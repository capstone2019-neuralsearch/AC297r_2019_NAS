2019-12-04 01:42:37,640 gpu device = 1
2019-12-04 01:42:37,640 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=96, cutout=False, cutout_length=16, data='../data', dataset='graphene', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name=None, gpu=1, grad_clip=5, gz_dtree=False, init_channels=36, layers=8, learning_rate=0.002, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-graphene-EXP-20191204-014237', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-12-04 01:42:40,842 param size = 1.250461MB
2019-12-04 01:42:40,845 epoch 0 lr 2.000000e-03
2019-12-04 01:42:43,713 train 000 1.900780e-01 -4.872297
2019-12-04 01:42:57,538 train 050 1.671624e-01 -4.810264
2019-12-04 01:43:11,277 train 100 9.844147e-02 -2.386009
2019-12-04 01:43:25,006 train 150 7.464581e-02 -1.561731
2019-12-04 01:43:38,737 train 200 6.259820e-02 -1.128271
2019-12-04 01:43:46,117 training loss; R2: 5.853063e-02 -0.988939
2019-12-04 01:43:46,259 valid 000 2.056829e-02 0.333317
2019-12-04 01:43:48,008 validation loss; R2: 2.221266e-02 0.293550
2019-12-04 01:43:48,032 epoch 1 lr 2.000000e-03
2019-12-04 01:43:48,479 train 000 2.096995e-02 0.207115
2019-12-04 01:44:02,131 train 050 2.175906e-02 0.280008
2019-12-04 01:44:15,488 train 100 2.252300e-02 0.272522
2019-12-04 01:44:28,848 train 150 2.131805e-02 0.315523
2019-12-04 01:44:42,211 train 200 2.052906e-02 0.344637
2019-12-04 01:44:48,220 training loss; R2: 2.004133e-02 0.354158
2019-12-04 01:44:48,357 valid 000 6.207690e-03 0.769002
2019-12-04 01:44:49,768 validation loss; R2: 8.777450e-03 0.723676
2019-12-04 01:44:49,793 epoch 2 lr 2.000000e-03
2019-12-04 01:44:50,144 train 000 7.833842e-03 0.482861
2019-12-04 01:45:03,508 train 050 1.764867e-02 0.486316
2019-12-04 01:45:16,869 train 100 1.557727e-02 0.517763
2019-12-04 01:45:30,231 train 150 1.430742e-02 0.532201
2019-12-04 01:45:43,590 train 200 1.365022e-02 0.547357
2019-12-04 01:45:49,597 training loss; R2: 1.335609e-02 0.558898
2019-12-04 01:45:49,733 valid 000 6.983411e-03 0.781217
2019-12-04 01:45:51,145 validation loss; R2: 7.408656e-03 0.740823
2019-12-04 01:45:51,169 epoch 3 lr 2.000000e-03
2019-12-04 01:45:51,519 train 000 2.821245e-02 0.222091
2019-12-04 01:46:04,867 train 050 1.280245e-02 0.595008
2019-12-04 01:46:18,223 train 100 1.238867e-02 0.600543
2019-12-04 01:46:31,570 train 150 1.185527e-02 0.604567
2019-12-04 01:46:44,926 train 200 1.132454e-02 0.613694
2019-12-04 01:46:50,932 training loss; R2: 1.126621e-02 0.620519
2019-12-04 01:46:51,068 valid 000 5.150689e-03 0.869742
2019-12-04 01:46:52,480 validation loss; R2: 5.080956e-03 0.823638
2019-12-04 01:46:52,504 epoch 4 lr 2.000000e-03
2019-12-04 01:46:52,860 train 000 6.646815e-03 0.838079
2019-12-04 01:47:06,222 train 050 1.211478e-02 0.651143
2019-12-04 01:47:19,585 train 100 1.078515e-02 0.655840
2019-12-04 01:47:32,943 train 150 1.064887e-02 0.651701
2019-12-04 01:47:46,304 train 200 1.036643e-02 0.658899
2019-12-04 01:47:52,313 training loss; R2: 1.034083e-02 0.659740
2019-12-04 01:47:52,451 valid 000 3.501859e-03 0.830658
2019-12-04 01:47:53,863 validation loss; R2: 5.529774e-03 0.810649
2019-12-04 01:47:53,887 epoch 5 lr 2.000000e-03
2019-12-04 01:47:54,239 train 000 1.086253e-02 0.783822
2019-12-04 01:48:07,589 train 050 8.405436e-03 0.700352
2019-12-04 01:48:20,945 train 100 8.778432e-03 0.687844
2019-12-04 01:48:34,295 train 150 8.752269e-03 0.689195
2019-12-04 01:48:47,641 train 200 8.868185e-03 0.693106
2019-12-04 01:48:53,644 training loss; R2: 8.960128e-03 0.696680
2019-12-04 01:48:53,786 valid 000 4.512914e-03 0.676345
2019-12-04 01:48:55,198 validation loss; R2: 6.542716e-03 0.764642
2019-12-04 01:48:55,222 epoch 6 lr 2.000000e-03
2019-12-04 01:48:55,574 train 000 1.795988e-02 0.350196
2019-12-04 01:49:08,920 train 050 9.073486e-03 0.653378
2019-12-04 01:49:22,266 train 100 8.523395e-03 0.696204
2019-12-04 01:49:35,615 train 150 9.069787e-03 0.690545
2019-12-04 01:49:48,967 train 200 9.162293e-03 0.696747
2019-12-04 01:49:54,967 training loss; R2: 9.055948e-03 0.697413
2019-12-04 01:49:55,105 valid 000 2.623927e-03 0.861683
2019-12-04 01:49:56,517 validation loss; R2: 4.657770e-03 0.850202
2019-12-04 01:49:56,542 epoch 7 lr 2.000000e-03
2019-12-04 01:49:56,893 train 000 1.110399e-02 0.758926
2019-12-04 01:50:10,244 train 050 8.464913e-03 0.719837
2019-12-04 01:50:23,598 train 100 8.183196e-03 0.729116
2019-12-04 01:50:36,949 train 150 7.994462e-03 0.728588
2019-12-04 01:50:50,301 train 200 8.189113e-03 0.729640
2019-12-04 01:50:56,302 training loss; R2: 8.174035e-03 0.730319
2019-12-04 01:50:56,446 valid 000 5.720940e-03 0.820292
2019-12-04 01:50:57,857 validation loss; R2: 5.267315e-03 0.813725
2019-12-04 01:50:57,881 epoch 8 lr 2.000000e-03
2019-12-04 01:50:58,231 train 000 8.437911e-03 0.764417
2019-12-04 01:51:11,580 train 050 8.454892e-03 0.706972
2019-12-04 01:51:24,927 train 100 8.098206e-03 0.720825
2019-12-04 01:51:38,279 train 150 7.827789e-03 0.730782
2019-12-04 01:51:51,629 train 200 7.817524e-03 0.734008
2019-12-04 01:51:57,631 training loss; R2: 7.710382e-03 0.735193
2019-12-04 01:51:57,768 valid 000 4.178079e-03 0.857655
2019-12-04 01:51:59,180 validation loss; R2: 3.942131e-03 0.865393
2019-12-04 01:51:59,204 epoch 9 lr 2.000000e-03
2019-12-04 01:51:59,558 train 000 6.695233e-03 0.516683
2019-12-04 01:52:12,910 train 050 6.191618e-03 0.763677
2019-12-04 01:52:26,259 train 100 7.008851e-03 0.750080
2019-12-04 01:52:39,610 train 150 7.065030e-03 0.748014
2019-12-04 01:52:52,961 train 200 7.267839e-03 0.746998
2019-12-04 01:52:58,968 training loss; R2: 7.350485e-03 0.748879
2019-12-04 01:52:59,112 valid 000 2.225532e-03 0.933127
2019-12-04 01:53:00,524 validation loss; R2: 4.542121e-03 0.839440
2019-12-04 01:53:00,548 epoch 10 lr 2.000000e-03
2019-12-04 01:53:00,901 train 000 7.117664e-03 0.752542
2019-12-04 01:53:14,244 train 050 8.389679e-03 0.737240
2019-12-04 01:53:27,587 train 100 7.796658e-03 0.736602
2019-12-04 01:53:40,931 train 150 7.536640e-03 0.739630
2019-12-04 01:53:54,269 train 200 7.451789e-03 0.743601
2019-12-04 01:54:00,271 training loss; R2: 7.353645e-03 0.747704
2019-12-04 01:54:00,411 valid 000 3.671395e-03 0.887592
2019-12-04 01:54:01,823 validation loss; R2: 3.628019e-03 0.870823
2019-12-04 01:54:01,847 epoch 11 lr 2.000000e-03
2019-12-04 01:54:02,198 train 000 9.417620e-03 0.687256
2019-12-04 01:54:15,543 train 050 7.112041e-03 0.743787
2019-12-04 01:54:28,882 train 100 7.382412e-03 0.750431
2019-12-04 01:54:42,222 train 150 7.192737e-03 0.754996
2019-12-04 01:54:55,566 train 200 7.083396e-03 0.757336
2019-12-04 01:55:01,563 training loss; R2: 7.081924e-03 0.759053
2019-12-04 01:55:01,701 valid 000 4.847335e-03 0.850982
2019-12-04 01:55:03,113 validation loss; R2: 4.191101e-03 0.849059
2019-12-04 01:55:03,138 epoch 12 lr 2.000000e-03
2019-12-04 01:55:03,490 train 000 9.383413e-03 0.630870
2019-12-04 01:55:16,825 train 050 7.081115e-03 0.752526
2019-12-04 01:55:30,161 train 100 7.352297e-03 0.755408
2019-12-04 01:55:43,493 train 150 7.012203e-03 0.761885
2019-12-04 01:55:56,824 train 200 6.988021e-03 0.764010
2019-12-04 01:56:02,820 training loss; R2: 7.038984e-03 0.764476
2019-12-04 01:56:02,955 valid 000 5.723756e-03 0.748284
2019-12-04 01:56:04,367 validation loss; R2: 4.485001e-03 0.845414
2019-12-04 01:56:04,393 epoch 13 lr 2.000000e-03
2019-12-04 01:56:04,746 train 000 6.295598e-03 0.799658
2019-12-04 01:56:18,082 train 050 6.470561e-03 0.789641
2019-12-04 01:56:31,413 train 100 6.540897e-03 0.777363
2019-12-04 01:56:44,741 train 150 6.358631e-03 0.773653
2019-12-04 01:56:58,072 train 200 6.408355e-03 0.775781
2019-12-04 01:57:04,070 training loss; R2: 6.521645e-03 0.774985
2019-12-04 01:57:04,213 valid 000 4.028116e-03 0.804276
2019-12-04 01:57:05,624 validation loss; R2: 4.512926e-03 0.843477
2019-12-04 01:57:05,649 epoch 14 lr 2.000000e-03
2019-12-04 01:57:06,001 train 000 9.973768e-03 0.835974
2019-12-04 01:57:19,339 train 050 7.466752e-03 0.756523
2019-12-04 01:57:32,679 train 100 6.890653e-03 0.777884
2019-12-04 01:57:46,023 train 150 6.564317e-03 0.781890
2019-12-04 01:57:59,358 train 200 6.528797e-03 0.779473
2019-12-04 01:58:05,356 training loss; R2: 6.459419e-03 0.781574
2019-12-04 01:58:05,498 valid 000 2.349123e-03 0.894805
2019-12-04 01:58:06,910 validation loss; R2: 3.289105e-03 0.890642
2019-12-04 01:58:06,935 epoch 15 lr 2.000000e-03
2019-12-04 01:58:07,288 train 000 3.660639e-03 0.796298
2019-12-04 01:58:20,614 train 050 6.868004e-03 0.769216
2019-12-04 01:58:33,937 train 100 6.534170e-03 0.780007
2019-12-04 01:58:47,255 train 150 6.585238e-03 0.777807
2019-12-04 01:59:00,568 train 200 6.658475e-03 0.780522
2019-12-04 01:59:06,556 training loss; R2: 6.649038e-03 0.779187
2019-12-04 01:59:06,698 valid 000 8.351441e-03 0.667236
2019-12-04 01:59:08,110 validation loss; R2: 5.539217e-03 0.806509
2019-12-04 01:59:08,136 epoch 16 lr 2.000000e-03
2019-12-04 01:59:08,487 train 000 9.387508e-03 0.848651
2019-12-04 01:59:21,797 train 050 6.139328e-03 0.801016
2019-12-04 01:59:35,110 train 100 6.460530e-03 0.798283
2019-12-04 01:59:48,419 train 150 6.143640e-03 0.794848
2019-12-04 02:00:01,731 train 200 6.214822e-03 0.792360
2019-12-04 02:00:07,715 training loss; R2: 6.100353e-03 0.792565
2019-12-04 02:00:07,854 valid 000 3.307020e-03 0.897459
2019-12-04 02:00:09,265 validation loss; R2: 3.277076e-03 0.890993
2019-12-04 02:00:09,290 epoch 17 lr 2.000000e-03
2019-12-04 02:00:09,641 train 000 5.604025e-03 0.825802
2019-12-04 02:00:22,953 train 050 6.005971e-03 0.796498
2019-12-04 02:00:36,265 train 100 6.047162e-03 0.800741
2019-12-04 02:00:49,578 train 150 5.960141e-03 0.801250
2019-12-04 02:01:02,888 train 200 6.094019e-03 0.797284
2019-12-04 02:01:08,874 training loss; R2: 6.061970e-03 0.797156
2019-12-04 02:01:09,011 valid 000 2.896964e-03 0.918448
2019-12-04 02:01:10,421 validation loss; R2: 3.728929e-03 0.877019
2019-12-04 02:01:10,447 epoch 18 lr 2.000000e-03
2019-12-04 02:01:10,797 train 000 4.477035e-03 0.808976
2019-12-04 02:01:24,103 train 050 6.888517e-03 0.773062
2019-12-04 02:01:37,405 train 100 6.047704e-03 0.801439
2019-12-04 02:01:50,698 train 150 6.062431e-03 0.803352
2019-12-04 02:02:03,992 train 200 5.923568e-03 0.802989
2019-12-04 02:02:09,969 training loss; R2: 5.919662e-03 0.803939
2019-12-04 02:02:10,114 valid 000 2.962203e-03 0.872925
2019-12-04 02:02:11,524 validation loss; R2: 3.268952e-03 0.881853
2019-12-04 02:02:11,550 epoch 19 lr 2.000000e-03
2019-12-04 02:02:11,899 train 000 6.185021e-03 0.829063
2019-12-04 02:02:25,184 train 050 5.994032e-03 0.795116
2019-12-04 02:02:38,471 train 100 6.114603e-03 0.789215
2019-12-04 02:02:51,756 train 150 5.921319e-03 0.796911
2019-12-04 02:03:05,044 train 200 5.964893e-03 0.797859
2019-12-04 02:03:11,016 training loss; R2: 6.047976e-03 0.794194
2019-12-04 02:03:11,154 valid 000 7.793465e-03 0.699511
2019-12-04 02:03:12,564 validation loss; R2: 7.088311e-03 0.742694
