2019-12-03 20:58:19,668 gpu device = 1
2019-12-03 20:58:19,668 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=96, cutout=False, cutout_length=16, data='../data', dataset='graphene', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name=None, gpu=1, grad_clip=5, gz_dtree=False, init_channels=36, layers=8, learning_rate=0.002, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-graphene-EXP-20191203-205819', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-12-03 20:58:22,917 param size = 1.002997MB
2019-12-03 20:58:22,920 epoch 0 lr 2.000000e-03
2019-12-03 20:58:25,477 train 000 1.023519e+00 -37.302772
2019-12-03 20:58:36,749 train 050 1.755563e-01 -5.533758
2019-12-03 20:58:47,961 train 100 1.067331e-01 -2.858245
2019-12-03 20:58:59,178 train 150 8.068472e-02 -1.880441
2019-12-03 20:59:10,395 train 200 6.809307e-02 -1.408574
2019-12-03 20:59:16,391 training loss; R2: 6.349273e-02 -1.244090
2019-12-03 20:59:16,518 valid 000 4.045818e-02 -0.454895
2019-12-03 20:59:18,047 validation loss; R2: 4.257344e-02 -0.462346
2019-12-03 20:59:18,065 epoch 1 lr 2.000000e-03
2019-12-03 20:59:18,406 train 000 2.977445e-02 -0.088174
2019-12-03 20:59:29,628 train 050 2.065996e-02 0.248038
2019-12-03 20:59:40,851 train 100 1.906433e-02 0.328676
2019-12-03 20:59:52,081 train 150 1.844887e-02 0.363098
2019-12-03 21:00:03,309 train 200 1.829078e-02 0.387798
2019-12-03 21:00:08,354 training loss; R2: 1.824605e-02 0.393637
2019-12-03 21:00:08,480 valid 000 7.255179e-02 0.001234
2019-12-03 21:00:09,690 validation loss; R2: 3.075898e-02 0.028554
2019-12-03 21:00:09,708 epoch 2 lr 2.000000e-03
2019-12-03 21:00:10,007 train 000 1.469341e-02 0.486882
2019-12-03 21:00:21,227 train 050 1.492695e-02 0.477084
2019-12-03 21:00:32,451 train 100 1.433936e-02 0.522760
2019-12-03 21:00:43,675 train 150 1.365074e-02 0.543467
2019-12-03 21:00:54,901 train 200 1.331072e-02 0.558476
2019-12-03 21:00:59,945 training loss; R2: 1.319951e-02 0.565743
2019-12-03 21:01:00,065 valid 000 6.737716e-03 0.738984
2019-12-03 21:01:01,276 validation loss; R2: 1.045190e-02 0.655621
2019-12-03 21:01:01,304 epoch 3 lr 2.000000e-03
2019-12-03 21:01:01,610 train 000 1.152067e-02 0.641928
2019-12-03 21:01:12,848 train 050 1.166690e-02 0.629807
2019-12-03 21:01:24,084 train 100 1.109587e-02 0.631884
2019-12-03 21:01:35,320 train 150 1.096231e-02 0.634192
2019-12-03 21:01:46,559 train 200 1.089003e-02 0.635835
2019-12-03 21:01:51,615 training loss; R2: 1.080611e-02 0.640860
2019-12-03 21:01:51,735 valid 000 5.257845e-03 0.819674
2019-12-03 21:01:52,946 validation loss; R2: 5.687346e-03 0.816551
2019-12-03 21:01:52,965 epoch 4 lr 2.000000e-03
2019-12-03 21:01:53,271 train 000 1.787157e-02 0.639527
2019-12-03 21:02:04,508 train 050 1.035053e-02 0.680614
2019-12-03 21:02:15,741 train 100 9.885355e-03 0.685701
2019-12-03 21:02:26,977 train 150 1.009400e-02 0.673133
2019-12-03 21:02:38,237 train 200 9.662989e-03 0.677478
2019-12-03 21:02:43,325 training loss; R2: 9.646499e-03 0.679338
2019-12-03 21:02:43,450 valid 000 8.011463e-03 0.707142
2019-12-03 21:02:44,664 validation loss; R2: 8.924535e-03 0.703843
2019-12-03 21:02:44,683 epoch 5 lr 2.000000e-03
2019-12-03 21:02:44,985 train 000 8.666441e-03 0.498096
2019-12-03 21:02:56,236 train 050 8.373571e-03 0.713042
2019-12-03 21:03:07,466 train 100 8.173059e-03 0.725045
2019-12-03 21:03:18,696 train 150 8.137029e-03 0.725286
2019-12-03 21:03:29,942 train 200 7.992552e-03 0.731436
2019-12-03 21:03:35,000 training loss; R2: 7.878024e-03 0.732286
2019-12-03 21:03:35,118 valid 000 5.422497e-03 0.871657
2019-12-03 21:03:36,327 validation loss; R2: 5.414360e-03 0.812687
2019-12-03 21:03:36,346 epoch 6 lr 2.000000e-03
2019-12-03 21:03:36,649 train 000 8.852704e-03 0.589015
2019-12-03 21:03:47,995 train 050 7.759647e-03 0.726576
2019-12-03 21:03:59,261 train 100 7.215955e-03 0.750372
2019-12-03 21:04:10,486 train 150 7.289992e-03 0.746715
2019-12-03 21:04:21,711 train 200 7.696249e-03 0.736987
2019-12-03 21:04:26,875 training loss; R2: 7.793324e-03 0.737300
2019-12-03 21:04:26,996 valid 000 5.209548e-03 0.790309
2019-12-03 21:04:28,207 validation loss; R2: 6.109061e-03 0.800772
2019-12-03 21:04:28,226 epoch 7 lr 2.000000e-03
2019-12-03 21:04:28,528 train 000 5.342452e-03 0.835091
2019-12-03 21:04:40,017 train 050 7.795888e-03 0.750016
2019-12-03 21:04:51,512 train 100 7.762902e-03 0.731671
2019-12-03 21:05:03,023 train 150 7.559156e-03 0.733011
2019-12-03 21:05:14,520 train 200 7.583002e-03 0.736164
2019-12-03 21:05:19,686 training loss; R2: 7.631537e-03 0.734902
2019-12-03 21:05:19,805 valid 000 2.787101e-03 0.870652
2019-12-03 21:05:21,016 validation loss; R2: 3.916087e-03 0.868962
2019-12-03 21:05:21,035 epoch 8 lr 2.000000e-03
2019-12-03 21:05:21,333 train 000 1.014352e-02 0.870941
2019-12-03 21:05:32,574 train 050 7.544189e-03 0.756911
2019-12-03 21:05:43,822 train 100 6.689117e-03 0.771748
2019-12-03 21:05:55,069 train 150 6.860160e-03 0.770490
2019-12-03 21:06:06,327 train 200 7.168133e-03 0.765277
2019-12-03 21:06:11,411 training loss; R2: 7.094941e-03 0.763884
2019-12-03 21:06:11,536 valid 000 4.381164e-03 0.837857
2019-12-03 21:06:12,748 validation loss; R2: 4.799293e-03 0.841243
2019-12-03 21:06:12,769 epoch 9 lr 2.000000e-03
2019-12-03 21:06:13,076 train 000 9.287058e-03 0.807502
2019-12-03 21:06:24,423 train 050 7.507235e-03 0.766460
2019-12-03 21:06:35,990 train 100 7.422897e-03 0.759230
2019-12-03 21:06:47,538 train 150 7.376495e-03 0.764109
2019-12-03 21:06:59,088 train 200 7.116300e-03 0.765609
2019-12-03 21:07:04,294 training loss; R2: 7.163628e-03 0.764307
2019-12-03 21:07:04,417 valid 000 8.340484e-03 0.739709
2019-12-03 21:07:05,631 validation loss; R2: 5.305659e-03 0.823865
2019-12-03 21:07:05,651 epoch 10 lr 2.000000e-03
2019-12-03 21:07:05,975 train 000 3.415996e-03 0.914344
2019-12-03 21:07:17,283 train 050 6.385199e-03 0.768230
2019-12-03 21:07:28,544 train 100 6.651131e-03 0.772393
2019-12-03 21:07:39,792 train 150 6.927819e-03 0.754487
2019-12-03 21:07:51,167 train 200 6.828362e-03 0.761483
2019-12-03 21:07:56,235 training loss; R2: 6.790742e-03 0.762189
2019-12-03 21:07:56,366 valid 000 1.103663e-02 0.500479
2019-12-03 21:07:57,576 validation loss; R2: 1.727613e-02 0.449155
2019-12-03 21:07:57,595 epoch 11 lr 2.000000e-03
2019-12-03 21:07:57,895 train 000 1.511457e-02 0.671682
2019-12-03 21:08:09,134 train 050 7.517522e-03 0.732342
2019-12-03 21:08:20,393 train 100 7.114276e-03 0.758385
2019-12-03 21:08:31,657 train 150 7.159244e-03 0.762808
2019-12-03 21:08:42,897 train 200 6.850256e-03 0.766455
2019-12-03 21:08:47,956 training loss; R2: 6.849036e-03 0.771349
2019-12-03 21:08:48,082 valid 000 7.539129e-03 0.851081
2019-12-03 21:08:49,292 validation loss; R2: 5.267654e-03 0.818287
2019-12-03 21:08:49,310 epoch 12 lr 2.000000e-03
2019-12-03 21:08:49,610 train 000 3.782524e-03 0.860407
2019-12-03 21:09:00,857 train 050 7.025912e-03 0.787920
2019-12-03 21:09:12,097 train 100 6.429530e-03 0.786913
2019-12-03 21:09:23,358 train 150 6.448270e-03 0.789742
2019-12-03 21:09:34,637 train 200 6.416466e-03 0.791562
2019-12-03 21:09:39,716 training loss; R2: 6.276680e-03 0.789297
2019-12-03 21:09:39,837 valid 000 3.407981e-03 0.843593
2019-12-03 21:09:41,047 validation loss; R2: 4.618179e-03 0.842867
2019-12-03 21:09:41,066 epoch 13 lr 2.000000e-03
2019-12-03 21:09:41,365 train 000 3.439193e-03 0.857618
2019-12-03 21:09:52,599 train 050 5.552956e-03 0.800616
2019-12-03 21:10:03,842 train 100 5.805236e-03 0.804955
2019-12-03 21:10:15,100 train 150 5.705319e-03 0.806355
2019-12-03 21:10:26,343 train 200 5.727199e-03 0.801847
2019-12-03 21:10:31,394 training loss; R2: 5.650521e-03 0.802611
2019-12-03 21:10:31,515 valid 000 8.804623e-01 -22.378051
2019-12-03 21:10:32,726 validation loss; R2: 9.073662e-01 -33.263330
2019-12-03 21:10:32,744 epoch 14 lr 2.000000e-03
2019-12-03 21:10:33,037 train 000 4.274803e-03 0.859230
2019-12-03 21:10:44,257 train 050 5.903502e-03 0.788821
2019-12-03 21:10:55,469 train 100 5.596198e-03 0.807502
2019-12-03 21:11:06,677 train 150 5.557752e-03 0.814579
2019-12-03 21:11:17,888 train 200 5.568511e-03 0.812703
2019-12-03 21:11:22,927 training loss; R2: 5.644197e-03 0.811054
2019-12-03 21:11:23,047 valid 000 2.797854e-01 -9.924960
2019-12-03 21:11:24,258 validation loss; R2: 2.825823e-01 -9.404212
2019-12-03 21:11:24,277 epoch 15 lr 2.000000e-03
2019-12-03 21:11:24,573 train 000 8.894728e-03 0.843603
2019-12-03 21:11:35,786 train 050 6.913901e-03 0.737064
2019-12-03 21:11:47,004 train 100 6.181309e-03 0.766425
2019-12-03 21:11:58,221 train 150 6.211606e-03 0.775146
2019-12-03 21:12:09,435 train 200 6.165822e-03 0.781987
2019-12-03 21:12:14,484 training loss; R2: 6.044525e-03 0.786586
2019-12-03 21:12:14,606 valid 000 8.396723e-01 -45.883495
2019-12-03 21:12:15,815 validation loss; R2: 8.067075e-01 -28.660312
2019-12-03 21:12:15,833 epoch 16 lr 2.000000e-03
2019-12-03 21:12:16,134 train 000 4.394575e-03 0.855807
2019-12-03 21:12:27,372 train 050 5.619663e-03 0.804068
2019-12-03 21:12:38,589 train 100 5.429816e-03 0.816816
2019-12-03 21:12:49,811 train 150 5.432608e-03 0.811911
2019-12-03 21:13:01,126 train 200 5.345124e-03 0.815981
2019-12-03 21:13:06,282 training loss; R2: 5.335395e-03 0.816786
2019-12-03 21:13:06,403 valid 000 5.949001e-01 -20.897686
2019-12-03 21:13:07,611 validation loss; R2: 5.992266e-01 -20.266804
2019-12-03 21:13:07,630 epoch 17 lr 2.000000e-03
2019-12-03 21:13:07,929 train 000 9.216827e-03 0.845140
2019-12-03 21:13:19,400 train 050 6.089598e-03 0.791388
2019-12-03 21:13:30,877 train 100 5.959194e-03 0.797171
2019-12-03 21:13:42,340 train 150 5.894990e-03 0.799779
2019-12-03 21:13:53,540 train 200 5.757381e-03 0.804118
2019-12-03 21:13:58,578 training loss; R2: 5.779584e-03 0.803540
2019-12-03 21:13:58,696 valid 000 8.954799e-01 -67.502754
2019-12-03 21:13:59,905 validation loss; R2: 8.684004e-01 -33.055838
2019-12-03 21:13:59,924 epoch 18 lr 2.000000e-03
2019-12-03 21:14:00,223 train 000 4.159658e-03 0.840319
2019-12-03 21:14:11,436 train 050 5.177332e-03 0.818831
2019-12-03 21:14:22,652 train 100 5.629086e-03 0.816560
2019-12-03 21:14:33,859 train 150 5.758109e-03 0.800496
2019-12-03 21:14:45,056 train 200 5.630355e-03 0.806092
2019-12-03 21:14:50,103 training loss; R2: 5.704424e-03 0.806562
2019-12-03 21:14:50,225 valid 000 8.270914e-01 -25.183247
2019-12-03 21:14:51,433 validation loss; R2: 8.290055e-01 -30.195902
2019-12-03 21:14:51,451 epoch 19 lr 2.000000e-03
2019-12-03 21:14:51,751 train 000 8.384359e-03 0.821315
2019-12-03 21:15:02,942 train 050 5.805160e-03 0.798514
2019-12-03 21:15:14,143 train 100 5.787008e-03 0.798293
2019-12-03 21:15:25,340 train 150 6.068843e-03 0.790386
2019-12-03 21:15:36,536 train 200 6.251418e-03 0.785493
2019-12-03 21:15:41,569 training loss; R2: 6.279511e-03 0.786712
2019-12-03 21:15:41,693 valid 000 5.181454e-01 -16.870605
2019-12-03 21:15:42,902 validation loss; R2: 4.829494e-01 -16.584529
