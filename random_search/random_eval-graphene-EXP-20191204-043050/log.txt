2019-12-04 04:30:50,803 gpu device = 1
2019-12-04 04:30:50,803 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=96, cutout=False, cutout_length=16, data='../data', dataset='graphene', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name=None, gpu=1, grad_clip=5, gz_dtree=False, init_channels=36, layers=8, learning_rate=0.002, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-graphene-EXP-20191204-043050', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-12-04 04:30:54,007 param size = 1.159021MB
2019-12-04 04:30:54,010 epoch 0 lr 2.000000e-03
2019-12-04 04:30:56,624 train 000 3.737138e-01 -14.652572
2019-12-04 04:31:09,194 train 050 1.481311e-01 -4.144467
2019-12-04 04:31:21,667 train 100 9.312078e-02 -2.226993
2019-12-04 04:31:34,135 train 150 7.123441e-02 -1.490208
2019-12-04 04:31:46,603 train 200 6.036463e-02 -1.075746
2019-12-04 04:31:53,279 training loss; R2: 5.666886e-02 -0.938424
2019-12-04 04:31:53,418 valid 000 1.425462e-02 0.522477
2019-12-04 04:31:55,002 validation loss; R2: 1.634245e-02 0.500494
2019-12-04 04:31:55,026 epoch 1 lr 2.000000e-03
2019-12-04 04:31:55,433 train 000 2.877296e-02 0.058930
2019-12-04 04:32:07,897 train 050 2.120711e-02 0.315239
2019-12-04 04:32:20,361 train 100 2.002592e-02 0.350394
2019-12-04 04:32:32,827 train 150 1.906864e-02 0.379010
2019-12-04 04:32:45,295 train 200 1.847937e-02 0.394441
2019-12-04 04:32:50,909 training loss; R2: 1.832154e-02 0.398961
2019-12-04 04:32:51,043 valid 000 7.742131e-03 0.708289
2019-12-04 04:32:52,358 validation loss; R2: 9.580413e-03 0.700489
2019-12-04 04:32:52,380 epoch 2 lr 2.000000e-03
2019-12-04 04:32:52,714 train 000 1.747061e-02 0.569863
2019-12-04 04:33:05,196 train 050 1.443572e-02 0.539013
2019-12-04 04:33:17,678 train 100 1.393550e-02 0.542724
2019-12-04 04:33:30,154 train 150 1.357436e-02 0.567560
2019-12-04 04:33:42,633 train 200 1.294790e-02 0.575483
2019-12-04 04:33:48,249 training loss; R2: 1.277564e-02 0.580836
2019-12-04 04:33:48,388 valid 000 8.803788e-03 0.765193
2019-12-04 04:33:49,703 validation loss; R2: 6.653220e-03 0.792802
2019-12-04 04:33:49,726 epoch 3 lr 2.000000e-03
2019-12-04 04:33:50,058 train 000 6.909262e-03 0.740240
2019-12-04 04:34:02,540 train 050 1.143467e-02 0.622193
2019-12-04 04:34:15,025 train 100 1.134458e-02 0.635468
2019-12-04 04:34:27,524 train 150 1.094889e-02 0.642934
2019-12-04 04:34:40,022 train 200 1.072710e-02 0.649557
2019-12-04 04:34:45,650 training loss; R2: 1.038842e-02 0.657920
2019-12-04 04:34:45,783 valid 000 4.869070e-03 0.815763
2019-12-04 04:34:47,099 validation loss; R2: 5.143764e-03 0.829462
2019-12-04 04:34:47,121 epoch 4 lr 2.000000e-03
2019-12-04 04:34:47,455 train 000 9.263825e-03 0.601953
2019-12-04 04:34:59,924 train 050 9.343939e-03 0.698394
2019-12-04 04:35:12,393 train 100 9.004153e-03 0.706961
2019-12-04 04:35:24,854 train 150 9.137680e-03 0.697547
2019-12-04 04:35:37,317 train 200 8.937741e-03 0.700808
2019-12-04 04:35:42,925 training loss; R2: 8.798650e-03 0.705738
2019-12-04 04:35:43,059 valid 000 2.847600e-03 0.859185
2019-12-04 04:35:44,374 validation loss; R2: 4.397856e-03 0.851158
2019-12-04 04:35:44,404 epoch 5 lr 2.000000e-03
2019-12-04 04:35:44,737 train 000 6.141739e-03 0.778873
2019-12-04 04:35:57,206 train 050 7.073652e-03 0.746118
2019-12-04 04:36:09,675 train 100 7.634494e-03 0.735948
2019-12-04 04:36:22,142 train 150 7.920608e-03 0.737461
2019-12-04 04:36:34,613 train 200 7.502206e-03 0.747453
2019-12-04 04:36:40,226 training loss; R2: 7.545493e-03 0.750258
2019-12-04 04:36:40,364 valid 000 1.153119e-02 0.774134
2019-12-04 04:36:41,678 validation loss; R2: 7.789287e-03 0.721684
2019-12-04 04:36:41,699 epoch 6 lr 2.000000e-03
2019-12-04 04:36:42,026 train 000 6.266472e-03 0.744411
2019-12-04 04:36:54,201 train 050 6.822967e-03 0.756638
2019-12-04 04:37:06,377 train 100 6.632844e-03 0.770862
2019-12-04 04:37:18,555 train 150 6.818666e-03 0.764109
2019-12-04 04:37:30,737 train 200 7.051762e-03 0.759906
2019-12-04 04:37:36,215 training loss; R2: 7.045835e-03 0.761457
2019-12-04 04:37:36,347 valid 000 3.940165e-03 0.889813
2019-12-04 04:37:37,661 validation loss; R2: 4.338542e-03 0.849447
2019-12-04 04:37:37,682 epoch 7 lr 2.000000e-03
2019-12-04 04:37:38,007 train 000 8.665360e-03 0.785488
2019-12-04 04:37:50,173 train 050 6.607875e-03 0.763298
2019-12-04 04:38:02,341 train 100 6.753619e-03 0.765682
2019-12-04 04:38:14,511 train 150 6.689371e-03 0.776181
2019-12-04 04:38:26,679 train 200 6.557796e-03 0.777905
2019-12-04 04:38:32,147 training loss; R2: 6.487916e-03 0.779426
2019-12-04 04:38:32,285 valid 000 6.253390e-03 0.884150
2019-12-04 04:38:33,599 validation loss; R2: 5.282971e-03 0.825709
2019-12-04 04:38:33,620 epoch 8 lr 2.000000e-03
2019-12-04 04:38:33,944 train 000 6.503901e-03 0.806751
2019-12-04 04:38:46,113 train 050 6.176749e-03 0.781277
2019-12-04 04:38:58,291 train 100 6.490992e-03 0.774446
2019-12-04 04:39:10,463 train 150 6.601570e-03 0.772536
2019-12-04 04:39:22,636 train 200 6.389960e-03 0.782481
2019-12-04 04:39:28,108 training loss; R2: 6.257089e-03 0.787427
2019-12-04 04:39:28,240 valid 000 2.740549e-03 0.909773
2019-12-04 04:39:29,554 validation loss; R2: 3.410353e-03 0.883887
2019-12-04 04:39:29,575 epoch 9 lr 2.000000e-03
2019-12-04 04:39:29,902 train 000 6.629331e-03 0.886833
2019-12-04 04:39:42,072 train 050 5.901611e-03 0.796527
2019-12-04 04:39:54,240 train 100 5.658736e-03 0.801289
2019-12-04 04:40:06,406 train 150 5.671843e-03 0.805609
2019-12-04 04:40:18,569 train 200 5.642659e-03 0.807955
2019-12-04 04:40:24,040 training loss; R2: 5.615013e-03 0.810237
2019-12-04 04:40:24,183 valid 000 3.136446e-03 0.879541
2019-12-04 04:40:25,500 validation loss; R2: 4.031852e-03 0.862781
2019-12-04 04:40:25,523 epoch 10 lr 2.000000e-03
2019-12-04 04:40:25,856 train 000 6.481123e-03 0.869193
2019-12-04 04:40:38,319 train 050 6.221383e-03 0.786491
2019-12-04 04:40:50,781 train 100 5.905069e-03 0.795883
2019-12-04 04:41:03,264 train 150 5.773806e-03 0.804527
2019-12-04 04:41:15,732 train 200 5.541433e-03 0.811235
2019-12-04 04:41:21,338 training loss; R2: 5.453433e-03 0.813594
2019-12-04 04:41:21,471 valid 000 5.035365e-03 0.907416
2019-12-04 04:41:22,786 validation loss; R2: 2.855246e-03 0.899418
2019-12-04 04:41:22,815 epoch 11 lr 2.000000e-03
2019-12-04 04:41:23,145 train 000 4.558854e-03 0.884206
2019-12-04 04:41:35,610 train 050 5.011711e-03 0.836729
2019-12-04 04:41:48,072 train 100 5.173106e-03 0.827136
2019-12-04 04:42:00,532 train 150 4.882252e-03 0.831851
2019-12-04 04:42:13,014 train 200 5.009708e-03 0.830103
2019-12-04 04:42:18,625 training loss; R2: 5.000841e-03 0.829832
2019-12-04 04:42:18,758 valid 000 5.288508e-03 0.896025
2019-12-04 04:42:20,073 validation loss; R2: 3.491379e-03 0.885276
2019-12-04 04:42:20,096 epoch 12 lr 2.000000e-03
2019-12-04 04:42:20,436 train 000 4.771009e-03 0.864880
2019-12-04 04:42:32,884 train 050 4.584348e-03 0.837483
2019-12-04 04:42:45,330 train 100 4.753272e-03 0.831170
2019-12-04 04:42:57,576 train 150 5.038553e-03 0.825730
2019-12-04 04:43:09,999 train 200 5.337898e-03 0.821432
2019-12-04 04:43:15,601 training loss; R2: 5.292013e-03 0.824095
2019-12-04 04:43:15,738 valid 000 2.771691e-03 0.857164
2019-12-04 04:43:17,053 validation loss; R2: 4.729309e-03 0.834921
2019-12-04 04:43:17,076 epoch 13 lr 2.000000e-03
2019-12-04 04:43:17,407 train 000 4.127126e-03 0.876819
2019-12-04 04:43:29,858 train 050 5.110204e-03 0.824340
2019-12-04 04:43:42,314 train 100 4.711465e-03 0.834519
2019-12-04 04:43:54,777 train 150 4.703064e-03 0.838653
2019-12-04 04:44:07,247 train 200 4.838487e-03 0.836888
2019-12-04 04:44:12,851 training loss; R2: 4.998028e-03 0.830408
2019-12-04 04:44:12,983 valid 000 2.822808e-03 0.859127
2019-12-04 04:44:14,297 validation loss; R2: 4.058571e-03 0.869720
2019-12-04 04:44:14,321 epoch 14 lr 2.000000e-03
2019-12-04 04:44:14,651 train 000 3.463793e-03 0.871670
2019-12-04 04:44:27,100 train 050 5.389365e-03 0.819106
2019-12-04 04:44:39,546 train 100 4.842838e-03 0.832027
2019-12-04 04:44:52,008 train 150 5.102585e-03 0.824119
2019-12-04 04:45:04,457 train 200 5.033370e-03 0.828966
2019-12-04 04:45:10,057 training loss; R2: 5.136976e-03 0.826515
2019-12-04 04:45:10,202 valid 000 2.489964e-03 0.927954
2019-12-04 04:45:11,517 validation loss; R2: 2.784820e-03 0.901573
2019-12-04 04:45:11,540 epoch 15 lr 2.000000e-03
2019-12-04 04:45:11,873 train 000 5.917439e-03 0.801676
2019-12-04 04:45:24,315 train 050 5.596107e-03 0.812162
2019-12-04 04:45:36,766 train 100 5.075695e-03 0.830039
2019-12-04 04:45:49,213 train 150 4.743804e-03 0.839551
2019-12-04 04:46:01,656 train 200 4.681014e-03 0.837679
2019-12-04 04:46:07,255 training loss; R2: 4.670833e-03 0.839525
2019-12-04 04:46:07,387 valid 000 4.368089e-03 0.936008
2019-12-04 04:46:08,702 validation loss; R2: 3.136827e-03 0.897929
2019-12-04 04:46:08,724 epoch 16 lr 2.000000e-03
2019-12-04 04:46:09,054 train 000 5.352980e-03 0.817426
2019-12-04 04:46:21,233 train 050 4.870845e-03 0.827144
2019-12-04 04:46:33,403 train 100 4.919860e-03 0.828696
2019-12-04 04:46:45,569 train 150 4.789790e-03 0.839094
2019-12-04 04:46:57,734 train 200 4.889966e-03 0.834140
2019-12-04 04:47:03,205 training loss; R2: 4.858044e-03 0.834523
2019-12-04 04:47:03,336 valid 000 2.290934e-03 0.900644
2019-12-04 04:47:04,649 validation loss; R2: 2.587260e-03 0.909411
2019-12-04 04:47:04,672 epoch 17 lr 2.000000e-03
2019-12-04 04:47:04,997 train 000 2.743452e-03 0.870625
2019-12-04 04:47:17,138 train 050 4.394681e-03 0.850820
2019-12-04 04:47:29,276 train 100 4.931321e-03 0.834016
2019-12-04 04:47:41,414 train 150 4.837116e-03 0.838628
2019-12-04 04:47:53,547 train 200 4.768370e-03 0.837598
2019-12-04 04:47:59,003 training loss; R2: 4.777933e-03 0.838014
2019-12-04 04:47:59,141 valid 000 5.814475e-03 0.848324
2019-12-04 04:48:00,454 validation loss; R2: 6.355714e-03 0.783004
2019-12-04 04:48:00,475 epoch 18 lr 2.000000e-03
2019-12-04 04:48:00,796 train 000 1.946385e-03 0.914978
2019-12-04 04:48:12,937 train 050 4.336360e-03 0.848375
2019-12-04 04:48:25,077 train 100 4.364187e-03 0.844712
2019-12-04 04:48:37,215 train 150 4.448421e-03 0.846179
2019-12-04 04:48:49,352 train 200 4.438448e-03 0.849647
2019-12-04 04:48:54,809 training loss; R2: 4.452749e-03 0.846655
2019-12-04 04:48:54,951 valid 000 2.755297e-03 0.925774
2019-12-04 04:48:56,263 validation loss; R2: 3.271037e-03 0.891660
2019-12-04 04:48:56,286 epoch 19 lr 2.000000e-03
2019-12-04 04:48:56,611 train 000 5.169838e-03 0.742663
2019-12-04 04:49:08,741 train 050 4.050618e-03 0.849737
2019-12-04 04:49:20,870 train 100 4.322084e-03 0.849055
2019-12-04 04:49:32,998 train 150 4.389584e-03 0.847642
2019-12-04 04:49:45,124 train 200 4.553394e-03 0.845373
2019-12-04 04:49:50,571 training loss; R2: 4.565648e-03 0.845055
2019-12-04 04:49:50,704 valid 000 2.426875e-03 0.853017
2019-12-04 04:49:52,016 validation loss; R2: 2.756488e-03 0.900637
