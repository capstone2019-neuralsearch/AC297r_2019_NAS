2019-12-04 05:49:43,221 gpu device = 1
2019-12-04 05:49:43,221 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=96, cutout=False, cutout_length=16, data='../data', dataset='graphene', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name=None, gpu=1, grad_clip=5, gz_dtree=False, init_channels=36, layers=8, learning_rate=0.002, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-graphene-EXP-20191204-054943', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-12-04 05:49:46,416 param size = 1.083349MB
2019-12-04 05:49:46,419 epoch 0 lr 2.000000e-03
2019-12-04 05:49:49,039 train 000 8.823624e-01 -15.400758
2019-12-04 05:50:01,580 train 050 2.670927e-01 -7.420854
2019-12-04 05:50:14,033 train 100 1.565273e-01 -3.901482
2019-12-04 05:50:26,511 train 150 1.201073e-01 -2.787787
2019-12-04 05:50:38,966 train 200 9.875331e-02 -2.125195
2019-12-04 05:50:45,677 training loss; R2: 9.150576e-02 -1.918347
2019-12-04 05:50:45,820 valid 000 2.864171e-02 0.368245
2019-12-04 05:50:47,468 validation loss; R2: 1.813348e-02 0.431523
2019-12-04 05:50:47,491 epoch 1 lr 2.000000e-03
2019-12-04 05:50:47,915 train 000 2.929038e-02 0.279021
2019-12-04 05:51:00,109 train 050 2.371436e-02 0.229902
2019-12-04 05:51:12,304 train 100 2.426819e-02 0.232739
2019-12-04 05:51:24,504 train 150 2.372807e-02 0.235239
2019-12-04 05:51:36,712 train 200 2.286180e-02 0.262191
2019-12-04 05:51:42,200 training loss; R2: 2.272059e-02 0.261577
2019-12-04 05:51:42,339 valid 000 3.304942e-02 -0.247691
2019-12-04 05:51:43,644 validation loss; R2: 3.791586e-02 -0.346663
2019-12-04 05:51:43,664 epoch 2 lr 2.000000e-03
2019-12-04 05:51:43,994 train 000 2.420847e-02 0.150018
2019-12-04 05:51:56,223 train 050 1.663376e-02 0.449509
2019-12-04 05:52:08,446 train 100 1.608569e-02 0.463299
2019-12-04 05:52:20,651 train 150 1.534471e-02 0.483306
2019-12-04 05:52:32,859 train 200 1.550835e-02 0.492140
2019-12-04 05:52:38,348 training loss; R2: 1.544666e-02 0.496318
2019-12-04 05:52:38,481 valid 000 8.756679e-03 0.685209
2019-12-04 05:52:39,786 validation loss; R2: 8.838607e-03 0.705041
2019-12-04 05:52:39,805 epoch 3 lr 2.000000e-03
2019-12-04 05:52:40,128 train 000 1.551955e-02 0.505304
2019-12-04 05:52:52,332 train 050 1.247913e-02 0.585614
2019-12-04 05:53:04,539 train 100 1.205590e-02 0.592141
2019-12-04 05:53:16,743 train 150 1.208191e-02 0.597580
2019-12-04 05:53:28,946 train 200 1.248940e-02 0.582017
2019-12-04 05:53:34,435 training loss; R2: 1.258563e-02 0.588042
2019-12-04 05:53:34,569 valid 000 5.316464e-03 0.704278
2019-12-04 05:53:35,875 validation loss; R2: 7.534995e-03 0.743006
2019-12-04 05:53:35,894 epoch 4 lr 2.000000e-03
2019-12-04 05:53:36,220 train 000 2.219196e-02 0.682439
2019-12-04 05:53:48,423 train 050 1.215000e-02 0.578699
2019-12-04 05:54:00,628 train 100 1.110344e-02 0.622166
2019-12-04 05:54:12,832 train 150 1.140675e-02 0.621324
2019-12-04 05:54:25,038 train 200 1.100810e-02 0.638689
2019-12-04 05:54:30,530 training loss; R2: 1.083104e-02 0.645100
2019-12-04 05:54:30,663 valid 000 2.170205e-02 0.713415
2019-12-04 05:54:31,968 validation loss; R2: 9.614042e-03 0.674866
2019-12-04 05:54:31,988 epoch 5 lr 2.000000e-03
2019-12-04 05:54:32,315 train 000 6.214958e-03 0.647498
2019-12-04 05:54:44,522 train 050 8.734334e-03 0.704995
2019-12-04 05:54:56,731 train 100 9.640887e-03 0.675235
2019-12-04 05:55:08,942 train 150 9.813053e-03 0.676834
2019-12-04 05:55:21,147 train 200 9.836122e-03 0.677091
2019-12-04 05:55:26,636 training loss; R2: 9.760103e-03 0.678244
2019-12-04 05:55:26,770 valid 000 9.815019e-03 0.695829
2019-12-04 05:55:28,076 validation loss; R2: 7.651427e-03 0.751671
2019-12-04 05:55:28,097 epoch 6 lr 2.000000e-03
2019-12-04 05:55:28,422 train 000 7.606748e-03 0.781121
2019-12-04 05:55:40,629 train 050 9.866070e-03 0.705329
2019-12-04 05:55:52,835 train 100 9.261403e-03 0.713380
2019-12-04 05:56:05,038 train 150 8.974869e-03 0.718720
2019-12-04 05:56:17,241 train 200 8.681370e-03 0.720462
2019-12-04 05:56:22,727 training loss; R2: 8.598459e-03 0.719373
2019-12-04 05:56:22,866 valid 000 4.329423e-03 0.875727
2019-12-04 05:56:24,172 validation loss; R2: 5.293664e-03 0.829894
2019-12-04 05:56:24,191 epoch 7 lr 2.000000e-03
2019-12-04 05:56:24,519 train 000 5.427775e-03 0.623932
2019-12-04 05:56:36,727 train 050 9.915487e-03 0.707758
2019-12-04 05:56:48,932 train 100 9.072611e-03 0.723445
2019-12-04 05:57:01,140 train 150 8.523228e-03 0.729353
2019-12-04 05:57:13,345 train 200 8.242784e-03 0.729143
2019-12-04 05:57:18,834 training loss; R2: 8.295144e-03 0.729457
2019-12-04 05:57:18,979 valid 000 5.031670e-03 0.826778
2019-12-04 05:57:20,286 validation loss; R2: 4.376051e-03 0.855978
2019-12-04 05:57:20,305 epoch 8 lr 2.000000e-03
2019-12-04 05:57:20,641 train 000 8.264714e-03 0.779927
2019-12-04 05:57:32,858 train 050 7.544332e-03 0.736263
2019-12-04 05:57:45,070 train 100 7.575026e-03 0.725900
2019-12-04 05:57:57,280 train 150 7.583050e-03 0.733139
2019-12-04 05:58:09,491 train 200 7.961447e-03 0.730759
2019-12-04 05:58:14,988 training loss; R2: 8.054090e-03 0.729586
2019-12-04 05:58:15,132 valid 000 1.936512e-02 0.504093
2019-12-04 05:58:16,438 validation loss; R2: 1.639563e-02 0.480848
2019-12-04 05:58:16,459 epoch 9 lr 2.000000e-03
2019-12-04 05:58:16,793 train 000 8.240702e-03 0.637303
2019-12-04 05:58:29,006 train 050 9.248893e-03 0.709923
2019-12-04 05:58:41,217 train 100 8.370979e-03 0.729666
2019-12-04 05:58:53,431 train 150 7.872535e-03 0.744221
2019-12-04 05:59:05,644 train 200 7.626186e-03 0.748749
2019-12-04 05:59:11,134 training loss; R2: 7.555513e-03 0.748852
2019-12-04 05:59:11,281 valid 000 4.923018e-03 0.880707
2019-12-04 05:59:12,587 validation loss; R2: 4.428897e-03 0.848658
2019-12-04 05:59:12,607 epoch 10 lr 2.000000e-03
2019-12-04 05:59:12,933 train 000 7.950475e-03 0.699545
2019-12-04 05:59:25,157 train 050 6.916399e-03 0.744643
2019-12-04 05:59:37,381 train 100 6.798395e-03 0.769542
2019-12-04 05:59:49,602 train 150 6.886302e-03 0.764141
2019-12-04 06:00:01,825 train 200 6.879968e-03 0.765253
2019-12-04 06:00:07,321 training loss; R2: 6.994848e-03 0.766675
2019-12-04 06:00:07,458 valid 000 1.346662e-02 0.656495
2019-12-04 06:00:08,763 validation loss; R2: 1.401281e-02 0.556173
2019-12-04 06:00:08,783 epoch 11 lr 2.000000e-03
2019-12-04 06:00:09,111 train 000 5.666439e-03 0.764325
2019-12-04 06:00:21,321 train 050 6.970707e-03 0.779202
2019-12-04 06:00:33,538 train 100 7.441166e-03 0.768973
2019-12-04 06:00:45,748 train 150 7.360298e-03 0.765503
2019-12-04 06:00:57,955 train 200 7.181905e-03 0.766525
2019-12-04 06:01:03,443 training loss; R2: 7.110336e-03 0.767146
2019-12-04 06:01:03,579 valid 000 2.507259e-03 0.866997
2019-12-04 06:01:04,885 validation loss; R2: 5.044539e-03 0.830059
2019-12-04 06:01:04,906 epoch 12 lr 2.000000e-03
2019-12-04 06:01:05,235 train 000 4.315644e-03 0.791337
2019-12-04 06:01:17,450 train 050 6.403215e-03 0.768190
2019-12-04 06:01:29,657 train 100 6.418870e-03 0.774310
2019-12-04 06:01:41,864 train 150 7.027945e-03 0.764006
2019-12-04 06:01:54,074 train 200 7.012175e-03 0.764442
2019-12-04 06:01:59,567 training loss; R2: 7.067805e-03 0.763789
2019-12-04 06:01:59,702 valid 000 7.526103e-03 0.664954
2019-12-04 06:02:01,008 validation loss; R2: 7.924613e-03 0.724119
2019-12-04 06:02:01,027 epoch 13 lr 2.000000e-03
2019-12-04 06:02:01,355 train 000 1.394647e-02 0.496164
2019-12-04 06:02:13,572 train 050 7.066090e-03 0.753102
2019-12-04 06:02:25,949 train 100 7.004958e-03 0.759051
2019-12-04 06:02:38,440 train 150 6.789249e-03 0.770807
2019-12-04 06:02:50,933 train 200 6.692667e-03 0.769543
2019-12-04 06:02:56,547 training loss; R2: 6.823518e-03 0.767827
2019-12-04 06:02:56,683 valid 000 1.209738e-02 0.627968
2019-12-04 06:02:57,990 validation loss; R2: 1.138163e-02 0.632740
2019-12-04 06:02:58,012 epoch 14 lr 2.000000e-03
2019-12-04 06:02:58,352 train 000 6.352227e-03 0.750873
2019-12-04 06:03:10,826 train 050 6.334612e-03 0.772142
2019-12-04 06:03:23,306 train 100 6.393770e-03 0.787139
2019-12-04 06:03:35,793 train 150 6.444612e-03 0.782621
2019-12-04 06:03:48,279 train 200 6.488068e-03 0.784579
2019-12-04 06:03:53,896 training loss; R2: 6.430560e-03 0.786052
2019-12-04 06:03:54,030 valid 000 1.770098e-02 0.688476
2019-12-04 06:03:55,337 validation loss; R2: 8.918860e-03 0.718258
2019-12-04 06:03:55,358 epoch 15 lr 2.000000e-03
2019-12-04 06:03:55,695 train 000 5.380839e-03 0.856259
2019-12-04 06:04:08,181 train 050 6.783226e-03 0.780086
2019-12-04 06:04:20,666 train 100 7.044975e-03 0.775469
2019-12-04 06:04:33,148 train 150 6.843567e-03 0.768780
2019-12-04 06:04:45,627 train 200 6.558086e-03 0.776882
2019-12-04 06:04:51,241 training loss; R2: 6.581825e-03 0.780326
2019-12-04 06:04:51,376 valid 000 3.259981e-03 0.911549
2019-12-04 06:04:52,683 validation loss; R2: 3.602226e-03 0.880076
2019-12-04 06:04:52,704 epoch 16 lr 2.000000e-03
2019-12-04 06:04:53,040 train 000 6.311168e-03 0.764712
2019-12-04 06:05:05,518 train 050 6.930769e-03 0.786119
2019-12-04 06:05:17,993 train 100 6.562651e-03 0.788896
2019-12-04 06:05:30,463 train 150 6.208502e-03 0.796920
2019-12-04 06:05:42,930 train 200 6.159448e-03 0.795885
2019-12-04 06:05:48,536 training loss; R2: 6.194792e-03 0.794042
2019-12-04 06:05:48,673 valid 000 3.823408e-02 -2.014145
2019-12-04 06:05:49,980 validation loss; R2: 3.069950e-02 -0.170318
2019-12-04 06:05:50,001 epoch 17 lr 2.000000e-03
2019-12-04 06:05:50,337 train 000 6.153896e-03 0.774687
2019-12-04 06:06:02,635 train 050 6.552626e-03 0.784028
2019-12-04 06:06:14,833 train 100 6.511788e-03 0.790199
2019-12-04 06:06:27,026 train 150 5.928491e-03 0.798319
2019-12-04 06:06:39,219 train 200 6.097874e-03 0.787372
2019-12-04 06:06:44,703 training loss; R2: 6.200495e-03 0.784707
2019-12-04 06:06:44,837 valid 000 5.412051e-01 -12.495249
2019-12-04 06:06:46,142 validation loss; R2: 5.641254e-01 -20.457258
2019-12-04 06:06:46,162 epoch 18 lr 2.000000e-03
2019-12-04 06:06:46,492 train 000 4.734791e-03 0.792955
2019-12-04 06:06:58,687 train 050 7.157772e-03 0.765669
2019-12-04 06:07:10,882 train 100 6.762817e-03 0.774152
2019-12-04 06:07:23,075 train 150 6.229491e-03 0.788107
2019-12-04 06:07:35,270 train 200 6.113710e-03 0.791859
2019-12-04 06:07:40,753 training loss; R2: 6.092609e-03 0.794387
2019-12-04 06:07:40,887 valid 000 1.351516e+00 -40.290971
2019-12-04 06:07:42,192 validation loss; R2: 1.329694e+00 -50.248406
2019-12-04 06:07:42,213 epoch 19 lr 2.000000e-03
2019-12-04 06:07:42,542 train 000 5.185180e-03 0.813368
2019-12-04 06:07:54,734 train 050 6.206383e-03 0.795406
2019-12-04 06:08:06,926 train 100 6.223133e-03 0.789499
2019-12-04 06:08:19,110 train 150 6.143663e-03 0.787968
2019-12-04 06:08:31,299 train 200 6.300084e-03 0.791354
2019-12-04 06:08:36,778 training loss; R2: 6.290170e-03 0.791105
2019-12-04 06:08:36,913 valid 000 7.454162e-01 -16.012796
2019-12-04 06:08:38,218 validation loss; R2: 7.586229e-01 -26.828133
