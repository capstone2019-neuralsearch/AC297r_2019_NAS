2019-12-03 17:43:53,561 gpu device = 1
2019-12-03 17:43:53,561 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=96, cutout=False, cutout_length=16, data='../data', dataset='graphene', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name=None, gpu=1, grad_clip=5, gz_dtree=False, init_channels=36, layers=8, learning_rate=0.002, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-graphene-EXP-20191203-174353', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-12-03 17:43:56,759 param size = 0.922213MB
2019-12-03 17:43:56,762 epoch 0 lr 2.000000e-03
2019-12-03 17:43:59,125 train 000 3.718729e-01 -17.178827
2019-12-03 17:44:08,574 train 050 1.575835e-01 -4.351802
2019-12-03 17:44:17,980 train 100 9.172573e-02 -2.088766
2019-12-03 17:44:27,382 train 150 6.872580e-02 -1.298083
2019-12-03 17:44:36,789 train 200 5.699715e-02 -0.899730
2019-12-03 17:44:41,954 training loss; R2: 5.338491e-02 -0.776192
2019-12-03 17:44:42,074 valid 000 1.837578e-02 0.572115
2019-12-03 17:44:43,420 validation loss; R2: 1.591031e-02 0.500670
2019-12-03 17:44:43,436 epoch 1 lr 2.000000e-03
2019-12-03 17:44:43,714 train 000 2.016727e-02 0.167670
2019-12-03 17:44:53,129 train 050 1.839800e-02 0.359273
2019-12-03 17:45:02,539 train 100 1.847654e-02 0.380280
2019-12-03 17:45:11,950 train 150 1.865037e-02 0.396345
2019-12-03 17:45:21,362 train 200 1.805631e-02 0.414695
2019-12-03 17:45:25,592 training loss; R2: 1.775200e-02 0.425966
2019-12-03 17:45:25,700 valid 000 8.636969e-03 0.666801
2019-12-03 17:45:26,736 validation loss; R2: 1.190590e-02 0.616576
2019-12-03 17:45:26,753 epoch 2 lr 2.000000e-03
2019-12-03 17:45:27,001 train 000 1.715359e-02 0.576570
2019-12-03 17:45:36,418 train 050 1.421812e-02 0.527806
2019-12-03 17:45:45,831 train 100 1.464929e-02 0.509923
2019-12-03 17:45:55,241 train 150 1.432514e-02 0.535230
2019-12-03 17:46:04,657 train 200 1.412873e-02 0.544338
2019-12-03 17:46:08,888 training loss; R2: 1.391162e-02 0.547295
2019-12-03 17:46:08,995 valid 000 6.743164e-03 0.687899
2019-12-03 17:46:10,030 validation loss; R2: 8.907693e-03 0.715935
2019-12-03 17:46:10,049 epoch 3 lr 2.000000e-03
2019-12-03 17:46:10,296 train 000 3.267802e-02 0.603783
2019-12-03 17:46:19,714 train 050 1.395191e-02 0.595063
2019-12-03 17:46:29,127 train 100 1.259446e-02 0.598902
2019-12-03 17:46:38,546 train 150 1.182024e-02 0.607699
2019-12-03 17:46:47,967 train 200 1.187801e-02 0.605678
2019-12-03 17:46:52,208 training loss; R2: 1.171958e-02 0.610554
2019-12-03 17:46:52,307 valid 000 5.410936e-03 0.851831
2019-12-03 17:46:53,341 validation loss; R2: 6.756492e-03 0.783310
2019-12-03 17:46:53,357 epoch 4 lr 2.000000e-03
2019-12-03 17:46:53,605 train 000 9.674261e-03 0.685500
2019-12-03 17:47:03,023 train 050 1.035073e-02 0.652691
2019-12-03 17:47:12,496 train 100 1.127768e-02 0.628106
2019-12-03 17:47:22,132 train 150 1.114501e-02 0.633471
2019-12-03 17:47:31,610 train 200 1.143837e-02 0.622955
2019-12-03 17:47:35,852 training loss; R2: 1.126022e-02 0.625628
2019-12-03 17:47:35,953 valid 000 6.036519e-03 0.792446
2019-12-03 17:47:36,987 validation loss; R2: 6.673049e-03 0.793620
2019-12-03 17:47:37,012 epoch 5 lr 2.000000e-03
2019-12-03 17:47:37,262 train 000 5.818971e-03 0.766604
2019-12-03 17:47:46,677 train 050 9.101680e-03 0.693002
2019-12-03 17:47:56,086 train 100 9.143726e-03 0.698299
2019-12-03 17:48:05,492 train 150 9.073826e-03 0.685704
2019-12-03 17:48:14,900 train 200 9.129373e-03 0.684969
2019-12-03 17:48:19,131 training loss; R2: 9.197942e-03 0.689366
2019-12-03 17:48:19,232 valid 000 6.290904e-03 0.736393
2019-12-03 17:48:20,266 validation loss; R2: 6.165691e-03 0.788689
2019-12-03 17:48:20,284 epoch 6 lr 2.000000e-03
2019-12-03 17:48:20,536 train 000 9.106304e-03 0.718954
2019-12-03 17:48:29,954 train 050 9.554724e-03 0.711732
2019-12-03 17:48:39,372 train 100 9.580037e-03 0.709950
2019-12-03 17:48:48,794 train 150 9.262317e-03 0.702034
2019-12-03 17:48:58,214 train 200 8.978796e-03 0.701448
2019-12-03 17:49:02,453 training loss; R2: 8.944801e-03 0.700669
2019-12-03 17:49:02,553 valid 000 5.870033e-03 0.779088
2019-12-03 17:49:03,587 validation loss; R2: 6.411203e-03 0.783306
2019-12-03 17:49:03,611 epoch 7 lr 2.000000e-03
2019-12-03 17:49:03,858 train 000 7.080574e-03 0.717150
2019-12-03 17:49:13,279 train 050 8.722949e-03 0.701051
2019-12-03 17:49:22,697 train 100 8.283717e-03 0.712756
2019-12-03 17:49:32,115 train 150 8.482622e-03 0.710186
2019-12-03 17:49:41,533 train 200 8.561792e-03 0.706121
2019-12-03 17:49:45,770 training loss; R2: 8.541433e-03 0.710791
2019-12-03 17:49:45,870 valid 000 1.151717e-02 0.779718
2019-12-03 17:49:46,904 validation loss; R2: 6.423050e-03 0.791680
2019-12-03 17:49:46,921 epoch 8 lr 2.000000e-03
2019-12-03 17:49:47,168 train 000 8.456282e-03 0.755908
2019-12-03 17:49:56,582 train 050 7.452850e-03 0.748515
2019-12-03 17:50:05,994 train 100 7.958069e-03 0.733721
2019-12-03 17:50:15,414 train 150 8.677776e-03 0.719639
2019-12-03 17:50:24,822 train 200 8.342622e-03 0.725800
2019-12-03 17:50:29,052 training loss; R2: 8.350457e-03 0.722542
2019-12-03 17:50:29,164 valid 000 6.025091e-03 0.812280
2019-12-03 17:50:30,198 validation loss; R2: 4.345626e-03 0.856871
2019-12-03 17:50:30,215 epoch 9 lr 2.000000e-03
2019-12-03 17:50:30,462 train 000 7.755237e-03 0.658146
2019-12-03 17:50:39,875 train 050 7.542987e-03 0.733792
2019-12-03 17:50:49,293 train 100 7.705095e-03 0.745521
2019-12-03 17:50:58,704 train 150 7.618498e-03 0.740211
2019-12-03 17:51:08,114 train 200 7.671997e-03 0.745655
2019-12-03 17:51:12,349 training loss; R2: 7.628961e-03 0.744765
2019-12-03 17:51:12,450 valid 000 3.606054e-03 0.878151
2019-12-03 17:51:13,485 validation loss; R2: 5.721695e-03 0.812533
2019-12-03 17:51:13,502 epoch 10 lr 2.000000e-03
2019-12-03 17:51:13,748 train 000 6.464825e-03 0.768282
2019-12-03 17:51:23,170 train 050 6.958664e-03 0.753420
2019-12-03 17:51:32,580 train 100 7.165132e-03 0.757078
2019-12-03 17:51:41,989 train 150 7.098245e-03 0.760060
2019-12-03 17:51:51,414 train 200 7.202216e-03 0.758645
2019-12-03 17:51:55,645 training loss; R2: 7.247460e-03 0.760080
2019-12-03 17:51:55,747 valid 000 3.357495e-03 0.797145
2019-12-03 17:51:56,782 validation loss; R2: 4.550525e-03 0.840027
2019-12-03 17:51:56,798 epoch 11 lr 2.000000e-03
2019-12-03 17:51:57,048 train 000 5.042166e-03 0.768828
2019-12-03 17:52:06,462 train 050 6.688101e-03 0.774860
2019-12-03 17:52:15,890 train 100 6.498064e-03 0.770511
2019-12-03 17:52:25,410 train 150 6.687207e-03 0.772042
2019-12-03 17:52:35,048 train 200 6.674213e-03 0.774547
2019-12-03 17:52:39,380 training loss; R2: 6.685519e-03 0.772449
2019-12-03 17:52:39,486 valid 000 2.623959e-03 0.938890
2019-12-03 17:52:40,524 validation loss; R2: 3.842852e-03 0.871057
2019-12-03 17:52:40,543 epoch 12 lr 2.000000e-03
2019-12-03 17:52:40,800 train 000 6.303869e-03 0.752142
2019-12-03 17:52:50,282 train 050 7.423440e-03 0.757817
2019-12-03 17:52:59,706 train 100 6.997787e-03 0.767486
2019-12-03 17:53:09,127 train 150 7.067022e-03 0.759830
2019-12-03 17:53:18,547 train 200 7.089663e-03 0.765425
2019-12-03 17:53:22,781 training loss; R2: 7.137698e-03 0.765545
2019-12-03 17:53:22,878 valid 000 4.400358e-03 0.801623
2019-12-03 17:53:23,915 validation loss; R2: 4.447908e-03 0.847125
2019-12-03 17:53:23,932 epoch 13 lr 2.000000e-03
2019-12-03 17:53:24,182 train 000 4.759673e-03 0.899590
2019-12-03 17:53:33,581 train 050 6.716213e-03 0.785526
2019-12-03 17:53:42,989 train 100 6.622217e-03 0.774642
2019-12-03 17:53:52,394 train 150 6.739726e-03 0.768922
2019-12-03 17:54:01,799 train 200 6.712223e-03 0.768878
2019-12-03 17:54:06,032 training loss; R2: 6.739773e-03 0.771844
2019-12-03 17:54:06,134 valid 000 4.021587e-03 0.865181
2019-12-03 17:54:07,169 validation loss; R2: 4.935376e-03 0.839120
2019-12-03 17:54:07,187 epoch 14 lr 2.000000e-03
2019-12-03 17:54:07,439 train 000 5.711671e-03 0.705083
2019-12-03 17:54:16,843 train 050 6.034009e-03 0.776343
2019-12-03 17:54:26,250 train 100 6.993133e-03 0.765959
2019-12-03 17:54:35,649 train 150 6.834723e-03 0.770220
2019-12-03 17:54:45,051 train 200 6.763659e-03 0.774287
2019-12-03 17:54:49,282 training loss; R2: 6.791766e-03 0.771050
2019-12-03 17:54:49,384 valid 000 2.918340e-03 0.876631
2019-12-03 17:54:50,419 validation loss; R2: 4.002281e-03 0.862770
2019-12-03 17:54:50,436 epoch 15 lr 2.000000e-03
2019-12-03 17:54:50,683 train 000 8.625495e-03 0.758958
2019-12-03 17:55:00,092 train 050 7.224524e-03 0.738165
2019-12-03 17:55:09,493 train 100 6.933493e-03 0.764199
2019-12-03 17:55:19,088 train 150 6.684229e-03 0.773350
2019-12-03 17:55:28,597 train 200 6.597181e-03 0.774637
2019-12-03 17:55:32,827 training loss; R2: 6.565400e-03 0.776871
2019-12-03 17:55:32,932 valid 000 3.173189e-03 0.900939
2019-12-03 17:55:33,967 validation loss; R2: 4.144941e-03 0.862920
2019-12-03 17:55:33,984 epoch 16 lr 2.000000e-03
2019-12-03 17:55:34,233 train 000 6.069811e-03 0.861040
2019-12-03 17:55:43,628 train 050 6.240416e-03 0.799554
2019-12-03 17:55:53,024 train 100 6.298720e-03 0.796099
2019-12-03 17:56:02,416 train 150 6.270253e-03 0.788223
2019-12-03 17:56:11,810 train 200 6.216948e-03 0.789767
2019-12-03 17:56:16,036 training loss; R2: 6.380351e-03 0.782183
2019-12-03 17:56:16,138 valid 000 3.414055e-03 0.796158
2019-12-03 17:56:17,172 validation loss; R2: 4.668123e-03 0.834311
2019-12-03 17:56:17,188 epoch 17 lr 2.000000e-03
2019-12-03 17:56:17,438 train 000 6.126227e-03 0.832329
2019-12-03 17:56:26,830 train 050 7.342531e-03 0.756190
2019-12-03 17:56:36,218 train 100 6.473799e-03 0.778290
2019-12-03 17:56:45,611 train 150 6.448191e-03 0.782199
2019-12-03 17:56:55,001 train 200 6.526735e-03 0.777793
2019-12-03 17:56:59,222 training loss; R2: 6.471223e-03 0.776506
2019-12-03 17:56:59,325 valid 000 3.839460e-03 0.791907
2019-12-03 17:57:00,360 validation loss; R2: 6.273111e-03 0.790346
2019-12-03 17:57:00,377 epoch 18 lr 2.000000e-03
2019-12-03 17:57:00,629 train 000 7.080090e-03 0.774442
2019-12-03 17:57:10,009 train 050 7.177417e-03 0.770511
2019-12-03 17:57:19,388 train 100 7.027437e-03 0.774647
2019-12-03 17:57:28,762 train 150 6.896164e-03 0.767530
2019-12-03 17:57:38,136 train 200 6.597567e-03 0.779731
2019-12-03 17:57:42,349 training loss; R2: 6.560754e-03 0.781259
2019-12-03 17:57:42,450 valid 000 2.354995e-03 0.869840
2019-12-03 17:57:43,487 validation loss; R2: 4.114970e-03 0.865664
2019-12-03 17:57:43,504 epoch 19 lr 2.000000e-03
2019-12-03 17:57:43,754 train 000 3.149008e-03 0.879573
2019-12-03 17:57:53,139 train 050 6.168108e-03 0.784217
2019-12-03 17:58:02,514 train 100 6.003744e-03 0.794239
2019-12-03 17:58:11,888 train 150 6.174012e-03 0.791177
2019-12-03 17:58:21,258 train 200 6.211329e-03 0.791471
2019-12-03 17:58:25,473 training loss; R2: 6.153452e-03 0.791115
2019-12-03 17:58:25,573 valid 000 9.376615e-03 0.679457
2019-12-03 17:58:26,606 validation loss; R2: 6.027957e-03 0.796793
