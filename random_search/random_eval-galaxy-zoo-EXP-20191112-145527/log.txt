2019-11-12 14:55:27,468 gpu device = 1
2019-11-12 14:55:27,468 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name='galaxy-zoo-cleaned', gpu=1, grad_clip=5, gz_dtree=False, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-galaxy-zoo-EXP-20191112-145527', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-11-12 14:55:39,099 param size = 0.297173MB
2019-11-12 14:55:39,103 epoch 0 lr 1.000000e-03
2019-11-12 14:55:41,308 train 000 4.529738e-01 -208.447252
2019-11-12 14:55:48,760 train 050 5.239921e-02 -9.824406
2019-11-12 14:55:56,152 train 100 3.996690e-02 -15.601400
2019-11-12 14:56:03,663 train 150 3.524993e-02 -10.622008
2019-11-12 14:56:11,172 train 200 3.276921e-02 -8.066018
2019-11-12 14:56:18,681 train 250 3.122162e-02 -6.499821
2019-11-12 14:56:26,209 train 300 2.996865e-02 -5.450312
2019-11-12 14:56:33,720 train 350 2.921400e-02 -4.695787
2019-11-12 14:56:41,086 train 400 2.853583e-02 -4.128509
2019-11-12 14:56:48,406 train 450 2.799627e-02 -3.684158
2019-11-12 14:56:55,906 train 500 2.756470e-02 -3.325064
2019-11-12 14:57:03,256 train 550 2.712054e-02 -3.033453
2019-11-12 14:57:10,581 train 600 2.674427e-02 -2.790226
2019-11-12 14:57:17,897 train 650 2.636401e-02 -2.579078
2019-11-12 14:57:25,320 train 700 2.603168e-02 -2.396006
2019-11-12 14:57:32,760 train 750 2.571913e-02 -2.238799
2019-11-12 14:57:40,287 train 800 2.548523e-02 -2.101381
2019-11-12 14:57:47,682 train 850 2.523796e-02 -1.976482
2019-11-12 14:57:50,712 training loss; R2: 2.515362e-02 -1.942167
2019-11-12 14:57:50,977 valid 000 2.586215e-02 -0.060959
2019-11-12 14:57:52,712 valid 050 2.624469e-02 -0.175838
2019-11-12 14:57:54,351 validation loss; R2: 2.615737e-02 -0.125513
2019-11-12 14:57:54,374 epoch 1 lr 1.000000e-03
2019-11-12 14:57:54,917 train 000 1.888925e-02 -0.114412
2019-11-12 14:58:02,329 train 050 2.097229e-02 0.036015
2019-11-12 14:58:09,788 train 100 2.082831e-02 0.033222
2019-11-12 14:58:17,104 train 150 2.072512e-02 -0.061377
2019-11-12 14:58:24,407 train 200 2.071455e-02 -0.043392
2019-11-12 14:58:31,610 train 250 2.063728e-02 -0.024497
2019-11-12 14:58:38,814 train 300 2.056650e-02 -0.011106
2019-11-12 14:58:46,025 train 350 2.050816e-02 0.000925
2019-11-12 14:58:53,226 train 400 2.042072e-02 0.004014
2019-11-12 14:59:00,429 train 450 2.031662e-02 0.005494
2019-11-12 14:59:07,640 train 500 2.029248e-02 0.010592
2019-11-12 14:59:14,843 train 550 2.024252e-02 0.013696
2019-11-12 14:59:22,048 train 600 2.018233e-02 0.018626
2019-11-12 14:59:29,261 train 650 2.015795e-02 0.021251
2019-11-12 14:59:36,471 train 700 2.009917e-02 0.025684
2019-11-12 14:59:43,686 train 750 2.000967e-02 0.030841
2019-11-12 14:59:50,896 train 800 1.995435e-02 0.031568
2019-11-12 14:59:58,101 train 850 1.989200e-02 0.032426
2019-11-12 15:00:00,259 training loss; R2: 1.985651e-02 0.033552
2019-11-12 15:00:00,536 valid 000 1.883307e-02 0.189010
2019-11-12 15:00:02,221 valid 050 1.744956e-02 0.162161
2019-11-12 15:00:03,732 validation loss; R2: 1.737818e-02 0.156601
2019-11-12 15:00:03,749 epoch 2 lr 1.000000e-03
2019-11-12 15:00:04,128 train 000 1.902027e-02 0.194755
2019-11-12 15:00:11,416 train 050 1.915830e-02 0.105879
2019-11-12 15:00:18,633 train 100 1.898064e-02 0.105619
2019-11-12 15:00:25,858 train 150 1.897104e-02 0.104250
2019-11-12 15:00:33,072 train 200 1.888998e-02 0.098714
2019-11-12 15:00:40,293 train 250 1.880235e-02 0.101176
2019-11-12 15:00:47,508 train 300 1.876042e-02 0.092370
2019-11-12 15:00:54,773 train 350 1.866184e-02 0.090616
2019-11-12 15:01:01,985 train 400 1.850467e-02 0.095248
2019-11-12 15:01:09,201 train 450 1.840119e-02 0.049987
2019-11-12 15:01:16,411 train 500 1.834530e-02 0.052934
2019-11-12 15:01:23,630 train 550 1.830237e-02 0.057234
2019-11-12 15:01:30,838 train 600 1.823585e-02 0.062642
2019-11-12 15:01:38,059 train 650 1.818578e-02 0.065389
2019-11-12 15:01:45,284 train 700 1.814166e-02 0.070083
2019-11-12 15:01:52,499 train 750 1.809973e-02 0.074733
2019-11-12 15:01:59,705 train 800 1.805705e-02 0.079472
2019-11-12 15:02:06,924 train 850 1.801266e-02 0.081477
2019-11-12 15:02:09,082 training loss; R2: 1.798680e-02 0.082787
2019-11-12 15:02:09,360 valid 000 1.520894e-02 0.278280
2019-11-12 15:02:11,078 valid 050 1.722918e-02 0.129944
2019-11-12 15:02:12,620 validation loss; R2: 1.720578e-02 0.144680
2019-11-12 15:02:12,636 epoch 3 lr 1.000000e-03
2019-11-12 15:02:13,025 train 000 1.441690e-02 0.187132
2019-11-12 15:02:20,309 train 050 1.724291e-02 0.151866
2019-11-12 15:02:27,594 train 100 1.708785e-02 0.134106
2019-11-12 15:02:34,935 train 150 1.712091e-02 0.127017
2019-11-12 15:02:42,181 train 200 1.711994e-02 0.136029
2019-11-12 15:02:49,481 train 250 1.707946e-02 0.123051
2019-11-12 15:02:56,727 train 300 1.704144e-02 0.121839
2019-11-12 15:03:04,048 train 350 1.698756e-02 0.111021
2019-11-12 15:03:11,391 train 400 1.692741e-02 0.110964
2019-11-12 15:03:18,659 train 450 1.688462e-02 0.112920
2019-11-12 15:03:25,906 train 500 1.680132e-02 0.117077
2019-11-12 15:03:33,243 train 550 1.678806e-02 0.116676
2019-11-12 15:03:40,581 train 600 1.673452e-02 0.118670
2019-11-12 15:03:47,837 train 650 1.665850e-02 0.123485
2019-11-12 15:03:55,115 train 700 1.663409e-02 0.126298
2019-11-12 15:04:02,371 train 750 1.658214e-02 0.121343
2019-11-12 15:04:09,615 train 800 1.655642e-02 0.123924
2019-11-12 15:04:16,987 train 850 1.649234e-02 0.127183
2019-11-12 15:04:19,148 training loss; R2: 1.648292e-02 0.128285
2019-11-12 15:04:19,438 valid 000 1.667951e-02 0.171870
2019-11-12 15:04:21,166 valid 050 1.580164e-02 0.176644
2019-11-12 15:04:22,706 validation loss; R2: 1.573885e-02 0.192272
2019-11-12 15:04:22,734 epoch 4 lr 1.000000e-03
2019-11-12 15:04:23,147 train 000 1.298368e-02 0.155954
2019-11-12 15:04:30,539 train 050 1.555872e-02 0.163431
2019-11-12 15:04:37,843 train 100 1.582543e-02 0.153832
2019-11-12 15:04:45,158 train 150 1.578407e-02 0.145915
2019-11-12 15:04:52,548 train 200 1.585403e-02 0.147491
2019-11-12 15:04:59,930 train 250 1.579828e-02 0.134213
2019-11-12 15:05:07,341 train 300 1.570521e-02 0.137838
2019-11-12 15:05:14,718 train 350 1.556155e-02 0.146783
2019-11-12 15:05:22,019 train 400 1.552364e-02 0.152357
2019-11-12 15:05:29,330 train 450 1.548238e-02 0.160400
2019-11-12 15:05:36,666 train 500 1.547476e-02 0.165449
2019-11-12 15:05:44,005 train 550 1.544509e-02 0.156662
2019-11-12 15:05:51,323 train 600 1.543741e-02 0.158670
2019-11-12 15:05:58,646 train 650 1.542391e-02 0.163107
2019-11-12 15:06:05,945 train 700 1.538295e-02 0.166018
2019-11-12 15:06:13,272 train 750 1.534716e-02 0.169549
2019-11-12 15:06:20,599 train 800 1.533001e-02 0.171505
2019-11-12 15:06:27,979 train 850 1.528580e-02 0.173531
2019-11-12 15:06:30,205 training loss; R2: 1.528413e-02 0.173631
2019-11-12 15:06:30,483 valid 000 1.597146e-02 0.220410
2019-11-12 15:06:32,208 valid 050 1.416265e-02 0.256488
2019-11-12 15:06:33,756 validation loss; R2: 1.419460e-02 0.240919
2019-11-12 15:06:33,773 epoch 5 lr 1.000000e-03
2019-11-12 15:06:34,166 train 000 1.637455e-02 0.261000
2019-11-12 15:06:41,504 train 050 1.488708e-02 0.221946
2019-11-12 15:06:48,952 train 100 1.509658e-02 0.205130
2019-11-12 15:06:56,289 train 150 1.493778e-02 0.203876
2019-11-12 15:07:03,689 train 200 1.484637e-02 0.211958
2019-11-12 15:07:11,114 train 250 1.476119e-02 0.216724
2019-11-12 15:07:18,427 train 300 1.471112e-02 0.119932
2019-11-12 15:07:25,743 train 350 1.467123e-02 0.130859
2019-11-12 15:07:33,070 train 400 1.462790e-02 0.141430
2019-11-12 15:07:40,415 train 450 1.459219e-02 0.150513
2019-11-12 15:07:47,817 train 500 1.455769e-02 0.158732
2019-11-12 15:07:55,099 train 550 1.458866e-02 0.160431
2019-11-12 15:08:02,428 train 600 1.455219e-02 0.163914
2019-11-12 15:08:09,779 train 650 1.452511e-02 0.170138
2019-11-12 15:08:17,058 train 700 1.448544e-02 0.175424
2019-11-12 15:08:24,348 train 750 1.444898e-02 0.178672
2019-11-12 15:08:31,710 train 800 1.444329e-02 0.180791
2019-11-12 15:08:39,014 train 850 1.443128e-02 0.184300
2019-11-12 15:08:41,181 training loss; R2: 1.442198e-02 0.185184
2019-11-12 15:08:41,487 valid 000 1.511341e-02 0.336227
2019-11-12 15:08:43,154 valid 050 1.275282e-02 0.284447
2019-11-12 15:08:44,658 validation loss; R2: 1.291932e-02 0.289238
2019-11-12 15:08:44,681 epoch 6 lr 1.000000e-03
2019-11-12 15:08:45,106 train 000 1.451869e-02 0.282742
2019-11-12 15:08:52,537 train 050 1.384992e-02 0.226613
2019-11-12 15:08:59,843 train 100 1.384830e-02 0.222554
2019-11-12 15:09:07,140 train 150 1.393695e-02 0.230383
2019-11-12 15:09:14,343 train 200 1.393422e-02 0.229726
2019-11-12 15:09:21,543 train 250 1.399353e-02 0.219553
2019-11-12 15:09:28,775 train 300 1.400717e-02 0.216227
2019-11-12 15:09:35,960 train 350 1.396231e-02 0.203555
2019-11-12 15:09:43,145 train 400 1.392950e-02 0.210517
2019-11-12 15:09:50,345 train 450 1.394334e-02 0.205966
2019-11-12 15:09:57,532 train 500 1.398722e-02 0.206336
2019-11-12 15:10:04,720 train 550 1.398218e-02 0.202390
2019-11-12 15:10:11,912 train 600 1.395072e-02 0.206845
2019-11-12 15:10:19,111 train 650 1.394431e-02 0.207628
2019-11-12 15:10:26,289 train 700 1.392128e-02 0.208954
2019-11-12 15:10:33,476 train 750 1.388851e-02 0.209991
2019-11-12 15:10:40,659 train 800 1.386893e-02 0.212925
2019-11-12 15:10:47,843 train 850 1.384031e-02 0.215595
2019-11-12 15:10:50,000 training loss; R2: 1.384433e-02 0.216354
2019-11-12 15:10:50,278 valid 000 1.101490e-02 0.351341
2019-11-12 15:10:51,974 valid 050 1.296888e-02 0.308692
2019-11-12 15:10:53,507 validation loss; R2: 1.298681e-02 0.298245
2019-11-12 15:10:53,529 epoch 7 lr 1.000000e-03
2019-11-12 15:10:53,944 train 000 1.389719e-02 0.311431
2019-11-12 15:11:01,400 train 050 1.338425e-02 0.254912
2019-11-12 15:11:08,742 train 100 1.355733e-02 0.248400
2019-11-12 15:11:16,081 train 150 1.347527e-02 0.253176
2019-11-12 15:11:23,420 train 200 1.351470e-02 0.257988
2019-11-12 15:11:30,741 train 250 1.358776e-02 0.255643
2019-11-12 15:11:38,083 train 300 1.356426e-02 0.240083
2019-11-12 15:11:45,404 train 350 1.355744e-02 0.241617
2019-11-12 15:11:52,687 train 400 1.350273e-02 0.237384
2019-11-12 15:12:00,022 train 450 1.349958e-02 0.237740
2019-11-12 15:12:07,358 train 500 1.348600e-02 0.238023
2019-11-12 15:12:14,712 train 550 1.347208e-02 0.236319
2019-11-12 15:12:22,064 train 600 1.345148e-02 0.237341
2019-11-12 15:12:29,325 train 650 1.343374e-02 0.239544
2019-11-12 15:12:36,717 train 700 1.343244e-02 0.241009
2019-11-12 15:12:44,048 train 750 1.342620e-02 0.239780
2019-11-12 15:12:51,404 train 800 1.343618e-02 0.239398
2019-11-12 15:12:58,708 train 850 1.341048e-02 0.240934
2019-11-12 15:13:00,888 training loss; R2: 1.339873e-02 0.241584
2019-11-12 15:13:01,180 valid 000 1.211476e-02 0.311900
2019-11-12 15:13:02,891 valid 050 1.146798e-02 0.343226
2019-11-12 15:13:04,449 validation loss; R2: 1.130603e-02 0.351649
2019-11-12 15:13:04,470 epoch 8 lr 1.000000e-03
2019-11-12 15:13:04,900 train 000 1.358024e-02 0.245280
2019-11-12 15:13:12,275 train 050 1.354533e-02 0.244261
2019-11-12 15:13:19,588 train 100 1.333715e-02 0.235219
2019-11-12 15:13:26,838 train 150 1.325712e-02 0.251784
2019-11-12 15:13:34,128 train 200 1.324402e-02 0.257608
2019-11-12 15:13:41,604 train 250 1.320521e-02 0.208433
2019-11-12 15:13:48,964 train 300 1.321821e-02 0.220128
2019-11-12 15:13:56,410 train 350 1.320652e-02 0.226355
2019-11-12 15:14:03,846 train 400 1.322470e-02 0.227788
2019-11-12 15:14:11,188 train 450 1.320520e-02 0.229627
2019-11-12 15:14:18,689 train 500 1.316220e-02 0.229376
2019-11-12 15:14:26,178 train 550 1.317327e-02 0.233283
2019-11-12 15:14:33,658 train 600 1.313221e-02 0.231679
2019-11-12 15:14:41,143 train 650 1.312658e-02 0.235219
2019-11-12 15:14:48,621 train 700 1.313497e-02 0.232339
2019-11-12 15:14:56,096 train 750 1.313101e-02 0.218114
2019-11-12 15:15:03,565 train 800 1.311284e-02 0.215078
2019-11-12 15:15:11,036 train 850 1.310470e-02 0.218961
2019-11-12 15:15:13,269 training loss; R2: 1.310252e-02 0.220258
2019-11-12 15:15:13,560 valid 000 8.760453e-03 0.371167
2019-11-12 15:15:15,246 valid 050 1.108870e-02 0.355720
2019-11-12 15:15:16,752 validation loss; R2: 1.101702e-02 0.342264
2019-11-12 15:15:16,769 epoch 9 lr 1.000000e-03
2019-11-12 15:15:17,195 train 000 1.378397e-02 0.083143
2019-11-12 15:15:24,509 train 050 1.301241e-02 0.258898
2019-11-12 15:15:31,851 train 100 1.314546e-02 0.240448
2019-11-12 15:15:39,370 train 150 1.305526e-02 0.255984
2019-11-12 15:15:46,860 train 200 1.297595e-02 0.259550
2019-11-12 15:15:54,346 train 250 1.292197e-02 0.262384
2019-11-12 15:16:01,830 train 300 1.292885e-02 0.257066
2019-11-12 15:16:09,310 train 350 1.289113e-02 0.255527
2019-11-12 15:16:16,790 train 400 1.289514e-02 0.259606
2019-11-12 15:16:24,276 train 450 1.285130e-02 0.258035
2019-11-12 15:16:31,758 train 500 1.284310e-02 0.240292
2019-11-12 15:16:39,234 train 550 1.286156e-02 0.241765
2019-11-12 15:16:46,722 train 600 1.286930e-02 0.243503
2019-11-12 15:16:54,204 train 650 1.286416e-02 0.245771
2019-11-12 15:17:01,697 train 700 1.288453e-02 0.246634
2019-11-12 15:17:09,176 train 750 1.290196e-02 0.244262
2019-11-12 15:17:16,377 train 800 1.289058e-02 0.240415
2019-11-12 15:17:23,551 train 850 1.289354e-02 0.240878
2019-11-12 15:17:25,739 training loss; R2: 1.289067e-02 0.241499
2019-11-12 15:17:26,004 valid 000 1.221508e-02 0.308600
2019-11-12 15:17:27,684 valid 050 1.185964e-02 0.316943
2019-11-12 15:17:29,208 validation loss; R2: 1.163551e-02 0.323281
2019-11-12 15:17:29,229 epoch 10 lr 1.000000e-03
2019-11-12 15:17:29,609 train 000 1.120187e-02 0.354909
2019-11-12 15:17:37,107 train 050 1.268124e-02 0.263907
2019-11-12 15:17:44,595 train 100 1.262132e-02 0.266862
2019-11-12 15:17:52,091 train 150 1.267395e-02 0.271438
2019-11-12 15:17:59,596 train 200 1.268164e-02 0.277024
2019-11-12 15:18:07,117 train 250 1.256659e-02 0.277262
2019-11-12 15:18:14,648 train 300 1.260682e-02 0.252739
2019-11-12 15:18:22,164 train 350 1.256551e-02 0.249947
2019-11-12 15:18:29,665 train 400 1.264447e-02 0.254819
2019-11-12 15:18:37,156 train 450 1.264738e-02 0.249275
2019-11-12 15:18:44,670 train 500 1.262742e-02 0.253469
2019-11-12 15:18:52,160 train 550 1.263278e-02 0.239896
2019-11-12 15:18:59,651 train 600 1.263338e-02 0.241368
2019-11-12 15:19:07,157 train 650 1.263393e-02 0.241909
2019-11-12 15:19:14,646 train 700 1.262849e-02 0.245761
2019-11-12 15:19:22,153 train 750 1.262064e-02 0.248686
2019-11-12 15:19:29,665 train 800 1.262178e-02 0.247710
2019-11-12 15:19:37,158 train 850 1.260011e-02 0.249114
2019-11-12 15:19:39,393 training loss; R2: 1.260079e-02 0.247711
2019-11-12 15:19:39,676 valid 000 1.107244e-02 0.308874
2019-11-12 15:19:41,365 valid 050 1.140919e-02 0.229058
2019-11-12 15:19:42,871 validation loss; R2: 1.134490e-02 0.285808
2019-11-12 15:19:42,898 epoch 11 lr 1.000000e-03
2019-11-12 15:19:43,270 train 000 1.284213e-02 0.337897
2019-11-12 15:19:50,578 train 050 1.224008e-02 0.289234
2019-11-12 15:19:57,886 train 100 1.217220e-02 0.290158
2019-11-12 15:20:05,118 train 150 1.234100e-02 0.288230
2019-11-12 15:20:12,356 train 200 1.249948e-02 0.275322
2019-11-12 15:20:19,638 train 250 1.252047e-02 0.271532
2019-11-12 15:20:27,045 train 300 1.247266e-02 0.266507
2019-11-12 15:20:34,555 train 350 1.247549e-02 0.263581
2019-11-12 15:20:42,057 train 400 1.250773e-02 0.267103
2019-11-12 15:20:49,582 train 450 1.249024e-02 0.183326
2019-11-12 15:20:57,125 train 500 1.245890e-02 0.193706
2019-11-12 15:21:04,674 train 550 1.246456e-02 0.203931
2019-11-12 15:21:12,140 train 600 1.245095e-02 0.206537
2019-11-12 15:21:19,443 train 650 1.242354e-02 0.189465
2019-11-12 15:21:26,825 train 700 1.243259e-02 0.191134
2019-11-12 15:21:34,072 train 750 1.240832e-02 0.196745
2019-11-12 15:21:41,470 train 800 1.241293e-02 0.197577
2019-11-12 15:21:48,715 train 850 1.240801e-02 0.202084
2019-11-12 15:21:50,871 training loss; R2: 1.240606e-02 0.201924
2019-11-12 15:21:51,166 valid 000 1.182153e-02 0.363225
2019-11-12 15:21:52,835 valid 050 1.137615e-02 0.297444
2019-11-12 15:21:54,353 validation loss; R2: 1.121249e-02 0.318165
2019-11-12 15:21:54,370 epoch 12 lr 1.000000e-03
2019-11-12 15:21:54,767 train 000 1.152511e-02 0.208291
2019-11-12 15:22:02,074 train 050 1.258585e-02 0.258375
2019-11-12 15:22:09,449 train 100 1.244043e-02 0.255097
2019-11-12 15:22:16,795 train 150 1.245791e-02 0.264788
2019-11-12 15:22:24,087 train 200 1.242711e-02 0.272728
2019-11-12 15:22:31,388 train 250 1.235087e-02 0.265669
2019-11-12 15:22:38,750 train 300 1.236131e-02 0.266815
2019-11-12 15:22:46,043 train 350 1.234610e-02 0.274962
2019-11-12 15:22:53,367 train 400 1.234550e-02 0.273718
2019-11-12 15:23:00,739 train 450 1.234917e-02 0.265059
2019-11-12 15:23:08,109 train 500 1.234011e-02 0.265862
2019-11-12 15:23:15,393 train 550 1.233790e-02 0.268125
2019-11-12 15:23:22,740 train 600 1.230280e-02 0.269314
2019-11-12 15:23:30,055 train 650 1.228152e-02 0.271482
2019-11-12 15:23:37,331 train 700 1.226818e-02 0.274019
2019-11-12 15:23:44,610 train 750 1.226903e-02 0.272826
2019-11-12 15:23:51,901 train 800 1.225294e-02 0.274880
2019-11-12 15:23:59,265 train 850 1.224100e-02 0.276498
2019-11-12 15:24:01,479 training loss; R2: 1.224393e-02 0.277331
2019-11-12 15:24:01,769 valid 000 1.165442e-02 0.186340
2019-11-12 15:24:03,487 valid 050 1.125181e-02 0.272410
2019-11-12 15:24:05,038 validation loss; R2: 1.132444e-02 0.266450
2019-11-12 15:24:05,060 epoch 13 lr 1.000000e-03
2019-11-12 15:24:05,495 train 000 1.133723e-02 0.334155
2019-11-12 15:24:13,025 train 050 1.228633e-02 0.281519
2019-11-12 15:24:20,337 train 100 1.223246e-02 0.284435
2019-11-12 15:24:27,687 train 150 1.217211e-02 0.267347
2019-11-12 15:24:35,042 train 200 1.215202e-02 0.274130
2019-11-12 15:24:42,403 train 250 1.212305e-02 0.283229
2019-11-12 15:24:49,733 train 300 1.210460e-02 0.279876
2019-11-12 15:24:57,073 train 350 1.202178e-02 0.284351
2019-11-12 15:25:04,372 train 400 1.203531e-02 0.286764
2019-11-12 15:25:11,735 train 450 1.204329e-02 0.290069
2019-11-12 15:25:19,108 train 500 1.205701e-02 0.290342
2019-11-12 15:25:26,424 train 550 1.205449e-02 0.290546
2019-11-12 15:25:33,795 train 600 1.207072e-02 0.291140
2019-11-12 15:25:41,097 train 650 1.206507e-02 0.289772
2019-11-12 15:25:48,462 train 700 1.209611e-02 0.286988
2019-11-12 15:25:55,818 train 750 1.211666e-02 0.286681
2019-11-12 15:26:03,332 train 800 1.212328e-02 0.286133
2019-11-12 15:26:10,693 train 850 1.211598e-02 0.286793
2019-11-12 15:26:12,962 training loss; R2: 1.212308e-02 0.287134
2019-11-12 15:26:13,246 valid 000 1.186271e-02 0.383153
2019-11-12 15:26:14,942 valid 050 1.135055e-02 0.360070
2019-11-12 15:26:16,438 validation loss; R2: 1.131905e-02 0.166598
2019-11-12 15:26:16,454 epoch 14 lr 1.000000e-03
2019-11-12 15:26:16,866 train 000 1.297566e-02 0.272954
2019-11-12 15:26:24,125 train 050 1.244701e-02 0.318545
2019-11-12 15:26:31,383 train 100 1.225415e-02 0.298874
2019-11-12 15:26:38,694 train 150 1.207309e-02 0.280133
2019-11-12 15:26:45,956 train 200 1.206137e-02 0.278781
2019-11-12 15:26:53,214 train 250 1.203202e-02 0.280400
2019-11-12 15:27:00,530 train 300 1.205909e-02 0.282987
2019-11-12 15:27:07,917 train 350 1.206827e-02 0.275573
2019-11-12 15:27:15,208 train 400 1.204710e-02 0.277313
2019-11-12 15:27:22,586 train 450 1.201754e-02 0.280227
2019-11-12 15:27:29,955 train 500 1.199573e-02 0.260761
2019-11-12 15:27:37,375 train 550 1.202767e-02 0.263936
2019-11-12 15:27:44,717 train 600 1.202301e-02 0.264434
2019-11-12 15:27:52,200 train 650 1.200177e-02 0.262750
2019-11-12 15:27:59,592 train 700 1.197352e-02 0.264277
2019-11-12 15:28:06,922 train 750 1.197862e-02 0.266998
2019-11-12 15:28:14,295 train 800 1.198357e-02 0.267768
2019-11-12 15:28:21,702 train 850 1.197204e-02 0.268712
2019-11-12 15:28:23,874 training loss; R2: 1.197116e-02 0.268959
2019-11-12 15:28:24,162 valid 000 1.288789e-02 0.366937
2019-11-12 15:28:25,864 valid 050 1.115346e-02 0.334577
2019-11-12 15:28:27,392 validation loss; R2: 1.103179e-02 0.323233
2019-11-12 15:28:27,414 epoch 15 lr 1.000000e-03
2019-11-12 15:28:27,829 train 000 1.073708e-02 0.419544
2019-11-12 15:28:35,153 train 050 1.175412e-02 0.304167
2019-11-12 15:28:42,538 train 100 1.201873e-02 0.311634
2019-11-12 15:28:49,964 train 150 1.202774e-02 0.307155
2019-11-12 15:28:57,331 train 200 1.195936e-02 0.307560
2019-11-12 15:29:04,604 train 250 1.194380e-02 0.307587
2019-11-12 15:29:11,920 train 300 1.194320e-02 0.307165
2019-11-12 15:29:19,264 train 350 1.192503e-02 0.304219
2019-11-12 15:29:26,545 train 400 1.191946e-02 0.302564
2019-11-12 15:29:33,826 train 450 1.194347e-02 0.302167
2019-11-12 15:29:41,216 train 500 1.194216e-02 0.300077
2019-11-12 15:29:48,491 train 550 1.193573e-02 0.295875
2019-11-12 15:29:55,837 train 600 1.191520e-02 0.295917
2019-11-12 15:30:03,142 train 650 1.192341e-02 0.288178
2019-11-12 15:30:10,417 train 700 1.188552e-02 0.289650
2019-11-12 15:30:17,667 train 750 1.187634e-02 0.244618
2019-11-12 15:30:24,961 train 800 1.188608e-02 0.245156
2019-11-12 15:30:32,241 train 850 1.187706e-02 0.250011
2019-11-12 15:30:34,406 training loss; R2: 1.186940e-02 0.250887
2019-11-12 15:30:34,698 valid 000 1.187778e-02 0.345079
2019-11-12 15:30:36,438 valid 050 1.079611e-02 0.350106
2019-11-12 15:30:37,979 validation loss; R2: 1.084846e-02 0.323161
2019-11-12 15:30:38,000 epoch 16 lr 1.000000e-03
2019-11-12 15:30:38,405 train 000 1.142968e-02 0.374205
2019-11-12 15:30:45,813 train 050 1.193008e-02 0.303635
2019-11-12 15:30:53,165 train 100 1.197791e-02 0.301659
2019-11-12 15:31:00,465 train 150 1.189626e-02 0.266681
2019-11-12 15:31:07,727 train 200 1.194785e-02 0.278166
2019-11-12 15:31:15,144 train 250 1.188806e-02 0.285217
2019-11-12 15:31:22,546 train 300 1.185163e-02 0.286001
2019-11-12 15:31:29,957 train 350 1.184658e-02 0.288328
2019-11-12 15:31:37,268 train 400 1.187243e-02 0.292317
2019-11-12 15:31:44,580 train 450 1.185520e-02 0.295403
2019-11-12 15:31:51,862 train 500 1.184550e-02 0.293249
2019-11-12 15:31:59,152 train 550 1.182031e-02 0.292091
2019-11-12 15:32:06,561 train 600 1.181195e-02 0.290552
2019-11-12 15:32:13,848 train 650 1.182206e-02 0.289642
2019-11-12 15:32:21,188 train 700 1.182315e-02 0.289132
2019-11-12 15:32:28,497 train 750 1.181851e-02 0.288978
2019-11-12 15:32:35,825 train 800 1.181609e-02 0.291603
2019-11-12 15:32:43,170 train 850 1.180603e-02 0.292516
2019-11-12 15:32:45,332 training loss; R2: 1.180744e-02 0.292815
2019-11-12 15:32:45,654 valid 000 1.131096e-02 0.324858
2019-11-12 15:32:47,308 valid 050 1.025008e-02 0.371169
2019-11-12 15:32:48,828 validation loss; R2: 1.032184e-02 0.343045
2019-11-12 15:32:48,852 epoch 17 lr 1.000000e-03
2019-11-12 15:32:49,255 train 000 1.085955e-02 0.326120
2019-11-12 15:32:56,478 train 050 1.187631e-02 0.294187
2019-11-12 15:33:03,719 train 100 1.181670e-02 0.301439
2019-11-12 15:33:11,003 train 150 1.167610e-02 0.293207
2019-11-12 15:33:18,269 train 200 1.165336e-02 0.294936
2019-11-12 15:33:25,499 train 250 1.164904e-02 0.300462
2019-11-12 15:33:32,773 train 300 1.165589e-02 0.305578
2019-11-12 15:33:40,010 train 350 1.168806e-02 0.299599
2019-11-12 15:33:47,374 train 400 1.169317e-02 0.300214
2019-11-12 15:33:54,603 train 450 1.168203e-02 0.303809
2019-11-12 15:34:01,882 train 500 1.168767e-02 0.304769
2019-11-12 15:34:09,175 train 550 1.170318e-02 0.304008
2019-11-12 15:34:16,410 train 600 1.170751e-02 0.301868
2019-11-12 15:34:23,734 train 650 1.171093e-02 0.304568
2019-11-12 15:34:31,013 train 700 1.168608e-02 0.304240
2019-11-12 15:34:38,453 train 750 1.166845e-02 0.305662
2019-11-12 15:34:45,795 train 800 1.166441e-02 0.304059
2019-11-12 15:34:53,050 train 850 1.168367e-02 0.302693
2019-11-12 15:34:55,255 training loss; R2: 1.170143e-02 0.302845
2019-11-12 15:34:55,538 valid 000 1.008939e-02 0.426139
2019-11-12 15:34:57,267 valid 050 1.029361e-02 0.367365
2019-11-12 15:34:58,828 validation loss; R2: 1.038146e-02 0.358831
2019-11-12 15:34:58,849 epoch 18 lr 1.000000e-03
2019-11-12 15:34:59,296 train 000 1.109193e-02 0.325489
2019-11-12 15:35:06,608 train 050 1.151406e-02 0.293880
2019-11-12 15:35:13,920 train 100 1.139891e-02 0.304781
2019-11-12 15:35:21,226 train 150 1.148503e-02 0.304219
2019-11-12 15:35:28,576 train 200 1.155466e-02 0.307053
2019-11-12 15:35:35,898 train 250 1.157045e-02 0.310042
2019-11-12 15:35:43,246 train 300 1.157499e-02 0.265837
2019-11-12 15:35:50,639 train 350 1.159506e-02 0.266684
2019-11-12 15:35:57,997 train 400 1.157092e-02 0.257098
2019-11-12 15:36:05,359 train 450 1.155106e-02 0.263993
2019-11-12 15:36:12,733 train 500 1.154869e-02 0.268972
2019-11-12 15:36:20,222 train 550 1.157512e-02 0.102633
2019-11-12 15:36:27,509 train 600 1.158897e-02 0.120811
2019-11-12 15:36:34,796 train 650 1.160083e-02 0.136024
2019-11-12 15:36:42,165 train 700 1.159983e-02 0.150577
2019-11-12 15:36:49,420 train 750 1.161252e-02 0.162370
2019-11-12 15:36:56,681 train 800 1.161919e-02 0.171507
2019-11-12 15:37:03,984 train 850 1.160694e-02 0.180481
2019-11-12 15:37:06,144 training loss; R2: 1.160878e-02 0.182284
2019-11-12 15:37:06,427 valid 000 1.113316e-02 0.333065
2019-11-12 15:37:08,167 valid 050 1.097026e-02 0.344023
2019-11-12 15:37:09,717 validation loss; R2: 1.097678e-02 0.339400
2019-11-12 15:37:09,738 epoch 19 lr 1.000000e-03
2019-11-12 15:37:10,164 train 000 1.205416e-02 0.306733
2019-11-12 15:37:17,391 train 050 1.185758e-02 0.314510
2019-11-12 15:37:24,828 train 100 1.215366e-02 0.280008
2019-11-12 15:37:32,184 train 150 1.196208e-02 0.285273
2019-11-12 15:37:39,484 train 200 1.191295e-02 0.222998
2019-11-12 15:37:46,758 train 250 1.189271e-02 0.239479
2019-11-12 15:37:54,125 train 300 1.183474e-02 0.253384
2019-11-12 15:38:01,535 train 350 1.179623e-02 0.262235
2019-11-12 15:38:08,896 train 400 1.178829e-02 0.262242
2019-11-12 15:38:16,250 train 450 1.174222e-02 0.270827
2019-11-12 15:38:23,516 train 500 1.171955e-02 0.269966
2019-11-12 15:38:30,816 train 550 1.172904e-02 0.267111
2019-11-12 15:38:38,141 train 600 1.170128e-02 0.259359
2019-11-12 15:38:45,454 train 650 1.168401e-02 0.262624
2019-11-12 15:38:52,716 train 700 1.167020e-02 0.264860
2019-11-12 15:38:59,984 train 750 1.165770e-02 0.266550
2019-11-12 15:39:07,438 train 800 1.164241e-02 0.270165
2019-11-12 15:39:14,786 train 850 1.164002e-02 0.269189
2019-11-12 15:39:16,941 training loss; R2: 1.163607e-02 0.269826
2019-11-12 15:39:17,225 valid 000 1.162673e-02 0.412684
2019-11-12 15:39:18,965 valid 050 1.038575e-02 0.260200
2019-11-12 15:39:20,517 validation loss; R2: 1.042797e-02 0.297893
