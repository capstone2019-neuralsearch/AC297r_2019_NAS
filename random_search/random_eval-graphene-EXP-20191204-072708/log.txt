2019-12-04 07:27:08,497 gpu device = 1
2019-12-04 07:27:08,497 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=96, cutout=False, cutout_length=16, data='../data', dataset='graphene', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name=None, gpu=1, grad_clip=5, gz_dtree=False, init_channels=36, layers=8, learning_rate=0.002, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-graphene-EXP-20191204-072708', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-12-04 07:27:11,684 param size = 1.228645MB
2019-12-04 07:27:11,687 epoch 0 lr 2.000000e-03
2019-12-04 07:27:14,423 train 000 1.808886e+00 -58.579568
2019-12-04 07:27:27,919 train 050 1.914578e-01 -4.982396
2019-12-04 07:27:41,297 train 100 1.192850e-01 -2.764373
2019-12-04 07:27:54,670 train 150 8.930401e-02 -1.843221
2019-12-04 07:28:07,975 train 200 7.372164e-02 -1.344669
2019-12-04 07:28:14,962 training loss; R2: 6.993013e-02 -1.245147
2019-12-04 07:28:15,110 valid 000 1.039651e-02 0.367804
2019-12-04 07:28:16,824 validation loss; R2: 1.932128e-02 0.392300
2019-12-04 07:28:16,845 epoch 1 lr 2.000000e-03
2019-12-04 07:28:17,269 train 000 2.078717e-02 -0.172149
2019-12-04 07:28:30,337 train 050 2.802977e-02 0.086459
2019-12-04 07:28:43,413 train 100 2.664839e-02 0.118686
2019-12-04 07:28:56,494 train 150 2.490784e-02 0.202338
2019-12-04 07:29:09,580 train 200 2.276407e-02 0.258547
2019-12-04 07:29:15,465 training loss; R2: 2.223527e-02 0.274961
2019-12-04 07:29:15,602 valid 000 8.118982e-03 0.687728
2019-12-04 07:29:16,995 validation loss; R2: 1.115276e-02 0.637812
2019-12-04 07:29:17,023 epoch 2 lr 2.000000e-03
2019-12-04 07:29:17,370 train 000 2.437015e-02 0.464672
2019-12-04 07:29:30,459 train 050 1.712786e-02 0.473556
2019-12-04 07:29:43,554 train 100 1.577020e-02 0.502860
2019-12-04 07:29:56,647 train 150 1.496983e-02 0.529219
2019-12-04 07:30:09,740 train 200 1.477122e-02 0.540330
2019-12-04 07:30:15,628 training loss; R2: 1.434083e-02 0.545368
2019-12-04 07:30:15,766 valid 000 5.387122e-03 0.679453
2019-12-04 07:30:17,159 validation loss; R2: 8.350954e-03 0.731273
2019-12-04 07:30:17,182 epoch 3 lr 2.000000e-03
2019-12-04 07:30:17,531 train 000 1.035023e-02 0.196385
2019-12-04 07:30:30,612 train 050 1.290954e-02 0.603379
2019-12-04 07:30:43,694 train 100 1.200404e-02 0.612514
2019-12-04 07:30:56,779 train 150 1.090816e-02 0.637780
2019-12-04 07:31:09,858 train 200 1.073231e-02 0.645802
2019-12-04 07:31:15,741 training loss; R2: 1.078471e-02 0.648768
2019-12-04 07:31:15,882 valid 000 8.503859e-03 0.789370
2019-12-04 07:31:17,274 validation loss; R2: 8.286112e-03 0.735341
2019-12-04 07:31:17,297 epoch 4 lr 2.000000e-03
2019-12-04 07:31:17,646 train 000 3.538841e-02 0.628513
2019-12-04 07:31:30,736 train 050 9.766892e-03 0.673629
2019-12-04 07:31:43,828 train 100 9.193429e-03 0.690237
2019-12-04 07:31:56,925 train 150 9.613398e-03 0.687921
2019-12-04 07:32:10,018 train 200 9.296370e-03 0.693538
2019-12-04 07:32:15,903 training loss; R2: 9.070308e-03 0.697064
2019-12-04 07:32:16,043 valid 000 5.282730e-03 0.821520
2019-12-04 07:32:17,436 validation loss; R2: 5.654114e-03 0.821603
2019-12-04 07:32:17,460 epoch 5 lr 2.000000e-03
2019-12-04 07:32:17,814 train 000 8.702666e-03 0.776512
2019-12-04 07:32:30,905 train 050 8.258403e-03 0.748841
2019-12-04 07:32:43,995 train 100 8.065755e-03 0.740084
2019-12-04 07:32:57,085 train 150 7.993127e-03 0.738013
2019-12-04 07:33:10,179 train 200 7.868758e-03 0.742584
2019-12-04 07:33:16,070 training loss; R2: 7.854863e-03 0.741021
2019-12-04 07:33:16,209 valid 000 4.459409e-03 0.875680
2019-12-04 07:33:17,602 validation loss; R2: 4.999566e-03 0.835630
2019-12-04 07:33:17,626 epoch 6 lr 2.000000e-03
2019-12-04 07:33:17,977 train 000 4.956195e-03 0.786876
2019-12-04 07:33:31,064 train 050 7.319599e-03 0.727815
2019-12-04 07:33:44,154 train 100 7.247997e-03 0.741496
2019-12-04 07:33:57,239 train 150 7.563394e-03 0.739307
2019-12-04 07:34:10,327 train 200 7.843855e-03 0.730409
2019-12-04 07:34:16,213 training loss; R2: 7.746220e-03 0.732173
2019-12-04 07:34:16,352 valid 000 1.966523e-02 0.665908
2019-12-04 07:34:17,745 validation loss; R2: 9.440885e-03 0.694128
2019-12-04 07:34:17,768 epoch 7 lr 2.000000e-03
2019-12-04 07:34:18,116 train 000 5.451410e-03 0.790484
2019-12-04 07:34:31,209 train 050 6.490656e-03 0.787316
2019-12-04 07:34:44,300 train 100 7.004511e-03 0.778733
2019-12-04 07:34:57,392 train 150 7.140808e-03 0.770760
2019-12-04 07:35:10,481 train 200 6.949977e-03 0.771446
2019-12-04 07:35:16,364 training loss; R2: 6.769699e-03 0.772941
2019-12-04 07:35:16,505 valid 000 5.444856e-03 0.905657
2019-12-04 07:35:17,898 validation loss; R2: 4.158145e-03 0.859105
2019-12-04 07:35:17,921 epoch 8 lr 2.000000e-03
2019-12-04 07:35:18,268 train 000 6.476670e-03 0.841854
2019-12-04 07:35:31,351 train 050 7.714040e-03 0.715974
2019-12-04 07:35:44,441 train 100 6.925552e-03 0.752745
2019-12-04 07:35:57,526 train 150 6.969151e-03 0.760011
2019-12-04 07:36:10,618 train 200 7.013097e-03 0.758562
2019-12-04 07:36:16,501 training loss; R2: 6.944543e-03 0.761573
2019-12-04 07:36:16,641 valid 000 4.089090e-03 0.858837
2019-12-04 07:36:18,034 validation loss; R2: 4.624714e-03 0.851491
2019-12-04 07:36:18,066 epoch 9 lr 2.000000e-03
2019-12-04 07:36:18,420 train 000 3.456851e-03 0.858822
2019-12-04 07:36:31,504 train 050 5.868627e-03 0.783074
2019-12-04 07:36:44,591 train 100 6.484454e-03 0.777972
2019-12-04 07:36:57,675 train 150 6.665418e-03 0.767375
2019-12-04 07:37:10,754 train 200 6.488582e-03 0.778882
2019-12-04 07:37:16,638 training loss; R2: 6.408378e-03 0.781185
2019-12-04 07:37:16,781 valid 000 2.785125e-03 0.943075
2019-12-04 07:37:18,175 validation loss; R2: 3.769097e-03 0.868220
2019-12-04 07:37:18,198 epoch 10 lr 2.000000e-03
2019-12-04 07:37:18,551 train 000 7.548432e-03 0.877062
2019-12-04 07:37:31,637 train 050 6.084168e-03 0.811633
2019-12-04 07:37:44,722 train 100 6.243141e-03 0.794739
2019-12-04 07:37:57,809 train 150 5.921002e-03 0.796945
2019-12-04 07:38:10,893 train 200 5.782861e-03 0.799688
2019-12-04 07:38:16,780 training loss; R2: 5.856159e-03 0.799716
2019-12-04 07:38:16,924 valid 000 3.553308e-03 0.905511
2019-12-04 07:38:18,318 validation loss; R2: 4.037300e-03 0.866649
2019-12-04 07:38:18,341 epoch 11 lr 2.000000e-03
2019-12-04 07:38:18,689 train 000 6.355103e-03 0.778681
2019-12-04 07:38:31,778 train 050 6.077308e-03 0.770082
2019-12-04 07:38:44,860 train 100 5.885044e-03 0.784959
2019-12-04 07:38:57,942 train 150 5.771477e-03 0.794023
2019-12-04 07:39:11,027 train 200 5.737626e-03 0.799324
2019-12-04 07:39:16,910 training loss; R2: 5.756363e-03 0.798965
2019-12-04 07:39:17,053 valid 000 3.916758e-03 0.810103
2019-12-04 07:39:18,447 validation loss; R2: 4.579295e-03 0.836453
2019-12-04 07:39:18,469 epoch 12 lr 2.000000e-03
2019-12-04 07:39:18,816 train 000 4.978961e-03 0.889805
2019-12-04 07:39:31,899 train 050 6.320769e-03 0.785196
2019-12-04 07:39:44,990 train 100 5.668139e-03 0.802290
2019-12-04 07:39:58,079 train 150 5.355484e-03 0.812591
2019-12-04 07:40:11,166 train 200 5.587508e-03 0.808624
2019-12-04 07:40:17,052 training loss; R2: 5.534033e-03 0.812869
2019-12-04 07:40:17,191 valid 000 6.603921e-03 0.781931
2019-12-04 07:40:18,585 validation loss; R2: 6.906325e-03 0.771623
2019-12-04 07:40:18,609 epoch 13 lr 2.000000e-03
2019-12-04 07:40:18,956 train 000 6.222177e-03 0.841981
2019-12-04 07:40:32,043 train 050 4.574245e-03 0.839101
2019-12-04 07:40:45,126 train 100 4.984130e-03 0.823512
2019-12-04 07:40:58,216 train 150 5.391989e-03 0.805998
2019-12-04 07:41:11,305 train 200 5.390151e-03 0.807728
2019-12-04 07:41:17,190 training loss; R2: 5.445236e-03 0.809639
2019-12-04 07:41:17,329 valid 000 8.606678e-03 0.666984
2019-12-04 07:41:18,722 validation loss; R2: 9.013995e-03 0.681167
2019-12-04 07:41:18,747 epoch 14 lr 2.000000e-03
2019-12-04 07:41:19,097 train 000 7.211385e-03 0.882674
2019-12-04 07:41:32,180 train 050 6.242368e-03 0.782922
2019-12-04 07:41:45,267 train 100 5.652656e-03 0.801092
2019-12-04 07:41:58,359 train 150 5.465111e-03 0.809737
2019-12-04 07:42:11,448 train 200 5.407232e-03 0.814151
2019-12-04 07:42:17,336 training loss; R2: 5.312198e-03 0.817283
2019-12-04 07:42:17,477 valid 000 2.270633e-03 0.945482
2019-12-04 07:42:18,869 validation loss; R2: 2.924841e-03 0.900182
2019-12-04 07:42:18,893 epoch 15 lr 2.000000e-03
2019-12-04 07:42:19,243 train 000 3.836733e-03 0.858174
2019-12-04 07:42:32,333 train 050 4.501928e-03 0.836957
2019-12-04 07:42:45,414 train 100 4.287201e-03 0.841517
2019-12-04 07:42:58,492 train 150 4.682093e-03 0.840420
2019-12-04 07:43:11,570 train 200 4.926846e-03 0.832742
2019-12-04 07:43:17,453 training loss; R2: 4.887538e-03 0.833559
2019-12-04 07:43:17,592 valid 000 2.711282e-03 0.939675
2019-12-04 07:43:18,985 validation loss; R2: 3.013889e-03 0.893753
2019-12-04 07:43:19,014 epoch 16 lr 2.000000e-03
2019-12-04 07:43:19,363 train 000 1.375936e-02 0.851199
2019-12-04 07:43:32,432 train 050 4.688891e-03 0.847455
2019-12-04 07:43:45,503 train 100 4.710225e-03 0.846277
2019-12-04 07:43:58,576 train 150 4.569120e-03 0.844410
2019-12-04 07:44:11,648 train 200 4.520458e-03 0.844964
2019-12-04 07:44:17,524 training loss; R2: 4.589627e-03 0.840955
2019-12-04 07:44:17,663 valid 000 2.363486e-03 0.854434
2019-12-04 07:44:19,056 validation loss; R2: 2.951812e-03 0.895446
2019-12-04 07:44:19,080 epoch 17 lr 2.000000e-03
2019-12-04 07:44:19,446 train 000 5.638490e-03 0.525825
2019-12-04 07:44:32,809 train 050 4.676166e-03 0.834149
2019-12-04 07:44:46,172 train 100 4.775232e-03 0.836251
2019-12-04 07:44:59,533 train 150 4.575105e-03 0.842511
2019-12-04 07:45:12,901 train 200 4.741003e-03 0.838357
2019-12-04 07:45:18,914 training loss; R2: 4.741325e-03 0.837470
2019-12-04 07:45:19,062 valid 000 9.987924e-03 0.767009
2019-12-04 07:45:20,456 validation loss; R2: 1.101413e-02 0.619291
2019-12-04 07:45:20,482 epoch 18 lr 2.000000e-03
2019-12-04 07:45:20,836 train 000 4.492653e-03 0.833894
2019-12-04 07:45:34,202 train 050 4.659695e-03 0.827109
2019-12-04 07:45:47,574 train 100 4.710662e-03 0.818693
2019-12-04 07:46:00,943 train 150 4.965852e-03 0.823226
2019-12-04 07:46:14,205 train 200 4.867024e-03 0.826363
2019-12-04 07:46:20,077 training loss; R2: 4.760811e-03 0.830706
2019-12-04 07:46:20,217 valid 000 7.917990e-03 0.705567
2019-12-04 07:46:21,609 validation loss; R2: 6.443663e-03 0.782433
2019-12-04 07:46:21,633 epoch 19 lr 2.000000e-03
2019-12-04 07:46:21,976 train 000 5.567765e-03 0.667107
2019-12-04 07:46:35,038 train 050 5.677818e-03 0.822264
2019-12-04 07:46:48,100 train 100 4.947438e-03 0.834018
2019-12-04 07:47:01,164 train 150 5.041237e-03 0.835993
2019-12-04 07:47:14,221 train 200 4.906577e-03 0.834018
2019-12-04 07:47:20,093 training loss; R2: 4.820577e-03 0.836952
2019-12-04 07:47:20,236 valid 000 3.326881e-03 0.829015
2019-12-04 07:47:21,628 validation loss; R2: 3.360560e-03 0.874678
