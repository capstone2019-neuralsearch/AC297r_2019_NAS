2019-11-12 23:37:04,958 gpu device = 1
2019-11-12 23:37:04,959 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name='galaxy-zoo-cleaned', gpu=1, grad_clip=5, gz_dtree=False, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-galaxy-zoo-EXP-20191112-233704', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-11-12 23:37:16,595 param size = 0.280853MB
2019-11-12 23:37:16,599 epoch 0 lr 1.000000e-03
2019-11-12 23:37:18,777 train 000 3.375640e-01 -239.962674
2019-11-12 23:37:26,144 train 050 4.521256e-02 -7.875337
2019-11-12 23:37:33,498 train 100 3.611473e-02 -4.215839
2019-11-12 23:37:40,846 train 150 3.241357e-02 -2.913190
2019-11-12 23:37:48,189 train 200 3.026185e-02 -2.239457
2019-11-12 23:37:55,529 train 250 2.898750e-02 -1.825807
2019-11-12 23:38:02,868 train 300 2.788459e-02 -1.536183
2019-11-12 23:38:10,206 train 350 2.704317e-02 -1.327170
2019-11-12 23:38:17,555 train 400 2.638142e-02 -1.174834
2019-11-12 23:38:24,904 train 450 2.584002e-02 -1.054113
2019-11-12 23:38:32,257 train 500 2.533208e-02 -0.948399
2019-11-12 23:38:39,628 train 550 2.495466e-02 -0.863156
2019-11-12 23:38:47,008 train 600 2.459678e-02 -0.785758
2019-11-12 23:38:54,356 train 650 2.423576e-02 -0.746296
2019-11-12 23:39:01,728 train 700 2.391262e-02 -0.688245
2019-11-12 23:39:09,004 train 750 2.363346e-02 -0.677344
2019-11-12 23:39:16,105 train 800 2.335803e-02 -0.643352
2019-11-12 23:39:23,222 train 850 2.307856e-02 -0.606857
2019-11-12 23:39:26,122 training loss; R2: 2.301341e-02 -0.594426
2019-11-12 23:39:26,434 valid 000 1.792336e-02 0.118902
2019-11-12 23:39:28,133 valid 050 1.764644e-02 0.084538
2019-11-12 23:39:29,778 validation loss; R2: 1.750668e-02 0.115803
2019-11-12 23:39:29,796 epoch 1 lr 1.000000e-03
2019-11-12 23:39:30,367 train 000 1.874907e-02 0.133779
2019-11-12 23:39:37,423 train 050 1.877590e-02 0.105687
2019-11-12 23:39:44,423 train 100 1.857320e-02 0.093969
2019-11-12 23:39:51,432 train 150 1.858850e-02 0.088571
2019-11-12 23:39:58,443 train 200 1.854849e-02 0.092788
2019-11-12 23:40:05,450 train 250 1.852016e-02 0.095848
2019-11-12 23:40:12,455 train 300 1.838866e-02 0.095837
2019-11-12 23:40:19,463 train 350 1.833068e-02 0.099595
2019-11-12 23:40:26,470 train 400 1.821789e-02 0.100331
2019-11-12 23:40:33,483 train 450 1.812333e-02 0.098142
2019-11-12 23:40:40,491 train 500 1.804828e-02 0.100679
2019-11-12 23:40:47,501 train 550 1.794877e-02 0.104487
2019-11-12 23:40:54,511 train 600 1.788695e-02 0.107363
2019-11-12 23:41:01,524 train 650 1.778719e-02 0.088960
2019-11-12 23:41:08,535 train 700 1.774979e-02 0.091430
2019-11-12 23:41:15,542 train 750 1.768481e-02 0.094830
2019-11-12 23:41:22,548 train 800 1.760347e-02 0.096683
2019-11-12 23:41:29,553 train 850 1.753292e-02 0.098569
2019-11-12 23:41:31,652 training loss; R2: 1.750107e-02 0.100225
2019-11-12 23:41:31,953 valid 000 1.362199e-02 0.313082
2019-11-12 23:41:33,658 valid 050 1.552262e-02 0.215113
2019-11-12 23:41:35,234 validation loss; R2: 1.561663e-02 0.201473
2019-11-12 23:41:35,255 epoch 2 lr 1.000000e-03
2019-11-12 23:41:35,680 train 000 1.659011e-02 0.195700
2019-11-12 23:41:42,885 train 050 1.598780e-02 0.101634
2019-11-12 23:41:50,069 train 100 1.605892e-02 0.125717
2019-11-12 23:41:57,184 train 150 1.609706e-02 0.146957
2019-11-12 23:42:04,300 train 200 1.598797e-02 0.130793
2019-11-12 23:42:11,342 train 250 1.595372e-02 0.137457
2019-11-12 23:42:18,397 train 300 1.592528e-02 0.136781
2019-11-12 23:42:25,522 train 350 1.588069e-02 0.145803
2019-11-12 23:42:32,560 train 400 1.587993e-02 0.147244
2019-11-12 23:42:39,593 train 450 1.577875e-02 0.150680
2019-11-12 23:42:46,626 train 500 1.568653e-02 0.149208
2019-11-12 23:42:53,653 train 550 1.562339e-02 0.155517
2019-11-12 23:43:00,681 train 600 1.556518e-02 0.158463
2019-11-12 23:43:07,840 train 650 1.552390e-02 0.160198
2019-11-12 23:43:14,898 train 700 1.545464e-02 0.163809
2019-11-12 23:43:21,970 train 750 1.536826e-02 0.165534
2019-11-12 23:43:29,056 train 800 1.530520e-02 0.169476
2019-11-12 23:43:36,193 train 850 1.526643e-02 0.170791
2019-11-12 23:43:38,295 training loss; R2: 1.525355e-02 0.171424
2019-11-12 23:43:38,599 valid 000 1.218687e-02 0.337490
2019-11-12 23:43:40,332 valid 050 1.415512e-02 0.234530
2019-11-12 23:43:41,891 validation loss; R2: 1.412941e-02 0.218984
2019-11-12 23:43:41,913 epoch 3 lr 1.000000e-03
2019-11-12 23:43:42,288 train 000 1.357776e-02 0.158314
2019-11-12 23:43:49,434 train 050 1.432993e-02 0.229806
2019-11-12 23:43:56,700 train 100 1.431004e-02 0.243085
2019-11-12 23:44:03,892 train 150 1.421484e-02 0.215021
2019-11-12 23:44:11,204 train 200 1.420633e-02 0.174769
2019-11-12 23:44:18,361 train 250 1.421131e-02 0.185148
2019-11-12 23:44:25,578 train 300 1.414588e-02 0.177608
2019-11-12 23:44:32,735 train 350 1.410833e-02 0.181199
2019-11-12 23:44:39,817 train 400 1.409458e-02 0.189963
2019-11-12 23:44:46,990 train 450 1.408512e-02 0.186707
2019-11-12 23:44:54,106 train 500 1.405846e-02 0.188784
2019-11-12 23:45:01,170 train 550 1.404473e-02 0.190227
2019-11-12 23:45:08,375 train 600 1.398754e-02 0.194537
2019-11-12 23:45:15,448 train 650 1.396694e-02 0.195636
2019-11-12 23:45:22,596 train 700 1.394320e-02 0.199830
2019-11-12 23:45:29,718 train 750 1.393654e-02 0.198394
2019-11-12 23:45:37,070 train 800 1.391770e-02 0.200661
2019-11-12 23:45:44,308 train 850 1.387718e-02 0.204402
2019-11-12 23:45:46,456 training loss; R2: 1.386779e-02 0.205640
2019-11-12 23:45:46,755 valid 000 1.361468e-02 0.308353
2019-11-12 23:45:48,490 valid 050 1.212636e-02 0.055552
2019-11-12 23:45:50,054 validation loss; R2: 1.190104e-02 0.140301
2019-11-12 23:45:50,078 epoch 4 lr 1.000000e-03
2019-11-12 23:45:50,484 train 000 1.364604e-02 0.114309
2019-11-12 23:45:57,733 train 050 1.343429e-02 0.225201
2019-11-12 23:46:05,028 train 100 1.328163e-02 0.229954
2019-11-12 23:46:12,313 train 150 1.333018e-02 0.209704
2019-11-12 23:46:19,609 train 200 1.326763e-02 0.186495
2019-11-12 23:46:26,889 train 250 1.323107e-02 0.197941
2019-11-12 23:46:34,166 train 300 1.322950e-02 0.204573
2019-11-12 23:46:41,445 train 350 1.323763e-02 0.211750
2019-11-12 23:46:48,730 train 400 1.327158e-02 0.217542
2019-11-12 23:46:56,011 train 450 1.325673e-02 0.224907
2019-11-12 23:47:03,294 train 500 1.322645e-02 0.227158
2019-11-12 23:47:10,574 train 550 1.320003e-02 0.225416
2019-11-12 23:47:17,866 train 600 1.321105e-02 0.226166
2019-11-12 23:47:25,151 train 650 1.320350e-02 0.228137
2019-11-12 23:47:32,438 train 700 1.317166e-02 0.232882
2019-11-12 23:47:39,726 train 750 1.317968e-02 0.234542
2019-11-12 23:47:47,027 train 800 1.315514e-02 0.236428
2019-11-12 23:47:54,314 train 850 1.314382e-02 0.237401
2019-11-12 23:47:56,495 training loss; R2: 1.313538e-02 0.238140
2019-11-12 23:47:56,801 valid 000 9.080866e-03 0.369599
2019-11-12 23:47:58,479 valid 050 1.140799e-02 0.359455
2019-11-12 23:48:00,017 validation loss; R2: 1.136165e-02 0.301806
2019-11-12 23:48:00,035 epoch 5 lr 1.000000e-03
2019-11-12 23:48:00,440 train 000 1.267968e-02 0.298883
2019-11-12 23:48:07,724 train 050 1.284879e-02 0.240523
2019-11-12 23:48:14,910 train 100 1.283127e-02 0.253466
2019-11-12 23:48:22,127 train 150 1.286538e-02 0.254626
2019-11-12 23:48:29,385 train 200 1.284554e-02 0.241248
2019-11-12 23:48:36,663 train 250 1.290625e-02 0.246110
2019-11-12 23:48:43,767 train 300 1.283794e-02 0.248924
2019-11-12 23:48:50,923 train 350 1.283071e-02 0.251171
2019-11-12 23:48:58,251 train 400 1.279323e-02 0.230579
2019-11-12 23:49:05,336 train 450 1.276377e-02 0.238078
2019-11-12 23:49:12,556 train 500 1.274390e-02 0.239156
2019-11-12 23:49:19,900 train 550 1.269372e-02 0.243066
2019-11-12 23:49:27,240 train 600 1.270068e-02 0.242755
2019-11-12 23:49:34,560 train 650 1.269473e-02 0.244838
2019-11-12 23:49:41,747 train 700 1.270208e-02 0.247263
2019-11-12 23:49:48,910 train 750 1.269259e-02 0.250236
2019-11-12 23:49:56,170 train 800 1.267087e-02 0.251111
2019-11-12 23:50:03,304 train 850 1.265031e-02 0.251209
2019-11-12 23:50:05,493 training loss; R2: 1.265779e-02 0.252222
2019-11-12 23:50:05,770 valid 000 2.613586e-02 -0.432837
2019-11-12 23:50:07,448 valid 050 2.795674e-02 -0.450344
2019-11-12 23:50:08,989 validation loss; R2: 2.775011e-02 -0.481209
2019-11-12 23:50:09,013 epoch 6 lr 1.000000e-03
2019-11-12 23:50:09,407 train 000 1.137745e-02 0.272240
2019-11-12 23:50:16,652 train 050 1.274634e-02 0.252457
2019-11-12 23:50:24,036 train 100 1.256241e-02 0.226408
2019-11-12 23:50:31,363 train 150 1.251646e-02 0.237025
2019-11-12 23:50:38,748 train 200 1.252288e-02 0.246971
2019-11-12 23:50:45,968 train 250 1.242363e-02 0.240903
2019-11-12 23:50:53,347 train 300 1.237854e-02 0.243263
2019-11-12 23:51:00,531 train 350 1.236169e-02 0.249130
2019-11-12 23:51:07,821 train 400 1.235111e-02 0.254422
2019-11-12 23:51:14,920 train 450 1.233657e-02 0.258507
2019-11-12 23:51:22,005 train 500 1.232709e-02 0.260516
2019-11-12 23:51:29,114 train 550 1.228097e-02 0.264994
2019-11-12 23:51:36,326 train 600 1.226778e-02 0.261662
2019-11-12 23:51:43,321 train 650 1.226814e-02 0.259357
2019-11-12 23:51:50,302 train 700 1.226206e-02 0.258646
2019-11-12 23:51:57,285 train 750 1.224772e-02 0.261111
2019-11-12 23:52:04,275 train 800 1.224789e-02 0.262748
2019-11-12 23:52:11,253 train 850 1.224512e-02 0.258780
2019-11-12 23:52:13,338 training loss; R2: 1.224617e-02 0.259943
2019-11-12 23:52:13,643 valid 000 7.374883e-02 -4.797991
2019-11-12 23:52:15,349 valid 050 7.687019e-02 -5.926000
2019-11-12 23:52:16,932 validation loss; R2: 7.704863e-02 -5.418389
2019-11-12 23:52:16,962 epoch 7 lr 1.000000e-03
2019-11-12 23:52:17,419 train 000 1.390762e-02 0.291987
2019-11-12 23:52:24,460 train 050 1.184373e-02 0.274322
2019-11-12 23:52:31,420 train 100 1.206030e-02 -1.722269
2019-11-12 23:52:38,409 train 150 1.198871e-02 -1.072327
2019-11-12 23:52:45,382 train 200 1.198398e-02 -0.733017
2019-11-12 23:52:52,348 train 250 1.201978e-02 -0.529703
2019-11-12 23:52:59,314 train 300 1.200472e-02 -0.398040
2019-11-12 23:53:06,295 train 350 1.200973e-02 -0.306204
2019-11-12 23:53:13,261 train 400 1.202997e-02 -0.231935
2019-11-12 23:53:20,222 train 450 1.204684e-02 -0.173561
2019-11-12 23:53:27,189 train 500 1.202605e-02 -0.125862
2019-11-12 23:53:34,162 train 550 1.199696e-02 -0.086138
2019-11-12 23:53:41,130 train 600 1.200755e-02 -0.053740
2019-11-12 23:53:48,087 train 650 1.200594e-02 -0.029799
2019-11-12 23:53:55,055 train 700 1.200194e-02 -0.007343
2019-11-12 23:54:02,025 train 750 1.200153e-02 0.012212
2019-11-12 23:54:09,001 train 800 1.199546e-02 0.028726
2019-11-12 23:54:15,975 train 850 1.197948e-02 0.039133
2019-11-12 23:54:18,057 training loss; R2: 1.198220e-02 0.044097
2019-11-12 23:54:18,361 valid 000 1.139011e+00 -54.875812
2019-11-12 23:54:20,036 valid 050 1.139296e+00 -71.671781
2019-11-12 23:54:21,567 validation loss; R2: 1.138831e+00 -66.953123
2019-11-12 23:54:21,585 epoch 8 lr 1.000000e-03
2019-11-12 23:54:22,019 train 000 1.208797e-02 0.308654
2019-11-12 23:54:29,418 train 050 1.188948e-02 0.299641
2019-11-12 23:54:36,772 train 100 1.191046e-02 0.302579
2019-11-12 23:54:43,981 train 150 1.187223e-02 0.305604
2019-11-12 23:54:51,179 train 200 1.173955e-02 0.302850
2019-11-12 23:54:58,465 train 250 1.175267e-02 0.305296
2019-11-12 23:55:05,822 train 300 1.177492e-02 0.303952
2019-11-12 23:55:13,073 train 350 1.178057e-02 0.297256
2019-11-12 23:55:20,298 train 400 1.180145e-02 0.294294
2019-11-12 23:55:27,449 train 450 1.179971e-02 0.295674
2019-11-12 23:55:34,670 train 500 1.182478e-02 0.294056
2019-11-12 23:55:41,809 train 550 1.182788e-02 0.291719
2019-11-12 23:55:48,991 train 600 1.181969e-02 0.289116
2019-11-12 23:55:56,168 train 650 1.181332e-02 0.285608
2019-11-12 23:56:03,540 train 700 1.179845e-02 0.284341
2019-11-12 23:56:10,945 train 750 1.180064e-02 0.286961
2019-11-12 23:56:18,125 train 800 1.178748e-02 0.286415
2019-11-12 23:56:25,307 train 850 1.179086e-02 0.288093
2019-11-12 23:56:27,430 training loss; R2: 1.178914e-02 0.287868
2019-11-12 23:56:27,736 valid 000 2.292515e+00 -120.226987
2019-11-12 23:56:29,407 valid 050 2.256089e+00 -197.097632
2019-11-12 23:56:30,926 validation loss; R2: 2.252499e+00 -192.284220
2019-11-12 23:56:30,943 epoch 9 lr 1.000000e-03
2019-11-12 23:56:31,354 train 000 9.429448e-03 0.411818
2019-11-12 23:56:38,593 train 050 1.166612e-02 0.285995
2019-11-12 23:56:45,841 train 100 1.169708e-02 0.298593
2019-11-12 23:56:53,041 train 150 1.168477e-02 0.300446
2019-11-12 23:57:00,360 train 200 1.165010e-02 0.302101
2019-11-12 23:57:07,586 train 250 1.166263e-02 -1.034949
2019-11-12 23:57:14,653 train 300 1.165039e-02 -0.815974
2019-11-12 23:57:22,019 train 350 1.166895e-02 -0.661129
2019-11-12 23:57:29,257 train 400 1.171324e-02 -0.542296
2019-11-12 23:57:36,523 train 450 1.169442e-02 -0.449988
2019-11-12 23:57:43,677 train 500 1.168035e-02 -0.378170
2019-11-12 23:57:50,854 train 550 1.165344e-02 -0.366397
2019-11-12 23:57:57,970 train 600 1.164930e-02 -0.310320
2019-11-12 23:58:05,103 train 650 1.162220e-02 -0.264783
2019-11-12 23:58:12,199 train 700 1.163355e-02 -0.221812
2019-11-12 23:58:19,266 train 750 1.162720e-02 -0.184585
2019-11-12 23:58:26,538 train 800 1.162871e-02 -0.154044
2019-11-12 23:58:33,911 train 850 1.163254e-02 -0.126960
2019-11-12 23:58:36,072 training loss; R2: 1.163097e-02 -0.119713
2019-11-12 23:58:36,349 valid 000 6.159666e+00 -925.292688
2019-11-12 23:58:38,034 valid 050 6.121632e+00 -821.561173
2019-11-12 23:58:39,569 validation loss; R2: 6.120304e+00 -810.791535
2019-11-12 23:58:39,593 epoch 10 lr 1.000000e-03
2019-11-12 23:58:40,024 train 000 1.386335e-02 0.316057
2019-11-12 23:58:47,455 train 050 1.126124e-02 0.312075
2019-11-12 23:58:54,614 train 100 1.140301e-02 0.314689
2019-11-12 23:59:01,910 train 150 1.140731e-02 0.314733
2019-11-12 23:59:08,932 train 200 1.140770e-02 0.317912
2019-11-12 23:59:15,933 train 250 1.147548e-02 0.295621
2019-11-12 23:59:22,926 train 300 1.145887e-02 0.294581
2019-11-12 23:59:29,938 train 350 1.144811e-02 0.296551
2019-11-12 23:59:36,937 train 400 1.140873e-02 0.296034
2019-11-12 23:59:43,926 train 450 1.140887e-02 0.297577
2019-11-12 23:59:50,930 train 500 1.140701e-02 0.295368
2019-11-12 23:59:57,931 train 550 1.144441e-02 0.191736
2019-11-13 00:00:04,928 train 600 1.142996e-02 0.203261
2019-11-13 00:00:11,928 train 650 1.140628e-02 0.191708
2019-11-13 00:00:18,925 train 700 1.142858e-02 0.201326
2019-11-13 00:00:25,918 train 750 1.144261e-02 0.204958
2019-11-13 00:00:32,918 train 800 1.143709e-02 0.211064
2019-11-13 00:00:39,918 train 850 1.143455e-02 0.213540
2019-11-13 00:00:42,012 training loss; R2: 1.143646e-02 0.212314
2019-11-13 00:00:42,328 valid 000 3.450049e+02 -26635.896297
2019-11-13 00:00:44,038 valid 050 3.451239e+02 -39290.095276
2019-11-13 00:00:45,567 validation loss; R2: 3.450768e+02 -41625.494291
2019-11-13 00:00:45,585 epoch 11 lr 1.000000e-03
2019-11-13 00:00:45,978 train 000 1.164139e-02 0.182172
2019-11-13 00:00:53,172 train 050 1.137341e-02 0.300509
2019-11-13 00:01:00,475 train 100 1.127923e-02 0.298560
2019-11-13 00:01:07,792 train 150 1.131339e-02 0.306780
2019-11-13 00:01:14,930 train 200 1.133375e-02 0.311228
2019-11-13 00:01:22,042 train 250 1.139085e-02 0.311497
2019-11-13 00:01:29,165 train 300 1.135525e-02 0.308772
2019-11-13 00:01:36,354 train 350 1.133006e-02 0.309811
2019-11-13 00:01:43,426 train 400 1.131579e-02 0.308238
2019-11-13 00:01:50,722 train 450 1.130653e-02 0.309840
2019-11-13 00:01:58,037 train 500 1.128650e-02 0.311193
2019-11-13 00:02:05,266 train 550 1.128777e-02 0.311456
2019-11-13 00:02:12,413 train 600 1.128615e-02 0.303434
2019-11-13 00:02:19,580 train 650 1.130992e-02 0.304506
2019-11-13 00:02:26,890 train 700 1.128896e-02 0.305033
2019-11-13 00:02:34,215 train 750 1.129980e-02 0.306785
2019-11-13 00:02:41,513 train 800 1.129461e-02 0.307860
2019-11-13 00:02:48,830 train 850 1.128552e-02 0.308141
2019-11-13 00:02:51,016 training loss; R2: 1.128357e-02 0.308658
2019-11-13 00:02:51,318 valid 000 7.417535e+01 -3733.499469
2019-11-13 00:02:52,987 valid 050 7.420139e+01 -5198.064226
2019-11-13 00:02:54,518 validation loss; R2: 7.419970e+01 -5015.913240
2019-11-13 00:02:54,542 epoch 12 lr 1.000000e-03
2019-11-13 00:02:54,979 train 000 1.186649e-02 0.352612
2019-11-13 00:03:02,208 train 050 1.096296e-02 0.312978
2019-11-13 00:03:09,462 train 100 1.113458e-02 0.299351
2019-11-13 00:03:16,724 train 150 1.114704e-02 0.312685
2019-11-13 00:03:23,944 train 200 1.111344e-02 0.126111
2019-11-13 00:03:31,002 train 250 1.110423e-02 0.156726
2019-11-13 00:03:38,067 train 300 1.116553e-02 0.181723
2019-11-13 00:03:45,243 train 350 1.112292e-02 0.204829
2019-11-13 00:03:52,574 train 400 1.107928e-02 0.223249
2019-11-13 00:03:59,816 train 450 1.106064e-02 0.228491
2019-11-13 00:04:07,186 train 500 1.102869e-02 0.236375
2019-11-13 00:04:14,542 train 550 1.103164e-02 0.241425
2019-11-13 00:04:21,889 train 600 1.107069e-02 0.248967
2019-11-13 00:04:29,206 train 650 1.109878e-02 0.254135
2019-11-13 00:04:36,414 train 700 1.108629e-02 0.243735
2019-11-13 00:04:43,719 train 750 1.110246e-02 0.247859
2019-11-13 00:04:50,975 train 800 1.110114e-02 0.253767
2019-11-13 00:04:58,192 train 850 1.111954e-02 0.254119
2019-11-13 00:05:00,303 training loss; R2: 1.112583e-02 0.254252
2019-11-13 00:05:00,587 valid 000 1.955712e+02 -13806.989322
2019-11-13 00:05:02,278 valid 050 1.954054e+02 -24303.591411
2019-11-13 00:05:03,813 validation loss; R2: 1.953546e+02 -24798.235798
2019-11-13 00:05:03,835 epoch 13 lr 1.000000e-03
2019-11-13 00:05:04,243 train 000 1.043266e-02 0.230864
2019-11-13 00:05:11,595 train 050 1.149984e-02 0.242616
2019-11-13 00:05:18,778 train 100 1.124913e-02 0.284137
2019-11-13 00:05:26,030 train 150 1.124423e-02 0.292657
2019-11-13 00:05:33,408 train 200 1.118931e-02 0.298014
2019-11-13 00:05:40,701 train 250 1.121247e-02 0.301581
2019-11-13 00:05:47,789 train 300 1.114155e-02 0.304901
2019-11-13 00:05:54,974 train 350 1.113110e-02 -1.415907
2019-11-13 00:06:02,057 train 400 1.112790e-02 -1.201693
2019-11-13 00:06:09,233 train 450 1.112791e-02 -1.034271
2019-11-13 00:06:16,435 train 500 1.113940e-02 -0.986575
2019-11-13 00:06:23,833 train 550 1.110645e-02 -0.864870
2019-11-13 00:06:31,207 train 600 1.109733e-02 -0.773562
2019-11-13 00:06:38,422 train 650 1.107765e-02 -0.689628
2019-11-13 00:06:45,605 train 700 1.108950e-02 -0.617737
2019-11-13 00:06:52,784 train 750 1.109304e-02 -0.554812
2019-11-13 00:07:00,003 train 800 1.109694e-02 -0.500527
2019-11-13 00:07:07,258 train 850 1.108615e-02 -0.453244
2019-11-13 00:07:09,351 training loss; R2: 1.109248e-02 -0.439806
2019-11-13 00:07:09,654 valid 000 2.321870e+02 -57095.150892
2019-11-13 00:07:11,332 valid 050 2.322063e+02 -44254.760470
2019-11-13 00:07:12,879 validation loss; R2: 2.322162e+02 -38804.450884
2019-11-13 00:07:12,906 epoch 14 lr 1.000000e-03
2019-11-13 00:07:13,354 train 000 1.058884e-02 0.095475
2019-11-13 00:07:20,422 train 050 1.085807e-02 0.335226
2019-11-13 00:07:27,480 train 100 1.107446e-02 0.326960
2019-11-13 00:07:34,543 train 150 1.118062e-02 0.317733
2019-11-13 00:07:41,573 train 200 1.125364e-02 0.313122
2019-11-13 00:07:48,664 train 250 1.129151e-02 0.299427
2019-11-13 00:07:55,812 train 300 1.121119e-02 0.305963
2019-11-13 00:08:03,164 train 350 1.117818e-02 0.308039
2019-11-13 00:08:10,515 train 400 1.116347e-02 0.302678
2019-11-13 00:08:17,861 train 450 1.114852e-02 0.305775
2019-11-13 00:08:24,961 train 500 1.113612e-02 0.308643
2019-11-13 00:08:32,042 train 550 1.112892e-02 0.307651
2019-11-13 00:08:39,425 train 600 1.114814e-02 0.309208
2019-11-13 00:08:46,733 train 650 1.115914e-02 0.310407
2019-11-13 00:08:53,798 train 700 1.114280e-02 0.308224
2019-11-13 00:09:00,952 train 750 1.116588e-02 0.305289
2019-11-13 00:09:08,306 train 800 1.115497e-02 0.306287
2019-11-13 00:09:15,651 train 850 1.116741e-02 0.307455
2019-11-13 00:09:17,843 training loss; R2: 1.117122e-02 0.307183
2019-11-13 00:09:18,128 valid 000 5.003381e+02 -133457.546621
2019-11-13 00:09:19,837 valid 050 5.008412e+02 -122875.369166
2019-11-13 00:09:21,360 validation loss; R2: 5.008583e+02 -130837.651233
2019-11-13 00:09:21,383 epoch 15 lr 1.000000e-03
2019-11-13 00:09:21,812 train 000 1.210451e-02 0.352085
2019-11-13 00:09:28,905 train 050 1.105544e-02 0.335032
2019-11-13 00:09:35,958 train 100 1.122493e-02 0.321492
2019-11-13 00:09:43,168 train 150 1.111985e-02 0.312366
2019-11-13 00:09:50,287 train 200 1.114543e-02 0.312352
2019-11-13 00:09:57,545 train 250 1.115590e-02 0.315286
2019-11-13 00:10:04,679 train 300 1.113925e-02 0.317813
2019-11-13 00:10:12,046 train 350 1.115230e-02 0.316117
2019-11-13 00:10:19,411 train 400 1.111308e-02 0.319402
2019-11-13 00:10:26,689 train 450 1.109336e-02 -1.484636
2019-11-13 00:10:33,749 train 500 1.109203e-02 -1.303114
2019-11-13 00:10:40,784 train 550 1.110746e-02 -1.158078
2019-11-13 00:10:47,949 train 600 1.111253e-02 -1.037305
2019-11-13 00:10:55,337 train 650 1.111663e-02 -0.931590
2019-11-13 00:11:02,764 train 700 1.112457e-02 -0.840837
2019-11-13 00:11:10,138 train 750 1.111577e-02 -0.761306
2019-11-13 00:11:17,515 train 800 1.110807e-02 -0.696390
2019-11-13 00:11:24,892 train 850 1.112419e-02 -0.643674
2019-11-13 00:11:27,088 training loss; R2: 1.114515e-02 -0.627124
2019-11-13 00:11:27,370 valid 000 1.631603e+01 -1497.166160
2019-11-13 00:11:29,064 valid 050 1.632495e+01 -2431.908454
2019-11-13 00:11:30,593 validation loss; R2: 1.632646e+01 -2345.255273
2019-11-13 00:11:30,616 epoch 16 lr 1.000000e-03
2019-11-13 00:11:31,051 train 000 1.106395e-02 0.330003
2019-11-13 00:11:38,190 train 050 1.147356e-02 0.289883
2019-11-13 00:11:45,544 train 100 1.105722e-02 0.313875
2019-11-13 00:11:52,709 train 150 1.118670e-02 0.316512
2019-11-13 00:12:00,040 train 200 1.120222e-02 0.324875
2019-11-13 00:12:07,325 train 250 1.116881e-02 0.309065
2019-11-13 00:12:14,506 train 300 1.108771e-02 0.303749
2019-11-13 00:12:21,837 train 350 1.109454e-02 0.305656
2019-11-13 00:12:28,953 train 400 1.111908e-02 0.310328
2019-11-13 00:12:36,135 train 450 1.111645e-02 0.312037
2019-11-13 00:12:43,405 train 500 1.111820e-02 0.307596
2019-11-13 00:12:50,652 train 550 1.107813e-02 0.306885
2019-11-13 00:12:57,753 train 600 1.109533e-02 0.309568
2019-11-13 00:13:04,775 train 650 1.115700e-02 0.308126
2019-11-13 00:13:11,796 train 700 1.117418e-02 0.306214
2019-11-13 00:13:18,807 train 750 1.116742e-02 0.308803
2019-11-13 00:13:25,883 train 800 1.117670e-02 0.307047
2019-11-13 00:13:32,888 train 850 1.118244e-02 0.307810
2019-11-13 00:13:34,983 training loss; R2: 1.117296e-02 0.305666
2019-11-13 00:13:35,296 valid 000 8.674234e+02 -79115.564554
2019-11-13 00:13:36,979 valid 050 8.670435e+02 -121904.085278
2019-11-13 00:13:38,508 validation loss; R2: 8.670135e+02 -115892.824180
2019-11-13 00:13:38,526 epoch 17 lr 1.000000e-03
2019-11-13 00:13:38,936 train 000 1.211580e-02 0.376070
2019-11-13 00:13:46,198 train 050 1.081369e-02 0.342072
2019-11-13 00:13:53,349 train 100 1.111498e-02 0.338998
2019-11-13 00:14:00,444 train 150 1.101638e-02 0.341561
2019-11-13 00:14:07,533 train 200 1.100655e-02 0.326489
2019-11-13 00:14:14,576 train 250 1.101000e-02 0.325485
2019-11-13 00:14:21,605 train 300 1.103781e-02 0.325444
2019-11-13 00:14:28,635 train 350 1.102084e-02 0.323197
2019-11-13 00:14:35,659 train 400 1.113467e-02 0.319119
2019-11-13 00:14:42,681 train 450 1.112427e-02 0.318859
2019-11-13 00:14:49,774 train 500 1.110470e-02 0.318970
2019-11-13 00:14:56,881 train 550 1.109446e-02 0.317853
2019-11-13 00:15:03,932 train 600 1.109966e-02 0.316945
2019-11-13 00:15:11,012 train 650 1.110771e-02 0.316464
2019-11-13 00:15:18,049 train 700 1.111846e-02 0.317904
2019-11-13 00:15:25,084 train 750 1.110999e-02 0.317549
2019-11-13 00:15:32,179 train 800 1.110686e-02 0.319651
2019-11-13 00:15:39,282 train 850 1.110505e-02 0.319545
2019-11-13 00:15:41,385 training loss; R2: 1.110176e-02 0.319733
2019-11-13 00:15:41,693 valid 000 2.641484e+03 -274466.868837
2019-11-13 00:15:43,451 valid 050 2.641261e+03 -323515.446150
2019-11-13 00:15:45,044 validation loss; R2: 2.641292e+03 -358509.613319
2019-11-13 00:15:45,068 epoch 18 lr 1.000000e-03
2019-11-13 00:15:45,498 train 000 9.011330e-03 0.347063
2019-11-13 00:15:52,771 train 050 1.083215e-02 0.245362
2019-11-13 00:15:59,998 train 100 1.083970e-02 0.294784
2019-11-13 00:16:07,339 train 150 1.110845e-02 0.306465
2019-11-13 00:16:14,619 train 200 1.116873e-02 0.306299
2019-11-13 00:16:21,767 train 250 1.111643e-02 0.311864
2019-11-13 00:16:29,134 train 300 1.110160e-02 0.313974
2019-11-13 00:16:36,364 train 350 1.111868e-02 0.311652
2019-11-13 00:16:43,631 train 400 1.110088e-02 0.313026
2019-11-13 00:16:50,844 train 450 1.108329e-02 0.313340
2019-11-13 00:16:58,104 train 500 1.106483e-02 0.313994
2019-11-13 00:17:05,256 train 550 1.105468e-02 0.309867
2019-11-13 00:17:12,344 train 600 1.103563e-02 0.312685
2019-11-13 00:17:19,600 train 650 1.102400e-02 0.313191
2019-11-13 00:17:26,807 train 700 1.104324e-02 0.314585
2019-11-13 00:17:34,016 train 750 1.104165e-02 0.314713
2019-11-13 00:17:41,278 train 800 1.103522e-02 0.316452
2019-11-13 00:17:48,394 train 850 1.102939e-02 0.317162
2019-11-13 00:17:50,573 training loss; R2: 1.103135e-02 0.315729
2019-11-13 00:17:50,903 valid 000 2.281034e+02 -81314.098916
2019-11-13 00:17:52,547 valid 050 2.279264e+02 -29227.463505
2019-11-13 00:17:54,099 validation loss; R2: 2.279282e+02 -28240.234910
2019-11-13 00:17:54,122 epoch 19 lr 1.000000e-03
2019-11-13 00:17:54,574 train 000 1.066778e-02 0.365752
2019-11-13 00:18:01,674 train 050 1.112400e-02 0.225221
2019-11-13 00:18:08,722 train 100 1.108129e-02 0.278229
2019-11-13 00:18:15,778 train 150 1.105727e-02 0.296396
2019-11-13 00:18:22,902 train 200 1.101040e-02 0.313248
2019-11-13 00:18:30,009 train 250 1.101072e-02 0.304765
2019-11-13 00:18:37,050 train 300 1.132217e-02 0.293568
2019-11-13 00:18:44,157 train 350 1.146189e-02 0.286786
2019-11-13 00:18:51,204 train 400 1.148593e-02 0.288960
2019-11-13 00:18:58,235 train 450 1.147142e-02 0.295184
2019-11-13 00:19:05,278 train 500 1.144430e-02 0.292870
2019-11-13 00:19:12,380 train 550 1.146130e-02 0.295774
2019-11-13 00:19:19,437 train 600 1.145599e-02 0.285137
2019-11-13 00:19:26,474 train 650 1.144158e-02 0.287673
2019-11-13 00:19:33,506 train 700 1.142887e-02 0.020663
2019-11-13 00:19:40,563 train 750 1.141725e-02 0.041178
2019-11-13 00:19:47,619 train 800 1.138993e-02 0.058411
2019-11-13 00:19:54,660 train 850 1.135725e-02 0.074835
2019-11-13 00:19:56,765 training loss; R2: 1.135271e-02 0.079832
2019-11-13 00:19:57,073 valid 000 2.390111e+00 -195.481412
2019-11-13 00:19:58,799 valid 050 2.395808e+00 -189.201157
2019-11-13 00:20:00,360 validation loss; R2: 2.397525e+00 -189.987508
