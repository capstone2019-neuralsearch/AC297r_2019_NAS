2019-11-13 16:53:07,449 gpu device = 1
2019-11-13 16:53:07,449 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name='galaxy-zoo-cleaned', gpu=1, grad_clip=5, gz_dtree=False, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-galaxy-zoo-EXP-20191113-165307', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-11-13 16:53:19,172 param size = 0.231989MB
2019-11-13 16:53:19,177 epoch 0 lr 1.000000e-03
2019-11-13 16:53:21,403 train 000 8.268057e-01 -157.242397
2019-11-13 16:53:27,473 train 050 6.459802e-02 -20.396194
2019-11-13 16:53:33,395 train 100 4.690316e-02 -10.691524
2019-11-13 16:53:39,308 train 150 4.025402e-02 -7.366493
2019-11-13 16:53:45,217 train 200 3.681803e-02 -5.665372
2019-11-13 16:53:51,138 train 250 3.468467e-02 -4.615200
2019-11-13 16:53:57,086 train 300 3.315827e-02 -4.833835
2019-11-13 16:54:03,045 train 350 3.199790e-02 -4.193100
2019-11-13 16:54:09,014 train 400 3.100907e-02 -3.698571
2019-11-13 16:54:14,964 train 450 3.022337e-02 -3.324511
2019-11-13 16:54:20,928 train 500 2.964060e-02 -3.017394
2019-11-13 16:54:26,882 train 550 2.909179e-02 -2.759303
2019-11-13 16:54:32,836 train 600 2.865447e-02 -2.542064
2019-11-13 16:54:38,791 train 650 2.817639e-02 -2.355244
2019-11-13 16:54:44,753 train 700 2.775398e-02 -2.194939
2019-11-13 16:54:50,763 train 750 2.740221e-02 -2.057230
2019-11-13 16:54:56,653 train 800 2.706695e-02 -1.935080
2019-11-13 16:55:02,598 train 850 2.673122e-02 -1.827572
2019-11-13 16:55:05,106 training loss; R2: 2.663682e-02 -1.796136
2019-11-13 16:55:05,442 valid 000 2.301367e-02 -0.014180
2019-11-13 16:55:07,215 valid 050 2.067827e-02 -0.025572
2019-11-13 16:55:08,881 validation loss; R2: 2.036063e-02 -0.025148
2019-11-13 16:55:08,896 epoch 1 lr 1.000000e-03
2019-11-13 16:55:09,444 train 000 2.265205e-02 -0.017274
2019-11-13 16:55:15,411 train 050 2.089805e-02 -0.031434
2019-11-13 16:55:21,463 train 100 2.091517e-02 -0.035958
2019-11-13 16:55:27,508 train 150 2.085937e-02 -0.044642
2019-11-13 16:55:33,365 train 200 2.067991e-02 -0.040353
2019-11-13 16:55:39,246 train 250 2.066502e-02 -0.078707
2019-11-13 16:55:45,166 train 300 2.043808e-02 -0.062965
2019-11-13 16:55:51,061 train 350 2.034819e-02 -0.053428
2019-11-13 16:55:57,024 train 400 2.020888e-02 -0.042179
2019-11-13 16:56:02,915 train 450 2.011186e-02 -0.033347
2019-11-13 16:56:08,883 train 500 2.007574e-02 -0.028137
2019-11-13 16:56:14,854 train 550 1.999394e-02 -0.022333
2019-11-13 16:56:21,042 train 600 1.990507e-02 -0.015976
2019-11-13 16:56:27,016 train 650 1.980861e-02 -0.010151
2019-11-13 16:56:32,978 train 700 1.969943e-02 -0.012248
2019-11-13 16:56:39,181 train 750 1.960403e-02 -0.006796
2019-11-13 16:56:45,220 train 800 1.954403e-02 -0.003155
2019-11-13 16:56:51,237 train 850 1.946800e-02 -0.001855
2019-11-13 16:56:53,014 training loss; R2: 1.946153e-02 -0.000773
2019-11-13 16:56:53,399 valid 000 1.961788e-02 -0.030116
2019-11-13 16:56:55,133 valid 050 1.667090e-02 0.189605
2019-11-13 16:56:56,807 validation loss; R2: 1.686876e-02 0.184325
2019-11-13 16:56:56,826 epoch 2 lr 1.000000e-03
2019-11-13 16:56:57,294 train 000 1.853885e-02 0.075666
2019-11-13 16:57:03,268 train 050 1.777822e-02 0.099674
2019-11-13 16:57:09,250 train 100 1.792040e-02 0.086522
2019-11-13 16:57:15,173 train 150 1.777640e-02 0.093173
2019-11-13 16:57:21,058 train 200 1.772121e-02 0.089212
2019-11-13 16:57:26,952 train 250 1.767692e-02 0.082009
2019-11-13 16:57:32,843 train 300 1.766548e-02 0.088200
2019-11-13 16:57:38,744 train 350 1.762356e-02 0.084814
2019-11-13 16:57:44,706 train 400 1.760627e-02 0.078132
2019-11-13 16:57:50,635 train 450 1.755415e-02 0.080973
2019-11-13 16:57:56,514 train 500 1.747659e-02 0.083321
2019-11-13 16:58:02,410 train 550 1.741485e-02 0.088184
2019-11-13 16:58:08,359 train 600 1.733374e-02 0.088917
2019-11-13 16:58:14,322 train 650 1.729440e-02 0.089731
2019-11-13 16:58:20,239 train 700 1.724680e-02 0.091978
2019-11-13 16:58:26,134 train 750 1.719477e-02 0.096013
2019-11-13 16:58:32,126 train 800 1.715187e-02 0.097977
2019-11-13 16:58:38,068 train 850 1.710185e-02 0.100208
2019-11-13 16:58:39,847 training loss; R2: 1.707303e-02 0.101503
2019-11-13 16:58:40,236 valid 000 1.285226e-02 0.279804
2019-11-13 16:58:41,999 valid 050 1.454655e-02 0.175942
2019-11-13 16:58:43,660 validation loss; R2: 1.465610e-02 0.187312
2019-11-13 16:58:43,674 epoch 3 lr 1.000000e-03
2019-11-13 16:58:44,116 train 000 1.437009e-02 -0.045368
2019-11-13 16:58:50,100 train 050 1.608853e-02 0.133884
2019-11-13 16:58:56,040 train 100 1.603850e-02 0.140303
2019-11-13 16:59:01,949 train 150 1.591383e-02 0.143393
2019-11-13 16:59:07,839 train 200 1.587414e-02 0.145980
2019-11-13 16:59:13,733 train 250 1.584688e-02 0.149568
2019-11-13 16:59:19,622 train 300 1.585667e-02 0.133750
2019-11-13 16:59:25,505 train 350 1.586979e-02 0.132941
2019-11-13 16:59:31,383 train 400 1.589400e-02 0.136478
2019-11-13 16:59:37,341 train 450 1.586132e-02 0.138930
2019-11-13 16:59:43,337 train 500 1.586889e-02 0.143540
2019-11-13 16:59:49,334 train 550 1.583784e-02 0.143329
2019-11-13 16:59:55,319 train 600 1.578914e-02 0.132602
2019-11-13 17:00:01,334 train 650 1.574872e-02 0.134809
2019-11-13 17:00:07,286 train 700 1.570720e-02 0.136961
2019-11-13 17:00:13,219 train 750 1.566647e-02 0.140251
2019-11-13 17:00:19,106 train 800 1.564450e-02 0.120650
2019-11-13 17:00:24,986 train 850 1.560014e-02 0.124707
2019-11-13 17:00:26,749 training loss; R2: 1.558241e-02 0.126030
2019-11-13 17:00:27,134 valid 000 1.304238e-02 0.203788
2019-11-13 17:00:28,993 valid 050 1.345238e-02 0.307708
2019-11-13 17:00:30,642 validation loss; R2: 1.353879e-02 0.299719
2019-11-13 17:00:30,655 epoch 4 lr 1.000000e-03
2019-11-13 17:00:31,108 train 000 1.385839e-02 0.271959
2019-11-13 17:00:36,988 train 050 1.524208e-02 0.195155
2019-11-13 17:00:42,870 train 100 1.480518e-02 0.204355
2019-11-13 17:00:48,858 train 150 1.492235e-02 0.054564
2019-11-13 17:00:54,862 train 200 1.486540e-02 0.090004
2019-11-13 17:01:00,831 train 250 1.487883e-02 0.115445
2019-11-13 17:01:06,747 train 300 1.481520e-02 -0.885418
2019-11-13 17:01:12,620 train 350 1.473107e-02 -0.727248
2019-11-13 17:01:18,530 train 400 1.473550e-02 -0.614943
2019-11-13 17:01:24,406 train 450 1.471781e-02 -0.525005
2019-11-13 17:01:30,286 train 500 1.473509e-02 -0.451484
2019-11-13 17:01:36,172 train 550 1.470345e-02 -0.391734
2019-11-13 17:01:42,036 train 600 1.466711e-02 -0.339934
2019-11-13 17:01:47,904 train 650 1.462453e-02 -0.297004
2019-11-13 17:01:53,783 train 700 1.457885e-02 -0.261619
2019-11-13 17:01:59,647 train 750 1.455505e-02 -0.232522
2019-11-13 17:02:05,520 train 800 1.453049e-02 -0.203976
2019-11-13 17:02:11,386 train 850 1.449521e-02 -0.181284
2019-11-13 17:02:13,143 training loss; R2: 1.448778e-02 -0.174014
2019-11-13 17:02:13,549 valid 000 1.441326e-02 0.322278
2019-11-13 17:02:15,309 valid 050 1.315036e-02 0.294584
2019-11-13 17:02:16,941 validation loss; R2: 1.301445e-02 0.233996
2019-11-13 17:02:16,954 epoch 5 lr 1.000000e-03
2019-11-13 17:02:17,379 train 000 1.384514e-02 0.317547
2019-11-13 17:02:23,008 train 050 1.409476e-02 0.178683
2019-11-13 17:02:28,602 train 100 1.399040e-02 0.200125
2019-11-13 17:02:34,197 train 150 1.393007e-02 0.221089
2019-11-13 17:02:39,802 train 200 1.395577e-02 0.218075
2019-11-13 17:02:45,390 train 250 1.389857e-02 0.218721
2019-11-13 17:02:50,983 train 300 1.387171e-02 0.212225
2019-11-13 17:02:56,570 train 350 1.393500e-02 0.215247
2019-11-13 17:03:02,171 train 400 1.392883e-02 0.214614
2019-11-13 17:03:07,765 train 450 1.391286e-02 0.213323
2019-11-13 17:03:13,361 train 500 1.392267e-02 0.185068
2019-11-13 17:03:18,951 train 550 1.394552e-02 0.188371
2019-11-13 17:03:24,536 train 600 1.392982e-02 0.188364
2019-11-13 17:03:30,165 train 650 1.391879e-02 0.191830
2019-11-13 17:03:36,023 train 700 1.388141e-02 0.194810
2019-11-13 17:03:41,886 train 750 1.385829e-02 0.198897
2019-11-13 17:03:47,746 train 800 1.385600e-02 0.199524
2019-11-13 17:03:53,612 train 850 1.384738e-02 0.201955
2019-11-13 17:03:55,367 training loss; R2: 1.383200e-02 0.202704
2019-11-13 17:03:55,741 valid 000 2.446184e-02 -0.538142
2019-11-13 17:03:57,540 valid 050 2.411892e-02 -0.645617
2019-11-13 17:03:59,195 validation loss; R2: 2.424127e-02 -0.638120
2019-11-13 17:03:59,209 epoch 6 lr 1.000000e-03
2019-11-13 17:03:59,639 train 000 1.535064e-02 0.242956
2019-11-13 17:04:05,588 train 050 1.342435e-02 0.256246
2019-11-13 17:04:11,599 train 100 1.329248e-02 0.246015
2019-11-13 17:04:17,540 train 150 1.339100e-02 0.252420
2019-11-13 17:04:23,480 train 200 1.340403e-02 0.231764
2019-11-13 17:04:29,415 train 250 1.339135e-02 0.234620
2019-11-13 17:04:35,345 train 300 1.335864e-02 0.191057
2019-11-13 17:04:41,283 train 350 1.339509e-02 0.201067
2019-11-13 17:04:47,153 train 400 1.339617e-02 0.208574
2019-11-13 17:04:53,022 train 450 1.336939e-02 0.211109
2019-11-13 17:04:58,898 train 500 1.335464e-02 0.216690
2019-11-13 17:05:04,762 train 550 1.333715e-02 0.211290
2019-11-13 17:05:10,630 train 600 1.331488e-02 0.214269
2019-11-13 17:05:16,509 train 650 1.332968e-02 0.215551
2019-11-13 17:05:22,380 train 700 1.331667e-02 0.213111
2019-11-13 17:05:28,246 train 750 1.328122e-02 0.212744
2019-11-13 17:05:34,112 train 800 1.326038e-02 0.214366
2019-11-13 17:05:39,981 train 850 1.323784e-02 0.213966
2019-11-13 17:05:41,736 training loss; R2: 1.324754e-02 0.214250
2019-11-13 17:05:42,134 valid 000 4.672986e-02 -2.631921
2019-11-13 17:05:43,924 valid 050 4.552586e-02 -2.395434
2019-11-13 17:05:45,518 validation loss; R2: 4.548975e-02 -2.563449
2019-11-13 17:05:45,531 epoch 7 lr 1.000000e-03
2019-11-13 17:05:45,969 train 000 1.552839e-02 0.291668
2019-11-13 17:05:51,888 train 050 1.281515e-02 0.251842
2019-11-13 17:05:57,787 train 100 1.274140e-02 0.211648
2019-11-13 17:06:03,661 train 150 1.284982e-02 0.223576
2019-11-13 17:06:09,556 train 200 1.287130e-02 0.223753
2019-11-13 17:06:15,421 train 250 1.289090e-02 0.233047
2019-11-13 17:06:21,289 train 300 1.288091e-02 0.240791
2019-11-13 17:06:27,168 train 350 1.286197e-02 0.241389
2019-11-13 17:06:33,031 train 400 1.285724e-02 0.246866
2019-11-13 17:06:38,900 train 450 1.286085e-02 0.244470
2019-11-13 17:06:44,764 train 500 1.283289e-02 0.224736
2019-11-13 17:06:50,658 train 550 1.281275e-02 0.200629
2019-11-13 17:06:56,526 train 600 1.282316e-02 0.208476
2019-11-13 17:07:02,391 train 650 1.281718e-02 0.211492
2019-11-13 17:07:08,256 train 700 1.280999e-02 0.211772
2019-11-13 17:07:14,121 train 750 1.282207e-02 0.216148
2019-11-13 17:07:19,998 train 800 1.279638e-02 0.172327
2019-11-13 17:07:25,865 train 850 1.279297e-02 0.171744
2019-11-13 17:07:27,644 training loss; R2: 1.278991e-02 0.173205
2019-11-13 17:07:28,033 valid 000 9.859442e-01 -58.074698
2019-11-13 17:07:29,798 valid 050 9.835481e-01 -112.456231
2019-11-13 17:07:31,427 validation loss; R2: 9.824099e-01 -119.798956
2019-11-13 17:07:31,444 epoch 8 lr 1.000000e-03
2019-11-13 17:07:31,943 train 000 1.112021e-02 0.327065
2019-11-13 17:07:37,988 train 050 1.259411e-02 0.275138
2019-11-13 17:07:43,924 train 100 1.260073e-02 0.262627
2019-11-13 17:07:49,804 train 150 1.255599e-02 0.273733
2019-11-13 17:07:55,710 train 200 1.259220e-02 0.260261
2019-11-13 17:08:01,590 train 250 1.252386e-02 0.243750
2019-11-13 17:08:07,480 train 300 1.251463e-02 0.247449
2019-11-13 17:08:13,353 train 350 1.246430e-02 0.254003
2019-11-13 17:08:19,231 train 400 1.248174e-02 0.255675
2019-11-13 17:08:25,106 train 450 1.247084e-02 0.259791
2019-11-13 17:08:30,976 train 500 1.242341e-02 0.261795
2019-11-13 17:08:36,843 train 550 1.240595e-02 0.259440
2019-11-13 17:08:42,729 train 600 1.239920e-02 0.262567
2019-11-13 17:08:48,594 train 650 1.239503e-02 0.264916
2019-11-13 17:08:54,499 train 700 1.240876e-02 0.265189
2019-11-13 17:09:00,363 train 750 1.240035e-02 0.262461
2019-11-13 17:09:06,228 train 800 1.239773e-02 0.263526
2019-11-13 17:09:12,094 train 850 1.239520e-02 0.264510
2019-11-13 17:09:13,852 training loss; R2: 1.240145e-02 0.261936
2019-11-13 17:09:14,215 valid 000 2.107462e-01 -18.962265
2019-11-13 17:09:16,010 valid 050 2.153777e-01 -19.775907
2019-11-13 17:09:17,579 validation loss; R2: 2.158228e-01 -19.638195
2019-11-13 17:09:17,599 epoch 9 lr 1.000000e-03
2019-11-13 17:09:18,048 train 000 1.088245e-02 0.364630
2019-11-13 17:09:23,941 train 050 1.272692e-02 0.248101
2019-11-13 17:09:29,934 train 100 1.263377e-02 0.245815
2019-11-13 17:09:35,894 train 150 1.247357e-02 0.266896
2019-11-13 17:09:41,792 train 200 1.246028e-02 0.269534
2019-11-13 17:09:47,671 train 250 1.242821e-02 0.275839
2019-11-13 17:09:53,577 train 300 1.238707e-02 0.272824
2019-11-13 17:09:59,442 train 350 1.238955e-02 0.270560
2019-11-13 17:10:05,324 train 400 1.233678e-02 0.272532
2019-11-13 17:10:11,192 train 450 1.231144e-02 0.275011
2019-11-13 17:10:17,058 train 500 1.232137e-02 0.276122
2019-11-13 17:10:22,927 train 550 1.231139e-02 0.278067
2019-11-13 17:10:28,807 train 600 1.229388e-02 0.276346
2019-11-13 17:10:34,665 train 650 1.225677e-02 0.277223
2019-11-13 17:10:40,521 train 700 1.222531e-02 0.279244
2019-11-13 17:10:46,379 train 750 1.221356e-02 0.276098
2019-11-13 17:10:52,238 train 800 1.218821e-02 0.276870
2019-11-13 17:10:58,101 train 850 1.217762e-02 0.276598
2019-11-13 17:10:59,856 training loss; R2: 1.217358e-02 0.277306
2019-11-13 17:11:00,254 valid 000 2.382505e+00 -109.284603
2019-11-13 17:11:02,016 valid 050 2.393459e+00 -170.998271
2019-11-13 17:11:03,616 validation loss; R2: 2.395632e+00 -177.696778
2019-11-13 17:11:03,639 epoch 10 lr 1.000000e-03
2019-11-13 17:11:04,143 train 000 9.883414e-03 -0.139485
2019-11-13 17:11:09,965 train 050 1.212551e-02 0.245208
2019-11-13 17:11:15,852 train 100 1.202316e-02 0.268382
2019-11-13 17:11:21,756 train 150 1.204984e-02 0.247654
2019-11-13 17:11:27,652 train 200 1.196760e-02 0.245478
2019-11-13 17:11:33,521 train 250 1.195897e-02 0.253358
2019-11-13 17:11:39,397 train 300 1.194376e-02 0.254571
2019-11-13 17:11:45,295 train 350 1.197203e-02 0.260649
2019-11-13 17:11:51,167 train 400 1.194718e-02 0.265198
2019-11-13 17:11:57,035 train 450 1.191187e-02 0.262605
2019-11-13 17:12:02,901 train 500 1.192893e-02 0.260517
2019-11-13 17:12:08,786 train 550 1.192085e-02 0.263537
2019-11-13 17:12:14,650 train 600 1.189901e-02 0.266589
2019-11-13 17:12:20,509 train 650 1.188190e-02 0.255378
2019-11-13 17:12:26,380 train 700 1.187066e-02 0.258598
2019-11-13 17:12:32,268 train 750 1.183150e-02 0.260817
2019-11-13 17:12:38,134 train 800 1.182926e-02 0.258823
2019-11-13 17:12:44,005 train 850 1.183075e-02 0.259736
2019-11-13 17:12:45,782 training loss; R2: 1.183806e-02 0.260144
2019-11-13 17:12:46,174 valid 000 1.706146e-01 -21.035704
2019-11-13 17:12:47,908 valid 050 1.777389e-01 -28.873479
2019-11-13 17:12:49,486 validation loss; R2: 1.779344e-01 -36.552241
2019-11-13 17:12:49,500 epoch 11 lr 1.000000e-03
2019-11-13 17:12:49,964 train 000 1.062899e-02 0.160089
2019-11-13 17:12:55,971 train 050 1.165239e-02 0.280087
2019-11-13 17:13:01,907 train 100 1.173157e-02 0.271202
2019-11-13 17:13:07,783 train 150 1.165305e-02 0.265908
2019-11-13 17:13:13,652 train 200 1.168770e-02 0.273453
2019-11-13 17:13:19,529 train 250 1.167675e-02 0.278606
2019-11-13 17:13:25,402 train 300 1.164068e-02 0.268916
2019-11-13 17:13:31,269 train 350 1.169698e-02 0.276740
2019-11-13 17:13:37,152 train 400 1.170339e-02 0.278240
2019-11-13 17:13:43,017 train 450 1.171238e-02 0.277682
2019-11-13 17:13:48,885 train 500 1.170562e-02 0.275842
2019-11-13 17:13:54,760 train 550 1.173101e-02 0.278347
2019-11-13 17:14:00,641 train 600 1.172326e-02 0.280715
2019-11-13 17:14:06,505 train 650 1.172670e-02 0.283992
2019-11-13 17:14:12,365 train 700 1.172166e-02 0.278110
2019-11-13 17:14:18,240 train 750 1.170705e-02 0.275109
2019-11-13 17:14:24,106 train 800 1.170843e-02 0.274283
2019-11-13 17:14:29,969 train 850 1.169833e-02 0.275123
2019-11-13 17:14:31,722 training loss; R2: 1.170328e-02 0.274192
2019-11-13 17:14:32,104 valid 000 7.267005e-01 -43.623669
2019-11-13 17:14:33,848 valid 050 7.025644e-01 -47.134440
2019-11-13 17:14:35,459 validation loss; R2: 7.035589e-01 -45.794139
2019-11-13 17:14:35,473 epoch 12 lr 1.000000e-03
2019-11-13 17:14:35,942 train 000 1.297896e-02 0.380041
2019-11-13 17:14:41,870 train 050 1.171133e-02 0.242375
2019-11-13 17:14:47,863 train 100 1.152074e-02 0.272113
2019-11-13 17:14:53,805 train 150 1.158834e-02 0.282707
2019-11-13 17:14:59,759 train 200 1.163311e-02 0.288070
2019-11-13 17:15:05,706 train 250 1.158026e-02 0.283683
2019-11-13 17:15:11,581 train 300 1.161886e-02 0.286065
2019-11-13 17:15:17,481 train 350 1.159605e-02 0.287235
2019-11-13 17:15:23,356 train 400 1.163325e-02 0.285567
2019-11-13 17:15:29,231 train 450 1.161864e-02 0.290447
2019-11-13 17:15:35,126 train 500 1.161209e-02 0.290116
2019-11-13 17:15:40,991 train 550 1.159967e-02 0.292860
2019-11-13 17:15:46,854 train 600 1.159531e-02 0.287755
2019-11-13 17:15:52,720 train 650 1.157660e-02 0.289880
2019-11-13 17:15:58,600 train 700 1.157445e-02 0.289266
2019-11-13 17:16:04,463 train 750 1.156069e-02 0.287523
2019-11-13 17:16:10,326 train 800 1.153743e-02 0.282756
2019-11-13 17:16:16,184 train 850 1.154673e-02 0.280278
2019-11-13 17:16:17,941 training loss; R2: 1.155221e-02 0.277643
2019-11-13 17:16:18,332 valid 000 2.892800e-01 -12.346929
2019-11-13 17:16:20,135 valid 050 2.847025e-01 -15.250591
2019-11-13 17:16:21,711 validation loss; R2: 2.836809e-01 -17.966131
2019-11-13 17:16:21,731 epoch 13 lr 1.000000e-03
2019-11-13 17:16:22,204 train 000 9.996364e-03 0.352257
2019-11-13 17:16:28,150 train 050 1.156975e-02 0.337293
2019-11-13 17:16:34,036 train 100 1.146668e-02 0.314615
2019-11-13 17:16:39,921 train 150 1.151316e-02 0.315124
2019-11-13 17:16:45,784 train 200 1.155654e-02 0.292118
2019-11-13 17:16:51,684 train 250 1.147708e-02 0.294172
2019-11-13 17:16:57,615 train 300 1.145813e-02 0.292213
2019-11-13 17:17:03,501 train 350 1.148097e-02 0.295451
2019-11-13 17:17:09,378 train 400 1.145713e-02 0.299161
2019-11-13 17:17:15,251 train 450 1.143803e-02 0.301811
2019-11-13 17:17:21,116 train 500 1.144175e-02 0.303016
2019-11-13 17:17:26,996 train 550 1.143296e-02 0.304548
2019-11-13 17:17:32,863 train 600 1.142189e-02 0.306151
2019-11-13 17:17:38,731 train 650 1.145764e-02 0.303995
2019-11-13 17:17:44,600 train 700 1.144287e-02 0.305363
2019-11-13 17:17:50,466 train 750 1.143357e-02 0.305513
2019-11-13 17:17:56,332 train 800 1.143860e-02 0.306433
2019-11-13 17:18:02,196 train 850 1.142882e-02 0.307206
2019-11-13 17:18:03,954 training loss; R2: 1.143306e-02 0.306923
2019-11-13 17:18:04,324 valid 000 1.984069e-01 -19.950820
2019-11-13 17:18:06,072 valid 050 1.914873e-01 -21.722328
2019-11-13 17:18:07,651 validation loss; R2: 1.913410e-01 -22.264276
2019-11-13 17:18:07,664 epoch 14 lr 1.000000e-03
2019-11-13 17:18:08,076 train 000 1.160011e-02 0.210310
2019-11-13 17:18:14,050 train 050 1.075188e-02 0.314672
2019-11-13 17:18:20,022 train 100 1.111586e-02 0.274168
2019-11-13 17:18:25,951 train 150 1.120068e-02 0.286965
2019-11-13 17:18:31,908 train 200 1.127886e-02 0.288768
2019-11-13 17:18:37,849 train 250 1.131218e-02 0.282757
2019-11-13 17:18:43,797 train 300 1.132798e-02 0.288165
2019-11-13 17:18:49,732 train 350 1.131948e-02 0.294360
2019-11-13 17:18:55,602 train 400 1.130453e-02 0.298213
2019-11-13 17:19:01,456 train 450 1.131336e-02 0.292398
2019-11-13 17:19:07,329 train 500 1.133681e-02 0.293227
2019-11-13 17:19:13,189 train 550 1.130219e-02 0.291928
2019-11-13 17:19:19,065 train 600 1.128829e-02 0.290795
2019-11-13 17:19:24,934 train 650 1.127782e-02 0.292869
2019-11-13 17:19:30,791 train 700 1.127025e-02 -1.390813
2019-11-13 17:19:36,661 train 750 1.126734e-02 -1.281428
2019-11-13 17:19:42,560 train 800 1.127178e-02 -1.181592
2019-11-13 17:19:48,427 train 850 1.125802e-02 -1.093208
2019-11-13 17:19:50,181 training loss; R2: 1.124776e-02 -1.069854
2019-11-13 17:19:50,585 valid 000 2.108988e-01 -35.295259
2019-11-13 17:19:52,336 valid 050 2.205991e-01 -20.064028
2019-11-13 17:19:53,917 validation loss; R2: 2.199470e-01 -19.268443
2019-11-13 17:19:53,930 epoch 15 lr 1.000000e-03
2019-11-13 17:19:54,356 train 000 1.252255e-02 0.357493
2019-11-13 17:20:00,384 train 050 1.116908e-02 0.321679
2019-11-13 17:20:06,327 train 100 1.122156e-02 0.324002
2019-11-13 17:20:12,200 train 150 1.120965e-02 0.327200
2019-11-13 17:20:18,065 train 200 1.121650e-02 0.317093
2019-11-13 17:20:23,956 train 250 1.120516e-02 0.322089
2019-11-13 17:20:29,819 train 300 1.120433e-02 0.322245
2019-11-13 17:20:35,688 train 350 1.119493e-02 0.323646
2019-11-13 17:20:41,570 train 400 1.119668e-02 0.323196
2019-11-13 17:20:47,436 train 450 1.115914e-02 0.319457
2019-11-13 17:20:53,292 train 500 1.114837e-02 0.319325
2019-11-13 17:20:59,149 train 550 1.116254e-02 0.316757
2019-11-13 17:21:05,011 train 600 1.116219e-02 0.314840
2019-11-13 17:21:10,867 train 650 1.118821e-02 0.313322
2019-11-13 17:21:16,735 train 700 1.117365e-02 0.314657
2019-11-13 17:21:22,608 train 750 1.118531e-02 0.311336
2019-11-13 17:21:28,467 train 800 1.117707e-02 0.308407
2019-11-13 17:21:34,324 train 850 1.118175e-02 0.306457
2019-11-13 17:21:36,080 training loss; R2: 1.118756e-02 0.306508
2019-11-13 17:21:36,451 valid 000 2.620604e-01 -21.796719
2019-11-13 17:21:38,215 valid 050 2.586514e-01 -46.712253
2019-11-13 17:21:39,791 validation loss; R2: 2.590265e-01 -50.444007
2019-11-13 17:21:39,811 epoch 16 lr 1.000000e-03
2019-11-13 17:21:40,252 train 000 9.383502e-03 0.370122
2019-11-13 17:21:46,107 train 050 1.085970e-02 0.316403
2019-11-13 17:21:51,998 train 100 1.106898e-02 0.327384
2019-11-13 17:21:57,861 train 150 1.107513e-02 0.305551
2019-11-13 17:22:03,736 train 200 1.112682e-02 0.303713
2019-11-13 17:22:09,628 train 250 1.115359e-02 0.307209
2019-11-13 17:22:15,487 train 300 1.118491e-02 0.307915
2019-11-13 17:22:21,342 train 350 1.120757e-02 0.311615
2019-11-13 17:22:27,199 train 400 1.117943e-02 0.316868
2019-11-13 17:22:33,072 train 450 1.115861e-02 0.294969
2019-11-13 17:22:38,928 train 500 1.114343e-02 0.294692
2019-11-13 17:22:44,778 train 550 1.111972e-02 0.290945
2019-11-13 17:22:50,625 train 600 1.108438e-02 0.295450
2019-11-13 17:22:56,483 train 650 1.107769e-02 0.295891
2019-11-13 17:23:02,340 train 700 1.107734e-02 0.297526
2019-11-13 17:23:08,218 train 750 1.107819e-02 0.295860
2019-11-13 17:23:14,079 train 800 1.107177e-02 0.298818
2019-11-13 17:23:19,943 train 850 1.105657e-02 0.294663
2019-11-13 17:23:21,695 training loss; R2: 1.105395e-02 0.295513
2019-11-13 17:23:22,070 valid 000 1.866728e+00 -147.429731
2019-11-13 17:23:23,806 valid 050 1.883651e+00 -142.207144
2019-11-13 17:23:25,397 validation loss; R2: 1.884415e+00 -137.343799
2019-11-13 17:23:25,410 epoch 17 lr 1.000000e-03
2019-11-13 17:23:25,829 train 000 8.161884e-03 0.446030
2019-11-13 17:23:31,796 train 050 1.073443e-02 0.319874
2019-11-13 17:23:37,739 train 100 1.095796e-02 0.328206
2019-11-13 17:23:43,677 train 150 1.102787e-02 0.329546
2019-11-13 17:23:49,584 train 200 1.106864e-02 0.323101
2019-11-13 17:23:55,509 train 250 1.106735e-02 0.302496
2019-11-13 17:24:01,436 train 300 1.107343e-02 0.302749
2019-11-13 17:24:07,351 train 350 1.108972e-02 0.304303
2019-11-13 17:24:13,241 train 400 1.106030e-02 0.306481
2019-11-13 17:24:19,154 train 450 1.103430e-02 0.303584
2019-11-13 17:24:25,056 train 500 1.100716e-02 0.307923
2019-11-13 17:24:30,948 train 550 1.101522e-02 0.311460
2019-11-13 17:24:36,850 train 600 1.101941e-02 0.303099
2019-11-13 17:24:42,760 train 650 1.102564e-02 0.306835
2019-11-13 17:24:48,623 train 700 1.103100e-02 0.308859
2019-11-13 17:24:54,482 train 750 1.100737e-02 0.310931
2019-11-13 17:25:00,341 train 800 1.098608e-02 0.313305
2019-11-13 17:25:06,112 train 850 1.099974e-02 0.312884
2019-11-13 17:25:07,789 training loss; R2: 1.099825e-02 0.274312
2019-11-13 17:25:08,161 valid 000 8.820447e-02 -2.312300
2019-11-13 17:25:09,901 valid 050 8.793306e-02 -3.215363
2019-11-13 17:25:11,505 validation loss; R2: 8.813344e-02 -3.335096
2019-11-13 17:25:11,522 epoch 18 lr 1.000000e-03
2019-11-13 17:25:11,979 train 000 1.083786e-02 0.322784
2019-11-13 17:25:17,927 train 050 1.087952e-02 0.340610
2019-11-13 17:25:23,901 train 100 1.103410e-02 0.311864
2019-11-13 17:25:29,857 train 150 1.101011e-02 0.320916
2019-11-13 17:25:35,804 train 200 1.099455e-02 0.305619
2019-11-13 17:25:41,748 train 250 1.102324e-02 0.313350
2019-11-13 17:25:47,659 train 300 1.098682e-02 0.313794
2019-11-13 17:25:53,591 train 350 1.099107e-02 0.316296
2019-11-13 17:25:59,457 train 400 1.099257e-02 0.315565
2019-11-13 17:26:05,340 train 450 1.097984e-02 0.318659
2019-11-13 17:26:11,199 train 500 1.094213e-02 0.322759
2019-11-13 17:26:17,077 train 550 1.094091e-02 0.323751
2019-11-13 17:26:22,949 train 600 1.091489e-02 0.327113
2019-11-13 17:26:28,810 train 650 1.093961e-02 0.318345
2019-11-13 17:26:34,672 train 700 1.092977e-02 0.314764
2019-11-13 17:26:40,548 train 750 1.095222e-02 0.315216
2019-11-13 17:26:46,410 train 800 1.096527e-02 0.315922
2019-11-13 17:26:52,266 train 850 1.096742e-02 0.315434
2019-11-13 17:26:54,018 training loss; R2: 1.097424e-02 0.316227
2019-11-13 17:26:54,416 valid 000 6.598180e-02 -2.214714
2019-11-13 17:26:56,160 valid 050 6.083418e-02 -1.691724
2019-11-13 17:26:57,710 validation loss; R2: 6.102209e-02 -1.668029
2019-11-13 17:26:57,726 epoch 19 lr 1.000000e-03
2019-11-13 17:26:58,173 train 000 1.038589e-02 0.169384
2019-11-13 17:27:04,179 train 050 1.083367e-02 0.294348
2019-11-13 17:27:10,129 train 100 1.073844e-02 0.316092
2019-11-13 17:27:16,011 train 150 1.068259e-02 0.314314
2019-11-13 17:27:21,866 train 200 1.064037e-02 0.315230
2019-11-13 17:27:27,743 train 250 1.074591e-02 0.318422
2019-11-13 17:27:33,602 train 300 1.079579e-02 0.318980
2019-11-13 17:27:39,487 train 350 1.081656e-02 0.290402
2019-11-13 17:27:45,350 train 400 1.082391e-02 0.294292
2019-11-13 17:27:51,207 train 450 1.085828e-02 0.296411
2019-11-13 17:27:57,071 train 500 1.086695e-02 0.300220
2019-11-13 17:28:02,959 train 550 1.087157e-02 0.303806
2019-11-13 17:28:08,785 train 600 1.085579e-02 0.308455
2019-11-13 17:28:14,377 train 650 1.085362e-02 0.309774
2019-11-13 17:28:19,966 train 700 1.085856e-02 0.310341
2019-11-13 17:28:25,548 train 750 1.086072e-02 0.298332
2019-11-13 17:28:31,150 train 800 1.085508e-02 0.297842
2019-11-13 17:28:36,736 train 850 1.085688e-02 0.301466
2019-11-13 17:28:38,412 training loss; R2: 1.085371e-02 0.302351
2019-11-13 17:28:38,773 valid 000 1.513326e-02 0.162730
2019-11-13 17:28:40,513 valid 050 1.563796e-02 0.041390
2019-11-13 17:28:42,100 validation loss; R2: 1.584113e-02 -0.044338
