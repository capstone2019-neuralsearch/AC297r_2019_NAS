2019-11-13 13:37:41,784 gpu device = 1
2019-11-13 13:37:41,785 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name='galaxy-zoo-cleaned', gpu=1, grad_clip=5, gz_dtree=False, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-galaxy-zoo-EXP-20191113-133741', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-11-13 13:37:53,386 param size = 0.311125MB
2019-11-13 13:37:53,390 epoch 0 lr 1.000000e-03
2019-11-13 13:37:55,663 train 000 2.848199e-01 -95.505456
2019-11-13 13:38:03,303 train 050 4.550730e-02 -7.715488
2019-11-13 13:38:10,626 train 100 3.623079e-02 -4.148185
2019-11-13 13:38:17,875 train 150 3.255683e-02 -2.884029
2019-11-13 13:38:25,191 train 200 3.075131e-02 -2.240051
2019-11-13 13:38:32,555 train 250 2.925870e-02 -1.831233
2019-11-13 13:38:39,810 train 300 2.836564e-02 -1.563190
2019-11-13 13:38:47,092 train 350 2.753748e-02 -1.365913
2019-11-13 13:38:54,413 train 400 2.680934e-02 -1.220544
2019-11-13 13:39:01,590 train 450 2.619821e-02 -1.088790
2019-11-13 13:39:08,965 train 500 2.563122e-02 -0.979659
2019-11-13 13:39:16,285 train 550 2.514159e-02 -0.888819
2019-11-13 13:39:23,525 train 600 2.473805e-02 -0.811065
2019-11-13 13:39:30,804 train 650 2.435445e-02 -0.744504
2019-11-13 13:39:38,017 train 700 2.401002e-02 -0.685667
2019-11-13 13:39:45,263 train 750 2.370625e-02 -0.636472
2019-11-13 13:39:52,586 train 800 2.337433e-02 -0.593217
2019-11-13 13:40:00,089 train 850 2.313753e-02 -0.556288
2019-11-13 13:40:03,072 training loss; R2: 2.305598e-02 -0.545056
2019-11-13 13:40:03,360 valid 000 1.982654e-02 0.214076
2019-11-13 13:40:05,027 valid 050 1.769333e-02 0.140782
2019-11-13 13:40:06,640 validation loss; R2: 1.772264e-02 0.150351
2019-11-13 13:40:06,663 epoch 1 lr 1.000000e-03
2019-11-13 13:40:07,291 train 000 1.696008e-02 0.179258
2019-11-13 13:40:14,412 train 050 1.849113e-02 0.025497
2019-11-13 13:40:21,545 train 100 1.856797e-02 0.071092
2019-11-13 13:40:28,662 train 150 1.836917e-02 0.090978
2019-11-13 13:40:35,776 train 200 1.829015e-02 0.099866
2019-11-13 13:40:42,879 train 250 1.816959e-02 0.096324
2019-11-13 13:40:49,988 train 300 1.805062e-02 0.101042
2019-11-13 13:40:57,089 train 350 1.799673e-02 0.101590
2019-11-13 13:41:04,189 train 400 1.786514e-02 0.103731
2019-11-13 13:41:11,309 train 450 1.772358e-02 0.079227
2019-11-13 13:41:18,430 train 500 1.760879e-02 0.088427
2019-11-13 13:41:25,541 train 550 1.752989e-02 0.095567
2019-11-13 13:41:32,652 train 600 1.742826e-02 0.028971
2019-11-13 13:41:39,755 train 650 1.731790e-02 0.040941
2019-11-13 13:41:46,870 train 700 1.721558e-02 0.049956
2019-11-13 13:41:53,977 train 750 1.713113e-02 0.057957
2019-11-13 13:42:01,082 train 800 1.704308e-02 0.065662
2019-11-13 13:42:08,188 train 850 1.693913e-02 0.074478
2019-11-13 13:42:10,315 training loss; R2: 1.692165e-02 0.076138
2019-11-13 13:42:10,630 valid 000 1.652991e-02 0.312177
2019-11-13 13:42:12,331 valid 050 1.597057e-02 0.146881
2019-11-13 13:42:13,871 validation loss; R2: 1.602782e-02 0.153854
2019-11-13 13:42:13,890 epoch 2 lr 1.000000e-03
2019-11-13 13:42:14,305 train 000 1.598400e-02 0.204226
2019-11-13 13:42:21,446 train 050 1.512236e-02 0.203645
2019-11-13 13:42:28,548 train 100 1.485998e-02 0.174874
2019-11-13 13:42:35,648 train 150 1.494398e-02 0.183910
2019-11-13 13:42:42,748 train 200 1.493587e-02 0.188298
2019-11-13 13:42:49,851 train 250 1.492405e-02 0.190059
2019-11-13 13:42:56,945 train 300 1.487480e-02 0.185885
2019-11-13 13:43:04,043 train 350 1.475435e-02 0.195771
2019-11-13 13:43:11,164 train 400 1.474521e-02 0.179520
2019-11-13 13:43:18,273 train 450 1.466924e-02 0.185261
2019-11-13 13:43:25,418 train 500 1.459454e-02 0.185295
2019-11-13 13:43:32,549 train 550 1.458371e-02 0.186372
2019-11-13 13:43:39,675 train 600 1.453583e-02 0.187425
2019-11-13 13:43:46,784 train 650 1.449485e-02 0.190809
2019-11-13 13:43:53,908 train 700 1.443290e-02 0.194173
2019-11-13 13:44:01,032 train 750 1.441096e-02 0.196626
2019-11-13 13:44:08,162 train 800 1.437004e-02 0.189415
2019-11-13 13:44:15,289 train 850 1.430767e-02 0.192890
2019-11-13 13:44:17,416 training loss; R2: 1.428408e-02 0.193373
2019-11-13 13:44:17,718 valid 000 1.320709e-02 0.199999
2019-11-13 13:44:19,470 valid 050 1.252377e-02 0.288535
2019-11-13 13:44:21,047 validation loss; R2: 1.249082e-02 0.224910
2019-11-13 13:44:21,072 epoch 3 lr 1.000000e-03
2019-11-13 13:44:21,469 train 000 1.636080e-02 -0.045063
2019-11-13 13:44:28,698 train 050 1.313381e-02 0.249261
2019-11-13 13:44:35,868 train 100 1.324833e-02 0.234385
2019-11-13 13:44:43,052 train 150 1.321514e-02 0.232682
2019-11-13 13:44:50,190 train 200 1.329499e-02 0.235273
2019-11-13 13:44:57,323 train 250 1.331237e-02 0.232901
2019-11-13 13:45:04,459 train 300 1.328553e-02 0.230436
2019-11-13 13:45:11,608 train 350 1.320812e-02 0.226242
2019-11-13 13:45:18,797 train 400 1.320440e-02 0.226933
2019-11-13 13:45:25,973 train 450 1.319512e-02 0.231054
2019-11-13 13:45:33,104 train 500 1.314087e-02 0.232946
2019-11-13 13:45:40,223 train 550 1.314538e-02 0.237537
2019-11-13 13:45:47,349 train 600 1.312067e-02 0.238834
2019-11-13 13:45:54,475 train 650 1.310267e-02 0.238346
2019-11-13 13:46:01,657 train 700 1.305814e-02 0.240313
2019-11-13 13:46:08,772 train 750 1.299953e-02 0.242302
2019-11-13 13:46:15,894 train 800 1.296379e-02 0.245102
2019-11-13 13:46:23,034 train 850 1.295237e-02 0.241888
2019-11-13 13:46:25,171 training loss; R2: 1.295141e-02 0.242474
2019-11-13 13:46:25,485 valid 000 1.131010e-02 0.095915
2019-11-13 13:46:27,218 valid 050 1.217080e-02 0.222288
2019-11-13 13:46:28,797 validation loss; R2: 1.213879e-02 0.231222
2019-11-13 13:46:28,815 epoch 4 lr 1.000000e-03
2019-11-13 13:46:29,217 train 000 1.297495e-02 0.375793
2019-11-13 13:46:36,359 train 050 1.204574e-02 0.280157
2019-11-13 13:46:43,481 train 100 1.201929e-02 0.284361
2019-11-13 13:46:50,607 train 150 1.218675e-02 0.277347
2019-11-13 13:46:57,737 train 200 1.230913e-02 0.282474
2019-11-13 13:47:04,873 train 250 1.232588e-02 0.283394
2019-11-13 13:47:12,019 train 300 1.234325e-02 0.280406
2019-11-13 13:47:19,158 train 350 1.235581e-02 0.281691
2019-11-13 13:47:26,281 train 400 1.236574e-02 0.278548
2019-11-13 13:47:33,394 train 450 1.234437e-02 0.278089
2019-11-13 13:47:40,501 train 500 1.232224e-02 0.278810
2019-11-13 13:47:47,613 train 550 1.232351e-02 0.272913
2019-11-13 13:47:54,743 train 600 1.231823e-02 0.275016
2019-11-13 13:48:01,919 train 650 1.229325e-02 0.273641
2019-11-13 13:48:09,040 train 700 1.229359e-02 0.259970
2019-11-13 13:48:16,214 train 750 1.226600e-02 0.261803
2019-11-13 13:48:23,334 train 800 1.225133e-02 0.263077
2019-11-13 13:48:30,462 train 850 1.224950e-02 0.264282
2019-11-13 13:48:32,598 training loss; R2: 1.224881e-02 0.265446
2019-11-13 13:48:32,914 valid 000 1.229359e-02 0.383165
2019-11-13 13:48:34,662 valid 050 1.096103e-02 0.273052
2019-11-13 13:48:36,228 validation loss; R2: 1.102213e-02 0.251055
2019-11-13 13:48:36,247 epoch 5 lr 1.000000e-03
2019-11-13 13:48:36,660 train 000 1.236826e-02 -0.045689
2019-11-13 13:48:43,985 train 050 1.204774e-02 0.297920
2019-11-13 13:48:51,204 train 100 1.210219e-02 0.258966
2019-11-13 13:48:58,382 train 150 1.202841e-02 0.261688
2019-11-13 13:49:05,533 train 200 1.188275e-02 0.263819
2019-11-13 13:49:12,645 train 250 1.190590e-02 0.273245
2019-11-13 13:49:19,753 train 300 1.184411e-02 0.247448
2019-11-13 13:49:26,874 train 350 1.183179e-02 0.251440
2019-11-13 13:49:34,003 train 400 1.184580e-02 0.256562
2019-11-13 13:49:41,109 train 450 1.185303e-02 0.226845
2019-11-13 13:49:48,261 train 500 1.184428e-02 0.234320
2019-11-13 13:49:55,437 train 550 1.182797e-02 0.235610
2019-11-13 13:50:02,558 train 600 1.180268e-02 0.242157
2019-11-13 13:50:09,665 train 650 1.179472e-02 0.248146
2019-11-13 13:50:16,832 train 700 1.180735e-02 0.240808
2019-11-13 13:50:23,940 train 750 1.179324e-02 0.245053
2019-11-13 13:50:31,061 train 800 1.177447e-02 0.247818
2019-11-13 13:50:38,176 train 850 1.177687e-02 0.250369
2019-11-13 13:50:40,306 training loss; R2: 1.178176e-02 0.251078
2019-11-13 13:50:40,619 valid 000 1.017231e-02 0.401636
2019-11-13 13:50:42,367 valid 050 1.048875e-02 0.330837
2019-11-13 13:50:43,918 validation loss; R2: 1.060689e-02 0.336378
2019-11-13 13:50:43,936 epoch 6 lr 1.000000e-03
2019-11-13 13:50:44,308 train 000 1.140176e-02 0.308278
2019-11-13 13:50:51,600 train 050 1.138152e-02 0.201852
2019-11-13 13:50:58,745 train 100 1.144910e-02 0.260011
2019-11-13 13:51:05,916 train 150 1.158987e-02 0.280707
2019-11-13 13:51:13,039 train 200 1.151035e-02 0.282359
2019-11-13 13:51:20,190 train 250 1.149159e-02 0.285656
2019-11-13 13:51:27,342 train 300 1.147063e-02 0.285000
2019-11-13 13:51:34,467 train 350 1.146708e-02 0.283854
2019-11-13 13:51:41,606 train 400 1.148086e-02 0.286246
2019-11-13 13:51:48,722 train 450 1.147538e-02 0.286584
2019-11-13 13:51:55,829 train 500 1.142484e-02 0.290497
2019-11-13 13:52:02,932 train 550 1.140388e-02 0.290405
2019-11-13 13:52:10,043 train 600 1.140377e-02 0.287931
2019-11-13 13:52:17,160 train 650 1.140867e-02 0.287845
2019-11-13 13:52:24,269 train 700 1.140045e-02 0.287978
2019-11-13 13:52:31,371 train 750 1.140179e-02 0.288014
2019-11-13 13:52:38,474 train 800 1.139941e-02 0.290343
2019-11-13 13:52:45,592 train 850 1.140197e-02 0.292631
2019-11-13 13:52:47,720 training loss; R2: 1.140017e-02 0.292121
2019-11-13 13:52:48,023 valid 000 8.170755e-03 0.382075
2019-11-13 13:52:49,740 valid 050 9.761724e-03 0.387204
2019-11-13 13:52:51,309 validation loss; R2: 9.858073e-03 0.332658
2019-11-13 13:52:51,327 epoch 7 lr 1.000000e-03
2019-11-13 13:52:51,709 train 000 9.888145e-03 0.301262
2019-11-13 13:52:58,882 train 050 1.131605e-02 0.276513
2019-11-13 13:53:06,036 train 100 1.117340e-02 0.295311
2019-11-13 13:53:13,354 train 150 1.118741e-02 0.307204
2019-11-13 13:53:20,476 train 200 1.126284e-02 0.283504
2019-11-13 13:53:27,610 train 250 1.124547e-02 0.281303
2019-11-13 13:53:34,752 train 300 1.118191e-02 0.291193
2019-11-13 13:53:41,880 train 350 1.117455e-02 0.293700
2019-11-13 13:53:49,006 train 400 1.117876e-02 0.299552
2019-11-13 13:53:56,118 train 450 1.115578e-02 0.303822
2019-11-13 13:54:03,228 train 500 1.111050e-02 0.305256
2019-11-13 13:54:10,339 train 550 1.107890e-02 0.307190
2019-11-13 13:54:17,453 train 600 1.110752e-02 0.308803
2019-11-13 13:54:24,580 train 650 1.108169e-02 0.307329
2019-11-13 13:54:31,692 train 700 1.104041e-02 0.309154
2019-11-13 13:54:38,801 train 750 1.103438e-02 0.312098
2019-11-13 13:54:45,912 train 800 1.105282e-02 0.312044
2019-11-13 13:54:53,034 train 850 1.104240e-02 0.310056
2019-11-13 13:54:55,163 training loss; R2: 1.103605e-02 0.310972
2019-11-13 13:54:55,482 valid 000 1.205807e-02 0.328819
2019-11-13 13:54:57,206 valid 050 1.053024e-02 0.285480
2019-11-13 13:54:58,802 validation loss; R2: 1.049194e-02 0.024069
2019-11-13 13:54:58,820 epoch 8 lr 1.000000e-03
2019-11-13 13:54:59,236 train 000 1.119147e-02 0.321961
2019-11-13 13:55:06,461 train 050 1.093743e-02 0.304932
2019-11-13 13:55:13,627 train 100 1.085776e-02 0.312237
2019-11-13 13:55:20,872 train 150 1.080061e-02 0.300194
2019-11-13 13:55:28,103 train 200 1.084479e-02 0.311869
2019-11-13 13:55:35,290 train 250 1.085663e-02 0.313027
2019-11-13 13:55:42,468 train 300 1.085430e-02 0.309185
2019-11-13 13:55:49,628 train 350 1.088451e-02 0.312971
2019-11-13 13:55:56,788 train 400 1.088148e-02 0.316658
2019-11-13 13:56:03,943 train 450 1.084966e-02 0.311252
2019-11-13 13:56:11,105 train 500 1.086547e-02 0.311151
2019-11-13 13:56:18,262 train 550 1.087369e-02 0.314535
2019-11-13 13:56:25,409 train 600 1.087764e-02 0.317661
2019-11-13 13:56:32,559 train 650 1.088652e-02 0.318317
2019-11-13 13:56:39,804 train 700 1.087701e-02 0.319441
2019-11-13 13:56:47,034 train 750 1.086823e-02 0.313224
2019-11-13 13:56:54,276 train 800 1.085669e-02 0.313889
2019-11-13 13:57:01,620 train 850 1.083886e-02 0.305439
2019-11-13 13:57:03,762 training loss; R2: 1.083241e-02 0.305789
2019-11-13 13:57:04,079 valid 000 1.592902e+00 -58.271733
2019-11-13 13:57:05,810 valid 050 1.600612e+00 -60.510547
2019-11-13 13:57:07,365 validation loss; R2: 1.602928e+00 -63.496390
2019-11-13 13:57:07,383 epoch 9 lr 1.000000e-03
2019-11-13 13:57:07,796 train 000 9.924494e-03 0.423669
2019-11-13 13:57:15,022 train 050 1.069796e-02 0.341069
2019-11-13 13:57:22,200 train 100 1.070413e-02 0.340081
2019-11-13 13:57:29,327 train 150 1.071713e-02 0.334960
2019-11-13 13:57:36,469 train 200 1.069599e-02 0.324793
2019-11-13 13:57:43,608 train 250 1.065829e-02 0.294096
2019-11-13 13:57:50,745 train 300 1.067314e-02 0.297218
2019-11-13 13:57:57,889 train 350 1.067256e-02 0.305474
2019-11-13 13:58:05,032 train 400 1.067778e-02 0.236916
2019-11-13 13:58:12,176 train 450 1.067059e-02 0.244655
2019-11-13 13:58:19,321 train 500 1.065641e-02 0.252872
2019-11-13 13:58:26,463 train 550 1.068572e-02 0.259946
2019-11-13 13:58:33,697 train 600 1.069590e-02 0.265945
2019-11-13 13:58:40,842 train 650 1.068110e-02 0.272523
2019-11-13 13:58:47,985 train 700 1.070068e-02 0.276321
2019-11-13 13:58:55,116 train 750 1.068569e-02 0.281749
2019-11-13 13:59:02,282 train 800 1.069011e-02 0.284704
2019-11-13 13:59:09,454 train 850 1.069830e-02 0.286245
2019-11-13 13:59:11,594 training loss; R2: 1.070649e-02 0.287318
2019-11-13 13:59:11,912 valid 000 3.897504e+01 -888.488418
2019-11-13 13:59:13,640 valid 050 3.877303e+01 -1339.205539
2019-11-13 13:59:15,236 validation loss; R2: 3.878504e+01 -1230.622853
2019-11-13 13:59:15,255 epoch 10 lr 1.000000e-03
2019-11-13 13:59:15,633 train 000 1.090789e-02 0.354821
2019-11-13 13:59:23,018 train 050 1.085855e-02 0.304642
2019-11-13 13:59:30,459 train 100 1.074153e-02 0.305274
2019-11-13 13:59:37,870 train 150 1.060868e-02 0.324155
2019-11-13 13:59:45,294 train 200 1.057576e-02 0.324456
2019-11-13 13:59:52,711 train 250 1.055589e-02 0.323388
2019-11-13 14:00:00,151 train 300 1.056391e-02 0.329306
2019-11-13 14:00:07,578 train 350 1.051712e-02 0.331301
2019-11-13 14:00:15,003 train 400 1.054116e-02 0.334739
2019-11-13 14:00:22,431 train 450 1.055112e-02 0.335455
2019-11-13 14:00:29,881 train 500 1.052410e-02 0.337737
2019-11-13 14:00:37,297 train 550 1.052413e-02 0.336483
2019-11-13 14:00:44,750 train 600 1.052880e-02 0.336666
2019-11-13 14:00:52,201 train 650 1.052097e-02 0.338904
2019-11-13 14:00:59,670 train 700 1.052674e-02 0.339183
2019-11-13 14:01:07,128 train 750 1.053144e-02 0.339392
2019-11-13 14:01:14,601 train 800 1.052386e-02 0.339456
2019-11-13 14:01:22,070 train 850 1.052170e-02 0.339257
2019-11-13 14:01:24,303 training loss; R2: 1.052769e-02 0.339686
2019-11-13 14:01:24,628 valid 000 3.856925e+00 -152.381937
2019-11-13 14:01:26,285 valid 050 3.852157e+00 -215.238677
2019-11-13 14:01:27,809 validation loss; R2: 3.854027e+00 -213.367354
2019-11-13 14:01:27,832 epoch 11 lr 1.000000e-03
2019-11-13 14:01:28,289 train 000 1.095349e-02 0.238167
2019-11-13 14:01:35,847 train 050 1.062136e-02 0.330495
2019-11-13 14:01:43,311 train 100 1.044060e-02 0.214397
2019-11-13 14:01:50,583 train 150 1.049960e-02 0.253367
2019-11-13 14:01:57,912 train 200 1.046955e-02 0.262092
2019-11-13 14:02:05,429 train 250 1.047386e-02 0.271533
2019-11-13 14:02:12,849 train 300 1.051119e-02 0.269896
2019-11-13 14:02:20,227 train 350 1.049918e-02 0.276825
2019-11-13 14:02:27,669 train 400 1.046425e-02 0.285149
2019-11-13 14:02:35,063 train 450 1.047074e-02 0.291559
2019-11-13 14:02:42,314 train 500 1.048961e-02 0.294803
2019-11-13 14:02:49,659 train 550 1.048423e-02 0.300914
2019-11-13 14:02:57,210 train 600 1.047546e-02 -1.014704
2019-11-13 14:03:04,749 train 650 1.048258e-02 -0.908942
2019-11-13 14:03:12,318 train 700 1.050402e-02 -0.822239
2019-11-13 14:03:19,842 train 750 1.052292e-02 -0.746716
2019-11-13 14:03:27,393 train 800 1.052307e-02 -0.679175
2019-11-13 14:03:34,874 train 850 1.052651e-02 -0.618157
2019-11-13 14:03:37,041 training loss; R2: 1.052363e-02 -0.601407
2019-11-13 14:03:37,339 valid 000 4.347368e+00 -2157.123127
2019-11-13 14:03:39,016 valid 050 4.357915e+00 -1625.658587
2019-11-13 14:03:40,564 validation loss; R2: 4.358093e+00 -1784.845978
2019-11-13 14:03:40,587 epoch 12 lr 1.000000e-03
2019-11-13 14:03:41,030 train 000 9.317097e-03 0.446206
2019-11-13 14:03:48,544 train 050 1.044584e-02 0.310027
2019-11-13 14:03:55,844 train 100 1.029193e-02 0.322866
2019-11-13 14:04:03,304 train 150 1.038848e-02 0.303379
2019-11-13 14:04:10,604 train 200 1.038570e-02 0.312959
2019-11-13 14:04:18,126 train 250 1.039288e-02 0.318795
2019-11-13 14:04:25,623 train 300 1.041662e-02 0.316789
2019-11-13 14:04:33,064 train 350 1.037535e-02 0.323401
2019-11-13 14:04:40,346 train 400 1.033423e-02 0.317403
2019-11-13 14:04:47,780 train 450 1.034519e-02 0.319973
2019-11-13 14:04:55,235 train 500 1.033657e-02 -0.708966
2019-11-13 14:05:02,665 train 550 1.034220e-02 -0.616768
2019-11-13 14:05:10,130 train 600 1.034446e-02 -0.535681
2019-11-13 14:05:17,698 train 650 1.033483e-02 -0.473578
2019-11-13 14:05:25,221 train 700 1.034102e-02 -0.413795
2019-11-13 14:05:32,706 train 750 1.034614e-02 -0.366769
2019-11-13 14:05:40,270 train 800 1.037621e-02 -0.325242
2019-11-13 14:05:47,658 train 850 1.036277e-02 -0.291123
2019-11-13 14:05:49,908 training loss; R2: 1.036825e-02 -0.280786
2019-11-13 14:05:50,199 valid 000 4.038389e+00 -83.378903
2019-11-13 14:05:51,898 valid 050 4.036027e+00 -95.973467
2019-11-13 14:05:53,441 validation loss; R2: 4.036715e+00 -95.673681
2019-11-13 14:05:53,463 epoch 13 lr 1.000000e-03
2019-11-13 14:05:53,915 train 000 1.066514e-02 0.360580
2019-11-13 14:06:01,097 train 050 1.052516e-02 0.278694
2019-11-13 14:06:08,283 train 100 1.038620e-02 0.270231
2019-11-13 14:06:15,436 train 150 1.050203e-02 0.282090
2019-11-13 14:06:22,751 train 200 1.053468e-02 0.246120
2019-11-13 14:06:29,969 train 250 1.047157e-02 0.266670
2019-11-13 14:06:37,182 train 300 1.049939e-02 0.281025
2019-11-13 14:06:44,288 train 350 1.049129e-02 0.288741
2019-11-13 14:06:51,370 train 400 1.055483e-02 0.295217
2019-11-13 14:06:58,464 train 450 1.053108e-02 0.301766
2019-11-13 14:07:05,554 train 500 1.047819e-02 0.300577
2019-11-13 14:07:12,641 train 550 1.048570e-02 0.301639
2019-11-13 14:07:19,746 train 600 1.051162e-02 0.301875
2019-11-13 14:07:26,856 train 650 1.048812e-02 0.298290
2019-11-13 14:07:33,999 train 700 1.047838e-02 0.296816
2019-11-13 14:07:41,083 train 750 1.049868e-02 0.296709
2019-11-13 14:07:48,166 train 800 1.048437e-02 0.296784
2019-11-13 14:07:55,246 train 850 1.047663e-02 0.299594
2019-11-13 14:07:57,365 training loss; R2: 1.047295e-02 0.299438
2019-11-13 14:07:57,686 valid 000 4.898103e+00 -1941.077060
2019-11-13 14:07:59,347 valid 050 4.907333e+00 -1197.777894
2019-11-13 14:08:00,886 validation loss; R2: 4.907868e+00 -1322.830380
2019-11-13 14:08:00,904 epoch 14 lr 1.000000e-03
2019-11-13 14:08:01,349 train 000 1.240697e-02 0.338123
2019-11-13 14:08:08,462 train 050 1.037589e-02 0.220012
2019-11-13 14:08:15,557 train 100 1.053018e-02 0.282002
2019-11-13 14:08:22,642 train 150 1.048011e-02 0.275689
2019-11-13 14:08:29,735 train 200 1.051535e-02 0.289495
2019-11-13 14:08:36,825 train 250 1.045452e-02 0.294863
2019-11-13 14:08:43,905 train 300 1.043504e-02 0.303658
2019-11-13 14:08:51,009 train 350 1.042686e-02 0.303994
2019-11-13 14:08:58,094 train 400 1.040748e-02 0.308259
2019-11-13 14:09:05,180 train 450 1.038858e-02 0.311719
2019-11-13 14:09:12,303 train 500 1.037690e-02 0.314996
2019-11-13 14:09:19,392 train 550 1.036122e-02 0.316836
2019-11-13 14:09:26,473 train 600 1.036834e-02 0.309987
2019-11-13 14:09:33,577 train 650 1.036287e-02 0.311580
2019-11-13 14:09:40,670 train 700 1.036331e-02 0.311252
2019-11-13 14:09:47,750 train 750 1.035774e-02 0.311582
2019-11-13 14:09:54,839 train 800 1.035541e-02 0.309056
2019-11-13 14:10:01,953 train 850 1.033140e-02 0.308203
2019-11-13 14:10:04,069 training loss; R2: 1.032687e-02 0.308687
2019-11-13 14:10:04,396 valid 000 2.341464e+01 -7400.243674
2019-11-13 14:10:06,157 valid 050 2.338642e+01 -6994.038504
2019-11-13 14:10:07,712 validation loss; R2: 2.339238e+01 -7878.001679
2019-11-13 14:10:07,731 epoch 15 lr 1.000000e-03
2019-11-13 14:10:08,154 train 000 1.378368e-02 0.341592
2019-11-13 14:10:15,509 train 050 1.023452e-02 0.337799
2019-11-13 14:10:22,665 train 100 1.017813e-02 0.336978
2019-11-13 14:10:29,791 train 150 1.011166e-02 0.348234
2019-11-13 14:10:36,914 train 200 1.009468e-02 0.167005
2019-11-13 14:10:44,085 train 250 1.007834e-02 0.189243
2019-11-13 14:10:51,218 train 300 1.007536e-02 0.212195
2019-11-13 14:10:58,347 train 350 1.011290e-02 0.224462
2019-11-13 14:11:05,475 train 400 1.016658e-02 0.236293
2019-11-13 14:11:12,623 train 450 1.018282e-02 0.249858
2019-11-13 14:11:19,808 train 500 1.018796e-02 0.257672
2019-11-13 14:11:26,994 train 550 1.017817e-02 0.240710
2019-11-13 14:11:34,175 train 600 1.016825e-02 0.248841
2019-11-13 14:11:41,357 train 650 1.018884e-02 0.256430
2019-11-13 14:11:48,485 train 700 1.017910e-02 0.256477
2019-11-13 14:11:55,607 train 750 1.019297e-02 0.254127
2019-11-13 14:12:02,746 train 800 1.019882e-02 0.259963
2019-11-13 14:12:09,882 train 850 1.020546e-02 0.264653
2019-11-13 14:12:12,016 training loss; R2: 1.020597e-02 0.264562
2019-11-13 14:12:12,342 valid 000 1.598142e+01 -3879.894996
2019-11-13 14:12:14,059 valid 050 1.592072e+01 -8834.080968
2019-11-13 14:12:15,637 validation loss; R2: 1.592332e+01 -6743.934133
2019-11-13 14:12:15,660 epoch 16 lr 1.000000e-03
2019-11-13 14:12:16,122 train 000 1.041431e-02 0.398131
2019-11-13 14:12:23,389 train 050 1.032991e-02 0.313087
2019-11-13 14:12:30,493 train 100 1.029044e-02 0.326999
2019-11-13 14:12:37,585 train 150 1.028607e-02 0.275773
2019-11-13 14:12:44,661 train 200 1.035388e-02 -0.671067
2019-11-13 14:12:51,736 train 250 1.035612e-02 -0.467662
2019-11-13 14:12:58,810 train 300 1.038089e-02 -0.344864
2019-11-13 14:13:05,897 train 350 1.035720e-02 -0.383395
2019-11-13 14:13:12,982 train 400 1.034462e-02 -0.297393
2019-11-13 14:13:20,052 train 450 1.031763e-02 -0.224133
2019-11-13 14:13:27,126 train 500 1.027835e-02 -0.169588
2019-11-13 14:13:34,205 train 550 1.027036e-02 -0.125617
2019-11-13 14:13:41,285 train 600 1.023056e-02 -0.086298
2019-11-13 14:13:48,355 train 650 1.020256e-02 -0.060158
2019-11-13 14:13:55,437 train 700 1.018768e-02 -0.031350
2019-11-13 14:14:02,520 train 750 1.018926e-02 -0.005943
2019-11-13 14:14:09,601 train 800 1.020468e-02 0.013724
2019-11-13 14:14:16,684 train 850 1.020607e-02 0.033738
2019-11-13 14:14:18,802 training loss; R2: 1.021670e-02 0.039232
2019-11-13 14:14:19,121 valid 000 8.540981e+01 -6259.012027
2019-11-13 14:14:20,787 valid 050 8.538285e+01 -8760.919422
2019-11-13 14:14:22,315 validation loss; R2: 8.539119e+01 -9996.120752
2019-11-13 14:14:22,338 epoch 17 lr 1.000000e-03
2019-11-13 14:14:22,738 train 000 8.408620e-03 0.453171
2019-11-13 14:14:29,978 train 050 1.017123e-02 0.292385
2019-11-13 14:14:37,447 train 100 1.025959e-02 0.317318
2019-11-13 14:14:44,834 train 150 1.027518e-02 0.321982
2019-11-13 14:14:52,004 train 200 1.024753e-02 0.323285
2019-11-13 14:14:59,199 train 250 1.020932e-02 0.329315
2019-11-13 14:15:06,370 train 300 1.014238e-02 0.333109
2019-11-13 14:15:13,700 train 350 1.012974e-02 0.324826
2019-11-13 14:15:20,856 train 400 1.013035e-02 0.321887
2019-11-13 14:15:28,040 train 450 1.013908e-02 0.315973
2019-11-13 14:15:35,264 train 500 1.013133e-02 0.318782
2019-11-13 14:15:42,633 train 550 1.012157e-02 0.320150
2019-11-13 14:15:49,957 train 600 1.013426e-02 0.319314
2019-11-13 14:15:57,202 train 650 1.014068e-02 0.319170
2019-11-13 14:16:04,417 train 700 1.016001e-02 0.320231
2019-11-13 14:16:11,712 train 750 1.015417e-02 0.320193
2019-11-13 14:16:18,940 train 800 1.015213e-02 0.321106
2019-11-13 14:16:26,243 train 850 1.014948e-02 0.320097
2019-11-13 14:16:28,397 training loss; R2: 1.015653e-02 0.318057
2019-11-13 14:16:28,710 valid 000 2.622551e+01 -8736.098343
2019-11-13 14:16:30,392 valid 050 2.619738e+01 -12895.665008
2019-11-13 14:16:31,933 validation loss; R2: 2.619772e+01 -13960.113670
2019-11-13 14:16:31,956 epoch 18 lr 1.000000e-03
2019-11-13 14:16:32,412 train 000 8.053259e-03 0.282092
2019-11-13 14:16:39,648 train 050 1.001297e-02 0.325831
2019-11-13 14:16:46,772 train 100 9.993991e-03 0.274746
2019-11-13 14:16:53,921 train 150 1.005031e-02 0.283066
2019-11-13 14:17:01,049 train 200 1.003165e-02 0.306074
2019-11-13 14:17:08,181 train 250 1.004128e-02 0.312906
2019-11-13 14:17:15,317 train 300 1.005026e-02 0.290354
2019-11-13 14:17:22,454 train 350 1.004819e-02 0.291003
2019-11-13 14:17:29,604 train 400 1.007872e-02 0.291633
2019-11-13 14:17:36,737 train 450 1.005861e-02 0.291302
2019-11-13 14:17:43,849 train 500 1.007426e-02 0.288928
2019-11-13 14:17:50,981 train 550 1.008671e-02 0.285389
2019-11-13 14:17:58,102 train 600 1.008259e-02 0.285415
2019-11-13 14:18:05,211 train 650 1.010107e-02 0.288306
2019-11-13 14:18:12,344 train 700 1.009405e-02 0.279416
2019-11-13 14:18:19,457 train 750 1.010159e-02 0.209753
2019-11-13 14:18:26,604 train 800 1.031091e-02 -0.131939
2019-11-13 14:18:33,694 train 850 1.034031e-02 -0.118212
2019-11-13 14:18:35,807 training loss; R2: 1.034395e-02 -0.113081
2019-11-13 14:18:36,142 valid 000 2.666700e+02 -38879.413903
2019-11-13 14:18:37,828 valid 050 2.667049e+02 -79620.083130
2019-11-13 14:18:39,378 validation loss; R2: 2.666940e+02 -87174.070730
2019-11-13 14:18:39,396 epoch 19 lr 1.000000e-03
2019-11-13 14:18:39,804 train 000 9.800503e-03 0.313092
2019-11-13 14:18:46,916 train 050 1.070322e-02 0.087870
2019-11-13 14:18:54,274 train 100 1.081263e-02 0.160799
2019-11-13 14:19:01,532 train 150 1.061378e-02 0.211600
2019-11-13 14:19:08,775 train 200 1.049773e-02 0.207737
2019-11-13 14:19:15,954 train 250 1.043633e-02 0.224677
2019-11-13 14:19:23,105 train 300 1.044662e-02 0.198937
2019-11-13 14:19:30,247 train 350 1.043517e-02 0.200998
2019-11-13 14:19:37,417 train 400 1.042592e-02 0.200166
2019-11-13 14:19:44,570 train 450 1.046499e-02 0.197138
2019-11-13 14:19:51,837 train 500 1.045905e-02 0.208831
2019-11-13 14:19:59,187 train 550 1.042588e-02 0.211353
2019-11-13 14:20:06,383 train 600 1.046061e-02 0.217528
2019-11-13 14:20:13,553 train 650 1.048500e-02 0.225701
2019-11-13 14:20:20,714 train 700 1.048014e-02 0.228030
2019-11-13 14:20:27,931 train 750 1.047774e-02 0.234436
2019-11-13 14:20:35,124 train 800 1.047247e-02 0.232630
2019-11-13 14:20:42,409 train 850 1.045305e-02 0.235428
2019-11-13 14:20:44,555 training loss; R2: 1.045412e-02 0.237075
2019-11-13 14:20:44,876 valid 000 5.810047e+01 -119560.917646
2019-11-13 14:20:46,597 valid 050 5.809983e+01 -98116.999248
2019-11-13 14:20:48,161 validation loss; R2: 5.809860e+01 -90901.587007
