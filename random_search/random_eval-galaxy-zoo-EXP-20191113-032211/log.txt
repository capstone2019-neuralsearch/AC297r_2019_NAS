2019-11-13 03:22:11,286 gpu device = 1
2019-11-13 03:22:11,286 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name='galaxy-zoo-cleaned', gpu=1, grad_clip=5, gz_dtree=False, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-galaxy-zoo-EXP-20191113-032211', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-11-13 03:22:22,781 param size = 0.334197MB
2019-11-13 03:22:22,786 epoch 0 lr 1.000000e-03
2019-11-13 03:22:25,059 train 000 4.813176e-01 -139.057260
2019-11-13 03:22:33,088 train 050 4.829919e-02 -15.024334
2019-11-13 03:22:41,220 train 100 3.786232e-02 -7.950319
2019-11-13 03:22:49,267 train 150 3.356774e-02 -5.467499
2019-11-13 03:22:57,313 train 200 3.113745e-02 -4.213992
2019-11-13 03:23:05,354 train 250 2.959390e-02 -3.425153
2019-11-13 03:23:13,590 train 300 2.861762e-02 -2.883301
2019-11-13 03:23:21,490 train 350 2.771127e-02 -2.584904
2019-11-13 03:23:29,513 train 400 2.702672e-02 -2.279741
2019-11-13 03:23:37,580 train 450 2.640002e-02 -2.049738
2019-11-13 03:23:45,737 train 500 2.587809e-02 -1.854683
2019-11-13 03:23:53,619 train 550 2.533669e-02 -1.688303
2019-11-13 03:24:01,488 train 600 2.493530e-02 -1.556393
2019-11-13 03:24:09,346 train 650 2.457873e-02 -1.447461
2019-11-13 03:24:17,199 train 700 2.420919e-02 -1.343464
2019-11-13 03:24:25,052 train 750 2.386314e-02 -1.254490
2019-11-13 03:24:32,902 train 800 2.354870e-02 -1.181503
2019-11-13 03:24:40,752 train 850 2.328489e-02 -1.112729
2019-11-13 03:24:43,938 training loss; R2: 2.320899e-02 -1.092157
2019-11-13 03:24:44,256 valid 000 2.063178e-02 0.084611
2019-11-13 03:24:46,030 valid 050 1.946358e-02 0.105596
2019-11-13 03:24:47,700 validation loss; R2: 1.928066e-02 0.090242
2019-11-13 03:24:47,718 epoch 1 lr 1.000000e-03
2019-11-13 03:24:48,315 train 000 1.637000e-02 0.167498
2019-11-13 03:24:56,172 train 050 1.858287e-02 0.019321
2019-11-13 03:25:04,034 train 100 1.853995e-02 0.035460
2019-11-13 03:25:11,899 train 150 1.830922e-02 0.048257
2019-11-13 03:25:19,757 train 200 1.808217e-02 0.061940
2019-11-13 03:25:27,615 train 250 1.800113e-02 0.060233
2019-11-13 03:25:35,477 train 300 1.790047e-02 0.059746
2019-11-13 03:25:43,347 train 350 1.784073e-02 0.060460
2019-11-13 03:25:51,215 train 400 1.773456e-02 0.067058
2019-11-13 03:25:59,073 train 450 1.764497e-02 0.071060
2019-11-13 03:26:06,929 train 500 1.759737e-02 0.073676
2019-11-13 03:26:14,791 train 550 1.748542e-02 0.080113
2019-11-13 03:26:22,651 train 600 1.739688e-02 0.077341
2019-11-13 03:26:30,511 train 650 1.734770e-02 0.039971
2019-11-13 03:26:38,371 train 700 1.727175e-02 0.047732
2019-11-13 03:26:46,233 train 750 1.717719e-02 0.052785
2019-11-13 03:26:54,098 train 800 1.705919e-02 0.057913
2019-11-13 03:27:01,956 train 850 1.696434e-02 0.064306
2019-11-13 03:27:04,309 training loss; R2: 1.695435e-02 0.065338
2019-11-13 03:27:04,610 valid 000 1.958632e-02 0.201099
2019-11-13 03:27:06,353 valid 050 1.712012e-02 0.095188
2019-11-13 03:27:07,921 validation loss; R2: 1.702150e-02 0.129303
2019-11-13 03:27:07,940 epoch 2 lr 1.000000e-03
2019-11-13 03:27:08,373 train 000 1.549507e-02 0.205018
2019-11-13 03:27:16,446 train 050 1.552287e-02 0.161469
2019-11-13 03:27:24,364 train 100 1.526621e-02 0.175261
2019-11-13 03:27:32,229 train 150 1.522220e-02 0.166593
2019-11-13 03:27:40,108 train 200 1.517027e-02 0.172661
2019-11-13 03:27:47,969 train 250 1.512985e-02 0.167102
2019-11-13 03:27:55,826 train 300 1.503412e-02 -1.123805
2019-11-13 03:28:03,694 train 350 1.496250e-02 -0.937914
2019-11-13 03:28:11,551 train 400 1.491398e-02 -0.798559
2019-11-13 03:28:19,441 train 450 1.486400e-02 -0.693102
2019-11-13 03:28:27,294 train 500 1.482696e-02 -0.609097
2019-11-13 03:28:35,150 train 550 1.476650e-02 -0.536039
2019-11-13 03:28:43,013 train 600 1.473351e-02 -0.475627
2019-11-13 03:28:50,867 train 650 1.466700e-02 -0.422697
2019-11-13 03:28:58,723 train 700 1.462045e-02 -0.379362
2019-11-13 03:29:06,577 train 750 1.455858e-02 -0.338878
2019-11-13 03:29:14,436 train 800 1.452067e-02 -0.306351
2019-11-13 03:29:22,289 train 850 1.450084e-02 -0.274160
2019-11-13 03:29:24,640 training loss; R2: 1.448350e-02 -0.265454
2019-11-13 03:29:24,952 valid 000 1.347357e-02 0.375149
2019-11-13 03:29:26,702 valid 050 1.282858e-02 0.300605
2019-11-13 03:29:28,292 validation loss; R2: 1.301378e-02 0.290490
2019-11-13 03:29:28,310 epoch 3 lr 1.000000e-03
2019-11-13 03:29:28,731 train 000 1.324916e-02 0.184942
2019-11-13 03:29:36,616 train 050 1.407108e-02 0.222783
2019-11-13 03:29:44,471 train 100 1.403702e-02 0.239944
2019-11-13 03:29:52,323 train 150 1.386360e-02 0.247277
2019-11-13 03:30:00,178 train 200 1.361462e-02 0.236540
2019-11-13 03:30:08,029 train 250 1.353372e-02 0.237283
2019-11-13 03:30:15,880 train 300 1.349399e-02 0.229295
2019-11-13 03:30:23,727 train 350 1.347458e-02 0.223607
2019-11-13 03:30:31,574 train 400 1.339349e-02 0.228679
2019-11-13 03:30:39,418 train 450 1.336155e-02 0.232714
2019-11-13 03:30:47,264 train 500 1.334449e-02 0.232202
2019-11-13 03:30:55,114 train 550 1.333470e-02 0.229184
2019-11-13 03:31:02,965 train 600 1.330036e-02 0.230152
2019-11-13 03:31:10,814 train 650 1.324914e-02 0.230664
2019-11-13 03:31:18,665 train 700 1.324025e-02 0.233642
2019-11-13 03:31:26,520 train 750 1.317558e-02 0.234613
2019-11-13 03:31:34,369 train 800 1.315245e-02 0.234706
2019-11-13 03:31:42,217 train 850 1.313553e-02 0.237520
2019-11-13 03:31:44,564 training loss; R2: 1.313351e-02 0.238025
2019-11-13 03:31:44,871 valid 000 1.269511e-02 -0.057337
2019-11-13 03:31:46,629 valid 050 1.156726e-02 0.333298
2019-11-13 03:31:48,205 validation loss; R2: 1.144374e-02 0.313938
2019-11-13 03:31:48,224 epoch 4 lr 1.000000e-03
2019-11-13 03:31:48,620 train 000 1.130067e-02 0.371278
2019-11-13 03:31:56,497 train 050 1.300749e-02 0.243611
2019-11-13 03:32:04,352 train 100 1.274156e-02 0.263326
2019-11-13 03:32:12,208 train 150 1.273550e-02 0.267927
2019-11-13 03:32:20,068 train 200 1.266514e-02 0.258009
2019-11-13 03:32:27,961 train 250 1.255199e-02 0.258151
2019-11-13 03:32:35,813 train 300 1.251493e-02 0.259744
2019-11-13 03:32:43,657 train 350 1.249013e-02 0.237333
2019-11-13 03:32:51,497 train 400 1.250730e-02 0.241507
2019-11-13 03:32:59,337 train 450 1.243946e-02 0.246854
2019-11-13 03:33:07,224 train 500 1.242426e-02 0.249057
2019-11-13 03:33:15,063 train 550 1.239146e-02 0.252034
2019-11-13 03:33:22,908 train 600 1.237344e-02 0.252917
2019-11-13 03:33:30,749 train 650 1.236264e-02 0.252580
2019-11-13 03:33:38,591 train 700 1.237352e-02 0.254195
2019-11-13 03:33:46,433 train 750 1.236769e-02 0.256580
2019-11-13 03:33:54,277 train 800 1.234274e-02 0.258248
2019-11-13 03:34:02,119 train 850 1.232318e-02 0.260364
2019-11-13 03:34:04,466 training loss; R2: 1.231827e-02 0.260738
2019-11-13 03:34:04,766 valid 000 1.251312e-02 0.116862
2019-11-13 03:34:06,498 valid 050 1.120773e-02 0.318596
2019-11-13 03:34:08,078 validation loss; R2: 1.115532e-02 0.322215
2019-11-13 03:34:08,097 epoch 5 lr 1.000000e-03
2019-11-13 03:34:08,517 train 000 1.364337e-02 0.326654
2019-11-13 03:34:16,383 train 050 1.178963e-02 0.249445
2019-11-13 03:34:24,221 train 100 1.191956e-02 0.207051
2019-11-13 03:34:32,055 train 150 1.189481e-02 0.237266
2019-11-13 03:34:39,891 train 200 1.187236e-02 0.252698
2019-11-13 03:34:47,735 train 250 1.185946e-02 0.256712
2019-11-13 03:34:55,600 train 300 1.184731e-02 0.263726
2019-11-13 03:35:03,439 train 350 1.183881e-02 0.263701
2019-11-13 03:35:11,281 train 400 1.186507e-02 0.267032
2019-11-13 03:35:19,126 train 450 1.185514e-02 0.261465
2019-11-13 03:35:26,966 train 500 1.185847e-02 0.261533
2019-11-13 03:35:34,806 train 550 1.185597e-02 0.263823
2019-11-13 03:35:42,644 train 600 1.181473e-02 0.265923
2019-11-13 03:35:50,480 train 650 1.180517e-02 0.268860
2019-11-13 03:35:58,315 train 700 1.179522e-02 0.270453
2019-11-13 03:36:06,153 train 750 1.177793e-02 0.271285
2019-11-13 03:36:13,992 train 800 1.175937e-02 0.271814
2019-11-13 03:36:21,832 train 850 1.175384e-02 0.273535
2019-11-13 03:36:24,181 training loss; R2: 1.175196e-02 0.273653
2019-11-13 03:36:24,477 valid 000 1.041938e-02 0.393601
2019-11-13 03:36:26,225 valid 050 1.060041e-02 0.369127
2019-11-13 03:36:27,795 validation loss; R2: 1.063132e-02 0.344518
2019-11-13 03:36:27,814 epoch 6 lr 1.000000e-03
2019-11-13 03:36:28,231 train 000 1.044704e-02 0.371436
2019-11-13 03:36:36,103 train 050 1.113879e-02 0.320776
2019-11-13 03:36:43,950 train 100 1.125102e-02 0.323095
2019-11-13 03:36:51,797 train 150 1.126204e-02 0.316626
2019-11-13 03:36:59,647 train 200 1.134644e-02 0.236118
2019-11-13 03:37:07,492 train 250 1.134928e-02 0.254769
2019-11-13 03:37:15,346 train 300 1.132161e-02 0.261630
2019-11-13 03:37:23,207 train 350 1.133816e-02 0.266322
2019-11-13 03:37:31,065 train 400 1.132999e-02 0.268872
2019-11-13 03:37:38,915 train 450 1.136615e-02 0.268030
2019-11-13 03:37:46,765 train 500 1.138373e-02 0.273094
2019-11-13 03:37:54,618 train 550 1.136181e-02 0.276632
2019-11-13 03:38:02,467 train 600 1.136235e-02 0.278112
2019-11-13 03:38:10,315 train 650 1.134914e-02 0.282599
2019-11-13 03:38:18,161 train 700 1.133483e-02 0.284778
2019-11-13 03:38:26,005 train 750 1.133558e-02 0.287335
2019-11-13 03:38:33,857 train 800 1.131791e-02 0.289230
2019-11-13 03:38:41,704 train 850 1.132681e-02 0.278206
2019-11-13 03:38:44,051 training loss; R2: 1.132726e-02 0.279128
2019-11-13 03:38:44,356 valid 000 1.263144e-02 0.312773
2019-11-13 03:38:46,101 valid 050 1.016373e-02 0.200203
2019-11-13 03:38:47,669 validation loss; R2: 1.022354e-02 0.129264
2019-11-13 03:38:47,688 epoch 7 lr 1.000000e-03
2019-11-13 03:38:48,078 train 000 1.050045e-02 0.320695
2019-11-13 03:38:56,027 train 050 1.082382e-02 0.305327
2019-11-13 03:39:03,973 train 100 1.089081e-02 0.316996
2019-11-13 03:39:11,810 train 150 1.089611e-02 0.312771
2019-11-13 03:39:19,638 train 200 1.088898e-02 0.173513
2019-11-13 03:39:27,468 train 250 1.087519e-02 0.195373
2019-11-13 03:39:35,293 train 300 1.088226e-02 0.213437
2019-11-13 03:39:43,122 train 350 1.088875e-02 0.231770
2019-11-13 03:39:50,953 train 400 1.090566e-02 0.225864
2019-11-13 03:39:58,785 train 450 1.090264e-02 0.223309
2019-11-13 03:40:06,615 train 500 1.087936e-02 0.234063
2019-11-13 03:40:14,444 train 550 1.089332e-02 0.235402
2019-11-13 03:40:22,275 train 600 1.089603e-02 0.241497
2019-11-13 03:40:30,108 train 650 1.089185e-02 0.247171
2019-11-13 03:40:37,934 train 700 1.090608e-02 0.252926
2019-11-13 03:40:45,759 train 750 1.090880e-02 0.251236
2019-11-13 03:40:53,587 train 800 1.088357e-02 0.255407
2019-11-13 03:41:01,418 train 850 1.091696e-02 0.258183
2019-11-13 03:41:03,759 training loss; R2: 1.091505e-02 0.259521
2019-11-13 03:41:04,065 valid 000 1.226027e-02 -0.073139
2019-11-13 03:41:05,809 valid 050 1.013808e-02 0.311682
2019-11-13 03:41:07,386 validation loss; R2: 1.015547e-02 0.301048
2019-11-13 03:41:07,405 epoch 8 lr 1.000000e-03
2019-11-13 03:41:07,812 train 000 1.202335e-02 0.428048
2019-11-13 03:41:15,690 train 050 1.103606e-02 0.334333
2019-11-13 03:41:23,554 train 100 1.095560e-02 0.325691
2019-11-13 03:41:31,521 train 150 1.095275e-02 0.332833
2019-11-13 03:41:39,445 train 200 1.092536e-02 0.323269
2019-11-13 03:41:47,384 train 250 1.096701e-02 0.322777
2019-11-13 03:41:55,304 train 300 1.089613e-02 0.325278
2019-11-13 03:42:03,252 train 350 1.081683e-02 0.329550
2019-11-13 03:42:11,132 train 400 1.079729e-02 0.316645
2019-11-13 03:42:19,049 train 450 1.079509e-02 0.296293
2019-11-13 03:42:27,073 train 500 1.081182e-02 0.299539
2019-11-13 03:42:35,023 train 550 1.080441e-02 0.302456
2019-11-13 03:42:42,895 train 600 1.077834e-02 0.304493
2019-11-13 03:42:50,760 train 650 1.076311e-02 0.304508
2019-11-13 03:42:58,642 train 700 1.075009e-02 0.308264
2019-11-13 03:43:06,508 train 750 1.073350e-02 0.305595
2019-11-13 03:43:14,450 train 800 1.072795e-02 0.305729
2019-11-13 03:43:22,340 train 850 1.072090e-02 0.307204
2019-11-13 03:43:24,728 training loss; R2: 1.071012e-02 0.307297
2019-11-13 03:43:25,021 valid 000 8.025005e-03 0.366774
2019-11-13 03:43:26,777 valid 050 9.523767e-03 0.234189
2019-11-13 03:43:28,357 validation loss; R2: 9.559581e-03 0.290829
2019-11-13 03:43:28,383 epoch 9 lr 1.000000e-03
2019-11-13 03:43:28,814 train 000 1.048136e-02 0.387860
2019-11-13 03:43:36,788 train 050 1.067361e-02 0.300621
2019-11-13 03:43:44,745 train 100 1.053997e-02 0.286519
2019-11-13 03:43:52,670 train 150 1.057001e-02 0.267415
2019-11-13 03:44:00,647 train 200 1.058700e-02 0.276970
2019-11-13 03:44:08,589 train 250 1.059818e-02 0.280729
2019-11-13 03:44:16,466 train 300 1.057811e-02 0.292201
2019-11-13 03:44:24,335 train 350 1.058441e-02 0.299848
2019-11-13 03:44:32,294 train 400 1.057782e-02 0.305047
2019-11-13 03:44:40,304 train 450 1.058015e-02 0.304555
2019-11-13 03:44:48,202 train 500 1.054883e-02 0.303258
2019-11-13 03:44:56,072 train 550 1.055209e-02 0.308197
2019-11-13 03:45:03,982 train 600 1.053002e-02 0.311787
2019-11-13 03:45:11,846 train 650 1.052410e-02 0.311964
2019-11-13 03:45:19,759 train 700 1.053248e-02 0.311297
2019-11-13 03:45:27,623 train 750 1.053513e-02 0.310171
2019-11-13 03:45:35,488 train 800 1.051784e-02 0.309483
2019-11-13 03:45:43,437 train 850 1.049738e-02 0.310314
2019-11-13 03:45:45,837 training loss; R2: 1.049293e-02 0.310950
2019-11-13 03:45:46,149 valid 000 6.940337e-03 0.331982
2019-11-13 03:45:47,890 valid 050 9.488308e-03 0.355308
2019-11-13 03:45:49,478 validation loss; R2: 9.617200e-03 0.350758
2019-11-13 03:45:49,499 epoch 10 lr 1.000000e-03
2019-11-13 03:45:49,927 train 000 9.640532e-03 0.377015
2019-11-13 03:45:57,983 train 050 1.032875e-02 0.333105
2019-11-13 03:46:05,867 train 100 1.032817e-02 0.329105
2019-11-13 03:46:13,805 train 150 1.041025e-02 0.337084
2019-11-13 03:46:21,723 train 200 1.040250e-02 0.339875
2019-11-13 03:46:29,605 train 250 1.036549e-02 0.334626
2019-11-13 03:46:37,535 train 300 1.042101e-02 0.323968
2019-11-13 03:46:45,493 train 350 1.041928e-02 0.315306
2019-11-13 03:46:53,390 train 400 1.044158e-02 0.319367
2019-11-13 03:47:01,289 train 450 1.040203e-02 0.323102
2019-11-13 03:47:09,171 train 500 1.038060e-02 0.327715
2019-11-13 03:47:17,031 train 550 1.035363e-02 0.329721
2019-11-13 03:47:24,947 train 600 1.036493e-02 0.331655
2019-11-13 03:47:32,826 train 650 1.036620e-02 0.333314
2019-11-13 03:47:40,687 train 700 1.036350e-02 0.332463
2019-11-13 03:47:48,645 train 750 1.036006e-02 0.334281
2019-11-13 03:47:56,571 train 800 1.032749e-02 0.336824
2019-11-13 03:48:04,501 train 850 1.032979e-02 0.336818
2019-11-13 03:48:06,855 training loss; R2: 1.033555e-02 0.336353
2019-11-13 03:48:07,150 valid 000 7.813138e-03 0.473947
2019-11-13 03:48:08,903 valid 050 9.558386e-03 -0.377984
2019-11-13 03:48:10,505 validation loss; R2: 9.662010e-03 -0.042916
2019-11-13 03:48:10,524 epoch 11 lr 1.000000e-03
2019-11-13 03:48:10,958 train 000 1.025135e-02 0.307542
2019-11-13 03:48:18,864 train 050 1.033518e-02 0.318894
2019-11-13 03:48:26,891 train 100 1.030354e-02 0.335767
2019-11-13 03:48:34,840 train 150 1.029208e-02 0.331867
2019-11-13 03:48:42,738 train 200 1.029440e-02 0.337227
2019-11-13 03:48:50,669 train 250 1.026910e-02 0.342556
2019-11-13 03:48:58,591 train 300 1.027457e-02 0.332108
2019-11-13 03:49:06,504 train 350 1.026304e-02 0.336421
2019-11-13 03:49:14,440 train 400 1.023407e-02 0.335891
2019-11-13 03:49:22,352 train 450 1.022133e-02 0.337746
2019-11-13 03:49:30,258 train 500 1.021828e-02 0.335707
2019-11-13 03:49:38,277 train 550 1.022173e-02 0.334270
2019-11-13 03:49:46,208 train 600 1.023118e-02 0.336547
2019-11-13 03:49:54,139 train 650 1.023571e-02 0.334014
2019-11-13 03:50:02,060 train 700 1.024160e-02 0.335670
2019-11-13 03:50:09,979 train 750 1.024177e-02 0.337507
2019-11-13 03:50:17,910 train 800 1.024680e-02 0.339087
2019-11-13 03:50:25,815 train 850 1.025255e-02 0.339969
2019-11-13 03:50:28,205 training loss; R2: 1.024472e-02 0.340606
2019-11-13 03:50:28,520 valid 000 1.092324e-02 0.351926
2019-11-13 03:50:30,270 valid 050 9.886923e-03 0.374934
2019-11-13 03:50:31,831 validation loss; R2: 9.989679e-03 0.364905
2019-11-13 03:50:31,850 epoch 12 lr 1.000000e-03
2019-11-13 03:50:32,247 train 000 9.820869e-03 0.020945
2019-11-13 03:50:40,201 train 050 1.019898e-02 0.327219
2019-11-13 03:50:48,176 train 100 1.012562e-02 0.331544
2019-11-13 03:50:56,132 train 150 1.022568e-02 0.336183
2019-11-13 03:51:04,130 train 200 1.022855e-02 0.336328
2019-11-13 03:51:12,081 train 250 1.022642e-02 0.340824
2019-11-13 03:51:20,026 train 300 1.020776e-02 0.341865
2019-11-13 03:51:27,948 train 350 1.021657e-02 0.339423
2019-11-13 03:51:35,895 train 400 1.022865e-02 0.339549
2019-11-13 03:51:43,800 train 450 1.024448e-02 0.333421
2019-11-13 03:51:51,760 train 500 1.020418e-02 0.331856
2019-11-13 03:51:59,672 train 550 1.018738e-02 0.334692
2019-11-13 03:52:07,587 train 600 1.016981e-02 0.337835
2019-11-13 03:52:15,493 train 650 1.015870e-02 0.336176
2019-11-13 03:52:23,446 train 700 1.015632e-02 0.331712
2019-11-13 03:52:31,387 train 750 1.016909e-02 0.335035
2019-11-13 03:52:39,340 train 800 1.016653e-02 0.328205
2019-11-13 03:52:47,318 train 850 1.016780e-02 0.326395
2019-11-13 03:52:49,712 training loss; R2: 1.016686e-02 0.327502
2019-11-13 03:52:50,035 valid 000 9.758976e-03 0.028648
2019-11-13 03:52:51,770 valid 050 9.247235e-03 0.352557
2019-11-13 03:52:53,350 validation loss; R2: 9.177343e-03 0.376056
2019-11-13 03:52:53,369 epoch 13 lr 1.000000e-03
2019-11-13 03:52:53,805 train 000 7.540255e-03 0.415312
2019-11-13 03:53:01,992 train 050 9.968840e-03 0.362873
2019-11-13 03:53:10,186 train 100 1.005328e-02 0.357148
2019-11-13 03:53:18,344 train 150 1.000899e-02 0.301840
2019-11-13 03:53:26,505 train 200 9.999982e-03 0.309475
2019-11-13 03:53:34,666 train 250 9.969232e-03 0.318403
2019-11-13 03:53:42,831 train 300 9.982575e-03 0.322840
2019-11-13 03:53:50,987 train 350 9.990562e-03 0.326191
2019-11-13 03:53:59,144 train 400 9.989850e-03 0.329474
2019-11-13 03:54:07,296 train 450 9.989865e-03 0.329477
2019-11-13 03:54:15,456 train 500 9.966779e-03 0.335167
2019-11-13 03:54:23,617 train 550 9.975621e-03 0.336883
2019-11-13 03:54:31,772 train 600 1.001337e-02 0.338090
2019-11-13 03:54:39,927 train 650 1.001020e-02 0.337388
2019-11-13 03:54:48,097 train 700 1.003354e-02 0.287781
2019-11-13 03:54:56,256 train 750 1.002066e-02 0.289772
2019-11-13 03:55:04,407 train 800 9.995956e-03 0.293887
2019-11-13 03:55:12,566 train 850 1.000158e-02 0.297367
2019-11-13 03:55:15,004 training loss; R2: 1.000117e-02 0.297674
2019-11-13 03:55:15,317 valid 000 1.107965e-02 0.453997
2019-11-13 03:55:17,003 valid 050 9.259257e-03 0.406757
2019-11-13 03:55:18,550 validation loss; R2: 9.246460e-03 0.409968
2019-11-13 03:55:18,568 epoch 14 lr 1.000000e-03
2019-11-13 03:55:18,986 train 000 9.243575e-03 0.380877
2019-11-13 03:55:27,037 train 050 9.961537e-03 0.366393
2019-11-13 03:55:34,958 train 100 9.916799e-03 0.310833
2019-11-13 03:55:42,817 train 150 9.976740e-03 0.329382
2019-11-13 03:55:50,779 train 200 9.879092e-03 0.341409
2019-11-13 03:55:58,819 train 250 9.875704e-03 0.346246
2019-11-13 03:56:07,065 train 300 9.896107e-03 0.345896
2019-11-13 03:56:15,311 train 350 9.892600e-03 0.346633
2019-11-13 03:56:23,548 train 400 9.939921e-03 0.344119
2019-11-13 03:56:31,780 train 450 9.908515e-03 0.343523
2019-11-13 03:56:40,012 train 500 9.904540e-03 0.344998
2019-11-13 03:56:48,240 train 550 9.938405e-03 0.347818
2019-11-13 03:56:56,398 train 600 9.938430e-03 0.347042
2019-11-13 03:57:04,584 train 650 9.931135e-03 0.349125
2019-11-13 03:57:12,503 train 700 9.932197e-03 0.345698
2019-11-13 03:57:20,464 train 750 9.940095e-03 0.347508
2019-11-13 03:57:28,502 train 800 9.940938e-03 0.346140
2019-11-13 03:57:36,547 train 850 9.947267e-03 0.346767
2019-11-13 03:57:38,901 training loss; R2: 9.938782e-03 0.347034
2019-11-13 03:57:39,198 valid 000 5.417185e+00 -546.040617
2019-11-13 03:57:40,924 valid 050 5.441665e+00 -475.139184
2019-11-13 03:57:42,489 validation loss; R2: 5.444983e+00 -452.127024
2019-11-13 03:57:42,508 epoch 15 lr 1.000000e-03
2019-11-13 03:57:42,924 train 000 9.526655e-03 0.452455
2019-11-13 03:57:50,913 train 050 9.741374e-03 0.359226
2019-11-13 03:57:59,000 train 100 9.641563e-03 0.366148
2019-11-13 03:58:07,173 train 150 9.689659e-03 0.366176
2019-11-13 03:58:15,328 train 200 9.774452e-03 0.315746
2019-11-13 03:58:23,518 train 250 9.803484e-03 0.258879
2019-11-13 03:58:31,460 train 300 9.827139e-03 0.276665
2019-11-13 03:58:39,592 train 350 9.833666e-03 0.188528
2019-11-13 03:58:47,748 train 400 9.858904e-03 0.209474
2019-11-13 03:58:55,756 train 450 9.854596e-03 0.227359
2019-11-13 03:59:03,819 train 500 9.868599e-03 0.240450
2019-11-13 03:59:11,782 train 550 9.876946e-03 0.254171
2019-11-13 03:59:19,968 train 600 9.876113e-03 0.262820
2019-11-13 03:59:28,208 train 650 9.891087e-03 0.269039
2019-11-13 03:59:36,155 train 700 9.873716e-03 0.277163
2019-11-13 03:59:44,101 train 750 9.869281e-03 0.284131
2019-11-13 03:59:52,159 train 800 9.886659e-03 0.289349
2019-11-13 04:00:00,357 train 850 9.880619e-03 0.291915
2019-11-13 04:00:02,817 training loss; R2: 9.885380e-03 0.291789
2019-11-13 04:00:03,101 valid 000 7.648732e+02 -34500.042428
2019-11-13 04:00:04,810 valid 050 7.652880e+02 -29925.990692
2019-11-13 04:00:06,364 validation loss; R2: 7.652262e+02 -32440.860130
2019-11-13 04:00:06,383 epoch 16 lr 1.000000e-03
2019-11-13 04:00:06,788 train 000 7.756547e-03 0.378249
2019-11-13 04:00:14,671 train 050 9.748663e-03 0.309653
2019-11-13 04:00:22,585 train 100 9.929365e-03 0.334922
2019-11-13 04:00:30,608 train 150 9.918745e-03 0.334194
2019-11-13 04:00:38,541 train 200 9.847199e-03 0.341991
2019-11-13 04:00:46,708 train 250 9.803527e-03 0.346589
2019-11-13 04:00:54,955 train 300 9.782615e-03 0.350044
2019-11-13 04:01:03,195 train 350 9.759976e-03 0.349296
2019-11-13 04:01:11,378 train 400 9.765099e-03 0.349305
2019-11-13 04:01:19,391 train 450 9.784882e-03 0.348519
2019-11-13 04:01:27,513 train 500 9.780235e-03 0.350412
2019-11-13 04:01:35,480 train 550 9.784127e-03 0.347948
2019-11-13 04:01:43,410 train 600 9.793089e-03 0.350909
2019-11-13 04:01:51,493 train 650 9.798852e-03 0.349950
2019-11-13 04:01:59,577 train 700 9.797654e-03 0.348292
2019-11-13 04:02:07,713 train 750 9.800667e-03 0.351497
2019-11-13 04:02:15,668 train 800 9.808755e-03 0.351759
2019-11-13 04:02:23,528 train 850 9.806559e-03 0.353475
2019-11-13 04:02:25,873 training loss; R2: 9.807202e-03 0.352717
2019-11-13 04:02:26,196 valid 000 1.614600e+02 -6317.769535
2019-11-13 04:02:27,908 valid 050 1.614681e+02 -6405.237105
2019-11-13 04:02:29,447 validation loss; R2: 1.614582e+02 -6179.765689
2019-11-13 04:02:29,466 epoch 17 lr 1.000000e-03
2019-11-13 04:02:29,861 train 000 1.082408e-02 0.391172
2019-11-13 04:02:37,715 train 050 9.584346e-03 0.362155
2019-11-13 04:02:45,851 train 100 9.657968e-03 0.362576
2019-11-13 04:02:54,056 train 150 9.657342e-03 0.368061
2019-11-13 04:03:02,149 train 200 9.660844e-03 0.372868
2019-11-13 04:03:10,001 train 250 9.708593e-03 0.369045
2019-11-13 04:03:17,844 train 300 9.709682e-03 0.356869
2019-11-13 04:03:25,684 train 350 9.708971e-03 0.357503
2019-11-13 04:03:33,524 train 400 9.715996e-03 0.359893
2019-11-13 04:03:41,354 train 450 9.727818e-03 0.361194
2019-11-13 04:03:49,207 train 500 9.724252e-03 0.356453
2019-11-13 04:03:57,049 train 550 9.715275e-03 0.357968
2019-11-13 04:04:04,882 train 600 9.734093e-03 0.352560
2019-11-13 04:04:12,719 train 650 9.719908e-03 0.352908
2019-11-13 04:04:20,555 train 700 9.719766e-03 0.343414
2019-11-13 04:04:28,443 train 750 9.733826e-03 0.344136
2019-11-13 04:04:36,288 train 800 9.738343e-03 0.334655
2019-11-13 04:04:44,406 train 850 9.743190e-03 0.334864
2019-11-13 04:04:46,864 training loss; R2: 9.738133e-03 0.334265
2019-11-13 04:04:47,154 valid 000 2.021953e+02 -9452.190386
2019-11-13 04:04:48,853 valid 050 2.018416e+02 -11523.904294
2019-11-13 04:04:50,363 validation loss; R2: 2.018370e+02 -11760.937295
2019-11-13 04:04:50,382 epoch 18 lr 1.000000e-03
2019-11-13 04:04:50,807 train 000 1.157063e-02 0.428806
2019-11-13 04:04:58,725 train 050 9.627181e-03 0.346819
2019-11-13 04:05:06,581 train 100 9.823016e-03 0.358843
2019-11-13 04:05:14,432 train 150 9.881497e-03 0.359326
2019-11-13 04:05:22,300 train 200 9.758512e-03 0.359689
2019-11-13 04:05:30,148 train 250 9.730515e-03 0.363017
2019-11-13 04:05:37,997 train 300 9.706552e-03 0.364304
2019-11-13 04:05:45,850 train 350 9.748918e-03 0.342408
2019-11-13 04:05:53,703 train 400 9.732262e-03 0.345876
2019-11-13 04:06:01,552 train 450 9.732799e-03 0.343827
2019-11-13 04:06:09,401 train 500 9.716101e-03 0.340751
2019-11-13 04:06:17,263 train 550 9.707677e-03 0.340391
2019-11-13 04:06:25,111 train 600 9.703508e-03 0.345017
2019-11-13 04:06:32,961 train 650 9.734000e-03 0.348634
2019-11-13 04:06:40,807 train 700 9.746899e-03 0.349542
2019-11-13 04:06:48,657 train 750 9.746685e-03 0.349666
2019-11-13 04:06:56,501 train 800 9.757911e-03 0.350631
2019-11-13 04:07:04,362 train 850 9.758447e-03 0.344157
2019-11-13 04:07:06,716 training loss; R2: 9.752828e-03 0.344626
2019-11-13 04:07:07,039 valid 000 1.222005e+02 -4996.719526
2019-11-13 04:07:08,763 valid 050 1.220658e+02 -5255.791882
2019-11-13 04:07:10,319 validation loss; R2: 1.220710e+02 -5154.337554
2019-11-13 04:07:10,339 epoch 19 lr 1.000000e-03
2019-11-13 04:07:10,771 train 000 9.093809e-03 0.415735
2019-11-13 04:07:18,835 train 050 9.798225e-03 0.356051
2019-11-13 04:07:26,851 train 100 9.660605e-03 0.360613
2019-11-13 04:07:34,783 train 150 9.735786e-03 0.358514
2019-11-13 04:07:42,726 train 200 9.716488e-03 0.363565
2019-11-13 04:07:50,679 train 250 9.647836e-03 0.359468
2019-11-13 04:07:58,623 train 300 9.694860e-03 0.308003
2019-11-13 04:08:06,574 train 350 9.701534e-03 0.314018
2019-11-13 04:08:14,582 train 400 9.696584e-03 0.319610
2019-11-13 04:08:22,554 train 450 9.678510e-03 0.320417
2019-11-13 04:08:30,529 train 500 9.692568e-03 0.326400
2019-11-13 04:08:38,453 train 550 9.663465e-03 0.331121
2019-11-13 04:08:46,370 train 600 9.684133e-03 0.335306
2019-11-13 04:08:54,357 train 650 9.687140e-03 0.338918
2019-11-13 04:09:02,286 train 700 9.692552e-03 0.340267
2019-11-13 04:09:10,282 train 750 9.718130e-03 0.341749
2019-11-13 04:09:18,352 train 800 9.723189e-03 0.343177
2019-11-13 04:09:26,392 train 850 9.730554e-03 0.342964
2019-11-13 04:09:28,766 training loss; R2: 9.723894e-03 0.343282
2019-11-13 04:09:29,071 valid 000 4.910019e+01 -3845.271564
2019-11-13 04:09:30,814 valid 050 4.924835e+01 -4905.773779
2019-11-13 04:09:32,389 validation loss; R2: 4.925013e+01 -4649.300202
