2019-12-04 02:23:54,286 gpu device = 1
2019-12-04 02:23:54,286 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=96, cutout=False, cutout_length=16, data='../data', dataset='graphene', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name=None, gpu=1, grad_clip=5, gz_dtree=False, init_channels=36, layers=8, learning_rate=0.002, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-graphene-EXP-20191204-022354', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-12-04 02:23:57,489 param size = 1.035037MB
2019-12-04 02:23:57,492 epoch 0 lr 2.000000e-03
2019-12-04 02:24:00,072 train 000 9.800795e-01 -30.128127
2019-12-04 02:24:11,493 train 050 2.668531e-01 -6.793594
2019-12-04 02:24:22,811 train 100 1.534720e-01 -3.612404
2019-12-04 02:24:34,131 train 150 1.130335e-01 -2.410145
2019-12-04 02:24:45,450 train 200 9.212245e-02 -1.799508
2019-12-04 02:24:51,599 training loss; R2: 8.593203e-02 -1.609286
2019-12-04 02:24:51,736 valid 000 1.411320e-02 0.086257
2019-12-04 02:24:53,244 validation loss; R2: 2.444944e-02 0.216190
2019-12-04 02:24:53,269 epoch 1 lr 2.000000e-03
2019-12-04 02:24:53,654 train 000 2.821802e-02 0.144297
2019-12-04 02:25:04,931 train 050 2.595553e-02 0.144721
2019-12-04 02:25:15,994 train 100 2.593970e-02 0.161835
2019-12-04 02:25:27,069 train 150 2.662648e-02 0.165362
2019-12-04 02:25:38,146 train 200 2.580699e-02 0.186427
2019-12-04 02:25:43,127 training loss; R2: 2.523977e-02 0.198245
2019-12-04 02:25:43,263 valid 000 1.389666e-02 0.505208
2019-12-04 02:25:44,451 validation loss; R2: 1.562930e-02 0.483800
2019-12-04 02:25:44,470 epoch 2 lr 2.000000e-03
2019-12-04 02:25:44,773 train 000 2.048334e-02 0.433043
2019-12-04 02:25:55,829 train 050 1.910431e-02 0.382569
2019-12-04 02:26:06,886 train 100 1.914238e-02 0.383676
2019-12-04 02:26:17,943 train 150 1.913464e-02 0.394805
2019-12-04 02:26:29,003 train 200 1.826617e-02 0.415751
2019-12-04 02:26:33,976 training loss; R2: 1.780368e-02 0.427102
2019-12-04 02:26:34,106 valid 000 4.991294e-03 0.561878
2019-12-04 02:26:35,295 validation loss; R2: 8.970273e-03 0.705049
2019-12-04 02:26:35,320 epoch 3 lr 2.000000e-03
2019-12-04 02:26:35,637 train 000 1.106598e-02 0.579850
2019-12-04 02:26:46,692 train 050 1.567585e-02 0.486503
2019-12-04 02:26:57,752 train 100 1.395549e-02 0.517155
2019-12-04 02:27:08,808 train 150 1.363630e-02 0.541200
2019-12-04 02:27:19,866 train 200 1.367654e-02 0.550502
2019-12-04 02:27:24,839 training loss; R2: 1.368667e-02 0.552870
2019-12-04 02:27:24,970 valid 000 1.547913e-02 0.781162
2019-12-04 02:27:26,159 validation loss; R2: 6.470893e-03 0.788394
2019-12-04 02:27:26,178 epoch 4 lr 2.000000e-03
2019-12-04 02:27:26,484 train 000 2.888945e-02 0.500475
2019-12-04 02:27:37,553 train 050 1.332058e-02 0.552940
2019-12-04 02:27:48,612 train 100 1.343860e-02 0.578432
2019-12-04 02:27:59,676 train 150 1.280831e-02 0.583010
2019-12-04 02:28:10,735 train 200 1.255443e-02 0.586055
2019-12-04 02:28:15,709 training loss; R2: 1.231205e-02 0.593495
2019-12-04 02:28:15,840 valid 000 1.884770e-02 0.659680
2019-12-04 02:28:17,029 validation loss; R2: 9.083365e-03 0.708193
2019-12-04 02:28:17,048 epoch 5 lr 2.000000e-03
2019-12-04 02:28:17,354 train 000 8.320407e-03 0.618653
2019-12-04 02:28:28,429 train 050 1.225004e-02 0.605752
2019-12-04 02:28:39,502 train 100 1.124100e-02 0.638995
2019-12-04 02:28:50,567 train 150 1.143471e-02 0.634063
2019-12-04 02:29:01,633 train 200 1.124294e-02 0.632470
2019-12-04 02:29:06,607 training loss; R2: 1.099603e-02 0.635398
2019-12-04 02:29:06,752 valid 000 1.316669e-02 0.803653
2019-12-04 02:29:07,941 validation loss; R2: 5.585427e-03 0.815740
2019-12-04 02:29:07,962 epoch 6 lr 2.000000e-03
2019-12-04 02:29:08,269 train 000 6.169083e-03 0.342121
2019-12-04 02:29:19,331 train 050 9.656564e-03 0.663235
2019-12-04 02:29:30,395 train 100 1.049476e-02 0.656125
2019-12-04 02:29:41,459 train 150 1.024836e-02 0.662904
2019-12-04 02:29:52,519 train 200 1.025526e-02 0.666474
2019-12-04 02:29:57,493 training loss; R2: 1.019055e-02 0.665412
2019-12-04 02:29:57,625 valid 000 6.993147e-03 0.824885
2019-12-04 02:29:58,814 validation loss; R2: 5.774622e-03 0.806793
2019-12-04 02:29:58,833 epoch 7 lr 2.000000e-03
2019-12-04 02:29:59,138 train 000 4.713560e-03 0.880721
2019-12-04 02:30:10,196 train 050 9.458850e-03 0.681433
2019-12-04 02:30:21,250 train 100 8.880331e-03 0.695826
2019-12-04 02:30:32,307 train 150 8.567928e-03 0.704401
2019-12-04 02:30:43,362 train 200 8.708013e-03 0.710232
2019-12-04 02:30:48,334 training loss; R2: 8.675803e-03 0.708492
2019-12-04 02:30:48,464 valid 000 3.749327e-03 0.913061
2019-12-04 02:30:49,653 validation loss; R2: 4.887094e-03 0.838425
2019-12-04 02:30:49,673 epoch 8 lr 2.000000e-03
2019-12-04 02:30:49,977 train 000 8.922782e-03 0.651616
2019-12-04 02:31:01,031 train 050 9.439983e-03 0.684017
2019-12-04 02:31:12,084 train 100 9.885840e-03 0.668792
2019-12-04 02:31:23,141 train 150 1.047132e-02 0.654578
2019-12-04 02:31:34,197 train 200 9.956312e-03 0.671970
2019-12-04 02:31:39,167 training loss; R2: 9.842461e-03 0.674501
2019-12-04 02:31:39,297 valid 000 4.511101e-03 0.733732
2019-12-04 02:31:40,486 validation loss; R2: 6.578890e-03 0.762416
2019-12-04 02:31:40,506 epoch 9 lr 2.000000e-03
2019-12-04 02:31:40,810 train 000 5.091171e-03 0.804875
2019-12-04 02:31:51,871 train 050 9.121406e-03 0.705751
2019-12-04 02:32:02,927 train 100 8.147640e-03 0.730951
2019-12-04 02:32:13,996 train 150 8.211692e-03 0.725681
2019-12-04 02:32:25,055 train 200 8.346822e-03 0.721493
2019-12-04 02:32:30,031 training loss; R2: 8.277983e-03 0.720982
2019-12-04 02:32:30,167 valid 000 7.956519e-03 0.832383
2019-12-04 02:32:31,356 validation loss; R2: 4.949187e-03 0.839029
2019-12-04 02:32:31,375 epoch 10 lr 2.000000e-03
2019-12-04 02:32:31,681 train 000 7.876346e-03 0.741030
2019-12-04 02:32:42,738 train 050 9.484268e-03 0.679496
2019-12-04 02:32:53,790 train 100 9.131549e-03 0.698804
2019-12-04 02:33:04,843 train 150 8.918446e-03 0.711178
2019-12-04 02:33:15,898 train 200 8.313719e-03 0.723660
2019-12-04 02:33:20,866 training loss; R2: 8.302209e-03 0.723468
2019-12-04 02:33:21,005 valid 000 8.240399e-03 0.833576
2019-12-04 02:33:22,194 validation loss; R2: 5.177501e-03 0.829111
2019-12-04 02:33:22,214 epoch 11 lr 2.000000e-03
2019-12-04 02:33:22,521 train 000 6.787398e-03 0.688771
2019-12-04 02:33:33,570 train 050 7.744961e-03 0.738568
2019-12-04 02:33:44,616 train 100 7.611100e-03 0.750807
2019-12-04 02:33:55,668 train 150 7.467797e-03 0.746729
2019-12-04 02:34:06,715 train 200 7.614442e-03 0.744909
2019-12-04 02:34:11,685 training loss; R2: 7.488833e-03 0.747829
2019-12-04 02:34:11,813 valid 000 3.444551e-03 0.877446
2019-12-04 02:34:13,003 validation loss; R2: 3.692865e-03 0.876269
2019-12-04 02:34:13,024 epoch 12 lr 2.000000e-03
2019-12-04 02:34:13,334 train 000 7.293600e-03 0.749170
2019-12-04 02:34:24,394 train 050 7.120099e-03 0.763833
2019-12-04 02:34:35,454 train 100 7.108013e-03 0.757530
2019-12-04 02:34:46,510 train 150 7.234573e-03 0.762570
2019-12-04 02:34:57,562 train 200 7.208911e-03 0.757770
2019-12-04 02:35:02,532 training loss; R2: 7.183679e-03 0.757155
2019-12-04 02:35:02,673 valid 000 2.014930e-03 0.915430
2019-12-04 02:35:03,862 validation loss; R2: 3.306739e-03 0.881320
2019-12-04 02:35:03,883 epoch 13 lr 2.000000e-03
2019-12-04 02:35:04,192 train 000 8.380688e-03 0.517738
2019-12-04 02:35:15,237 train 050 7.248565e-03 0.742204
2019-12-04 02:35:26,278 train 100 7.497804e-03 0.748278
2019-12-04 02:35:37,325 train 150 7.344674e-03 0.746051
2019-12-04 02:35:48,366 train 200 7.849619e-03 0.734454
2019-12-04 02:35:53,339 training loss; R2: 7.844750e-03 0.733710
2019-12-04 02:35:53,469 valid 000 3.661309e-03 0.919271
2019-12-04 02:35:54,658 validation loss; R2: 4.700871e-03 0.839011
2019-12-04 02:35:54,677 epoch 14 lr 2.000000e-03
2019-12-04 02:35:54,982 train 000 2.027695e-02 0.706322
2019-12-04 02:36:06,029 train 050 7.821335e-03 0.723412
2019-12-04 02:36:17,067 train 100 7.016066e-03 0.751358
2019-12-04 02:36:28,108 train 150 6.860134e-03 0.765875
2019-12-04 02:36:39,147 train 200 6.927827e-03 0.764960
2019-12-04 02:36:44,110 training loss; R2: 7.098450e-03 0.760484
2019-12-04 02:36:44,241 valid 000 3.364657e-03 0.820002
2019-12-04 02:36:45,430 validation loss; R2: 3.828016e-03 0.864762
2019-12-04 02:36:45,450 epoch 15 lr 2.000000e-03
2019-12-04 02:36:45,752 train 000 6.143173e-03 0.835281
2019-12-04 02:36:56,795 train 050 8.392195e-03 0.743776
2019-12-04 02:37:07,833 train 100 8.419184e-03 0.734681
2019-12-04 02:37:18,868 train 150 7.819003e-03 0.750257
2019-12-04 02:37:29,904 train 200 7.361489e-03 0.762013
2019-12-04 02:37:34,868 training loss; R2: 7.127738e-03 0.766334
2019-12-04 02:37:34,998 valid 000 4.530143e-03 0.903593
2019-12-04 02:37:36,187 validation loss; R2: 3.779452e-03 0.871272
2019-12-04 02:37:36,207 epoch 16 lr 2.000000e-03
2019-12-04 02:37:36,510 train 000 5.495671e-03 0.889625
2019-12-04 02:37:47,551 train 050 7.222489e-03 0.761206
2019-12-04 02:37:58,590 train 100 7.059172e-03 0.766685
2019-12-04 02:38:09,633 train 150 6.815697e-03 0.768989
2019-12-04 02:38:20,672 train 200 7.003042e-03 0.769488
2019-12-04 02:38:25,635 training loss; R2: 6.963603e-03 0.770498
2019-12-04 02:38:25,766 valid 000 4.496107e-03 0.943223
2019-12-04 02:38:26,955 validation loss; R2: 2.993030e-03 0.887621
2019-12-04 02:38:26,980 epoch 17 lr 2.000000e-03
2019-12-04 02:38:27,286 train 000 4.801320e-03 0.778720
2019-12-04 02:38:38,326 train 050 6.257279e-03 0.778511
2019-12-04 02:38:49,363 train 100 6.505900e-03 0.771591
2019-12-04 02:39:00,404 train 150 6.684990e-03 0.779882
2019-12-04 02:39:11,441 train 200 6.534474e-03 0.778271
2019-12-04 02:39:16,406 training loss; R2: 6.552859e-03 0.779130
2019-12-04 02:39:16,537 valid 000 6.528014e-03 0.852414
2019-12-04 02:39:17,725 validation loss; R2: 4.949106e-03 0.818865
2019-12-04 02:39:17,745 epoch 18 lr 2.000000e-03
2019-12-04 02:39:18,050 train 000 3.151886e-03 0.884438
2019-12-04 02:39:29,087 train 050 6.497584e-03 0.787808
2019-12-04 02:39:40,119 train 100 6.586614e-03 0.781492
2019-12-04 02:39:51,153 train 150 6.582020e-03 0.783516
2019-12-04 02:40:02,187 train 200 6.387081e-03 0.789599
2019-12-04 02:40:07,149 training loss; R2: 6.412113e-03 0.791719
2019-12-04 02:40:07,281 valid 000 2.937921e-03 0.870432
2019-12-04 02:40:08,469 validation loss; R2: 3.944133e-03 0.869282
2019-12-04 02:40:08,490 epoch 19 lr 2.000000e-03
2019-12-04 02:40:08,792 train 000 7.778236e-03 0.731571
2019-12-04 02:40:19,826 train 050 6.023595e-03 0.804093
2019-12-04 02:40:30,857 train 100 6.401199e-03 0.783698
2019-12-04 02:40:41,891 train 150 6.136478e-03 0.790432
2019-12-04 02:40:52,927 train 200 6.172238e-03 0.792245
2019-12-04 02:40:57,887 training loss; R2: 6.244317e-03 0.793717
2019-12-04 02:40:58,020 valid 000 4.412423e-03 0.898930
2019-12-04 02:40:59,209 validation loss; R2: 4.396630e-03 0.854213
