2019-11-13 10:13:23,486 gpu device = 1
2019-11-13 10:13:23,486 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name='galaxy-zoo-cleaned', gpu=1, grad_clip=5, gz_dtree=False, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-galaxy-zoo-EXP-20191113-101323', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-11-13 10:13:35,040 param size = 0.266453MB
2019-11-13 10:13:35,044 epoch 0 lr 1.000000e-03
2019-11-13 10:13:37,231 train 000 3.008535e-01 -210.766367
2019-11-13 10:13:44,189 train 050 4.421015e-02 -10.612226
2019-11-13 10:13:50,977 train 100 3.562794e-02 -5.614196
2019-11-13 10:13:57,746 train 150 3.209088e-02 -3.921810
2019-11-13 10:14:04,493 train 200 3.029891e-02 -3.006570
2019-11-13 10:14:11,131 train 250 2.902921e-02 -2.439625
2019-11-13 10:14:17,818 train 300 2.811965e-02 -2.061182
2019-11-13 10:14:24,315 train 350 2.736093e-02 -1.782723
2019-11-13 10:14:31,139 train 400 2.681073e-02 -1.575246
2019-11-13 10:14:37,630 train 450 2.632106e-02 -1.422352
2019-11-13 10:14:44,236 train 500 2.586673e-02 -1.283272
2019-11-13 10:14:50,722 train 550 2.544674e-02 -1.167581
2019-11-13 10:14:57,201 train 600 2.504422e-02 -1.072342
2019-11-13 10:15:03,677 train 650 2.468796e-02 -0.988072
2019-11-13 10:15:10,148 train 700 2.438096e-02 -0.913856
2019-11-13 10:15:16,619 train 750 2.412641e-02 -0.865871
2019-11-13 10:15:23,096 train 800 2.388343e-02 -0.808360
2019-11-13 10:15:29,566 train 850 2.365945e-02 -0.756726
2019-11-13 10:15:32,283 training loss; R2: 2.358976e-02 -0.743145
2019-11-13 10:15:32,605 valid 000 1.914920e-02 0.042113
2019-11-13 10:15:34,301 valid 050 1.987873e-02 0.022206
2019-11-13 10:15:35,921 validation loss; R2: 1.994308e-02 0.028387
2019-11-13 10:15:35,938 epoch 1 lr 1.000000e-03
2019-11-13 10:15:36,519 train 000 1.847471e-02 0.075660
2019-11-13 10:15:42,987 train 050 1.988512e-02 0.087373
2019-11-13 10:15:49,464 train 100 1.970777e-02 -0.001809
2019-11-13 10:15:55,930 train 150 1.951193e-02 0.020486
2019-11-13 10:16:02,403 train 200 1.944689e-02 0.034691
2019-11-13 10:16:08,870 train 250 1.938047e-02 0.030693
2019-11-13 10:16:15,340 train 300 1.930380e-02 0.037441
2019-11-13 10:16:21,816 train 350 1.931629e-02 0.042561
2019-11-13 10:16:28,288 train 400 1.925762e-02 0.043438
2019-11-13 10:16:34,751 train 450 1.914550e-02 0.045409
2019-11-13 10:16:41,219 train 500 1.905667e-02 0.051277
2019-11-13 10:16:47,691 train 550 1.894858e-02 0.058387
2019-11-13 10:16:54,167 train 600 1.886825e-02 0.063880
2019-11-13 10:17:00,637 train 650 1.876340e-02 0.070627
2019-11-13 10:17:07,104 train 700 1.864991e-02 0.068637
2019-11-13 10:17:13,571 train 750 1.855692e-02 0.072594
2019-11-13 10:17:20,041 train 800 1.846578e-02 0.077536
2019-11-13 10:17:26,566 train 850 1.836352e-02 0.083174
2019-11-13 10:17:28,508 training loss; R2: 1.833217e-02 0.084331
2019-11-13 10:17:28,798 valid 000 1.711064e-02 0.129237
2019-11-13 10:17:30,506 valid 050 1.601763e-02 0.152265
2019-11-13 10:17:32,070 validation loss; R2: 1.588167e-02 0.150000
2019-11-13 10:17:32,087 epoch 2 lr 1.000000e-03
2019-11-13 10:17:32,454 train 000 1.676675e-02 0.164375
2019-11-13 10:17:39,160 train 050 1.705553e-02 0.138464
2019-11-13 10:17:45,747 train 100 1.695043e-02 0.148684
2019-11-13 10:17:52,242 train 150 1.679056e-02 0.152884
2019-11-13 10:17:58,745 train 200 1.669410e-02 0.156836
2019-11-13 10:18:05,241 train 250 1.659969e-02 0.157958
2019-11-13 10:18:11,731 train 300 1.658782e-02 0.159662
2019-11-13 10:18:18,231 train 350 1.652212e-02 0.165931
2019-11-13 10:18:24,723 train 400 1.647284e-02 0.169547
2019-11-13 10:18:31,217 train 450 1.646607e-02 0.169995
2019-11-13 10:18:37,710 train 500 1.644067e-02 0.171201
2019-11-13 10:18:44,201 train 550 1.640680e-02 0.172841
2019-11-13 10:18:50,699 train 600 1.636805e-02 0.172806
2019-11-13 10:18:57,191 train 650 1.631631e-02 0.172580
2019-11-13 10:19:03,680 train 700 1.626437e-02 0.171721
2019-11-13 10:19:10,166 train 750 1.618532e-02 0.171388
2019-11-13 10:19:16,646 train 800 1.613439e-02 0.171974
2019-11-13 10:19:23,132 train 850 1.610582e-02 0.174389
2019-11-13 10:19:25,069 training loss; R2: 1.608320e-02 0.175215
2019-11-13 10:19:25,377 valid 000 1.263713e-02 0.031462
2019-11-13 10:19:27,078 valid 050 1.417766e-02 0.256810
2019-11-13 10:19:28,632 validation loss; R2: 1.408229e-02 0.268424
2019-11-13 10:19:28,649 epoch 3 lr 1.000000e-03
2019-11-13 10:19:29,062 train 000 1.381321e-02 0.288160
2019-11-13 10:19:35,909 train 050 1.476444e-02 0.025710
2019-11-13 10:19:42,766 train 100 1.508195e-02 0.102676
2019-11-13 10:19:49,557 train 150 1.494704e-02 0.143160
2019-11-13 10:19:56,212 train 200 1.482732e-02 0.159361
2019-11-13 10:20:02,991 train 250 1.485137e-02 0.164474
2019-11-13 10:20:09,818 train 300 1.475963e-02 0.054184
2019-11-13 10:20:16,522 train 350 1.470981e-02 0.068424
2019-11-13 10:20:23,219 train 400 1.469856e-02 0.087514
2019-11-13 10:20:30,009 train 450 1.468118e-02 0.098529
2019-11-13 10:20:36,693 train 500 1.468390e-02 0.101147
2019-11-13 10:20:43,537 train 550 1.466705e-02 0.111114
2019-11-13 10:20:50,334 train 600 1.463703e-02 0.121978
2019-11-13 10:20:57,164 train 650 1.462324e-02 0.130434
2019-11-13 10:21:03,940 train 700 1.458502e-02 0.137263
2019-11-13 10:21:10,605 train 750 1.453764e-02 0.144771
2019-11-13 10:21:17,456 train 800 1.449587e-02 0.150255
2019-11-13 10:21:24,286 train 850 1.447800e-02 0.153239
2019-11-13 10:21:26,279 training loss; R2: 1.447010e-02 0.153759
2019-11-13 10:21:26,553 valid 000 1.296308e-02 0.277431
2019-11-13 10:21:28,223 valid 050 1.227331e-02 0.298985
2019-11-13 10:21:29,726 validation loss; R2: 1.232963e-02 0.292432
2019-11-13 10:21:29,748 epoch 4 lr 1.000000e-03
2019-11-13 10:21:30,147 train 000 1.366126e-02 0.297168
2019-11-13 10:21:36,989 train 050 1.399395e-02 0.224627
2019-11-13 10:21:43,758 train 100 1.396090e-02 0.216503
2019-11-13 10:21:50,404 train 150 1.387333e-02 0.209921
2019-11-13 10:21:57,205 train 200 1.386531e-02 -0.170278
2019-11-13 10:22:03,855 train 250 1.382225e-02 -0.087882
2019-11-13 10:22:10,580 train 300 1.378598e-02 -0.031826
2019-11-13 10:22:17,362 train 350 1.374918e-02 0.010900
2019-11-13 10:22:24,052 train 400 1.377369e-02 0.037702
2019-11-13 10:22:30,914 train 450 1.372226e-02 0.055177
2019-11-13 10:22:37,778 train 500 1.367806e-02 0.073939
2019-11-13 10:22:44,555 train 550 1.364496e-02 0.087253
2019-11-13 10:22:51,144 train 600 1.364182e-02 0.097915
2019-11-13 10:22:57,924 train 650 1.365065e-02 0.110692
2019-11-13 10:23:04,560 train 700 1.361730e-02 0.120012
2019-11-13 10:23:11,307 train 750 1.359014e-02 0.128968
2019-11-13 10:23:18,080 train 800 1.355975e-02 0.138897
2019-11-13 10:23:24,934 train 850 1.353165e-02 0.147711
2019-11-13 10:23:26,928 training loss; R2: 1.352527e-02 0.147705
2019-11-13 10:23:27,171 valid 000 1.236964e-02 0.131896
2019-11-13 10:23:28,855 valid 050 1.219507e-02 0.334982
2019-11-13 10:23:30,379 validation loss; R2: 1.215737e-02 0.340658
2019-11-13 10:23:30,395 epoch 5 lr 1.000000e-03
2019-11-13 10:23:30,739 train 000 1.544674e-02 0.265900
2019-11-13 10:23:37,367 train 050 1.339572e-02 0.252221
2019-11-13 10:23:44,005 train 100 1.322274e-02 0.258650
2019-11-13 10:23:50,712 train 150 1.317983e-02 0.258856
2019-11-13 10:23:57,228 train 200 1.313102e-02 0.253178
2019-11-13 10:24:03,917 train 250 1.309340e-02 0.256332
2019-11-13 10:24:10,709 train 300 1.308485e-02 0.259223
2019-11-13 10:24:17,248 train 350 1.305977e-02 0.257316
2019-11-13 10:24:23,864 train 400 1.306885e-02 0.252966
2019-11-13 10:24:30,559 train 450 1.308306e-02 0.254719
2019-11-13 10:24:37,301 train 500 1.306350e-02 0.258537
2019-11-13 10:24:44,088 train 550 1.304339e-02 0.260547
2019-11-13 10:24:50,724 train 600 1.305037e-02 0.247320
2019-11-13 10:24:57,209 train 650 1.302914e-02 0.249104
2019-11-13 10:25:03,848 train 700 1.304237e-02 0.252852
2019-11-13 10:25:10,335 train 750 1.304076e-02 0.254786
2019-11-13 10:25:16,812 train 800 1.302730e-02 0.255036
2019-11-13 10:25:23,284 train 850 1.301034e-02 0.255446
2019-11-13 10:25:25,218 training loss; R2: 1.300623e-02 0.256207
2019-11-13 10:25:25,534 valid 000 1.283711e-02 0.332659
2019-11-13 10:25:27,217 valid 050 1.181592e-02 0.344045
2019-11-13 10:25:28,722 validation loss; R2: 1.179389e-02 0.342679
2019-11-13 10:25:28,744 epoch 6 lr 1.000000e-03
2019-11-13 10:25:29,198 train 000 1.116559e-02 0.343857
2019-11-13 10:25:35,979 train 050 1.225275e-02 0.266803
2019-11-13 10:25:42,463 train 100 1.250202e-02 0.277294
2019-11-13 10:25:48,942 train 150 1.253682e-02 0.258925
2019-11-13 10:25:55,409 train 200 1.253785e-02 0.255279
2019-11-13 10:26:01,872 train 250 1.256264e-02 0.256280
2019-11-13 10:26:08,330 train 300 1.256736e-02 0.259339
2019-11-13 10:26:14,800 train 350 1.261870e-02 0.262246
2019-11-13 10:26:21,263 train 400 1.263301e-02 0.256504
2019-11-13 10:26:27,727 train 450 1.260117e-02 0.253910
2019-11-13 10:26:34,188 train 500 1.260167e-02 0.251883
2019-11-13 10:26:40,649 train 550 1.258316e-02 0.256491
2019-11-13 10:26:47,116 train 600 1.258736e-02 0.258282
2019-11-13 10:26:53,636 train 650 1.259127e-02 0.261565
2019-11-13 10:27:00,101 train 700 1.260563e-02 0.264266
2019-11-13 10:27:06,576 train 750 1.258930e-02 0.060203
2019-11-13 10:27:13,050 train 800 1.255401e-02 0.073740
2019-11-13 10:27:19,522 train 850 1.256105e-02 0.086298
2019-11-13 10:27:21,455 training loss; R2: 1.255657e-02 0.089706
2019-11-13 10:27:21,764 valid 000 1.274270e-02 0.365065
2019-11-13 10:27:23,491 valid 050 1.199560e-02 0.342679
2019-11-13 10:27:25,042 validation loss; R2: 1.200092e-02 0.344561
2019-11-13 10:27:25,060 epoch 7 lr 1.000000e-03
2019-11-13 10:27:25,432 train 000 1.190135e-02 0.263317
2019-11-13 10:27:32,223 train 050 1.215495e-02 0.314448
2019-11-13 10:27:38,872 train 100 1.217036e-02 0.311438
2019-11-13 10:27:45,354 train 150 1.217362e-02 0.296678
2019-11-13 10:27:51,827 train 200 1.221567e-02 0.284804
2019-11-13 10:27:58,293 train 250 1.227577e-02 0.281871
2019-11-13 10:28:04,762 train 300 1.228915e-02 0.281133
2019-11-13 10:28:11,240 train 350 1.227567e-02 0.274337
2019-11-13 10:28:17,717 train 400 1.226640e-02 0.277740
2019-11-13 10:28:24,188 train 450 1.226739e-02 0.281076
2019-11-13 10:28:30,658 train 500 1.228945e-02 0.282066
2019-11-13 10:28:37,124 train 550 1.226434e-02 0.284011
2019-11-13 10:28:43,594 train 600 1.226338e-02 0.282665
2019-11-13 10:28:50,066 train 650 1.225381e-02 0.281333
2019-11-13 10:28:56,639 train 700 1.223239e-02 0.277008
2019-11-13 10:29:03,103 train 750 1.224769e-02 0.277492
2019-11-13 10:29:09,566 train 800 1.225057e-02 0.271903
2019-11-13 10:29:16,035 train 850 1.223559e-02 0.271865
2019-11-13 10:29:17,969 training loss; R2: 1.223968e-02 0.272215
2019-11-13 10:29:18,274 valid 000 5.410187e-02 -3.190614
2019-11-13 10:29:19,997 valid 050 5.453160e-02 -3.437557
2019-11-13 10:29:21,552 validation loss; R2: 5.465143e-02 -7.121216
2019-11-13 10:29:21,577 epoch 8 lr 1.000000e-03
2019-11-13 10:29:21,972 train 000 1.143235e-02 0.223968
2019-11-13 10:29:28,568 train 050 1.185103e-02 0.315686
2019-11-13 10:29:35,070 train 100 1.201238e-02 0.319902
2019-11-13 10:29:41,618 train 150 1.217264e-02 0.294092
2019-11-13 10:29:48,098 train 200 1.219464e-02 0.283663
2019-11-13 10:29:54,580 train 250 1.213991e-02 0.288106
2019-11-13 10:30:01,056 train 300 1.216076e-02 0.283976
2019-11-13 10:30:07,616 train 350 1.210928e-02 0.287497
2019-11-13 10:30:14,083 train 400 1.209889e-02 0.289047
2019-11-13 10:30:20,550 train 450 1.208040e-02 0.287849
2019-11-13 10:30:27,016 train 500 1.208400e-02 0.290462
2019-11-13 10:30:33,482 train 550 1.208014e-02 0.284145
2019-11-13 10:30:39,944 train 600 1.208708e-02 0.280589
2019-11-13 10:30:46,415 train 650 1.209711e-02 0.281745
2019-11-13 10:30:52,878 train 700 1.209917e-02 0.283169
2019-11-13 10:30:59,373 train 750 1.206273e-02 0.284142
2019-11-13 10:31:05,838 train 800 1.204686e-02 0.285240
2019-11-13 10:31:12,313 train 850 1.204523e-02 0.285093
2019-11-13 10:31:14,247 training loss; R2: 1.204467e-02 0.285499
2019-11-13 10:31:14,551 valid 000 6.113943e-01 -18.653039
2019-11-13 10:31:16,272 valid 050 6.129327e-01 -21.982058
2019-11-13 10:31:17,826 validation loss; R2: 6.121343e-01 -23.886969
2019-11-13 10:31:17,850 epoch 9 lr 1.000000e-03
2019-11-13 10:31:18,221 train 000 1.224362e-02 0.374559
2019-11-13 10:31:24,787 train 050 1.179200e-02 0.321645
2019-11-13 10:31:31,265 train 100 1.175590e-02 0.290590
2019-11-13 10:31:37,755 train 150 1.185297e-02 0.284987
2019-11-13 10:31:44,228 train 200 1.191727e-02 0.288402
2019-11-13 10:31:50,701 train 250 1.190909e-02 0.284525
2019-11-13 10:31:57,163 train 300 1.195472e-02 0.289148
2019-11-13 10:32:03,631 train 350 1.194194e-02 0.280139
2019-11-13 10:32:10,104 train 400 1.198288e-02 0.202855
2019-11-13 10:32:16,567 train 450 1.194502e-02 0.194598
2019-11-13 10:32:23,034 train 500 1.194027e-02 0.204874
2019-11-13 10:32:29,500 train 550 1.191625e-02 0.211056
2019-11-13 10:32:35,970 train 600 1.189514e-02 0.213183
2019-11-13 10:32:42,442 train 650 1.188726e-02 0.220593
2019-11-13 10:32:48,906 train 700 1.188751e-02 0.227184
2019-11-13 10:32:55,369 train 750 1.187263e-02 0.231840
2019-11-13 10:33:01,847 train 800 1.185509e-02 0.234150
2019-11-13 10:33:08,325 train 850 1.184308e-02 0.238492
2019-11-13 10:33:10,260 training loss; R2: 1.183419e-02 0.239961
2019-11-13 10:33:10,567 valid 000 4.978220e-01 -20.135871
2019-11-13 10:33:12,298 valid 050 4.918962e-01 -32.320946
2019-11-13 10:33:13,859 validation loss; R2: 4.919101e-01 -33.350472
2019-11-13 10:33:13,883 epoch 10 lr 1.000000e-03
2019-11-13 10:33:14,252 train 000 1.266276e-02 0.292469
2019-11-13 10:33:20,949 train 050 1.197573e-02 0.311882
2019-11-13 10:33:27,525 train 100 1.179216e-02 0.314048
2019-11-13 10:33:34,035 train 150 1.173077e-02 -0.463485
2019-11-13 10:33:40,511 train 200 1.166805e-02 -0.269099
2019-11-13 10:33:46,990 train 250 1.167513e-02 -0.185081
2019-11-13 10:33:53,462 train 300 1.169452e-02 -0.100251
2019-11-13 10:33:59,930 train 350 1.167176e-02 -0.041088
2019-11-13 10:34:06,396 train 400 1.169930e-02 0.002647
2019-11-13 10:34:12,859 train 450 1.168333e-02 0.035857
2019-11-13 10:34:19,322 train 500 1.164882e-02 0.062721
2019-11-13 10:34:25,786 train 550 1.164492e-02 0.085076
2019-11-13 10:34:32,254 train 600 1.167311e-02 0.103938
2019-11-13 10:34:38,719 train 650 1.168344e-02 0.114333
2019-11-13 10:34:45,311 train 700 1.166034e-02 0.125878
2019-11-13 10:34:51,780 train 750 1.166653e-02 0.138569
2019-11-13 10:34:58,346 train 800 1.164334e-02 0.151045
2019-11-13 10:35:04,826 train 850 1.165636e-02 0.160421
2019-11-13 10:35:06,758 training loss; R2: 1.166233e-02 0.163277
2019-11-13 10:35:07,077 valid 000 4.278962e-01 -22.332419
2019-11-13 10:35:08,808 valid 050 4.213349e-01 -26.074468
2019-11-13 10:35:10,359 validation loss; R2: 4.219442e-01 -24.790536
2019-11-13 10:35:10,381 epoch 11 lr 1.000000e-03
2019-11-13 10:35:10,792 train 000 1.027791e-02 0.367231
2019-11-13 10:35:17,514 train 050 1.139397e-02 0.320377
2019-11-13 10:35:24,030 train 100 1.155492e-02 0.322110
2019-11-13 10:35:30,543 train 150 1.157167e-02 0.324888
2019-11-13 10:35:37,055 train 200 1.159712e-02 0.324827
2019-11-13 10:35:43,553 train 250 1.156808e-02 0.325736
2019-11-13 10:35:50,049 train 300 1.154085e-02 0.321317
2019-11-13 10:35:56,522 train 350 1.149914e-02 0.299785
2019-11-13 10:36:02,995 train 400 1.151488e-02 0.300532
2019-11-13 10:36:09,469 train 450 1.152370e-02 0.293344
2019-11-13 10:36:15,948 train 500 1.154700e-02 0.291593
2019-11-13 10:36:22,422 train 550 1.152803e-02 0.290814
2019-11-13 10:36:28,893 train 600 1.150453e-02 0.294861
2019-11-13 10:36:35,373 train 650 1.150611e-02 0.296767
2019-11-13 10:36:41,850 train 700 1.151597e-02 0.298048
2019-11-13 10:36:48,325 train 750 1.151453e-02 0.298084
2019-11-13 10:36:54,802 train 800 1.150324e-02 0.299136
2019-11-13 10:37:01,274 train 850 1.152234e-02 0.300780
2019-11-13 10:37:03,209 training loss; R2: 1.153175e-02 0.293166
2019-11-13 10:37:03,514 valid 000 3.298637e-01 -17.463249
2019-11-13 10:37:05,215 valid 050 3.402303e-01 -17.877909
2019-11-13 10:37:06,749 validation loss; R2: 3.392889e-01 -18.292420
2019-11-13 10:37:06,765 epoch 12 lr 1.000000e-03
2019-11-13 10:37:07,151 train 000 1.039766e-02 0.418428
2019-11-13 10:37:13,683 train 050 1.135821e-02 0.285164
2019-11-13 10:37:20,229 train 100 1.140976e-02 0.290000
2019-11-13 10:37:26,817 train 150 1.142944e-02 0.296345
2019-11-13 10:37:33,293 train 200 1.137479e-02 0.293476
2019-11-13 10:37:39,751 train 250 1.138700e-02 0.301085
2019-11-13 10:37:46,204 train 300 1.139926e-02 0.306735
2019-11-13 10:37:52,666 train 350 1.140725e-02 0.309338
2019-11-13 10:37:59,118 train 400 1.139097e-02 0.310802
2019-11-13 10:38:05,571 train 450 1.140754e-02 0.312070
2019-11-13 10:38:12,027 train 500 1.142853e-02 0.312824
2019-11-13 10:38:18,481 train 550 1.143928e-02 0.312642
2019-11-13 10:38:24,938 train 600 1.142420e-02 0.314582
2019-11-13 10:38:31,387 train 650 1.143447e-02 0.314306
2019-11-13 10:38:37,841 train 700 1.143132e-02 0.312201
2019-11-13 10:38:44,294 train 750 1.142808e-02 0.309917
2019-11-13 10:38:50,749 train 800 1.142060e-02 0.312475
2019-11-13 10:38:57,212 train 850 1.141957e-02 0.311469
2019-11-13 10:38:59,141 training loss; R2: 1.141697e-02 0.311565
2019-11-13 10:38:59,456 valid 000 1.179875e-02 0.301109
2019-11-13 10:39:01,188 valid 050 1.228929e-02 0.333563
2019-11-13 10:39:02,751 validation loss; R2: 1.225163e-02 0.316472
2019-11-13 10:39:02,774 epoch 13 lr 1.000000e-03
2019-11-13 10:39:03,151 train 000 1.349836e-02 0.291376
2019-11-13 10:39:09,911 train 050 1.125856e-02 0.310620
2019-11-13 10:39:16,642 train 100 1.146920e-02 0.295844
2019-11-13 10:39:23,206 train 150 1.142930e-02 0.308627
2019-11-13 10:39:29,946 train 200 1.140536e-02 0.314994
2019-11-13 10:39:36,490 train 250 1.139339e-02 0.317224
2019-11-13 10:39:42,983 train 300 1.136203e-02 0.317435
2019-11-13 10:39:49,468 train 350 1.137783e-02 0.321289
2019-11-13 10:39:55,951 train 400 1.136024e-02 0.312703
2019-11-13 10:40:02,427 train 450 1.136225e-02 0.309070
2019-11-13 10:40:08,903 train 500 1.137495e-02 0.310549
2019-11-13 10:40:15,383 train 550 1.137125e-02 0.237439
2019-11-13 10:40:21,854 train 600 1.139362e-02 0.242455
2019-11-13 10:40:28,326 train 650 1.140688e-02 0.248123
2019-11-13 10:40:34,792 train 700 1.141530e-02 0.245278
2019-11-13 10:40:41,255 train 750 1.143200e-02 0.249137
2019-11-13 10:40:47,722 train 800 1.141260e-02 0.254065
2019-11-13 10:40:54,187 train 850 1.140006e-02 0.257699
2019-11-13 10:40:56,125 training loss; R2: 1.139580e-02 0.258877
2019-11-13 10:40:56,443 valid 000 1.520429e-02 0.102790
2019-11-13 10:40:58,165 valid 050 1.446893e-02 0.219155
2019-11-13 10:40:59,710 validation loss; R2: 1.438277e-02 0.208181
2019-11-13 10:40:59,727 epoch 14 lr 1.000000e-03
2019-11-13 10:41:00,130 train 000 1.215526e-02 0.198556
2019-11-13 10:41:06,638 train 050 1.115475e-02 0.307166
2019-11-13 10:41:13,116 train 100 1.128491e-02 0.251299
2019-11-13 10:41:19,582 train 150 1.127419e-02 0.238203
2019-11-13 10:41:26,046 train 200 1.124632e-02 0.261305
2019-11-13 10:41:32,516 train 250 1.125169e-02 0.274531
2019-11-13 10:41:38,980 train 300 1.128089e-02 0.282407
2019-11-13 10:41:45,444 train 350 1.127620e-02 0.282188
2019-11-13 10:41:51,906 train 400 1.126237e-02 0.287247
2019-11-13 10:41:58,387 train 450 1.123399e-02 0.289175
2019-11-13 10:42:04,850 train 500 1.124174e-02 0.291043
2019-11-13 10:42:11,310 train 550 1.127057e-02 0.293483
2019-11-13 10:42:17,761 train 600 1.127903e-02 0.295983
2019-11-13 10:42:24,225 train 650 1.131436e-02 0.296900
2019-11-13 10:42:30,682 train 700 1.133495e-02 0.299018
2019-11-13 10:42:37,129 train 750 1.134193e-02 0.295889
2019-11-13 10:42:43,579 train 800 1.132984e-02 0.292320
2019-11-13 10:42:50,026 train 850 1.131717e-02 0.293483
2019-11-13 10:42:51,958 training loss; R2: 1.132868e-02 0.293600
2019-11-13 10:42:52,259 valid 000 1.339222e-02 0.311445
2019-11-13 10:42:53,987 valid 050 1.239911e-02 0.297380
2019-11-13 10:42:55,542 validation loss; R2: 1.222543e-02 0.278366
2019-11-13 10:42:55,565 epoch 15 lr 1.000000e-03
2019-11-13 10:42:55,972 train 000 9.519764e-03 0.359506
2019-11-13 10:43:02,468 train 050 1.120020e-02 0.296877
2019-11-13 10:43:08,919 train 100 1.151219e-02 0.244294
2019-11-13 10:43:15,376 train 150 1.143613e-02 0.271805
2019-11-13 10:43:21,913 train 200 1.133979e-02 0.282556
2019-11-13 10:43:28,369 train 250 1.133701e-02 0.278727
2019-11-13 10:43:34,820 train 300 1.136290e-02 0.285360
2019-11-13 10:43:41,270 train 350 1.138100e-02 0.293194
2019-11-13 10:43:47,720 train 400 1.131416e-02 0.297419
2019-11-13 10:43:54,174 train 450 1.135018e-02 0.293734
2019-11-13 10:44:00,628 train 500 1.135983e-02 0.291432
2019-11-13 10:44:07,076 train 550 1.136676e-02 0.290372
2019-11-13 10:44:13,529 train 600 1.137346e-02 0.290394
2019-11-13 10:44:19,984 train 650 1.137017e-02 0.293383
2019-11-13 10:44:26,439 train 700 1.135352e-02 0.294468
2019-11-13 10:44:32,891 train 750 1.132583e-02 0.294587
2019-11-13 10:44:39,344 train 800 1.133913e-02 0.294590
2019-11-13 10:44:45,802 train 850 1.133003e-02 0.295946
2019-11-13 10:44:47,737 training loss; R2: 1.132427e-02 0.296309
2019-11-13 10:44:48,037 valid 000 4.121628e-01 -33.537284
2019-11-13 10:44:49,760 valid 050 4.228846e-01 -32.765243
2019-11-13 10:44:51,312 validation loss; R2: 4.231458e-01 -32.779287
2019-11-13 10:44:51,329 epoch 16 lr 1.000000e-03
2019-11-13 10:44:51,726 train 000 1.253069e-02 0.416204
2019-11-13 10:44:58,224 train 050 1.118134e-02 0.327893
2019-11-13 10:45:04,695 train 100 1.121725e-02 0.318270
2019-11-13 10:45:11,161 train 150 1.128169e-02 0.297133
2019-11-13 10:45:17,706 train 200 1.123380e-02 0.303604
2019-11-13 10:45:24,176 train 250 1.120634e-02 0.299056
2019-11-13 10:45:30,641 train 300 1.123084e-02 0.301989
2019-11-13 10:45:37,107 train 350 1.122762e-02 0.303146
2019-11-13 10:45:43,577 train 400 1.121090e-02 0.306573
2019-11-13 10:45:50,041 train 450 1.122157e-02 0.292585
2019-11-13 10:45:56,508 train 500 1.124641e-02 0.291976
2019-11-13 10:46:02,971 train 550 1.124882e-02 0.295767
2019-11-13 10:46:09,444 train 600 1.122313e-02 0.292851
2019-11-13 10:46:15,902 train 650 1.122997e-02 0.292683
2019-11-13 10:46:22,365 train 700 1.123328e-02 0.295616
2019-11-13 10:46:28,826 train 750 1.124409e-02 0.290615
2019-11-13 10:46:35,285 train 800 1.125341e-02 0.293157
2019-11-13 10:46:41,737 train 850 1.124617e-02 0.295410
2019-11-13 10:46:43,666 training loss; R2: 1.124423e-02 0.295379
2019-11-13 10:46:43,956 valid 000 7.244931e+01 -9237.016115
2019-11-13 10:46:45,666 valid 050 7.236261e+01 -7904.236268
2019-11-13 10:46:47,233 validation loss; R2: 7.234253e+01 -8545.895251
2019-11-13 10:46:47,250 epoch 17 lr 1.000000e-03
2019-11-13 10:46:47,650 train 000 1.177956e-02 0.419445
2019-11-13 10:46:54,205 train 050 1.119175e-02 0.334719
2019-11-13 10:47:00,712 train 100 1.109550e-02 0.328082
2019-11-13 10:47:07,433 train 150 1.114366e-02 0.323495
2019-11-13 10:47:13,941 train 200 1.122646e-02 0.322074
2019-11-13 10:47:20,438 train 250 1.122945e-02 0.318986
2019-11-13 10:47:27,013 train 300 1.123472e-02 0.311397
2019-11-13 10:47:33,569 train 350 1.127582e-02 0.312797
2019-11-13 10:47:40,075 train 400 1.129008e-02 0.314048
2019-11-13 10:47:46,815 train 450 1.129861e-02 0.314399
2019-11-13 10:47:53,407 train 500 1.127334e-02 0.310822
2019-11-13 10:47:59,953 train 550 1.126240e-02 0.310635
2019-11-13 10:48:06,445 train 600 1.127614e-02 0.312396
2019-11-13 10:48:12,936 train 650 1.124490e-02 0.312892
2019-11-13 10:48:19,425 train 700 1.125225e-02 0.310531
2019-11-13 10:48:25,916 train 750 1.123953e-02 0.311816
2019-11-13 10:48:32,596 train 800 1.123020e-02 0.313641
2019-11-13 10:48:39,087 train 850 1.123285e-02 0.311167
2019-11-13 10:48:41,025 training loss; R2: 1.124391e-02 0.310454
2019-11-13 10:48:41,333 valid 000 1.998699e+01 -1206.764666
2019-11-13 10:48:43,054 valid 050 1.999974e+01 -1189.095834
2019-11-13 10:48:44,640 validation loss; R2: 2.000007e+01 -1236.326624
2019-11-13 10:48:44,664 epoch 18 lr 1.000000e-03
2019-11-13 10:48:45,062 train 000 9.621885e-03 0.427821
2019-11-13 10:48:51,663 train 050 1.107108e-02 0.312653
2019-11-13 10:48:58,349 train 100 1.105835e-02 0.299648
2019-11-13 10:49:04,789 train 150 1.110917e-02 0.230078
2019-11-13 10:49:11,233 train 200 1.113468e-02 0.251575
2019-11-13 10:49:17,669 train 250 1.110569e-02 0.221097
2019-11-13 10:49:24,105 train 300 1.106150e-02 0.233929
2019-11-13 10:49:30,552 train 350 1.107596e-02 0.249347
2019-11-13 10:49:36,986 train 400 1.107806e-02 0.254663
2019-11-13 10:49:43,420 train 450 1.106526e-02 0.262688
2019-11-13 10:49:49,857 train 500 1.106115e-02 0.265332
2019-11-13 10:49:56,301 train 550 1.107528e-02 0.270248
2019-11-13 10:50:02,738 train 600 1.106800e-02 0.269131
2019-11-13 10:50:09,177 train 650 1.106501e-02 0.272340
2019-11-13 10:50:15,611 train 700 1.106137e-02 0.275884
2019-11-13 10:50:22,050 train 750 1.106613e-02 0.277490
2019-11-13 10:50:28,485 train 800 1.107306e-02 0.272230
2019-11-13 10:50:34,920 train 850 1.109619e-02 0.272352
2019-11-13 10:50:36,843 training loss; R2: 1.110479e-02 0.273128
2019-11-13 10:50:37,144 valid 000 3.917687e+01 -2392.748359
2019-11-13 10:50:38,785 valid 050 3.921741e+01 -1936.636612
2019-11-13 10:50:40,320 validation loss; R2: 3.921150e+01 -2004.140699
2019-11-13 10:50:40,342 epoch 19 lr 1.000000e-03
2019-11-13 10:50:40,757 train 000 1.071322e-02 0.381347
2019-11-13 10:50:47,305 train 050 1.117346e-02 0.329648
2019-11-13 10:50:53,840 train 100 1.113693e-02 0.322670
2019-11-13 10:51:00,315 train 150 1.107291e-02 0.299426
2019-11-13 10:51:06,789 train 200 1.112050e-02 0.302156
2019-11-13 10:51:13,391 train 250 1.118425e-02 0.304217
2019-11-13 10:51:20,207 train 300 1.128727e-02 0.305601
2019-11-13 10:51:27,007 train 350 1.135036e-02 0.308136
2019-11-13 10:51:33,786 train 400 1.134602e-02 0.310197
2019-11-13 10:51:40,567 train 450 1.134790e-02 0.162915
2019-11-13 10:51:47,350 train 500 1.134434e-02 0.179418
2019-11-13 10:51:54,135 train 550 1.132586e-02 0.063543
2019-11-13 10:52:00,933 train 600 1.129831e-02 0.084658
2019-11-13 10:52:07,720 train 650 1.130277e-02 0.100935
2019-11-13 10:52:14,506 train 700 1.129969e-02 0.114632
2019-11-13 10:52:21,285 train 750 1.128779e-02 0.119568
2019-11-13 10:52:28,070 train 800 1.126178e-02 0.131870
2019-11-13 10:52:34,855 train 850 1.124453e-02 0.143431
2019-11-13 10:52:36,877 training loss; R2: 1.124673e-02 0.146754
2019-11-13 10:52:37,177 valid 000 5.781312e+00 -663.258157
2019-11-13 10:52:38,863 valid 050 5.789859e+00 -1299.457509
2019-11-13 10:52:40,407 validation loss; R2: 5.789001e+00 -1013.948538
