2019-11-13 15:35:35,303 gpu device = 1
2019-11-13 15:35:35,304 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name='galaxy-zoo-cleaned', gpu=1, grad_clip=5, gz_dtree=False, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-galaxy-zoo-EXP-20191113-153535', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-11-13 15:35:46,748 param size = 0.298229MB
2019-11-13 15:35:46,752 epoch 0 lr 1.000000e-03
2019-11-13 15:35:48,963 train 000 6.292800e-01 -134.977874
2019-11-13 15:35:56,177 train 050 5.364052e-02 -11.918212
2019-11-13 15:36:03,477 train 100 4.144742e-02 -6.606533
2019-11-13 15:36:10,751 train 150 3.657643e-02 -4.714260
2019-11-13 15:36:18,035 train 200 3.403269e-02 -3.720761
2019-11-13 15:36:25,303 train 250 3.225389e-02 -3.110025
2019-11-13 15:36:32,505 train 300 3.092762e-02 -2.660538
2019-11-13 15:36:39,770 train 350 2.991621e-02 -2.319083
2019-11-13 15:36:47,108 train 400 2.911908e-02 -2.071698
2019-11-13 15:36:54,273 train 450 2.842391e-02 -1.877996
2019-11-13 15:37:01,460 train 500 2.782171e-02 -1.722398
2019-11-13 15:37:08,759 train 550 2.728522e-02 -1.587999
2019-11-13 15:37:16,098 train 600 2.676212e-02 -1.475059
2019-11-13 15:37:23,384 train 650 2.635402e-02 -1.381033
2019-11-13 15:37:30,556 train 700 2.598363e-02 -1.368055
2019-11-13 15:37:37,741 train 750 2.558633e-02 -1.284097
2019-11-13 15:37:45,147 train 800 2.524912e-02 -1.209321
2019-11-13 15:37:52,525 train 850 2.494566e-02 -1.142463
2019-11-13 15:37:55,455 training loss; R2: 2.486889e-02 -1.125311
2019-11-13 15:37:55,747 valid 000 2.236204e-02 -0.054459
2019-11-13 15:37:57,387 valid 050 1.864553e-02 0.044595
2019-11-13 15:37:58,985 validation loss; R2: 1.908910e-02 0.044856
2019-11-13 15:37:59,007 epoch 1 lr 1.000000e-03
2019-11-13 15:37:59,532 train 000 2.360039e-02 -0.017836
2019-11-13 15:38:06,623 train 050 1.949713e-02 -0.039943
2019-11-13 15:38:13,851 train 100 1.950935e-02 -0.013104
2019-11-13 15:38:20,909 train 150 1.933884e-02 0.004123
2019-11-13 15:38:27,991 train 200 1.923743e-02 0.003778
2019-11-13 15:38:35,061 train 250 1.911759e-02 0.006058
2019-11-13 15:38:42,113 train 300 1.912266e-02 0.011093
2019-11-13 15:38:49,163 train 350 1.907456e-02 0.005433
2019-11-13 15:38:56,208 train 400 1.897081e-02 0.013973
2019-11-13 15:39:03,261 train 450 1.889020e-02 0.015590
2019-11-13 15:39:10,312 train 500 1.884965e-02 0.016035
2019-11-13 15:39:17,363 train 550 1.875719e-02 0.020520
2019-11-13 15:39:24,417 train 600 1.866555e-02 0.019636
2019-11-13 15:39:31,460 train 650 1.858857e-02 0.022996
2019-11-13 15:39:38,508 train 700 1.850412e-02 0.027416
2019-11-13 15:39:45,565 train 750 1.843203e-02 0.028603
2019-11-13 15:39:52,633 train 800 1.835413e-02 0.032585
2019-11-13 15:39:59,676 train 850 1.831655e-02 0.034449
2019-11-13 15:40:01,782 training loss; R2: 1.830430e-02 0.036312
2019-11-13 15:40:02,108 valid 000 1.446225e-02 0.111473
2019-11-13 15:40:03,777 valid 050 1.674115e-02 0.142795
2019-11-13 15:40:05,299 validation loss; R2: 1.685789e-02 0.156196
2019-11-13 15:40:05,315 epoch 2 lr 1.000000e-03
2019-11-13 15:40:05,693 train 000 1.855525e-02 0.191699
2019-11-13 15:40:12,726 train 050 1.702018e-02 0.098871
2019-11-13 15:40:19,759 train 100 1.684642e-02 -0.237658
2019-11-13 15:40:26,790 train 150 1.666506e-02 -0.113848
2019-11-13 15:40:33,895 train 200 1.665538e-02 -0.050934
2019-11-13 15:40:40,984 train 250 1.657763e-02 -0.024602
2019-11-13 15:40:48,030 train 300 1.657505e-02 0.000862
2019-11-13 15:40:55,065 train 350 1.656113e-02 0.017421
2019-11-13 15:41:02,109 train 400 1.651354e-02 0.029337
2019-11-13 15:41:09,152 train 450 1.648004e-02 0.028168
2019-11-13 15:41:16,186 train 500 1.648658e-02 0.037268
2019-11-13 15:41:23,218 train 550 1.642772e-02 0.046200
2019-11-13 15:41:30,248 train 600 1.638603e-02 0.051032
2019-11-13 15:41:37,276 train 650 1.632449e-02 0.054293
2019-11-13 15:41:44,353 train 700 1.629071e-02 0.059373
2019-11-13 15:41:51,375 train 750 1.625577e-02 0.066767
2019-11-13 15:41:58,405 train 800 1.620310e-02 0.074718
2019-11-13 15:42:05,425 train 850 1.613855e-02 0.080234
2019-11-13 15:42:07,523 training loss; R2: 1.612341e-02 0.081826
2019-11-13 15:42:07,853 valid 000 1.608308e-02 0.026339
2019-11-13 15:42:09,564 valid 050 1.501258e-02 0.167764
2019-11-13 15:42:11,114 validation loss; R2: 1.496505e-02 0.117856
2019-11-13 15:42:11,131 epoch 3 lr 1.000000e-03
2019-11-13 15:42:11,530 train 000 1.467548e-02 0.163919
2019-11-13 15:42:18,616 train 050 1.560781e-02 0.178882
2019-11-13 15:42:25,657 train 100 1.519488e-02 0.193075
2019-11-13 15:42:32,696 train 150 1.527493e-02 0.168331
2019-11-13 15:42:39,729 train 200 1.516257e-02 0.162442
2019-11-13 15:42:46,770 train 250 1.509138e-02 0.170949
2019-11-13 15:42:53,806 train 300 1.506997e-02 0.169204
2019-11-13 15:43:00,849 train 350 1.508852e-02 0.169685
2019-11-13 15:43:07,919 train 400 1.507578e-02 0.165364
2019-11-13 15:43:14,960 train 450 1.501625e-02 0.169139
2019-11-13 15:43:21,996 train 500 1.501481e-02 0.168009
2019-11-13 15:43:29,031 train 550 1.500205e-02 0.169044
2019-11-13 15:43:36,069 train 600 1.496549e-02 0.169238
2019-11-13 15:43:43,115 train 650 1.493690e-02 0.164617
2019-11-13 15:43:50,155 train 700 1.486278e-02 0.168804
2019-11-13 15:43:57,183 train 750 1.483428e-02 0.171763
2019-11-13 15:44:04,242 train 800 1.478061e-02 0.165758
2019-11-13 15:44:11,287 train 850 1.476475e-02 0.168268
2019-11-13 15:44:13,394 training loss; R2: 1.475780e-02 0.168588
2019-11-13 15:44:13,709 valid 000 1.459983e-02 0.292966
2019-11-13 15:44:15,422 valid 050 1.429125e-02 0.179476
2019-11-13 15:44:16,977 validation loss; R2: 1.417194e-02 0.169237
2019-11-13 15:44:17,001 epoch 4 lr 1.000000e-03
2019-11-13 15:44:17,380 train 000 1.228155e-02 0.206043
2019-11-13 15:44:24,544 train 050 1.445860e-02 0.204782
2019-11-13 15:44:31,591 train 100 1.435167e-02 0.196700
2019-11-13 15:44:38,638 train 150 1.429748e-02 0.204696
2019-11-13 15:44:45,691 train 200 1.425630e-02 0.216496
2019-11-13 15:44:52,724 train 250 1.416344e-02 0.218771
2019-11-13 15:44:59,753 train 300 1.410424e-02 0.212704
2019-11-13 15:45:06,791 train 350 1.412317e-02 0.210640
2019-11-13 15:45:13,822 train 400 1.405445e-02 0.213798
2019-11-13 15:45:20,853 train 450 1.398869e-02 0.216399
2019-11-13 15:45:27,877 train 500 1.396251e-02 0.146370
2019-11-13 15:45:34,921 train 550 1.394428e-02 0.153665
2019-11-13 15:45:41,948 train 600 1.392275e-02 0.152505
2019-11-13 15:45:48,965 train 650 1.394010e-02 0.158713
2019-11-13 15:45:55,986 train 700 1.391560e-02 0.160602
2019-11-13 15:46:03,052 train 750 1.390461e-02 0.152475
2019-11-13 15:46:10,078 train 800 1.387254e-02 0.142057
2019-11-13 15:46:17,100 train 850 1.384541e-02 0.144987
2019-11-13 15:46:19,199 training loss; R2: 1.384328e-02 0.145973
2019-11-13 15:46:19,509 valid 000 1.336958e-02 0.392038
2019-11-13 15:46:21,259 valid 050 1.235351e-02 0.325147
2019-11-13 15:46:22,797 validation loss; R2: 1.230758e-02 0.321385
2019-11-13 15:46:22,820 epoch 5 lr 1.000000e-03
2019-11-13 15:46:23,247 train 000 1.234728e-02 0.327682
2019-11-13 15:46:30,531 train 050 1.306013e-02 0.272163
2019-11-13 15:46:37,619 train 100 1.313715e-02 0.230293
2019-11-13 15:46:44,758 train 150 1.329354e-02 0.222901
2019-11-13 15:46:51,772 train 200 1.323936e-02 0.233006
2019-11-13 15:46:58,783 train 250 1.327114e-02 0.153482
2019-11-13 15:47:05,801 train 300 1.336553e-02 0.109831
2019-11-13 15:47:12,817 train 350 1.337737e-02 0.128319
2019-11-13 15:47:19,832 train 400 1.333751e-02 0.130747
2019-11-13 15:47:26,838 train 450 1.332976e-02 0.139270
2019-11-13 15:47:33,904 train 500 1.332438e-02 0.151171
2019-11-13 15:47:40,920 train 550 1.327391e-02 0.156858
2019-11-13 15:47:47,942 train 600 1.326582e-02 0.163296
2019-11-13 15:47:54,948 train 650 1.326737e-02 0.166861
2019-11-13 15:48:01,957 train 700 1.323668e-02 0.162873
2019-11-13 15:48:08,966 train 750 1.322552e-02 0.167107
2019-11-13 15:48:15,977 train 800 1.319609e-02 0.171756
2019-11-13 15:48:22,991 train 850 1.319198e-02 0.178018
2019-11-13 15:48:25,135 training loss; R2: 1.319376e-02 0.178447
2019-11-13 15:48:25,456 valid 000 1.231109e-02 0.335780
2019-11-13 15:48:27,227 valid 050 1.184495e-02 0.261157
2019-11-13 15:48:28,753 validation loss; R2: 1.190715e-02 0.264757
2019-11-13 15:48:28,770 epoch 6 lr 1.000000e-03
2019-11-13 15:48:29,188 train 000 1.398128e-02 -1.521742
2019-11-13 15:48:36,381 train 050 1.296938e-02 0.223315
2019-11-13 15:48:43,408 train 100 1.297027e-02 0.240263
2019-11-13 15:48:50,445 train 150 1.290862e-02 0.248787
2019-11-13 15:48:57,457 train 200 1.294762e-02 0.247951
2019-11-13 15:49:04,472 train 250 1.294284e-02 0.232661
2019-11-13 15:49:11,483 train 300 1.287104e-02 0.230926
2019-11-13 15:49:18,498 train 350 1.287812e-02 0.237621
2019-11-13 15:49:25,520 train 400 1.285884e-02 0.234293
2019-11-13 15:49:32,547 train 450 1.281807e-02 0.235385
2019-11-13 15:49:39,569 train 500 1.279666e-02 0.234226
2019-11-13 15:49:46,592 train 550 1.277303e-02 0.236448
2019-11-13 15:49:53,606 train 600 1.276956e-02 0.239248
2019-11-13 15:50:00,626 train 650 1.273383e-02 0.241419
2019-11-13 15:50:07,646 train 700 1.273161e-02 0.241403
2019-11-13 15:50:14,721 train 750 1.269867e-02 0.240161
2019-11-13 15:50:21,733 train 800 1.270787e-02 0.240527
2019-11-13 15:50:28,752 train 850 1.269843e-02 0.241163
2019-11-13 15:50:30,855 training loss; R2: 1.268867e-02 0.242065
2019-11-13 15:50:31,165 valid 000 1.430547e-02 -0.502567
2019-11-13 15:50:32,873 valid 050 1.145479e-02 0.289221
2019-11-13 15:50:34,425 validation loss; R2: 1.129407e-02 0.286552
2019-11-13 15:50:34,442 epoch 7 lr 1.000000e-03
2019-11-13 15:50:34,842 train 000 1.079135e-02 0.357267
2019-11-13 15:50:42,011 train 050 1.216499e-02 0.249601
2019-11-13 15:50:49,095 train 100 1.243069e-02 0.266630
2019-11-13 15:50:56,115 train 150 1.244942e-02 0.273885
2019-11-13 15:51:03,131 train 200 1.243750e-02 0.271480
2019-11-13 15:51:10,163 train 250 1.245108e-02 0.268391
2019-11-13 15:51:17,192 train 300 1.243427e-02 0.258494
2019-11-13 15:51:24,201 train 350 1.239050e-02 0.262595
2019-11-13 15:51:31,219 train 400 1.236009e-02 0.264952
2019-11-13 15:51:38,232 train 450 1.236423e-02 0.268938
2019-11-13 15:51:45,249 train 500 1.234489e-02 0.267587
2019-11-13 15:51:52,259 train 550 1.235221e-02 0.261317
2019-11-13 15:51:59,274 train 600 1.234031e-02 0.257819
2019-11-13 15:52:06,294 train 650 1.231917e-02 0.258714
2019-11-13 15:52:13,320 train 700 1.229555e-02 0.258324
2019-11-13 15:52:20,328 train 750 1.228505e-02 0.261613
2019-11-13 15:52:27,340 train 800 1.229777e-02 0.260888
2019-11-13 15:52:34,359 train 850 1.228411e-02 0.257678
2019-11-13 15:52:36,458 training loss; R2: 1.228180e-02 0.252206
2019-11-13 15:52:36,785 valid 000 1.051760e-02 0.329977
2019-11-13 15:52:38,471 valid 050 1.095076e-02 0.352985
2019-11-13 15:52:40,016 validation loss; R2: 1.104143e-02 0.318431
2019-11-13 15:52:40,033 epoch 8 lr 1.000000e-03
2019-11-13 15:52:40,426 train 000 1.055752e-02 0.364630
2019-11-13 15:52:47,476 train 050 1.191620e-02 0.305452
2019-11-13 15:52:54,503 train 100 1.210223e-02 0.282715
2019-11-13 15:53:01,508 train 150 1.220398e-02 0.284478
2019-11-13 15:53:08,510 train 200 1.214632e-02 0.284453
2019-11-13 15:53:15,511 train 250 1.214165e-02 0.272714
2019-11-13 15:53:22,515 train 300 1.216017e-02 0.261267
2019-11-13 15:53:29,518 train 350 1.213131e-02 0.263112
2019-11-13 15:53:36,525 train 400 1.214523e-02 0.263001
2019-11-13 15:53:43,542 train 450 1.211805e-02 0.267457
2019-11-13 15:53:50,567 train 500 1.210899e-02 0.267666
2019-11-13 15:53:57,574 train 550 1.209052e-02 0.251478
2019-11-13 15:54:04,576 train 600 1.208216e-02 0.251730
2019-11-13 15:54:11,580 train 650 1.207255e-02 0.249706
2019-11-13 15:54:18,584 train 700 1.202074e-02 0.246342
2019-11-13 15:54:25,589 train 750 1.199902e-02 0.248250
2019-11-13 15:54:32,618 train 800 1.200669e-02 0.250719
2019-11-13 15:54:39,652 train 850 1.198917e-02 0.251911
2019-11-13 15:54:41,750 training loss; R2: 1.198197e-02 0.251892
2019-11-13 15:54:42,074 valid 000 7.497088e-02 -3.198788
2019-11-13 15:54:43,810 valid 050 7.045885e-02 -4.165448
2019-11-13 15:54:45,362 validation loss; R2: 7.025893e-02 -3.857898
2019-11-13 15:54:45,379 epoch 9 lr 1.000000e-03
2019-11-13 15:54:45,778 train 000 1.202584e-02 0.319377
2019-11-13 15:54:52,903 train 050 1.198783e-02 -31.267371
2019-11-13 15:54:59,954 train 100 1.191539e-02 -15.662741
2019-11-13 15:55:06,970 train 150 1.173906e-02 -10.395701
2019-11-13 15:55:13,980 train 200 1.173283e-02 -7.738972
2019-11-13 15:55:20,978 train 250 1.176714e-02 -6.143943
2019-11-13 15:55:27,978 train 300 1.179179e-02 -5.082665
2019-11-13 15:55:35,023 train 350 1.176450e-02 -4.317423
2019-11-13 15:55:42,015 train 400 1.178874e-02 -3.742894
2019-11-13 15:55:49,004 train 450 1.177891e-02 -3.297458
2019-11-13 15:55:55,990 train 500 1.178950e-02 -2.938997
2019-11-13 15:56:02,982 train 550 1.175273e-02 -2.645503
2019-11-13 15:56:09,976 train 600 1.173370e-02 -2.403162
2019-11-13 15:56:16,975 train 650 1.173640e-02 -2.203740
2019-11-13 15:56:23,967 train 700 1.174991e-02 -2.026846
2019-11-13 15:56:30,959 train 750 1.174675e-02 -1.874701
2019-11-13 15:56:37,950 train 800 1.173378e-02 -1.739844
2019-11-13 15:56:44,939 train 850 1.172560e-02 -1.620174
2019-11-13 15:56:47,029 training loss; R2: 1.172967e-02 -1.586769
2019-11-13 15:56:47,348 valid 000 5.786983e-02 -0.665487
2019-11-13 15:56:49,069 valid 050 4.969699e-02 -0.681552
2019-11-13 15:56:50,621 validation loss; R2: 5.011408e-02 -0.694644
2019-11-13 15:56:50,644 epoch 10 lr 1.000000e-03
2019-11-13 15:56:51,025 train 000 9.593995e-03 0.409348
2019-11-13 15:56:58,235 train 050 1.158657e-02 0.302797
2019-11-13 15:57:05,288 train 100 1.153176e-02 0.301753
2019-11-13 15:57:12,344 train 150 1.152280e-02 0.300878
2019-11-13 15:57:19,346 train 200 1.150350e-02 0.305461
2019-11-13 15:57:26,352 train 250 1.152079e-02 0.300497
2019-11-13 15:57:33,351 train 300 1.152523e-02 0.282980
2019-11-13 15:57:40,359 train 350 1.153662e-02 0.279024
2019-11-13 15:57:47,358 train 400 1.151805e-02 0.285527
2019-11-13 15:57:54,358 train 450 1.149823e-02 0.285440
2019-11-13 15:58:01,357 train 500 1.150886e-02 0.283529
2019-11-13 15:58:08,356 train 550 1.149394e-02 0.283789
2019-11-13 15:58:15,359 train 600 1.151325e-02 0.285696
2019-11-13 15:58:22,365 train 650 1.150118e-02 0.289469
2019-11-13 15:58:29,373 train 700 1.148656e-02 0.291475
2019-11-13 15:58:36,379 train 750 1.147213e-02 0.286411
2019-11-13 15:58:43,385 train 800 1.146787e-02 0.202436
2019-11-13 15:58:50,398 train 850 1.145605e-02 0.207509
2019-11-13 15:58:52,495 training loss; R2: 1.146337e-02 0.209363
2019-11-13 15:58:52,811 valid 000 1.247067e-02 0.326740
2019-11-13 15:58:54,534 valid 050 1.535895e-02 0.236812
2019-11-13 15:58:56,102 validation loss; R2: 1.542152e-02 0.228747
2019-11-13 15:58:56,129 epoch 11 lr 1.000000e-03
2019-11-13 15:58:56,561 train 000 1.276110e-02 0.187845
2019-11-13 15:59:03,680 train 050 1.107116e-02 0.301077
2019-11-13 15:59:10,694 train 100 1.117916e-02 0.294114
2019-11-13 15:59:17,706 train 150 1.121110e-02 0.304634
2019-11-13 15:59:24,710 train 200 1.127557e-02 0.302499
2019-11-13 15:59:31,711 train 250 1.125736e-02 0.296097
2019-11-13 15:59:38,713 train 300 1.125047e-02 0.285552
2019-11-13 15:59:45,711 train 350 1.124087e-02 0.289247
2019-11-13 15:59:52,723 train 400 1.121120e-02 0.292314
2019-11-13 15:59:59,724 train 450 1.120024e-02 0.294108
2019-11-13 16:00:06,720 train 500 1.121763e-02 0.296175
2019-11-13 16:00:13,720 train 550 1.122501e-02 0.297401
2019-11-13 16:00:20,718 train 600 1.121000e-02 0.296829
2019-11-13 16:00:27,718 train 650 1.120895e-02 0.296174
2019-11-13 16:00:34,720 train 700 1.123281e-02 0.293553
2019-11-13 16:00:41,716 train 750 1.124159e-02 0.294993
2019-11-13 16:00:48,710 train 800 1.123752e-02 0.293569
2019-11-13 16:00:55,702 train 850 1.124461e-02 0.293142
2019-11-13 16:00:57,854 training loss; R2: 1.123422e-02 0.294205
2019-11-13 16:00:58,174 valid 000 2.341653e-02 -0.328066
2019-11-13 16:00:59,825 valid 050 2.432813e-02 -0.271833
2019-11-13 16:01:01,350 validation loss; R2: 2.428645e-02 -0.270146
2019-11-13 16:01:01,370 epoch 12 lr 1.000000e-03
2019-11-13 16:01:01,844 train 000 1.196984e-02 0.311980
2019-11-13 16:01:08,903 train 050 1.111770e-02 -1.616990
2019-11-13 16:01:15,890 train 100 1.102174e-02 -0.681651
2019-11-13 16:01:22,882 train 150 1.117604e-02 -0.364162
2019-11-13 16:01:29,882 train 200 1.110438e-02 -0.187543
2019-11-13 16:01:36,869 train 250 1.114731e-02 -0.086887
2019-11-13 16:01:43,861 train 300 1.117732e-02 -0.019653
2019-11-13 16:01:50,854 train 350 1.115847e-02 0.031680
2019-11-13 16:01:57,848 train 400 1.114390e-02 0.066587
2019-11-13 16:02:04,838 train 450 1.112980e-02 0.094188
2019-11-13 16:02:11,834 train 500 1.110182e-02 0.069881
2019-11-13 16:02:18,828 train 550 1.109368e-02 0.089817
2019-11-13 16:02:25,873 train 600 1.110383e-02 0.104853
2019-11-13 16:02:32,869 train 650 1.109090e-02 0.122830
2019-11-13 16:02:39,862 train 700 1.109171e-02 0.137232
2019-11-13 16:02:46,856 train 750 1.109693e-02 0.148043
2019-11-13 16:02:53,848 train 800 1.109244e-02 0.158157
2019-11-13 16:03:00,844 train 850 1.110074e-02 0.164546
2019-11-13 16:03:02,933 training loss; R2: 1.110649e-02 0.167350
2019-11-13 16:03:03,237 valid 000 9.441318e-03 0.408728
2019-11-13 16:03:04,963 valid 050 9.833176e-03 0.339964
2019-11-13 16:03:06,515 validation loss; R2: 9.866173e-03 0.363162
2019-11-13 16:03:06,537 epoch 13 lr 1.000000e-03
2019-11-13 16:03:06,953 train 000 1.042581e-02 0.128159
2019-11-13 16:03:14,065 train 050 1.093180e-02 0.315358
2019-11-13 16:03:21,120 train 100 1.091289e-02 0.276142
2019-11-13 16:03:28,199 train 150 1.097102e-02 0.279777
2019-11-13 16:03:35,305 train 200 1.104704e-02 0.287741
2019-11-13 16:03:42,309 train 250 1.100196e-02 0.294530
2019-11-13 16:03:49,311 train 300 1.101525e-02 0.297852
2019-11-13 16:03:56,313 train 350 1.101101e-02 0.301250
2019-11-13 16:04:03,316 train 400 1.098830e-02 0.300841
2019-11-13 16:04:10,308 train 450 1.099685e-02 0.300028
2019-11-13 16:04:17,302 train 500 1.099576e-02 0.304100
2019-11-13 16:04:24,298 train 550 1.098807e-02 0.306091
2019-11-13 16:04:31,287 train 600 1.097406e-02 0.307380
2019-11-13 16:04:38,280 train 650 1.095937e-02 0.307457
2019-11-13 16:04:45,267 train 700 1.094461e-02 0.309507
2019-11-13 16:04:52,257 train 750 1.093832e-02 0.308626
2019-11-13 16:04:59,259 train 800 1.094491e-02 0.310861
2019-11-13 16:05:06,380 train 850 1.094900e-02 0.101821
2019-11-13 16:05:08,472 training loss; R2: 1.093710e-02 0.106268
2019-11-13 16:05:08,790 valid 000 8.281997e-03 0.408658
2019-11-13 16:05:10,505 valid 050 9.486311e-03 0.380240
2019-11-13 16:05:12,059 validation loss; R2: 9.578186e-03 0.382697
2019-11-13 16:05:12,076 epoch 14 lr 1.000000e-03
2019-11-13 16:05:12,467 train 000 1.248988e-02 0.311117
2019-11-13 16:05:19,656 train 050 1.091075e-02 0.349643
2019-11-13 16:05:26,668 train 100 1.094054e-02 0.295430
2019-11-13 16:05:33,666 train 150 1.086074e-02 0.297745
2019-11-13 16:05:40,660 train 200 1.093448e-02 0.302939
2019-11-13 16:05:47,684 train 250 1.088221e-02 0.308951
2019-11-13 16:05:54,680 train 300 1.091877e-02 0.285119
2019-11-13 16:06:01,680 train 350 1.094270e-02 0.290393
2019-11-13 16:06:08,670 train 400 1.093302e-02 0.280089
2019-11-13 16:06:15,660 train 450 1.094783e-02 0.282382
2019-11-13 16:06:22,650 train 500 1.093656e-02 0.281382
2019-11-13 16:06:29,636 train 550 1.094595e-02 0.285229
2019-11-13 16:06:36,640 train 600 1.093355e-02 0.284317
2019-11-13 16:06:43,630 train 650 1.091012e-02 0.287603
2019-11-13 16:06:50,627 train 700 1.090639e-02 0.288044
2019-11-13 16:06:57,618 train 750 1.087533e-02 0.287995
2019-11-13 16:07:04,617 train 800 1.085450e-02 0.289770
2019-11-13 16:07:11,619 train 850 1.085873e-02 0.292346
2019-11-13 16:07:13,715 training loss; R2: 1.085972e-02 0.292030
2019-11-13 16:07:14,028 valid 000 1.094315e-02 0.344998
2019-11-13 16:07:15,743 valid 050 9.685545e-03 0.361223
2019-11-13 16:07:17,293 validation loss; R2: 9.848987e-03 0.338402
2019-11-13 16:07:17,309 epoch 15 lr 1.000000e-03
2019-11-13 16:07:17,722 train 000 1.017441e-02 0.338296
2019-11-13 16:07:24,760 train 050 1.089175e-02 0.285858
2019-11-13 16:07:31,782 train 100 1.060178e-02 0.287391
2019-11-13 16:07:38,767 train 150 1.059621e-02 0.298399
2019-11-13 16:07:45,751 train 200 1.069769e-02 0.220165
2019-11-13 16:07:52,741 train 250 1.074521e-02 0.241367
2019-11-13 16:07:59,732 train 300 1.074959e-02 0.254188
2019-11-13 16:08:06,722 train 350 1.082114e-02 0.255934
2019-11-13 16:08:13,707 train 400 1.089253e-02 0.251648
2019-11-13 16:08:20,692 train 450 1.094145e-02 0.258535
2019-11-13 16:08:27,680 train 500 1.098128e-02 0.265913
2019-11-13 16:08:34,662 train 550 1.097049e-02 0.264772
2019-11-13 16:08:41,643 train 600 1.095669e-02 0.269527
2019-11-13 16:08:48,624 train 650 1.095749e-02 0.273998
2019-11-13 16:08:55,603 train 700 1.096086e-02 0.278499
2019-11-13 16:09:02,586 train 750 1.092727e-02 0.282422
2019-11-13 16:09:09,568 train 800 1.093203e-02 0.284204
2019-11-13 16:09:16,550 train 850 1.091384e-02 0.286155
2019-11-13 16:09:18,635 training loss; R2: 1.091369e-02 0.286675
2019-11-13 16:09:18,947 valid 000 1.156743e-02 0.262557
2019-11-13 16:09:20,662 valid 050 9.485286e-03 0.365770
2019-11-13 16:09:22,211 validation loss; R2: 9.436746e-03 0.385876
2019-11-13 16:09:22,235 epoch 16 lr 1.000000e-03
2019-11-13 16:09:22,658 train 000 1.046849e-02 0.374636
2019-11-13 16:09:29,769 train 050 1.086504e-02 -2.308249
2019-11-13 16:09:36,774 train 100 1.076605e-02 -1.027323
2019-11-13 16:09:43,773 train 150 1.075741e-02 -0.607275
2019-11-13 16:09:50,803 train 200 1.075462e-02 -0.398816
2019-11-13 16:09:57,794 train 250 1.078564e-02 -0.269957
2019-11-13 16:10:04,777 train 300 1.081304e-02 -0.173847
2019-11-13 16:10:11,762 train 350 1.078223e-02 -0.099734
2019-11-13 16:10:18,744 train 400 1.076894e-02 -0.049855
2019-11-13 16:10:25,727 train 450 1.078014e-02 -0.021836
2019-11-13 16:10:32,711 train 500 1.075371e-02 0.006666
2019-11-13 16:10:39,702 train 550 1.078182e-02 0.033397
2019-11-13 16:10:46,685 train 600 1.077654e-02 0.060198
2019-11-13 16:10:53,677 train 650 1.075713e-02 0.071662
2019-11-13 16:11:00,665 train 700 1.076757e-02 0.089340
2019-11-13 16:11:07,650 train 750 1.075630e-02 0.102906
2019-11-13 16:11:14,631 train 800 1.074491e-02 0.116522
2019-11-13 16:11:21,615 train 850 1.074562e-02 0.127850
2019-11-13 16:11:23,709 training loss; R2: 1.074295e-02 0.131855
2019-11-13 16:11:24,036 valid 000 4.023579e-02 -1.054057
2019-11-13 16:11:25,757 valid 050 4.176337e-02 -1.513304
2019-11-13 16:11:27,280 validation loss; R2: 4.216593e-02 -1.553895
2019-11-13 16:11:27,298 epoch 17 lr 1.000000e-03
2019-11-13 16:11:27,666 train 000 1.029071e-02 0.404424
2019-11-13 16:11:34,737 train 050 1.052730e-02 0.361631
2019-11-13 16:11:41,778 train 100 1.044676e-02 0.348965
2019-11-13 16:11:48,782 train 150 1.059480e-02 0.328486
2019-11-13 16:11:55,791 train 200 1.075311e-02 0.325496
2019-11-13 16:12:02,789 train 250 1.075248e-02 0.317479
2019-11-13 16:12:09,791 train 300 1.070054e-02 0.318179
2019-11-13 16:12:16,798 train 350 1.063152e-02 0.314600
2019-11-13 16:12:23,798 train 400 1.062053e-02 0.317710
2019-11-13 16:12:30,794 train 450 1.059908e-02 0.311280
2019-11-13 16:12:37,801 train 500 1.059668e-02 0.313523
2019-11-13 16:12:44,800 train 550 1.060023e-02 0.314906
2019-11-13 16:12:51,798 train 600 1.059852e-02 0.313172
2019-11-13 16:12:58,852 train 650 1.059687e-02 0.315061
2019-11-13 16:13:05,858 train 700 1.058638e-02 0.310461
2019-11-13 16:13:12,860 train 750 1.059276e-02 0.312006
2019-11-13 16:13:19,852 train 800 1.058780e-02 0.313692
2019-11-13 16:13:26,857 train 850 1.059772e-02 0.314095
2019-11-13 16:13:28,949 training loss; R2: 1.060157e-02 0.314222
2019-11-13 16:13:29,262 valid 000 1.819616e-02 0.034409
2019-11-13 16:13:30,970 valid 050 1.739283e-02 -0.070959
2019-11-13 16:13:32,533 validation loss; R2: 1.722299e-02 -0.083384
2019-11-13 16:13:32,550 epoch 18 lr 1.000000e-03
2019-11-13 16:13:32,958 train 000 1.181235e-02 0.330255
2019-11-13 16:13:40,012 train 050 1.013137e-02 0.322499
2019-11-13 16:13:47,001 train 100 1.023771e-02 0.329127
2019-11-13 16:13:53,987 train 150 1.039388e-02 0.333610
2019-11-13 16:14:00,972 train 200 1.038102e-02 0.331560
2019-11-13 16:14:07,955 train 250 1.046786e-02 0.331466
2019-11-13 16:14:14,941 train 300 1.051727e-02 0.318134
2019-11-13 16:14:21,981 train 350 1.053447e-02 0.315430
2019-11-13 16:14:28,974 train 400 1.050273e-02 0.309444
2019-11-13 16:14:36,004 train 450 1.048733e-02 0.311602
2019-11-13 16:14:43,002 train 500 1.050005e-02 0.314350
2019-11-13 16:14:49,992 train 550 1.048604e-02 0.314795
2019-11-13 16:14:56,985 train 600 1.052263e-02 0.313885
2019-11-13 16:15:03,983 train 650 1.060258e-02 0.315076
2019-11-13 16:15:10,970 train 700 1.061503e-02 0.317077
2019-11-13 16:15:17,960 train 750 1.060838e-02 0.318720
2019-11-13 16:15:25,008 train 800 1.060836e-02 0.308469
2019-11-13 16:15:32,000 train 850 1.061615e-02 0.310193
2019-11-13 16:15:34,086 training loss; R2: 1.061960e-02 0.310832
2019-11-13 16:15:34,404 valid 000 1.218924e-02 0.438699
2019-11-13 16:15:36,144 valid 050 1.090777e-02 0.389826
2019-11-13 16:15:37,752 validation loss; R2: 1.083432e-02 0.385972
2019-11-13 16:15:37,779 epoch 19 lr 1.000000e-03
2019-11-13 16:15:38,231 train 000 9.505801e-03 0.342930
2019-11-13 16:15:45,266 train 050 1.041314e-02 0.323452
2019-11-13 16:15:52,250 train 100 1.043537e-02 0.320356
2019-11-13 16:15:59,235 train 150 1.050221e-02 0.333220
2019-11-13 16:16:06,222 train 200 1.059502e-02 0.319982
2019-11-13 16:16:13,201 train 250 1.060570e-02 0.315367
2019-11-13 16:16:20,182 train 300 1.061040e-02 0.318537
2019-11-13 16:16:27,166 train 350 1.058509e-02 0.313919
2019-11-13 16:16:34,155 train 400 1.061966e-02 0.319091
2019-11-13 16:16:41,141 train 450 1.061150e-02 0.248202
2019-11-13 16:16:48,125 train 500 1.059478e-02 0.251967
2019-11-13 16:16:55,111 train 550 1.057580e-02 0.261832
2019-11-13 16:17:02,098 train 600 1.056732e-02 0.265945
2019-11-13 16:17:09,078 train 650 1.056119e-02 0.270873
2019-11-13 16:17:16,056 train 700 1.055614e-02 0.277363
2019-11-13 16:17:23,036 train 750 1.054938e-02 0.277927
2019-11-13 16:17:30,017 train 800 1.053581e-02 0.281303
2019-11-13 16:17:37,000 train 850 1.050805e-02 0.284287
2019-11-13 16:17:39,085 training loss; R2: 1.051462e-02 0.280259
2019-11-13 16:17:39,404 valid 000 9.128405e-03 0.389135
2019-11-13 16:17:41,125 valid 050 9.767724e-03 0.379396
2019-11-13 16:17:42,673 validation loss; R2: 9.713093e-03 0.392210
