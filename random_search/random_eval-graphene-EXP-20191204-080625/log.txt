2019-12-04 08:06:25,053 gpu device = 1
2019-12-04 08:06:25,053 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=96, cutout=False, cutout_length=16, data='../data', dataset='graphene', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name=None, gpu=1, grad_clip=5, gz_dtree=False, init_channels=36, layers=8, learning_rate=0.002, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-graphene-EXP-20191204-080625', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-12-04 08:06:28,307 param size = 1.317061MB
2019-12-04 08:06:28,311 epoch 0 lr 2.000000e-03
2019-12-04 08:06:31,113 train 000 5.392258e-01 -23.864424
2019-12-04 08:06:44,883 train 050 1.774396e-01 -4.675587
2019-12-04 08:06:58,367 train 100 1.047768e-01 -2.337608
2019-12-04 08:07:11,843 train 150 7.768658e-02 -1.493497
2019-12-04 08:07:25,315 train 200 6.365689e-02 -1.046430
2019-12-04 08:07:32,561 training loss; R2: 5.909443e-02 -0.902723
2019-12-04 08:07:32,708 valid 000 1.395861e-02 0.400608
2019-12-04 08:07:34,399 validation loss; R2: 2.068508e-02 0.354146
2019-12-04 08:07:34,423 epoch 1 lr 2.000000e-03
2019-12-04 08:07:35,011 train 000 2.196780e-02 0.253111
2019-12-04 08:07:48,526 train 050 1.791855e-02 0.411534
2019-12-04 08:08:02,009 train 100 1.703043e-02 0.451550
2019-12-04 08:08:15,493 train 150 1.574113e-02 0.494947
2019-12-04 08:08:28,984 train 200 1.506034e-02 0.511754
2019-12-04 08:08:35,051 training loss; R2: 1.497611e-02 0.516563
2019-12-04 08:08:35,197 valid 000 1.254935e-02 0.418437
2019-12-04 08:08:36,619 validation loss; R2: 1.117190e-02 0.603266
2019-12-04 08:08:36,645 epoch 2 lr 2.000000e-03
2019-12-04 08:08:37,004 train 000 9.016777e-03 0.616851
2019-12-04 08:08:50,479 train 050 1.095796e-02 0.628708
2019-12-04 08:09:03,955 train 100 1.028572e-02 0.653768
2019-12-04 08:09:17,434 train 150 9.871748e-03 0.666404
2019-12-04 08:09:30,912 train 200 9.644136e-03 0.675889
2019-12-04 08:09:36,978 training loss; R2: 9.542683e-03 0.679221
2019-12-04 08:09:37,120 valid 000 4.578625e-03 0.652180
2019-12-04 08:09:38,543 validation loss; R2: 8.627082e-03 0.711651
2019-12-04 08:09:38,570 epoch 3 lr 2.000000e-03
2019-12-04 08:09:38,927 train 000 6.832058e-03 0.759161
2019-12-04 08:09:52,409 train 050 9.459878e-03 0.691654
2019-12-04 08:10:05,893 train 100 9.279309e-03 0.682736
2019-12-04 08:10:19,382 train 150 9.072179e-03 0.697245
2019-12-04 08:10:32,860 train 200 8.603362e-03 0.711224
2019-12-04 08:10:38,924 training loss; R2: 8.694534e-03 0.707726
2019-12-04 08:10:39,066 valid 000 9.325191e-03 0.542209
2019-12-04 08:10:40,488 validation loss; R2: 1.019666e-02 0.647223
2019-12-04 08:10:40,515 epoch 4 lr 2.000000e-03
2019-12-04 08:10:40,873 train 000 9.427301e-03 0.735672
2019-12-04 08:10:54,347 train 050 7.793007e-03 0.722658
2019-12-04 08:11:07,826 train 100 7.868124e-03 0.724153
2019-12-04 08:11:21,304 train 150 8.238400e-03 0.716744
2019-12-04 08:11:34,782 train 200 8.435087e-03 0.715636
2019-12-04 08:11:40,844 training loss; R2: 8.396232e-03 0.715402
2019-12-04 08:11:40,985 valid 000 4.317728e-03 0.733890
2019-12-04 08:11:42,407 validation loss; R2: 4.334611e-03 0.843919
2019-12-04 08:11:42,434 epoch 5 lr 2.000000e-03
2019-12-04 08:11:42,793 train 000 5.540488e-03 0.785849
2019-12-04 08:11:56,270 train 050 6.274867e-03 0.779763
2019-12-04 08:12:09,737 train 100 6.332049e-03 0.783338
2019-12-04 08:12:23,213 train 150 6.479991e-03 0.774631
2019-12-04 08:12:36,689 train 200 6.600533e-03 0.777290
2019-12-04 08:12:42,748 training loss; R2: 6.761937e-03 0.773453
2019-12-04 08:12:42,892 valid 000 2.619788e-03 0.884700
2019-12-04 08:12:44,314 validation loss; R2: 4.092344e-03 0.850761
2019-12-04 08:12:44,341 epoch 6 lr 2.000000e-03
2019-12-04 08:12:44,696 train 000 5.280430e-03 0.813719
2019-12-04 08:12:58,167 train 050 6.016141e-03 0.784272
2019-12-04 08:13:11,639 train 100 6.114353e-03 0.783264
2019-12-04 08:13:25,113 train 150 6.097719e-03 0.789740
2019-12-04 08:13:38,587 train 200 6.151904e-03 0.784429
2019-12-04 08:13:44,645 training loss; R2: 6.426416e-03 0.777206
2019-12-04 08:13:44,790 valid 000 8.077600e-03 0.686639
2019-12-04 08:13:46,213 validation loss; R2: 8.812306e-03 0.683620
2019-12-04 08:13:46,240 epoch 7 lr 2.000000e-03
2019-12-04 08:13:46,601 train 000 8.630618e-03 0.821851
2019-12-04 08:14:00,073 train 050 7.256921e-03 0.749076
2019-12-04 08:14:13,541 train 100 7.311231e-03 0.744183
2019-12-04 08:14:27,011 train 150 6.908903e-03 0.762935
2019-12-04 08:14:40,482 train 200 6.544575e-03 0.773708
2019-12-04 08:14:46,541 training loss; R2: 6.467514e-03 0.776816
2019-12-04 08:14:46,684 valid 000 5.692509e-03 0.851578
2019-12-04 08:14:48,106 validation loss; R2: 4.537303e-03 0.850051
2019-12-04 08:14:48,133 epoch 8 lr 2.000000e-03
2019-12-04 08:14:48,492 train 000 4.857888e-03 0.803700
2019-12-04 08:15:01,959 train 050 5.719281e-03 0.791352
2019-12-04 08:15:15,426 train 100 5.769155e-03 0.796025
2019-12-04 08:15:28,893 train 150 5.827206e-03 0.801490
2019-12-04 08:15:42,357 train 200 5.852935e-03 0.798927
2019-12-04 08:15:48,414 training loss; R2: 5.743076e-03 0.802841
2019-12-04 08:15:48,555 valid 000 2.488202e-03 0.816470
2019-12-04 08:15:49,977 validation loss; R2: 3.530037e-03 0.871932
2019-12-04 08:15:50,004 epoch 9 lr 2.000000e-03
2019-12-04 08:15:50,363 train 000 3.723742e-03 0.820123
2019-12-04 08:16:03,837 train 050 4.907109e-03 0.848201
2019-12-04 08:16:17,308 train 100 4.993691e-03 0.836891
2019-12-04 08:16:30,777 train 150 4.982839e-03 0.835496
2019-12-04 08:16:44,245 train 200 5.002813e-03 0.829487
2019-12-04 08:16:50,306 training loss; R2: 4.944849e-03 0.831097
2019-12-04 08:16:50,447 valid 000 2.665488e-03 0.887121
2019-12-04 08:16:51,870 validation loss; R2: 3.805099e-03 0.862264
2019-12-04 08:16:51,898 epoch 10 lr 2.000000e-03
2019-12-04 08:16:52,258 train 000 6.197264e-03 0.864847
2019-12-04 08:17:05,723 train 050 5.335470e-03 0.812631
2019-12-04 08:17:19,189 train 100 5.084680e-03 0.827617
2019-12-04 08:17:32,652 train 150 4.881794e-03 0.834198
2019-12-04 08:17:46,117 train 200 4.944191e-03 0.832732
2019-12-04 08:17:52,172 training loss; R2: 4.946353e-03 0.833527
2019-12-04 08:17:52,314 valid 000 9.004267e-02 -3.276784
2019-12-04 08:17:53,734 validation loss; R2: 9.486665e-02 -2.479441
2019-12-04 08:17:53,760 epoch 11 lr 2.000000e-03
2019-12-04 08:17:54,116 train 000 3.914454e-03 0.834073
2019-12-04 08:18:07,574 train 050 5.790593e-03 0.803905
2019-12-04 08:18:21,031 train 100 5.412224e-03 0.817473
2019-12-04 08:18:34,483 train 150 5.221872e-03 0.828599
2019-12-04 08:18:47,932 train 200 4.930010e-03 0.835613
2019-12-04 08:18:53,982 training loss; R2: 4.880008e-03 0.835698
2019-12-04 08:18:54,124 valid 000 3.263343e-02 -0.001866
2019-12-04 08:18:55,544 validation loss; R2: 3.331723e-02 -0.083872
2019-12-04 08:18:55,570 epoch 12 lr 2.000000e-03
2019-12-04 08:18:55,929 train 000 4.146198e-03 0.842975
2019-12-04 08:19:09,376 train 050 4.455593e-03 0.845929
2019-12-04 08:19:22,824 train 100 4.627054e-03 0.833829
2019-12-04 08:19:36,272 train 150 4.866824e-03 0.830752
2019-12-04 08:19:49,717 train 200 4.933952e-03 0.831575
2019-12-04 08:19:55,764 training loss; R2: 4.965688e-03 0.832697
2019-12-04 08:19:55,905 valid 000 3.454520e-02 -0.218230
2019-12-04 08:19:57,326 validation loss; R2: 3.733723e-02 -0.191179
2019-12-04 08:19:57,353 epoch 13 lr 2.000000e-03
2019-12-04 08:19:57,711 train 000 2.911133e-03 0.787273
2019-12-04 08:20:11,151 train 050 6.109716e-03 0.792037
2019-12-04 08:20:24,584 train 100 5.292560e-03 0.816461
2019-12-04 08:20:38,017 train 150 4.881537e-03 0.831692
2019-12-04 08:20:51,446 train 200 4.756391e-03 0.837390
2019-12-04 08:20:57,490 training loss; R2: 4.783293e-03 0.837617
2019-12-04 08:20:57,631 valid 000 4.272256e-02 -0.091187
2019-12-04 08:20:59,051 validation loss; R2: 3.578382e-02 -0.134136
2019-12-04 08:20:59,082 epoch 14 lr 2.000000e-03
2019-12-04 08:20:59,438 train 000 3.627314e-03 0.850143
2019-12-04 08:21:12,866 train 050 4.778698e-03 0.823992
2019-12-04 08:21:26,297 train 100 4.940999e-03 0.823520
2019-12-04 08:21:39,724 train 150 4.749991e-03 0.833073
2019-12-04 08:21:53,144 train 200 4.637316e-03 0.838630
2019-12-04 08:21:59,183 training loss; R2: 4.592840e-03 0.841059
2019-12-04 08:21:59,325 valid 000 1.872748e-02 -0.384406
2019-12-04 08:22:00,745 validation loss; R2: 4.195785e-02 -0.349132
2019-12-04 08:22:00,771 epoch 15 lr 2.000000e-03
2019-12-04 08:22:01,127 train 000 4.399486e-03 0.869494
2019-12-04 08:22:14,548 train 050 4.911797e-03 0.836781
2019-12-04 08:22:27,963 train 100 4.503622e-03 0.844725
2019-12-04 08:22:41,373 train 150 4.478422e-03 0.840430
2019-12-04 08:22:54,783 train 200 4.431351e-03 0.845195
2019-12-04 08:23:00,815 training loss; R2: 4.368818e-03 0.847994
2019-12-04 08:23:00,956 valid 000 2.233543e-02 -0.007269
2019-12-04 08:23:02,376 validation loss; R2: 3.186723e-02 -0.019114
2019-12-04 08:23:02,400 epoch 16 lr 2.000000e-03
2019-12-04 08:23:02,763 train 000 2.637707e-03 0.949998
2019-12-04 08:23:16,171 train 050 4.353383e-03 0.859540
2019-12-04 08:23:29,573 train 100 4.425965e-03 0.862675
2019-12-04 08:23:42,974 train 150 4.626010e-03 0.855025
2019-12-04 08:23:56,372 train 200 4.531877e-03 0.851147
2019-12-04 08:24:02,397 training loss; R2: 4.453167e-03 0.850492
2019-12-04 08:24:02,539 valid 000 1.448847e-01 -1.005953
2019-12-04 08:24:03,958 validation loss; R2: 1.178792e-01 -3.471508
2019-12-04 08:24:03,984 epoch 17 lr 2.000000e-03
2019-12-04 08:24:04,339 train 000 4.013377e-03 0.899489
2019-12-04 08:24:17,739 train 050 4.433475e-03 0.865220
2019-12-04 08:24:31,140 train 100 4.426182e-03 0.858443
2019-12-04 08:24:44,536 train 150 4.288641e-03 0.855597
2019-12-04 08:24:57,933 train 200 4.221713e-03 0.854781
2019-12-04 08:25:03,959 training loss; R2: 4.290427e-03 0.853766
2019-12-04 08:25:04,102 valid 000 5.578035e-02 0.001379
2019-12-04 08:25:05,521 validation loss; R2: 3.200960e-02 -0.035209
2019-12-04 08:25:05,548 epoch 18 lr 2.000000e-03
2019-12-04 08:25:05,906 train 000 3.869334e-03 0.871408
2019-12-04 08:25:19,300 train 050 4.190345e-03 0.847456
2019-12-04 08:25:32,692 train 100 4.172862e-03 0.847608
2019-12-04 08:25:46,086 train 150 4.294073e-03 0.848648
2019-12-04 08:25:59,474 train 200 4.302595e-03 0.852595
2019-12-04 08:26:05,500 training loss; R2: 4.276372e-03 0.854188
2019-12-04 08:26:05,641 valid 000 6.752876e-02 -0.595801
2019-12-04 08:26:07,061 validation loss; R2: 5.403798e-02 -0.754987
2019-12-04 08:26:07,088 epoch 19 lr 2.000000e-03
2019-12-04 08:26:07,447 train 000 3.916134e-03 0.859803
2019-12-04 08:26:20,842 train 050 4.407589e-03 0.849686
2019-12-04 08:26:34,239 train 100 4.603684e-03 0.842358
2019-12-04 08:26:47,627 train 150 4.486644e-03 0.846323
2019-12-04 08:27:01,016 train 200 4.384303e-03 0.847125
2019-12-04 08:27:07,040 training loss; R2: 4.336573e-03 0.847840
2019-12-04 08:27:07,184 valid 000 8.765852e-02 -1.466144
2019-12-04 08:27:08,603 validation loss; R2: 8.173219e-02 -1.729124
