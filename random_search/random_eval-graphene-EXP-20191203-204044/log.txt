2019-12-03 20:40:44,138 gpu device = 1
2019-12-03 20:40:44,138 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=96, cutout=False, cutout_length=16, data='../data', dataset='graphene', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name=None, gpu=1, grad_clip=5, gz_dtree=False, init_channels=36, layers=8, learning_rate=0.002, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-graphene-EXP-20191203-204044', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-12-03 20:40:47,376 param size = 1.064485MB
2019-12-03 20:40:47,379 epoch 0 lr 2.000000e-03
2019-12-03 20:40:49,983 train 000 5.487478e-01 -24.085853
2019-12-03 20:41:01,694 train 050 1.663863e-01 -4.828478
2019-12-03 20:41:13,364 train 100 9.912481e-02 -2.416263
2019-12-03 20:41:25,035 train 150 7.569437e-02 -1.592610
2019-12-03 20:41:36,718 train 200 6.351986e-02 -1.147122
2019-12-03 20:41:42,979 training loss; R2: 5.951415e-02 -1.015757
2019-12-03 20:41:43,104 valid 000 3.792513e-02 0.367316
2019-12-03 20:41:44,624 validation loss; R2: 1.761285e-02 0.454724
2019-12-03 20:41:44,646 epoch 1 lr 2.000000e-03
2019-12-03 20:41:45,001 train 000 2.159061e-02 0.085872
2019-12-03 20:41:56,673 train 050 2.219027e-02 0.237223
2019-12-03 20:42:08,360 train 100 2.164823e-02 0.282879
2019-12-03 20:42:20,042 train 150 2.096561e-02 0.314811
2019-12-03 20:42:31,715 train 200 2.076556e-02 0.331236
2019-12-03 20:42:36,961 training loss; R2: 2.042296e-02 0.339138
2019-12-03 20:42:37,082 valid 000 7.347183e-03 0.584531
2019-12-03 20:42:38,290 validation loss; R2: 1.160401e-02 0.617812
2019-12-03 20:42:38,330 epoch 2 lr 2.000000e-03
2019-12-03 20:42:38,645 train 000 2.540775e-02 0.471443
2019-12-03 20:42:50,022 train 050 1.768958e-02 0.429431
2019-12-03 20:43:01,398 train 100 1.521665e-02 0.494619
2019-12-03 20:43:12,771 train 150 1.534071e-02 0.509846
2019-12-03 20:43:24,147 train 200 1.477876e-02 0.514984
2019-12-03 20:43:29,264 training loss; R2: 1.438704e-02 0.525341
2019-12-03 20:43:29,384 valid 000 9.173525e-03 0.715132
2019-12-03 20:43:30,592 validation loss; R2: 6.948286e-03 0.766060
2019-12-03 20:43:30,613 epoch 3 lr 2.000000e-03
2019-12-03 20:43:30,919 train 000 7.417347e-03 0.657403
2019-12-03 20:43:42,296 train 050 1.122636e-02 0.615066
2019-12-03 20:43:53,673 train 100 1.210453e-02 0.607514
2019-12-03 20:44:05,040 train 150 1.184592e-02 0.607188
2019-12-03 20:44:16,419 train 200 1.155577e-02 0.618721
2019-12-03 20:44:21,534 training loss; R2: 1.144067e-02 0.621336
2019-12-03 20:44:21,660 valid 000 6.489202e-03 0.799136
2019-12-03 20:44:22,866 validation loss; R2: 6.774465e-03 0.775211
2019-12-03 20:44:22,887 epoch 4 lr 2.000000e-03
2019-12-03 20:44:23,185 train 000 7.603696e-03 0.568020
2019-12-03 20:44:34,558 train 050 1.142922e-02 0.641342
2019-12-03 20:44:45,930 train 100 1.034119e-02 0.662438
2019-12-03 20:44:57,302 train 150 1.010764e-02 0.666748
2019-12-03 20:45:08,674 train 200 9.911499e-03 0.669163
2019-12-03 20:45:13,788 training loss; R2: 9.837652e-03 0.667493
2019-12-03 20:45:13,911 valid 000 5.134605e-03 0.693504
2019-12-03 20:45:15,117 validation loss; R2: 6.320149e-03 0.782833
2019-12-03 20:45:15,139 epoch 5 lr 2.000000e-03
2019-12-03 20:45:15,440 train 000 6.651757e-03 0.750501
2019-12-03 20:45:26,811 train 050 9.092694e-03 0.692315
2019-12-03 20:45:38,180 train 100 9.290613e-03 0.671557
2019-12-03 20:45:49,552 train 150 9.597099e-03 0.666659
2019-12-03 20:46:00,931 train 200 9.429989e-03 0.676710
2019-12-03 20:46:06,055 training loss; R2: 9.279385e-03 0.682658
2019-12-03 20:46:06,176 valid 000 4.209412e-03 0.820748
2019-12-03 20:46:07,383 validation loss; R2: 4.628262e-03 0.842520
2019-12-03 20:46:07,412 epoch 6 lr 2.000000e-03
2019-12-03 20:46:07,727 train 000 6.257096e-03 0.786938
2019-12-03 20:46:19,103 train 050 7.600316e-03 0.742066
2019-12-03 20:46:30,477 train 100 7.379676e-03 0.748469
2019-12-03 20:46:41,863 train 150 7.450569e-03 0.744907
2019-12-03 20:46:53,236 train 200 7.666271e-03 0.745602
2019-12-03 20:46:58,349 training loss; R2: 7.689060e-03 0.747603
2019-12-03 20:46:58,477 valid 000 7.171323e-03 0.774554
2019-12-03 20:46:59,683 validation loss; R2: 6.337516e-03 0.769911
2019-12-03 20:46:59,704 epoch 7 lr 2.000000e-03
2019-12-03 20:47:00,008 train 000 7.975307e-03 0.677376
2019-12-03 20:47:11,372 train 050 8.190444e-03 0.723342
2019-12-03 20:47:22,744 train 100 8.138355e-03 0.731384
2019-12-03 20:47:34,114 train 150 7.788532e-03 0.739257
2019-12-03 20:47:45,483 train 200 7.667821e-03 0.742129
2019-12-03 20:47:50,597 training loss; R2: 8.052610e-03 0.724342
2019-12-03 20:47:50,725 valid 000 5.138524e-03 0.829594
2019-12-03 20:47:51,932 validation loss; R2: 4.668550e-03 0.845409
2019-12-03 20:47:51,954 epoch 8 lr 2.000000e-03
2019-12-03 20:47:52,252 train 000 6.684260e-03 0.635280
2019-12-03 20:48:03,618 train 050 8.989624e-03 0.698353
2019-12-03 20:48:14,985 train 100 8.741066e-03 0.702484
2019-12-03 20:48:26,350 train 150 8.393051e-03 0.720116
2019-12-03 20:48:37,712 train 200 8.265800e-03 0.727746
2019-12-03 20:48:42,824 training loss; R2: 8.328067e-03 0.724397
2019-12-03 20:48:42,949 valid 000 7.075672e-03 0.801272
2019-12-03 20:48:44,156 validation loss; R2: 7.014460e-03 0.765536
2019-12-03 20:48:44,178 epoch 9 lr 2.000000e-03
2019-12-03 20:48:44,476 train 000 7.593425e-03 0.701544
2019-12-03 20:48:55,835 train 050 7.081079e-03 0.736912
2019-12-03 20:49:07,203 train 100 7.040748e-03 0.751342
2019-12-03 20:49:18,564 train 150 7.373205e-03 0.748076
2019-12-03 20:49:29,929 train 200 7.418245e-03 0.747376
2019-12-03 20:49:35,038 training loss; R2: 7.388892e-03 0.749607
2019-12-03 20:49:35,167 valid 000 6.241169e-03 0.884993
2019-12-03 20:49:36,373 validation loss; R2: 4.535256e-03 0.829358
2019-12-03 20:49:36,395 epoch 10 lr 2.000000e-03
2019-12-03 20:49:36,695 train 000 5.965400e-03 0.861096
2019-12-03 20:49:48,058 train 050 6.837452e-03 0.768036
2019-12-03 20:49:59,429 train 100 6.975689e-03 0.763245
2019-12-03 20:50:10,789 train 150 7.107541e-03 0.765902
2019-12-03 20:50:22,151 train 200 7.154708e-03 0.766289
2019-12-03 20:50:27,258 training loss; R2: 7.114586e-03 0.766997
2019-12-03 20:50:27,388 valid 000 6.964394e-03 0.885661
2019-12-03 20:50:28,593 validation loss; R2: 3.495251e-03 0.884772
2019-12-03 20:50:28,614 epoch 11 lr 2.000000e-03
2019-12-03 20:50:28,916 train 000 5.383926e-03 0.677896
2019-12-03 20:50:40,270 train 050 6.348718e-03 0.783917
2019-12-03 20:50:51,625 train 100 6.596174e-03 0.784170
2019-12-03 20:51:02,983 train 150 6.536188e-03 0.785308
2019-12-03 20:51:14,339 train 200 6.495367e-03 0.787886
2019-12-03 20:51:19,445 training loss; R2: 6.416851e-03 0.788915
2019-12-03 20:51:19,562 valid 000 2.550856e-03 0.941060
2019-12-03 20:51:20,768 validation loss; R2: 2.971150e-03 0.900370
2019-12-03 20:51:20,789 epoch 12 lr 2.000000e-03
2019-12-03 20:51:21,087 train 000 5.651724e-03 0.877879
2019-12-03 20:51:32,439 train 050 5.687384e-03 0.803615
2019-12-03 20:51:43,799 train 100 6.193198e-03 0.792768
2019-12-03 20:51:55,165 train 150 6.106548e-03 0.792635
2019-12-03 20:52:06,520 train 200 6.239320e-03 0.790168
2019-12-03 20:52:11,624 training loss; R2: 6.188760e-03 0.788987
2019-12-03 20:52:11,746 valid 000 2.298958e-03 0.922486
2019-12-03 20:52:12,952 validation loss; R2: 3.200988e-03 0.896960
2019-12-03 20:52:12,974 epoch 13 lr 2.000000e-03
2019-12-03 20:52:13,270 train 000 6.865770e-03 0.853671
2019-12-03 20:52:24,629 train 050 6.712412e-03 0.778388
2019-12-03 20:52:35,981 train 100 6.789921e-03 0.767331
2019-12-03 20:52:47,334 train 150 6.747289e-03 0.773125
2019-12-03 20:52:58,688 train 200 6.623792e-03 0.776149
2019-12-03 20:53:03,793 training loss; R2: 6.529983e-03 0.779664
2019-12-03 20:53:03,911 valid 000 4.558155e-03 0.857083
2019-12-03 20:53:05,117 validation loss; R2: 4.876859e-03 0.838804
2019-12-03 20:53:05,138 epoch 14 lr 2.000000e-03
2019-12-03 20:53:05,439 train 000 4.452843e-03 0.569155
2019-12-03 20:53:16,794 train 050 5.898378e-03 0.803388
2019-12-03 20:53:28,151 train 100 5.903874e-03 0.803377
2019-12-03 20:53:39,503 train 150 5.884902e-03 0.805099
2019-12-03 20:53:50,859 train 200 5.730484e-03 0.804199
2019-12-03 20:53:55,967 training loss; R2: 5.783067e-03 0.805893
2019-12-03 20:53:56,086 valid 000 2.770394e-02 -0.011033
2019-12-03 20:53:57,291 validation loss; R2: 3.297073e-02 -0.042583
2019-12-03 20:53:57,311 epoch 15 lr 2.000000e-03
2019-12-03 20:53:57,610 train 000 5.306277e-03 0.908602
2019-12-03 20:54:08,969 train 050 5.343792e-03 0.807106
2019-12-03 20:54:20,327 train 100 5.528282e-03 0.811106
2019-12-03 20:54:31,677 train 150 5.631202e-03 0.801957
2019-12-03 20:54:43,026 train 200 5.768030e-03 0.800018
2019-12-03 20:54:48,132 training loss; R2: 5.811785e-03 0.800614
2019-12-03 20:54:48,252 valid 000 3.801228e-02 -0.317637
2019-12-03 20:54:49,457 validation loss; R2: 4.130951e-02 -0.394685
2019-12-03 20:54:49,478 epoch 16 lr 2.000000e-03
2019-12-03 20:54:49,776 train 000 8.136266e-03 0.759447
2019-12-03 20:55:01,118 train 050 5.782817e-03 0.800278
2019-12-03 20:55:12,466 train 100 6.026369e-03 0.801156
2019-12-03 20:55:23,816 train 150 5.963808e-03 0.794099
2019-12-03 20:55:35,164 train 200 5.831785e-03 0.795223
2019-12-03 20:55:40,271 training loss; R2: 5.779641e-03 0.799058
2019-12-03 20:55:40,391 valid 000 5.221964e-02 -0.044485
2019-12-03 20:55:41,596 validation loss; R2: 3.187350e-02 -0.014069
2019-12-03 20:55:41,617 epoch 17 lr 2.000000e-03
2019-12-03 20:55:41,914 train 000 3.991251e-03 0.854976
2019-12-03 20:55:53,265 train 050 5.024360e-03 0.820082
2019-12-03 20:56:04,626 train 100 5.502047e-03 0.813700
2019-12-03 20:56:15,974 train 150 5.636669e-03 0.812166
2019-12-03 20:56:27,316 train 200 5.701504e-03 0.803506
2019-12-03 20:56:32,423 training loss; R2: 5.779432e-03 0.804036
2019-12-03 20:56:32,541 valid 000 1.503525e-02 -0.001925
2019-12-03 20:56:33,746 validation loss; R2: 3.139922e-02 0.007239
2019-12-03 20:56:33,767 epoch 18 lr 2.000000e-03
2019-12-03 20:56:34,071 train 000 7.086805e-03 0.727101
2019-12-03 20:56:45,427 train 050 5.587126e-03 0.810105
2019-12-03 20:56:56,790 train 100 5.705256e-03 0.813165
2019-12-03 20:57:08,144 train 150 5.555810e-03 0.815928
2019-12-03 20:57:19,491 train 200 5.511231e-03 0.816583
2019-12-03 20:57:24,596 training loss; R2: 5.606825e-03 0.816635
2019-12-03 20:57:24,715 valid 000 2.815909e-03 0.831237
2019-12-03 20:57:25,921 validation loss; R2: 5.342954e-03 0.820361
2019-12-03 20:57:25,943 epoch 19 lr 2.000000e-03
2019-12-03 20:57:26,244 train 000 3.865825e-03 0.733885
2019-12-03 20:57:37,580 train 050 6.037227e-03 0.798192
2019-12-03 20:57:48,936 train 100 5.843789e-03 0.809477
2019-12-03 20:58:00,297 train 150 5.756862e-03 0.810064
2019-12-03 20:58:11,656 train 200 5.593019e-03 0.813956
2019-12-03 20:58:16,763 training loss; R2: 5.523668e-03 0.815678
2019-12-03 20:58:16,879 valid 000 5.114351e-02 -0.095967
2019-12-03 20:58:18,085 validation loss; R2: 3.512975e-02 -0.148356
