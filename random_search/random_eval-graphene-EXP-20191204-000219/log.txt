2019-12-04 00:02:19,761 gpu device = 1
2019-12-04 00:02:19,761 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=96, cutout=False, cutout_length=16, data='../data', dataset='graphene', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name=None, gpu=1, grad_clip=5, gz_dtree=False, init_channels=36, layers=8, learning_rate=0.002, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-graphene-EXP-20191204-000219', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-12-04 00:02:22,986 param size = 1.350757MB
2019-12-04 00:02:22,989 epoch 0 lr 2.000000e-03
2019-12-04 00:02:25,884 train 000 3.432006e-01 -3.491632
2019-12-04 00:02:41,311 train 050 2.067185e-01 -4.638609
2019-12-04 00:02:56,475 train 100 1.198522e-01 -2.401474
2019-12-04 00:03:11,633 train 150 9.038291e-02 -1.621610
2019-12-04 00:03:26,796 train 200 7.536528e-02 -1.190216
2019-12-04 00:03:34,765 training loss; R2: 7.027796e-02 -1.046890
2019-12-04 00:03:34,912 valid 000 7.574638e-03 0.344948
2019-12-04 00:03:36,788 validation loss; R2: 2.043983e-02 0.344365
2019-12-04 00:03:36,813 epoch 1 lr 2.000000e-03
2019-12-04 00:03:37,423 train 000 2.217897e-02 0.156112
2019-12-04 00:03:52,593 train 050 2.497923e-02 0.241535
2019-12-04 00:04:07,749 train 100 2.807610e-02 0.096089
2019-12-04 00:04:22,912 train 150 2.794858e-02 0.069811
2019-12-04 00:04:38,069 train 200 2.585112e-02 0.156060
2019-12-04 00:04:44,889 training loss; R2: 2.497835e-02 0.183757
2019-12-04 00:04:45,039 valid 000 1.001318e-02 0.446810
2019-12-04 00:04:46,597 validation loss; R2: 1.299192e-02 0.577711
2019-12-04 00:04:46,625 epoch 2 lr 2.000000e-03
2019-12-04 00:04:47,014 train 000 1.961469e-02 0.512128
2019-12-04 00:05:02,175 train 050 1.662367e-02 0.447679
2019-12-04 00:05:17,345 train 100 1.739513e-02 0.436095
2019-12-04 00:05:32,513 train 150 1.591000e-02 0.481222
2019-12-04 00:05:47,684 train 200 1.526357e-02 0.504761
2019-12-04 00:05:54,508 training loss; R2: 1.486787e-02 0.516677
2019-12-04 00:05:54,658 valid 000 5.369442e-03 0.767533
2019-12-04 00:05:56,216 validation loss; R2: 6.625372e-03 0.787449
2019-12-04 00:05:56,243 epoch 3 lr 2.000000e-03
2019-12-04 00:05:56,632 train 000 9.044283e-03 0.746312
2019-12-04 00:06:11,813 train 050 1.182247e-02 0.630033
2019-12-04 00:06:26,993 train 100 1.149693e-02 0.631684
2019-12-04 00:06:42,172 train 150 1.094909e-02 0.648703
2019-12-04 00:06:56,959 train 200 1.084570e-02 0.648248
2019-12-04 00:07:03,605 training loss; R2: 1.076384e-02 0.646058
2019-12-04 00:07:03,748 valid 000 5.216246e-03 0.742347
2019-12-04 00:07:05,305 validation loss; R2: 7.164460e-03 0.770725
2019-12-04 00:07:05,332 epoch 4 lr 2.000000e-03
2019-12-04 00:07:05,711 train 000 7.465312e-03 0.770728
2019-12-04 00:07:20,497 train 050 1.032262e-02 0.645699
2019-12-04 00:07:35,280 train 100 1.020874e-02 0.655180
2019-12-04 00:07:50,064 train 150 9.826274e-03 0.671134
2019-12-04 00:08:04,851 train 200 9.845778e-03 0.673617
2019-12-04 00:08:11,501 training loss; R2: 9.719619e-03 0.678536
2019-12-04 00:08:11,644 valid 000 3.894739e-03 0.852700
2019-12-04 00:08:13,201 validation loss; R2: 4.782129e-03 0.842963
2019-12-04 00:08:13,228 epoch 5 lr 2.000000e-03
2019-12-04 00:08:13,606 train 000 6.134534e-03 0.746415
2019-12-04 00:08:28,386 train 050 8.629531e-03 0.704569
2019-12-04 00:08:43,166 train 100 8.574720e-03 0.701796
2019-12-04 00:08:57,945 train 150 8.761947e-03 0.702464
2019-12-04 00:09:12,724 train 200 8.541104e-03 0.709130
2019-12-04 00:09:19,368 training loss; R2: 8.334134e-03 0.716941
2019-12-04 00:09:19,523 valid 000 6.977760e-03 0.808425
2019-12-04 00:09:21,080 validation loss; R2: 4.635847e-03 0.840021
2019-12-04 00:09:21,107 epoch 6 lr 2.000000e-03
2019-12-04 00:09:21,491 train 000 1.015811e-02 0.393648
2019-12-04 00:09:36,264 train 050 9.167405e-03 0.681212
2019-12-04 00:09:51,040 train 100 8.593231e-03 0.698765
2019-12-04 00:10:05,849 train 150 8.249143e-03 0.705876
2019-12-04 00:10:20,634 train 200 8.200561e-03 0.719370
2019-12-04 00:10:27,281 training loss; R2: 8.224494e-03 0.721244
2019-12-04 00:10:27,427 valid 000 4.831983e-03 0.795266
2019-12-04 00:10:28,984 validation loss; R2: 5.557288e-03 0.812611
2019-12-04 00:10:29,011 epoch 7 lr 2.000000e-03
2019-12-04 00:10:29,392 train 000 1.080606e-02 0.733365
2019-12-04 00:10:44,177 train 050 7.911888e-03 0.723940
2019-12-04 00:10:58,968 train 100 7.481661e-03 0.744379
2019-12-04 00:11:13,754 train 150 7.899117e-03 0.737816
2019-12-04 00:11:28,540 train 200 8.046288e-03 0.733777
2019-12-04 00:11:35,190 training loss; R2: 8.007943e-03 0.732507
2019-12-04 00:11:35,332 valid 000 8.079696e-03 0.830499
2019-12-04 00:11:36,890 validation loss; R2: 4.220404e-03 0.860486
2019-12-04 00:11:36,916 epoch 8 lr 2.000000e-03
2019-12-04 00:11:37,296 train 000 8.593366e-03 0.699476
2019-12-04 00:11:52,077 train 050 7.455940e-03 0.759013
2019-12-04 00:12:06,859 train 100 7.482910e-03 0.760468
2019-12-04 00:12:21,640 train 150 7.412688e-03 0.759346
2019-12-04 00:12:36,425 train 200 7.343034e-03 0.757598
2019-12-04 00:12:43,071 training loss; R2: 7.289494e-03 0.756344
2019-12-04 00:12:43,214 valid 000 4.276994e-03 0.864145
2019-12-04 00:12:44,771 validation loss; R2: 5.544178e-03 0.807036
2019-12-04 00:12:44,798 epoch 9 lr 2.000000e-03
2019-12-04 00:12:45,178 train 000 4.303924e-03 0.806340
2019-12-04 00:12:59,955 train 050 6.430241e-03 0.789256
2019-12-04 00:13:14,729 train 100 6.970246e-03 0.764604
2019-12-04 00:13:29,508 train 150 7.081154e-03 0.764526
2019-12-04 00:13:44,284 train 200 6.899245e-03 0.768950
2019-12-04 00:13:50,933 training loss; R2: 6.856323e-03 0.769487
2019-12-04 00:13:51,076 valid 000 3.463021e-03 0.876862
2019-12-04 00:13:52,633 validation loss; R2: 4.011448e-03 0.860836
2019-12-04 00:13:52,660 epoch 10 lr 2.000000e-03
2019-12-04 00:13:53,047 train 000 5.899786e-03 0.644033
2019-12-04 00:14:07,826 train 050 6.751342e-03 0.773461
2019-12-04 00:14:22,599 train 100 6.486483e-03 0.775654
2019-12-04 00:14:37,371 train 150 6.720159e-03 0.775107
2019-12-04 00:14:52,145 train 200 6.459456e-03 0.780012
2019-12-04 00:14:58,788 training loss; R2: 6.438394e-03 0.782257
2019-12-04 00:14:58,932 valid 000 2.598962e-03 0.889691
2019-12-04 00:15:00,489 validation loss; R2: 3.219372e-03 0.888272
2019-12-04 00:15:00,516 epoch 11 lr 2.000000e-03
2019-12-04 00:15:00,895 train 000 3.968196e-03 0.867650
2019-12-04 00:15:15,666 train 050 6.622924e-03 0.788109
2019-12-04 00:15:30,436 train 100 6.343246e-03 0.795272
2019-12-04 00:15:45,208 train 150 6.724020e-03 0.784638
2019-12-04 00:15:59,977 train 200 6.469301e-03 0.782916
2019-12-04 00:16:06,618 training loss; R2: 6.510909e-03 0.781329
2019-12-04 00:16:06,778 valid 000 5.557294e-03 0.752295
2019-12-04 00:16:08,335 validation loss; R2: 6.855429e-03 0.773592
2019-12-04 00:16:08,362 epoch 12 lr 2.000000e-03
2019-12-04 00:16:08,741 train 000 3.327365e-03 0.835242
2019-12-04 00:16:23,501 train 050 6.359080e-03 0.788888
2019-12-04 00:16:38,269 train 100 6.165009e-03 0.796420
2019-12-04 00:16:53,037 train 150 5.960686e-03 0.803506
2019-12-04 00:17:07,807 train 200 5.849744e-03 0.805790
2019-12-04 00:17:14,447 training loss; R2: 5.983985e-03 0.799646
2019-12-04 00:17:14,589 valid 000 5.457595e-03 0.704700
2019-12-04 00:17:16,146 validation loss; R2: 4.752552e-03 0.842152
2019-12-04 00:17:16,172 epoch 13 lr 2.000000e-03
2019-12-04 00:17:16,557 train 000 5.130635e-03 0.852164
2019-12-04 00:17:31,326 train 050 6.873762e-03 0.755829
2019-12-04 00:17:46,086 train 100 6.385234e-03 0.776258
2019-12-04 00:18:00,853 train 150 5.919367e-03 0.788495
2019-12-04 00:18:15,617 train 200 5.974771e-03 0.791081
2019-12-04 00:18:22,259 training loss; R2: 6.063246e-03 0.789943
2019-12-04 00:18:22,400 valid 000 5.046965e-03 0.887863
2019-12-04 00:18:23,957 validation loss; R2: 4.784828e-03 0.833158
2019-12-04 00:18:23,982 epoch 14 lr 2.000000e-03
2019-12-04 00:18:24,358 train 000 3.034286e-03 0.896414
2019-12-04 00:18:39,118 train 050 6.603616e-03 0.788800
2019-12-04 00:18:53,884 train 100 6.553327e-03 0.787362
2019-12-04 00:19:08,648 train 150 6.331700e-03 0.786980
2019-12-04 00:19:23,413 train 200 6.127941e-03 0.792110
2019-12-04 00:19:30,053 training loss; R2: 6.256335e-03 0.789386
2019-12-04 00:19:30,195 valid 000 6.064397e-03 0.866434
2019-12-04 00:19:31,752 validation loss; R2: 3.442781e-03 0.884916
2019-12-04 00:19:31,779 epoch 15 lr 2.000000e-03
2019-12-04 00:19:32,161 train 000 5.787276e-03 0.844589
2019-12-04 00:19:46,925 train 050 5.413443e-03 0.810726
2019-12-04 00:20:01,687 train 100 5.626557e-03 0.807519
2019-12-04 00:20:16,449 train 150 5.585254e-03 0.810778
2019-12-04 00:20:31,214 train 200 5.557138e-03 0.810438
2019-12-04 00:20:37,852 training loss; R2: 5.687532e-03 0.809290
2019-12-04 00:20:37,994 valid 000 7.392020e-03 0.689478
2019-12-04 00:20:39,551 validation loss; R2: 6.733507e-03 0.770912
2019-12-04 00:20:39,578 epoch 16 lr 2.000000e-03
2019-12-04 00:20:39,958 train 000 5.049486e-03 0.783098
2019-12-04 00:20:54,720 train 050 5.841034e-03 0.801263
2019-12-04 00:21:09,474 train 100 5.898045e-03 0.804058
2019-12-04 00:21:24,227 train 150 5.639640e-03 0.805201
2019-12-04 00:21:38,980 train 200 5.636658e-03 0.806808
2019-12-04 00:21:45,615 training loss; R2: 5.619708e-03 0.808257
2019-12-04 00:21:45,759 valid 000 2.051874e-03 0.926160
2019-12-04 00:21:47,315 validation loss; R2: 2.868215e-03 0.904912
2019-12-04 00:21:47,342 epoch 17 lr 2.000000e-03
2019-12-04 00:21:47,720 train 000 5.401774e-03 0.864419
2019-12-04 00:22:02,461 train 050 5.916385e-03 0.816178
2019-12-04 00:22:17,207 train 100 5.799593e-03 0.803172
2019-12-04 00:22:31,946 train 150 5.898992e-03 0.803847
2019-12-04 00:22:46,685 train 200 5.744018e-03 0.808362
2019-12-04 00:22:53,310 training loss; R2: 5.790029e-03 0.806735
2019-12-04 00:22:53,452 valid 000 4.174789e-03 0.812249
2019-12-04 00:22:55,008 validation loss; R2: 4.486576e-03 0.844988
2019-12-04 00:22:55,041 epoch 18 lr 2.000000e-03
2019-12-04 00:22:55,425 train 000 4.187727e-03 0.871617
2019-12-04 00:23:10,165 train 050 5.462240e-03 0.825270
2019-12-04 00:23:25,234 train 100 5.940158e-03 0.808709
2019-12-04 00:23:40,348 train 150 6.315120e-03 0.797780
2019-12-04 00:23:55,456 train 200 6.139400e-03 0.797135
2019-12-04 00:24:02,252 training loss; R2: 6.111315e-03 0.798655
2019-12-04 00:24:02,400 valid 000 4.149844e-03 0.865277
2019-12-04 00:24:03,957 validation loss; R2: 4.033071e-03 0.865670
2019-12-04 00:24:03,984 epoch 19 lr 2.000000e-03
2019-12-04 00:24:04,378 train 000 4.779797e-03 0.807159
2019-12-04 00:24:19,471 train 050 6.040235e-03 0.797015
2019-12-04 00:24:34,563 train 100 5.831569e-03 0.797071
2019-12-04 00:24:49,656 train 150 5.806800e-03 0.802004
2019-12-04 00:25:04,751 train 200 5.848428e-03 0.802025
2019-12-04 00:25:11,540 training loss; R2: 5.887577e-03 0.801154
2019-12-04 00:25:11,692 valid 000 4.943591e+00 -218.726250
2019-12-04 00:25:13,248 validation loss; R2: 4.916529e+00 -170.700494
