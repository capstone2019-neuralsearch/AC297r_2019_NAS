2019-11-13 15:02:41,043 gpu device = 1
2019-11-13 15:02:41,043 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name='galaxy-zoo-cleaned', gpu=1, grad_clip=5, gz_dtree=False, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-galaxy-zoo-EXP-20191113-150240', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-11-13 15:02:52,495 param size = 0.201237MB
2019-11-13 15:02:52,498 epoch 0 lr 1.000000e-03
2019-11-13 15:02:54,586 train 000 2.246753e+00 -1254.829748
2019-11-13 15:03:00,209 train 050 1.161315e-01 -45.670482
2019-11-13 15:03:05,702 train 100 7.587284e-02 -24.616400
2019-11-13 15:03:11,306 train 150 6.059554e-02 -16.975082
2019-11-13 15:03:16,849 train 200 5.265251e-02 -13.141939
2019-11-13 15:03:22,479 train 250 4.761750e-02 -10.690103
2019-11-13 15:03:28,158 train 300 4.415460e-02 -9.067116
2019-11-13 15:03:33,663 train 350 4.156839e-02 -7.869811
2019-11-13 15:03:39,207 train 400 3.955956e-02 -6.961423
2019-11-13 15:03:44,700 train 450 3.796847e-02 -6.258327
2019-11-13 15:03:50,205 train 500 3.672464e-02 -5.689891
2019-11-13 15:03:55,697 train 550 3.562346e-02 -5.199464
2019-11-13 15:04:01,242 train 600 3.468804e-02 -4.797972
2019-11-13 15:04:06,725 train 650 3.385574e-02 -4.454894
2019-11-13 15:04:12,240 train 700 3.308634e-02 -4.167061
2019-11-13 15:04:17,777 train 750 3.242755e-02 -3.905622
2019-11-13 15:04:23,310 train 800 3.184349e-02 -3.677291
2019-11-13 15:04:28,730 train 850 3.128059e-02 -3.469002
2019-11-13 15:04:31,002 training loss; R2: 3.112763e-02 -3.415625
2019-11-13 15:04:31,302 valid 000 2.516966e-02 0.102338
2019-11-13 15:04:32,983 valid 050 2.122094e-02 0.057733
2019-11-13 15:04:34,582 validation loss; R2: 2.119312e-02 0.070016
2019-11-13 15:04:34,596 epoch 1 lr 1.000000e-03
2019-11-13 15:04:35,084 train 000 2.392576e-02 -0.109871
2019-11-13 15:04:40,386 train 050 2.240096e-02 -0.200753
2019-11-13 15:04:45,761 train 100 2.222419e-02 -0.251716
2019-11-13 15:04:51,140 train 150 2.202033e-02 -0.227854
2019-11-13 15:04:56,505 train 200 2.191505e-02 -0.204086
2019-11-13 15:05:01,881 train 250 2.168513e-02 -0.191665
2019-11-13 15:05:07,269 train 300 2.164095e-02 -0.202616
2019-11-13 15:05:12,638 train 350 2.144006e-02 -0.189302
2019-11-13 15:05:18,012 train 400 2.132370e-02 -0.187712
2019-11-13 15:05:23,400 train 450 2.122021e-02 -0.185901
2019-11-13 15:05:28,777 train 500 2.110951e-02 -0.176800
2019-11-13 15:05:34,144 train 550 2.104937e-02 -0.168239
2019-11-13 15:05:39,522 train 600 2.095569e-02 -0.159357
2019-11-13 15:05:44,896 train 650 2.090371e-02 -0.156982
2019-11-13 15:05:50,264 train 700 2.085105e-02 -0.305995
2019-11-13 15:05:55,643 train 750 2.078330e-02 -0.285493
2019-11-13 15:06:01,006 train 800 2.072010e-02 -0.273499
2019-11-13 15:06:06,375 train 850 2.065900e-02 -0.258725
2019-11-13 15:06:07,979 training loss; R2: 2.064280e-02 -0.254643
2019-11-13 15:06:08,331 valid 000 2.194618e-02 -0.009667
2019-11-13 15:06:10,005 valid 050 1.954026e-02 0.076800
2019-11-13 15:06:11,536 validation loss; R2: 1.947165e-02 0.059724
2019-11-13 15:06:11,547 epoch 2 lr 1.000000e-03
2019-11-13 15:06:11,924 train 000 1.760028e-02 0.170258
2019-11-13 15:06:17,084 train 050 1.923112e-02 -0.058600
2019-11-13 15:06:22,260 train 100 1.910625e-02 -0.117711
2019-11-13 15:06:27,429 train 150 1.906845e-02 -0.081855
2019-11-13 15:06:32,592 train 200 1.910176e-02 -0.064043
2019-11-13 15:06:37,750 train 250 1.896481e-02 -0.077347
2019-11-13 15:06:42,912 train 300 1.889348e-02 -0.061244
2019-11-13 15:06:48,083 train 350 1.878751e-02 -0.054719
2019-11-13 15:06:53,253 train 400 1.878645e-02 -0.054088
2019-11-13 15:06:58,416 train 450 1.872957e-02 -0.049114
2019-11-13 15:07:03,582 train 500 1.867817e-02 -0.048867
2019-11-13 15:07:08,860 train 550 1.863377e-02 -0.038299
2019-11-13 15:07:14,232 train 600 1.860312e-02 -0.030304
2019-11-13 15:07:19,603 train 650 1.853740e-02 -0.026740
2019-11-13 15:07:24,976 train 700 1.850347e-02 -0.021439
2019-11-13 15:07:30,346 train 750 1.847729e-02 -0.018656
2019-11-13 15:07:35,742 train 800 1.843225e-02 -0.013735
2019-11-13 15:07:41,114 train 850 1.840217e-02 -0.009527
2019-11-13 15:07:42,719 training loss; R2: 1.838965e-02 -0.008686
2019-11-13 15:07:43,040 valid 000 1.369686e-02 0.020703
2019-11-13 15:07:44,762 valid 050 1.502807e-02 0.236648
2019-11-13 15:07:46,331 validation loss; R2: 1.519526e-02 0.222921
2019-11-13 15:07:46,353 epoch 3 lr 1.000000e-03
2019-11-13 15:07:46,746 train 000 1.804020e-02 0.170495
2019-11-13 15:07:52,158 train 050 1.702486e-02 0.109348
2019-11-13 15:07:57,542 train 100 1.722036e-02 0.041321
2019-11-13 15:08:02,923 train 150 1.711348e-02 0.061037
2019-11-13 15:08:08,293 train 200 1.720970e-02 0.043405
2019-11-13 15:08:13,671 train 250 1.720911e-02 0.055434
2019-11-13 15:08:19,057 train 300 1.718372e-02 0.066070
2019-11-13 15:08:24,437 train 350 1.711661e-02 0.071859
2019-11-13 15:08:29,804 train 400 1.707752e-02 0.077692
2019-11-13 15:08:35,196 train 450 1.702056e-02 0.081517
2019-11-13 15:08:40,564 train 500 1.696418e-02 0.078211
2019-11-13 15:08:45,952 train 550 1.692589e-02 0.070411
2019-11-13 15:08:51,329 train 600 1.685583e-02 0.074045
2019-11-13 15:08:56,709 train 650 1.681165e-02 0.079619
2019-11-13 15:09:02,079 train 700 1.677504e-02 0.082713
2019-11-13 15:09:07,446 train 750 1.673289e-02 0.086962
2019-11-13 15:09:12,814 train 800 1.665479e-02 0.090707
2019-11-13 15:09:18,185 train 850 1.661872e-02 0.092809
2019-11-13 15:09:19,792 training loss; R2: 1.660950e-02 0.092790
2019-11-13 15:09:20,129 valid 000 1.256570e-02 0.346071
2019-11-13 15:09:21,873 valid 050 1.340306e-02 -0.092764
2019-11-13 15:09:23,438 validation loss; R2: 1.339901e-02 0.081342
2019-11-13 15:09:23,455 epoch 4 lr 1.000000e-03
2019-11-13 15:09:23,808 train 000 1.468924e-02 0.288265
2019-11-13 15:09:29,209 train 050 1.593564e-02 0.099231
2019-11-13 15:09:34,628 train 100 1.567872e-02 0.107275
2019-11-13 15:09:40,013 train 150 1.566471e-02 0.095236
2019-11-13 15:09:45,387 train 200 1.564436e-02 0.111520
2019-11-13 15:09:50,769 train 250 1.556383e-02 0.124406
2019-11-13 15:09:56,145 train 300 1.562909e-02 0.129093
2019-11-13 15:10:01,522 train 350 1.558934e-02 0.135285
2019-11-13 15:10:06,902 train 400 1.559569e-02 0.137933
2019-11-13 15:10:12,278 train 450 1.557527e-02 0.112431
2019-11-13 15:10:17,658 train 500 1.551287e-02 0.120290
2019-11-13 15:10:23,034 train 550 1.552801e-02 0.122447
2019-11-13 15:10:28,411 train 600 1.548939e-02 0.124837
2019-11-13 15:10:33,791 train 650 1.547974e-02 0.128046
2019-11-13 15:10:39,164 train 700 1.546513e-02 0.133595
2019-11-13 15:10:44,538 train 750 1.542363e-02 0.132727
2019-11-13 15:10:49,913 train 800 1.539927e-02 0.136403
2019-11-13 15:10:55,286 train 850 1.534670e-02 0.138809
2019-11-13 15:10:56,891 training loss; R2: 1.532987e-02 0.133625
2019-11-13 15:10:57,218 valid 000 1.244251e-02 0.327673
2019-11-13 15:10:58,933 valid 050 1.289298e-02 0.271407
2019-11-13 15:11:00,525 validation loss; R2: 1.283688e-02 0.272096
2019-11-13 15:11:00,538 epoch 5 lr 1.000000e-03
2019-11-13 15:11:00,925 train 000 1.159649e-02 0.303918
2019-11-13 15:11:06,331 train 050 1.460485e-02 0.197567
2019-11-13 15:11:11,791 train 100 1.468871e-02 0.177275
2019-11-13 15:11:17,190 train 150 1.469233e-02 0.177043
2019-11-13 15:11:22,597 train 200 1.464910e-02 0.186542
2019-11-13 15:11:28,005 train 250 1.467009e-02 0.189789
2019-11-13 15:11:33,421 train 300 1.461974e-02 0.188985
2019-11-13 15:11:38,826 train 350 1.461888e-02 0.186111
2019-11-13 15:11:44,233 train 400 1.461409e-02 0.189650
2019-11-13 15:11:49,641 train 450 1.460917e-02 0.187032
2019-11-13 15:11:55,070 train 500 1.461230e-02 0.187762
2019-11-13 15:12:00,492 train 550 1.460708e-02 0.185448
2019-11-13 15:12:05,888 train 600 1.460365e-02 0.183403
2019-11-13 15:12:11,279 train 650 1.458477e-02 0.182163
2019-11-13 15:12:16,682 train 700 1.456369e-02 0.182513
2019-11-13 15:12:22,083 train 750 1.455959e-02 0.185293
2019-11-13 15:12:27,486 train 800 1.453876e-02 0.186738
2019-11-13 15:12:32,884 train 850 1.450622e-02 0.185748
2019-11-13 15:12:34,501 training loss; R2: 1.449159e-02 0.186345
2019-11-13 15:12:34,826 valid 000 1.276438e-02 0.278391
2019-11-13 15:12:36,543 valid 050 1.175247e-02 0.325531
2019-11-13 15:12:38,115 validation loss; R2: 1.174469e-02 0.328926
2019-11-13 15:12:38,126 epoch 6 lr 1.000000e-03
2019-11-13 15:12:38,488 train 000 1.604437e-02 0.205589
2019-11-13 15:12:43,755 train 050 1.429756e-02 0.208621
2019-11-13 15:12:49,004 train 100 1.409933e-02 0.188492
2019-11-13 15:12:54,514 train 150 1.410138e-02 0.198018
2019-11-13 15:12:59,926 train 200 1.395826e-02 0.188807
2019-11-13 15:13:05,322 train 250 1.396313e-02 0.201847
2019-11-13 15:13:10,727 train 300 1.401316e-02 0.208002
2019-11-13 15:13:16,138 train 350 1.400272e-02 0.205991
2019-11-13 15:13:21,532 train 400 1.399394e-02 -1.957595
2019-11-13 15:13:26,933 train 450 1.398852e-02 -1.714995
2019-11-13 15:13:32,336 train 500 1.397239e-02 -1.528209
2019-11-13 15:13:37,728 train 550 1.393569e-02 -1.370941
2019-11-13 15:13:43,130 train 600 1.390299e-02 -1.238091
2019-11-13 15:13:48,531 train 650 1.387068e-02 -1.123954
2019-11-13 15:13:53,924 train 700 1.385117e-02 -1.028977
2019-11-13 15:13:59,318 train 750 1.387869e-02 -0.951614
2019-11-13 15:14:04,715 train 800 1.385632e-02 -0.880420
2019-11-13 15:14:10,110 train 850 1.383196e-02 -0.817257
2019-11-13 15:14:11,727 training loss; R2: 1.382439e-02 -0.798796
2019-11-13 15:14:12,062 valid 000 1.436523e-02 0.312311
2019-11-13 15:14:13,824 valid 050 1.182539e-02 0.311182
2019-11-13 15:14:15,376 validation loss; R2: 1.198645e-02 0.323679
2019-11-13 15:14:15,388 epoch 7 lr 1.000000e-03
2019-11-13 15:14:15,762 train 000 1.178496e-02 0.271996
2019-11-13 15:14:21,068 train 050 1.357020e-02 0.235059
2019-11-13 15:14:26,489 train 100 1.362779e-02 0.213781
2019-11-13 15:14:31,687 train 150 1.372041e-02 0.207465
2019-11-13 15:14:36,879 train 200 1.367260e-02 0.207180
2019-11-13 15:14:42,068 train 250 1.361160e-02 0.217438
2019-11-13 15:14:47,466 train 300 1.354824e-02 0.207291
2019-11-13 15:14:52,862 train 350 1.353035e-02 0.213780
2019-11-13 15:14:58,266 train 400 1.351074e-02 0.218807
2019-11-13 15:15:03,666 train 450 1.352080e-02 0.218351
2019-11-13 15:15:09,055 train 500 1.348357e-02 0.219603
2019-11-13 15:15:14,433 train 550 1.343510e-02 0.224317
2019-11-13 15:15:19,805 train 600 1.340521e-02 0.225832
2019-11-13 15:15:25,184 train 650 1.337887e-02 0.225947
2019-11-13 15:15:30,562 train 700 1.340282e-02 0.226451
2019-11-13 15:15:35,946 train 750 1.340697e-02 0.227431
2019-11-13 15:15:41,328 train 800 1.339441e-02 0.227143
2019-11-13 15:15:46,698 train 850 1.340495e-02 0.226407
2019-11-13 15:15:48,307 training loss; R2: 1.340098e-02 0.226931
2019-11-13 15:15:48,643 valid 000 1.050592e-01 -4.090011
2019-11-13 15:15:50,346 valid 050 1.034376e-01 -11.414678
2019-11-13 15:15:51,918 validation loss; R2: 1.034508e-01 -11.759018
2019-11-13 15:15:51,929 epoch 8 lr 1.000000e-03
2019-11-13 15:15:52,275 train 000 1.313821e-02 0.165440
2019-11-13 15:15:57,773 train 050 1.332474e-02 0.188087
2019-11-13 15:16:03,228 train 100 1.316244e-02 0.183431
2019-11-13 15:16:08,681 train 150 1.327347e-02 0.204319
2019-11-13 15:16:14,136 train 200 1.323732e-02 0.208654
2019-11-13 15:16:19,513 train 250 1.330051e-02 0.211459
2019-11-13 15:16:24,881 train 300 1.329588e-02 0.219256
2019-11-13 15:16:30,260 train 350 1.326299e-02 0.221555
2019-11-13 15:16:35,653 train 400 1.321900e-02 0.224901
2019-11-13 15:16:41,035 train 450 1.320375e-02 0.228306
2019-11-13 15:16:46,417 train 500 1.319310e-02 0.225429
2019-11-13 15:16:51,821 train 550 1.314016e-02 0.227554
2019-11-13 15:16:57,201 train 600 1.311774e-02 0.230185
2019-11-13 15:17:02,584 train 650 1.311362e-02 0.230651
2019-11-13 15:17:07,959 train 700 1.309291e-02 0.221562
2019-11-13 15:17:13,343 train 750 1.308995e-02 0.223126
2019-11-13 15:17:18,711 train 800 1.308265e-02 0.225707
2019-11-13 15:17:24,085 train 850 1.308363e-02 0.207000
2019-11-13 15:17:25,688 training loss; R2: 1.308525e-02 0.208073
2019-11-13 15:17:26,034 valid 000 1.241548e+00 -91.954221
2019-11-13 15:17:27,777 valid 050 1.242303e+00 -64.505412
2019-11-13 15:17:29,338 validation loss; R2: 1.244437e+00 -66.986344
2019-11-13 15:17:29,357 epoch 9 lr 1.000000e-03
2019-11-13 15:17:29,754 train 000 1.445146e-02 0.229483
2019-11-13 15:17:35,247 train 050 1.323097e-02 0.268148
2019-11-13 15:17:40,725 train 100 1.307212e-02 0.264780
2019-11-13 15:17:46,165 train 150 1.297844e-02 0.247361
2019-11-13 15:17:51,537 train 200 1.292565e-02 0.255141
2019-11-13 15:17:56,919 train 250 1.291978e-02 0.256109
2019-11-13 15:18:02,295 train 300 1.289563e-02 0.253105
2019-11-13 15:18:07,667 train 350 1.286939e-02 0.255943
2019-11-13 15:18:13,055 train 400 1.284470e-02 0.252083
2019-11-13 15:18:18,448 train 450 1.286148e-02 0.254119
2019-11-13 15:18:23,840 train 500 1.287059e-02 0.252221
2019-11-13 15:18:29,249 train 550 1.285569e-02 0.251075
2019-11-13 15:18:34,648 train 600 1.285989e-02 0.251634
2019-11-13 15:18:40,035 train 650 1.286423e-02 0.253614
2019-11-13 15:18:45,419 train 700 1.283972e-02 0.255448
2019-11-13 15:18:50,807 train 750 1.282298e-02 0.253616
2019-11-13 15:18:56,191 train 800 1.281801e-02 0.252978
2019-11-13 15:19:01,577 train 850 1.281169e-02 0.250799
2019-11-13 15:19:03,186 training loss; R2: 1.280753e-02 -0.274477
2019-11-13 15:19:03,524 valid 000 1.098339e-01 -4.933441
2019-11-13 15:19:05,244 valid 050 1.019423e-01 -5.883106
2019-11-13 15:19:06,806 validation loss; R2: 1.020073e-01 -6.145449
2019-11-13 15:19:06,826 epoch 10 lr 1.000000e-03
2019-11-13 15:19:07,223 train 000 1.232360e-02 0.334176
2019-11-13 15:19:12,696 train 050 1.251694e-02 0.281772
2019-11-13 15:19:18,121 train 100 1.245017e-02 0.219456
2019-11-13 15:19:23,345 train 150 1.247459e-02 0.206452
2019-11-13 15:19:28,760 train 200 1.249149e-02 0.221355
2019-11-13 15:19:34,176 train 250 1.252434e-02 0.229899
2019-11-13 15:19:39,568 train 300 1.247278e-02 0.235662
2019-11-13 15:19:44,986 train 350 1.242622e-02 0.193878
2019-11-13 15:19:50,367 train 400 1.247324e-02 0.203005
2019-11-13 15:19:55,759 train 450 1.250119e-02 0.200975
2019-11-13 15:20:01,145 train 500 1.250920e-02 0.206539
2019-11-13 15:20:06,534 train 550 1.248355e-02 0.208465
2019-11-13 15:20:11,925 train 600 1.247514e-02 0.214944
2019-11-13 15:20:17,310 train 650 1.249423e-02 0.220281
2019-11-13 15:20:22,699 train 700 1.251263e-02 0.221909
2019-11-13 15:20:28,085 train 750 1.252754e-02 0.220350
2019-11-13 15:20:33,477 train 800 1.252389e-02 0.220626
2019-11-13 15:20:38,872 train 850 1.252635e-02 0.030033
2019-11-13 15:20:40,483 training loss; R2: 1.253162e-02 0.034710
2019-11-13 15:20:40,826 valid 000 4.843153e-02 -2.308670
2019-11-13 15:20:42,546 valid 050 5.469964e-02 -5.189051
2019-11-13 15:20:44,124 validation loss; R2: 5.472963e-02 -4.265992
2019-11-13 15:20:44,143 epoch 11 lr 1.000000e-03
2019-11-13 15:20:44,539 train 000 1.150738e-02 0.342070
2019-11-13 15:20:50,031 train 050 1.226104e-02 0.296390
2019-11-13 15:20:55,592 train 100 1.230501e-02 0.285907
2019-11-13 15:21:01,145 train 150 1.229694e-02 0.274605
2019-11-13 15:21:06,653 train 200 1.223861e-02 0.274522
2019-11-13 15:21:12,145 train 250 1.229954e-02 0.265239
2019-11-13 15:21:17,635 train 300 1.229316e-02 0.266142
2019-11-13 15:21:23,134 train 350 1.229826e-02 0.265914
2019-11-13 15:21:28,605 train 400 1.230362e-02 0.266354
2019-11-13 15:21:34,088 train 450 1.230662e-02 0.269018
2019-11-13 15:21:39,508 train 500 1.231931e-02 0.264609
2019-11-13 15:21:44,901 train 550 1.234977e-02 0.264714
2019-11-13 15:21:50,303 train 600 1.236514e-02 0.264125
2019-11-13 15:21:55,700 train 650 1.238233e-02 0.262522
2019-11-13 15:22:01,099 train 700 1.237519e-02 0.261403
2019-11-13 15:22:06,491 train 750 1.237153e-02 0.263369
2019-11-13 15:22:11,888 train 800 1.236659e-02 0.264700
2019-11-13 15:22:17,277 train 850 1.237468e-02 0.258594
2019-11-13 15:22:18,891 training loss; R2: 1.237751e-02 0.259520
2019-11-13 15:22:19,235 valid 000 1.821740e+01 -1579.517234
2019-11-13 15:22:20,973 valid 050 1.830523e+01 -2248.569463
2019-11-13 15:22:22,529 validation loss; R2: 1.830525e+01 -2368.285711
2019-11-13 15:22:22,546 epoch 12 lr 1.000000e-03
2019-11-13 15:22:22,950 train 000 1.221870e-02 0.289557
2019-11-13 15:22:28,204 train 050 1.205406e-02 0.288447
2019-11-13 15:22:33,700 train 100 1.207257e-02 0.240287
2019-11-13 15:22:39,201 train 150 1.212067e-02 0.229938
2019-11-13 15:22:44,694 train 200 1.208684e-02 0.238308
2019-11-13 15:22:50,193 train 250 1.217684e-02 0.229771
2019-11-13 15:22:55,691 train 300 1.228980e-02 0.237140
2019-11-13 15:23:01,191 train 350 1.225039e-02 0.229524
2019-11-13 15:23:06,687 train 400 1.228572e-02 0.226589
2019-11-13 15:23:12,187 train 450 1.231144e-02 0.230433
2019-11-13 15:23:17,466 train 500 1.229015e-02 0.234586
2019-11-13 15:23:22,941 train 550 1.225692e-02 0.234361
2019-11-13 15:23:28,446 train 600 1.225884e-02 0.236524
2019-11-13 15:23:33,950 train 650 1.225729e-02 0.240519
2019-11-13 15:23:39,457 train 700 1.223680e-02 0.244586
2019-11-13 15:23:44,946 train 750 1.223375e-02 0.247466
2019-11-13 15:23:50,445 train 800 1.221107e-02 0.249418
2019-11-13 15:23:55,925 train 850 1.219730e-02 0.250655
2019-11-13 15:23:57,553 training loss; R2: 1.219179e-02 0.251965
2019-11-13 15:23:57,891 valid 000 3.013314e+00 -1983.750604
2019-11-13 15:23:59,619 valid 050 3.010058e+00 -565.922676
2019-11-13 15:24:01,194 validation loss; R2: 3.010826e+00 -584.437408
2019-11-13 15:24:01,204 epoch 13 lr 1.000000e-03
2019-11-13 15:24:01,573 train 000 1.198645e-02 0.263875
2019-11-13 15:24:06,867 train 050 1.188040e-02 0.258294
2019-11-13 15:24:12,161 train 100 1.212105e-02 0.268067
2019-11-13 15:24:17,659 train 150 1.206123e-02 0.274069
2019-11-13 15:24:23,209 train 200 1.201011e-02 0.273036
2019-11-13 15:24:28,811 train 250 1.195944e-02 0.270168
2019-11-13 15:24:34,237 train 300 1.191603e-02 0.270788
2019-11-13 15:24:39,823 train 350 1.195856e-02 0.270742
2019-11-13 15:24:45,433 train 400 1.193903e-02 0.275986
2019-11-13 15:24:50,940 train 450 1.194628e-02 0.273544
2019-11-13 15:24:56,472 train 500 1.193254e-02 0.274334
2019-11-13 15:25:02,013 train 550 1.194347e-02 0.276768
2019-11-13 15:25:07,548 train 600 1.194921e-02 0.274027
2019-11-13 15:25:13,050 train 650 1.196053e-02 0.274991
2019-11-13 15:25:18,573 train 700 1.197882e-02 0.272631
2019-11-13 15:25:24,244 train 750 1.198415e-02 0.267859
2019-11-13 15:25:29,768 train 800 1.197407e-02 0.270468
2019-11-13 15:25:35,271 train 850 1.197708e-02 0.268698
2019-11-13 15:25:36,915 training loss; R2: 1.197954e-02 0.268935
2019-11-13 15:25:37,252 valid 000 2.247625e+01 -1228.847891
2019-11-13 15:25:38,972 valid 050 2.253410e+01 -1682.209411
2019-11-13 15:25:40,578 validation loss; R2: 2.253074e+01 -1568.379182
2019-11-13 15:25:40,592 epoch 14 lr 1.000000e-03
2019-11-13 15:25:41,005 train 000 1.406464e-02 0.264414
2019-11-13 15:25:46,430 train 050 1.226482e-02 0.225831
2019-11-13 15:25:51,727 train 100 1.203821e-02 0.254085
2019-11-13 15:25:57,254 train 150 1.201319e-02 -0.029194
2019-11-13 15:26:02,759 train 200 1.193506e-02 0.040751
2019-11-13 15:26:08,267 train 250 1.201285e-02 0.085137
2019-11-13 15:26:13,772 train 300 1.190069e-02 0.121929
2019-11-13 15:26:19,276 train 350 1.190651e-02 0.144002
2019-11-13 15:26:24,799 train 400 1.185806e-02 0.163724
2019-11-13 15:26:30,313 train 450 1.184415e-02 0.170914
2019-11-13 15:26:35,841 train 500 1.182906e-02 0.185315
2019-11-13 15:26:41,346 train 550 1.183374e-02 0.196444
2019-11-13 15:26:46,852 train 600 1.184025e-02 0.205218
2019-11-13 15:26:52,355 train 650 1.184172e-02 0.212958
2019-11-13 15:26:57,852 train 700 1.186682e-02 0.219060
2019-11-13 15:27:03,356 train 750 1.185974e-02 0.222884
2019-11-13 15:27:08,855 train 800 1.185228e-02 0.227300
2019-11-13 15:27:14,355 train 850 1.185684e-02 0.230936
2019-11-13 15:27:15,993 training loss; R2: 1.185388e-02 0.232137
2019-11-13 15:27:16,313 valid 000 7.888951e+00 -627.521711
2019-11-13 15:27:18,012 valid 050 7.873771e+00 -840.357616
2019-11-13 15:27:19,534 validation loss; R2: 7.874693e+00 -846.326099
2019-11-13 15:27:19,551 epoch 15 lr 1.000000e-03
2019-11-13 15:27:19,942 train 000 1.179374e-02 0.341886
2019-11-13 15:27:25,179 train 050 1.190621e-02 0.293735
2019-11-13 15:27:30,416 train 100 1.174991e-02 0.293039
2019-11-13 15:27:35,737 train 150 1.177834e-02 0.288336
2019-11-13 15:27:41,289 train 200 1.176794e-02 0.299385
2019-11-13 15:27:46,778 train 250 1.180465e-02 0.281598
2019-11-13 15:27:52,261 train 300 1.182113e-02 0.266110
2019-11-13 15:27:57,823 train 350 1.184088e-02 0.269271
2019-11-13 15:28:03,349 train 400 1.180575e-02 0.273172
2019-11-13 15:28:08,884 train 450 1.181552e-02 0.273182
2019-11-13 15:28:14,414 train 500 1.183633e-02 0.274460
2019-11-13 15:28:19,969 train 550 1.181804e-02 0.274894
2019-11-13 15:28:25,526 train 600 1.179621e-02 0.273948
2019-11-13 15:28:31,089 train 650 1.177707e-02 0.277248
2019-11-13 15:28:36,760 train 700 1.178745e-02 0.278301
2019-11-13 15:28:42,490 train 750 1.178546e-02 0.279115
2019-11-13 15:28:48,206 train 800 1.177713e-02 0.276434
2019-11-13 15:28:53,930 train 850 1.176907e-02 0.276766
2019-11-13 15:28:55,633 training loss; R2: 1.176508e-02 0.276669
2019-11-13 15:28:55,957 valid 000 6.079037e+00 -462.199634
2019-11-13 15:28:57,607 valid 050 6.078320e+00 -654.865139
2019-11-13 15:28:59,139 validation loss; R2: 6.075776e+00 -659.247362
2019-11-13 15:28:59,153 epoch 16 lr 1.000000e-03
2019-11-13 15:28:59,573 train 000 1.061250e-02 0.376308
2019-11-13 15:29:04,980 train 050 1.152886e-02 0.305755
2019-11-13 15:29:10,391 train 100 1.169191e-02 0.294150
2019-11-13 15:29:15,804 train 150 1.178534e-02 0.290516
2019-11-13 15:29:21,232 train 200 1.181472e-02 0.298179
2019-11-13 15:29:26,668 train 250 1.177739e-02 0.274942
2019-11-13 15:29:32,213 train 300 1.175116e-02 0.273674
2019-11-13 15:29:37,688 train 350 1.172610e-02 0.279076
2019-11-13 15:29:43,178 train 400 1.172999e-02 0.281483
2019-11-13 15:29:48,602 train 450 1.169847e-02 0.283995
2019-11-13 15:29:53,977 train 500 1.169610e-02 0.285575
2019-11-13 15:29:59,425 train 550 1.166479e-02 0.284513
2019-11-13 15:30:04,933 train 600 1.165065e-02 0.283658
2019-11-13 15:30:10,390 train 650 1.165769e-02 0.279507
2019-11-13 15:30:15,891 train 700 1.163896e-02 0.279422
2019-11-13 15:30:21,392 train 750 1.164539e-02 0.282598
2019-11-13 15:30:26,897 train 800 1.164132e-02 0.283311
2019-11-13 15:30:32,313 train 850 1.163546e-02 0.283578
2019-11-13 15:30:33,969 training loss; R2: 1.162843e-02 0.283011
2019-11-13 15:30:34,301 valid 000 5.288821e+01 -16699.891817
2019-11-13 15:30:36,024 valid 050 5.290548e+01 -8290.522370
2019-11-13 15:30:37,526 validation loss; R2: 5.289050e+01 -7668.742920
2019-11-13 15:30:37,537 epoch 17 lr 1.000000e-03
2019-11-13 15:30:37,893 train 000 9.017047e-03 0.446475
2019-11-13 15:30:43,121 train 050 1.137015e-02 0.301201
2019-11-13 15:30:48,346 train 100 1.160497e-02 0.284238
2019-11-13 15:30:53,647 train 150 1.150761e-02 0.294337
2019-11-13 15:30:59,097 train 200 1.150428e-02 0.298108
2019-11-13 15:31:04,615 train 250 1.154091e-02 0.299126
2019-11-13 15:31:10,095 train 300 1.161921e-02 0.267845
2019-11-13 15:31:15,585 train 350 1.162414e-02 0.192206
2019-11-13 15:31:21,066 train 400 1.159525e-02 0.208940
2019-11-13 15:31:26,497 train 450 1.158381e-02 0.216273
2019-11-13 15:31:31,897 train 500 1.155929e-02 0.225683
2019-11-13 15:31:37,290 train 550 1.159139e-02 0.228627
2019-11-13 15:31:42,680 train 600 1.161044e-02 0.234866
2019-11-13 15:31:48,085 train 650 1.160532e-02 0.240318
2019-11-13 15:31:53,480 train 700 1.156138e-02 0.243733
2019-11-13 15:31:58,907 train 750 1.153268e-02 0.247647
2019-11-13 15:32:04,307 train 800 1.153364e-02 0.245380
2019-11-13 15:32:09,708 train 850 1.153661e-02 0.248709
2019-11-13 15:32:11,321 training loss; R2: 1.153520e-02 0.248355
2019-11-13 15:32:11,655 valid 000 3.154109e+02 -29533.479426
2019-11-13 15:32:13,323 valid 050 3.154896e+02 -14400.962811
2019-11-13 15:32:14,869 validation loss; R2: 3.154900e+02 -14489.433759
2019-11-13 15:32:14,880 epoch 18 lr 1.000000e-03
2019-11-13 15:32:15,272 train 000 1.023045e-02 0.397151
2019-11-13 15:32:20,508 train 050 1.149255e-02 0.273750
2019-11-13 15:32:25,745 train 100 1.172753e-02 0.274620
2019-11-13 15:32:30,996 train 150 1.167469e-02 0.280405
2019-11-13 15:32:36,239 train 200 1.167237e-02 0.275998
2019-11-13 15:32:41,500 train 250 1.164041e-02 0.272948
2019-11-13 15:32:46,761 train 300 1.161285e-02 0.280344
2019-11-13 15:32:52,025 train 350 1.160128e-02 0.282860
2019-11-13 15:32:57,500 train 400 1.158143e-02 0.280424
2019-11-13 15:33:02,981 train 450 1.153223e-02 0.287225
2019-11-13 15:33:08,537 train 500 1.151538e-02 0.291204
2019-11-13 15:33:14,047 train 550 1.148544e-02 0.290909
2019-11-13 15:33:19,600 train 600 1.148424e-02 0.281189
2019-11-13 15:33:25,139 train 650 1.146272e-02 0.285068
2019-11-13 15:33:30,714 train 700 1.144435e-02 0.287702
2019-11-13 15:33:36,277 train 750 1.143450e-02 0.290109
2019-11-13 15:33:41,801 train 800 1.142961e-02 0.292487
2019-11-13 15:33:47,339 train 850 1.141617e-02 0.293063
2019-11-13 15:33:48,985 training loss; R2: 1.141423e-02 0.292483
2019-11-13 15:33:49,343 valid 000 1.120923e+01 -789.775359
2019-11-13 15:33:51,068 valid 050 1.118763e+01 -1073.643822
2019-11-13 15:33:52,625 validation loss; R2: 1.118976e+01 -1114.499276
2019-11-13 15:33:52,641 epoch 19 lr 1.000000e-03
2019-11-13 15:33:52,996 train 000 1.309384e-02 0.323512
2019-11-13 15:33:58,269 train 050 1.127335e-02 0.327890
2019-11-13 15:34:03,706 train 100 1.100696e-02 0.335042
2019-11-13 15:34:09,238 train 150 1.116977e-02 0.317486
2019-11-13 15:34:14,824 train 200 1.115559e-02 0.277847
2019-11-13 15:34:20,407 train 250 1.117945e-02 0.281800
2019-11-13 15:34:25,972 train 300 1.113970e-02 0.287458
2019-11-13 15:34:31,547 train 350 1.113414e-02 0.289119
2019-11-13 15:34:37,105 train 400 1.112849e-02 0.289717
2019-11-13 15:34:42,707 train 450 1.114063e-02 0.294778
2019-11-13 15:34:48,292 train 500 1.113471e-02 0.294315
2019-11-13 15:34:53,853 train 550 1.111882e-02 0.297096
2019-11-13 15:34:59,413 train 600 1.109545e-02 0.301875
2019-11-13 15:35:04,990 train 650 1.110032e-02 0.301651
2019-11-13 15:35:10,546 train 700 1.108096e-02 0.297590
2019-11-13 15:35:16,143 train 750 1.108258e-02 0.300690
2019-11-13 15:35:21,957 train 800 1.107494e-02 0.304024
2019-11-13 15:35:27,549 train 850 1.106434e-02 0.306653
2019-11-13 15:35:29,210 training loss; R2: 1.106471e-02 0.305064
2019-11-13 15:35:29,558 valid 000 2.729030e-01 -45.502162
2019-11-13 15:35:31,285 valid 050 2.728579e-01 -42.050652
2019-11-13 15:35:32,865 validation loss; R2: 2.731169e-01 -43.591294
