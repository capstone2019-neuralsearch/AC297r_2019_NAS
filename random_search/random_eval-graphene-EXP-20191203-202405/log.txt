2019-12-03 20:24:05,567 gpu device = 1
2019-12-03 20:24:05,567 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=96, cutout=False, cutout_length=16, data='../data', dataset='graphene', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name=None, gpu=1, grad_clip=5, gz_dtree=False, init_channels=36, layers=8, learning_rate=0.002, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-graphene-EXP-20191203-202405', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-12-03 20:24:08,777 param size = 1.066069MB
2019-12-03 20:24:08,781 epoch 0 lr 2.000000e-03
2019-12-03 20:24:11,214 train 000 3.737047e-01 -12.920932
2019-12-03 20:24:22,313 train 050 9.948965e-02 -2.159714
2019-12-03 20:24:33,371 train 100 6.405810e-02 -1.034900
2019-12-03 20:24:44,237 train 150 5.062529e-02 -0.624668
2019-12-03 20:24:55,005 train 200 4.335183e-02 -0.393776
2019-12-03 20:25:00,824 training loss; R2: 4.118739e-02 -0.323063
2019-12-03 20:25:00,947 valid 000 1.537262e-02 0.594534
2019-12-03 20:25:02,424 validation loss; R2: 1.486369e-02 0.518953
2019-12-03 20:25:02,444 epoch 1 lr 2.000000e-03
2019-12-03 20:25:02,760 train 000 1.827370e-02 0.337651
2019-12-03 20:25:13,528 train 050 1.858640e-02 0.408487
2019-12-03 20:25:24,298 train 100 1.608244e-02 0.479378
2019-12-03 20:25:35,065 train 150 1.515733e-02 0.504065
2019-12-03 20:25:45,829 train 200 1.441114e-02 0.510361
2019-12-03 20:25:50,668 training loss; R2: 1.420383e-02 0.525962
2019-12-03 20:25:50,787 valid 000 8.112320e-03 0.737853
2019-12-03 20:25:51,949 validation loss; R2: 7.230756e-03 0.749845
2019-12-03 20:25:51,968 epoch 2 lr 2.000000e-03
2019-12-03 20:25:52,258 train 000 9.444967e-03 0.421435
2019-12-03 20:26:03,020 train 050 9.258882e-03 0.681264
2019-12-03 20:26:13,780 train 100 9.438144e-03 0.676284
2019-12-03 20:26:24,542 train 150 9.426626e-03 0.682992
2019-12-03 20:26:35,311 train 200 9.382142e-03 0.689650
2019-12-03 20:26:40,153 training loss; R2: 9.155633e-03 0.695693
2019-12-03 20:26:40,265 valid 000 7.476071e-03 0.867936
2019-12-03 20:26:41,427 validation loss; R2: 4.283592e-03 0.858812
2019-12-03 20:26:41,447 epoch 3 lr 2.000000e-03
2019-12-03 20:26:41,732 train 000 5.967102e-03 0.706547
2019-12-03 20:26:52,493 train 050 7.120939e-03 0.744630
2019-12-03 20:27:03,263 train 100 8.322299e-03 0.729752
2019-12-03 20:27:14,031 train 150 8.416939e-03 0.729715
2019-12-03 20:27:24,794 train 200 8.170335e-03 0.731734
2019-12-03 20:27:29,640 training loss; R2: 7.981181e-03 0.734125
2019-12-03 20:27:29,760 valid 000 5.471505e-03 0.856465
2019-12-03 20:27:30,923 validation loss; R2: 4.532337e-03 0.846619
2019-12-03 20:27:30,944 epoch 4 lr 2.000000e-03
2019-12-03 20:27:31,234 train 000 7.672830e-03 0.769397
2019-12-03 20:27:41,996 train 050 7.611219e-03 0.753194
2019-12-03 20:27:52,756 train 100 7.257449e-03 0.764515
2019-12-03 20:28:03,515 train 150 7.281191e-03 0.762227
2019-12-03 20:28:14,281 train 200 7.110199e-03 0.764552
2019-12-03 20:28:19,118 training loss; R2: 6.980712e-03 0.768831
2019-12-03 20:28:19,232 valid 000 2.514610e-03 0.875491
2019-12-03 20:28:20,395 validation loss; R2: 3.831632e-03 0.863624
2019-12-03 20:28:20,422 epoch 5 lr 2.000000e-03
2019-12-03 20:28:20,707 train 000 5.652245e-03 0.866243
2019-12-03 20:28:31,466 train 050 6.475282e-03 0.781075
2019-12-03 20:28:42,229 train 100 6.078041e-03 0.790069
2019-12-03 20:28:52,990 train 150 6.086619e-03 0.792056
2019-12-03 20:29:03,752 train 200 6.454502e-03 0.779922
2019-12-03 20:29:08,588 training loss; R2: 6.575389e-03 0.778822
2019-12-03 20:29:08,704 valid 000 2.819216e-03 0.922906
2019-12-03 20:29:09,867 validation loss; R2: 3.690706e-03 0.873824
2019-12-03 20:29:09,888 epoch 6 lr 2.000000e-03
2019-12-03 20:29:10,172 train 000 6.816948e-03 0.734182
2019-12-03 20:29:20,942 train 050 6.458058e-03 0.793065
2019-12-03 20:29:31,710 train 100 6.640722e-03 0.782735
2019-12-03 20:29:42,475 train 150 6.410576e-03 0.776567
2019-12-03 20:29:53,245 train 200 6.320598e-03 0.784211
2019-12-03 20:29:58,085 training loss; R2: 6.371456e-03 0.781698
2019-12-03 20:29:58,197 valid 000 3.129724e-03 0.880740
2019-12-03 20:29:59,358 validation loss; R2: 3.581071e-03 0.876849
2019-12-03 20:29:59,379 epoch 7 lr 2.000000e-03
2019-12-03 20:29:59,663 train 000 6.722227e-03 0.718201
2019-12-03 20:30:10,425 train 050 6.244296e-03 0.795332
2019-12-03 20:30:21,191 train 100 6.031107e-03 0.791405
2019-12-03 20:30:31,954 train 150 6.004198e-03 0.791806
2019-12-03 20:30:42,716 train 200 5.861047e-03 0.796053
2019-12-03 20:30:47,556 training loss; R2: 5.783568e-03 0.798963
2019-12-03 20:30:47,674 valid 000 3.345215e-03 0.842999
2019-12-03 20:30:48,835 validation loss; R2: 4.327976e-03 0.845126
2019-12-03 20:30:48,861 epoch 8 lr 2.000000e-03
2019-12-03 20:30:49,147 train 000 5.524682e-03 0.745989
2019-12-03 20:30:59,907 train 050 5.633471e-03 0.788365
2019-12-03 20:31:10,671 train 100 5.921374e-03 0.793605
2019-12-03 20:31:21,433 train 150 6.021511e-03 0.792819
2019-12-03 20:31:32,207 train 200 5.827385e-03 0.798700
2019-12-03 20:31:37,049 training loss; R2: 5.824941e-03 0.797345
2019-12-03 20:31:37,167 valid 000 2.756245e-03 0.825570
2019-12-03 20:31:38,330 validation loss; R2: 3.854923e-03 0.864670
2019-12-03 20:31:38,351 epoch 9 lr 2.000000e-03
2019-12-03 20:31:38,637 train 000 9.231901e-03 0.772379
2019-12-03 20:31:49,411 train 050 5.813162e-03 0.816126
2019-12-03 20:32:00,188 train 100 5.488273e-03 0.824836
2019-12-03 20:32:10,957 train 150 5.459767e-03 0.825347
2019-12-03 20:32:21,727 train 200 5.189615e-03 0.824764
2019-12-03 20:32:26,568 training loss; R2: 5.144120e-03 0.825877
2019-12-03 20:32:26,686 valid 000 2.574723e-03 0.861601
2019-12-03 20:32:27,848 validation loss; R2: 4.524951e-03 0.852444
2019-12-03 20:32:27,869 epoch 10 lr 2.000000e-03
2019-12-03 20:32:28,159 train 000 4.117817e-03 0.851665
2019-12-03 20:32:38,932 train 050 5.125323e-03 0.827823
2019-12-03 20:32:49,702 train 100 4.957701e-03 0.827350
2019-12-03 20:33:00,472 train 150 4.798252e-03 0.834881
2019-12-03 20:33:11,239 train 200 4.950478e-03 0.832137
2019-12-03 20:33:16,083 training loss; R2: 4.881888e-03 0.833240
2019-12-03 20:33:16,203 valid 000 1.613265e-03 0.933775
2019-12-03 20:33:17,364 validation loss; R2: 3.270534e-03 0.891515
2019-12-03 20:33:17,385 epoch 11 lr 2.000000e-03
2019-12-03 20:33:17,671 train 000 3.635405e-03 0.863350
2019-12-03 20:33:28,437 train 050 4.619890e-03 0.844678
2019-12-03 20:33:39,207 train 100 4.530315e-03 0.840839
2019-12-03 20:33:49,969 train 150 4.796332e-03 0.837219
2019-12-03 20:34:00,736 train 200 5.008306e-03 0.831389
2019-12-03 20:34:05,575 training loss; R2: 5.057557e-03 0.829889
2019-12-03 20:34:05,690 valid 000 2.243755e-03 0.878933
2019-12-03 20:34:06,853 validation loss; R2: 2.797962e-03 0.902449
2019-12-03 20:34:06,873 epoch 12 lr 2.000000e-03
2019-12-03 20:34:07,160 train 000 3.817590e-03 0.937226
2019-12-03 20:34:17,927 train 050 4.735195e-03 0.854813
2019-12-03 20:34:28,698 train 100 4.722955e-03 0.839613
2019-12-03 20:34:39,465 train 150 4.677281e-03 0.840597
2019-12-03 20:34:50,230 train 200 4.619546e-03 0.841867
2019-12-03 20:34:55,070 training loss; R2: 4.675934e-03 0.841541
2019-12-03 20:34:55,187 valid 000 4.257308e-03 0.886455
2019-12-03 20:34:56,349 validation loss; R2: 3.622024e-03 0.873782
2019-12-03 20:34:56,369 epoch 13 lr 2.000000e-03
2019-12-03 20:34:56,653 train 000 4.761549e-03 0.796484
2019-12-03 20:35:07,421 train 050 4.406192e-03 0.851842
2019-12-03 20:35:18,185 train 100 4.543710e-03 0.838619
2019-12-03 20:35:28,996 train 150 4.872809e-03 0.833395
2019-12-03 20:35:39,827 train 200 5.070123e-03 0.830240
2019-12-03 20:35:44,664 training loss; R2: 4.974637e-03 0.833182
2019-12-03 20:35:44,788 valid 000 3.831201e-03 0.792542
2019-12-03 20:35:45,950 validation loss; R2: 4.006264e-03 0.862497
2019-12-03 20:35:45,970 epoch 14 lr 2.000000e-03
2019-12-03 20:35:46,262 train 000 3.691508e-03 0.761395
2019-12-03 20:35:57,032 train 050 4.662860e-03 0.843616
2019-12-03 20:36:07,804 train 100 4.711607e-03 0.843200
2019-12-03 20:36:18,570 train 150 4.572184e-03 0.845818
2019-12-03 20:36:29,335 train 200 4.513185e-03 0.848153
2019-12-03 20:36:34,175 training loss; R2: 4.499210e-03 0.847321
2019-12-03 20:36:34,300 valid 000 4.604453e-03 0.919683
2019-12-03 20:36:35,463 validation loss; R2: 3.197165e-03 0.891952
2019-12-03 20:36:35,484 epoch 15 lr 2.000000e-03
2019-12-03 20:36:35,766 train 000 2.991066e-03 0.827382
2019-12-03 20:36:46,517 train 050 4.181653e-03 0.853565
2019-12-03 20:36:57,270 train 100 4.343645e-03 0.854871
2019-12-03 20:37:08,025 train 150 4.297672e-03 0.852968
2019-12-03 20:37:18,784 train 200 4.390096e-03 0.851244
2019-12-03 20:37:23,618 training loss; R2: 4.400807e-03 0.850181
2019-12-03 20:37:23,740 valid 000 2.188779e-01 -2.355449
2019-12-03 20:37:24,902 validation loss; R2: 1.904543e-01 -5.496322
2019-12-03 20:37:24,927 epoch 16 lr 2.000000e-03
2019-12-03 20:37:25,215 train 000 1.111554e-02 0.843761
2019-12-03 20:37:35,974 train 050 4.544788e-03 0.848820
2019-12-03 20:37:46,734 train 100 4.473801e-03 0.845926
2019-12-03 20:37:57,487 train 150 4.639467e-03 0.842343
2019-12-03 20:38:08,241 train 200 4.601696e-03 0.842235
2019-12-03 20:38:13,078 training loss; R2: 4.565133e-03 0.844157
2019-12-03 20:38:13,202 valid 000 7.455521e-01 -20.303030
2019-12-03 20:38:14,364 validation loss; R2: 7.457531e-01 -28.030857
2019-12-03 20:38:14,384 epoch 17 lr 2.000000e-03
2019-12-03 20:38:14,678 train 000 5.749127e-03 0.914277
2019-12-03 20:38:25,429 train 050 4.656176e-03 0.847330
2019-12-03 20:38:36,175 train 100 4.606916e-03 0.853666
2019-12-03 20:38:46,918 train 150 4.461026e-03 0.850028
2019-12-03 20:38:57,657 train 200 4.365473e-03 0.849864
2019-12-03 20:39:02,491 training loss; R2: 4.327109e-03 0.850175
2019-12-03 20:39:02,609 valid 000 6.781789e-03 0.727900
2019-12-03 20:39:03,772 validation loss; R2: 5.529039e-03 0.793206
2019-12-03 20:39:03,793 epoch 18 lr 2.000000e-03
2019-12-03 20:39:04,082 train 000 3.386038e-03 0.866309
2019-12-03 20:39:14,821 train 050 4.288977e-03 0.826524
2019-12-03 20:39:25,560 train 100 4.013206e-03 0.848292
2019-12-03 20:39:36,300 train 150 4.127897e-03 0.850541
2019-12-03 20:39:47,041 train 200 4.212203e-03 0.849292
2019-12-03 20:39:51,867 training loss; R2: 4.210698e-03 0.850716
2019-12-03 20:39:51,995 valid 000 3.437800e-03 0.843443
2019-12-03 20:39:53,159 validation loss; R2: 4.139663e-03 0.851682
2019-12-03 20:39:53,179 epoch 19 lr 2.000000e-03
2019-12-03 20:39:53,467 train 000 3.376531e-03 0.798024
2019-12-03 20:40:04,205 train 050 5.005051e-03 0.844239
2019-12-03 20:40:14,934 train 100 4.989723e-03 0.832954
2019-12-03 20:40:25,665 train 150 4.916407e-03 0.835531
2019-12-03 20:40:36,389 train 200 4.710391e-03 0.837499
2019-12-03 20:40:41,214 training loss; R2: 4.743869e-03 0.839016
2019-12-03 20:40:41,329 valid 000 2.260279e-03 0.904758
2019-12-03 20:40:42,490 validation loss; R2: 3.194639e-03 0.894054
