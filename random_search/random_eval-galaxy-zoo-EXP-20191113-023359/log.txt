2019-11-13 02:33:59,588 gpu device = 1
2019-11-13 02:33:59,588 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name='galaxy-zoo-cleaned', gpu=1, grad_clip=5, gz_dtree=False, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-galaxy-zoo-EXP-20191113-023359', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-11-13 02:34:11,058 param size = 0.332373MB
2019-11-13 02:34:11,063 epoch 0 lr 1.000000e-03
2019-11-13 02:34:13,340 train 000 2.601172e-01 -68.306591
2019-11-13 02:34:21,927 train 050 4.349917e-02 -5.473550
2019-11-13 02:34:30,375 train 100 3.428584e-02 -2.996264
2019-11-13 02:34:38,820 train 150 3.091566e-02 -2.118969
2019-11-13 02:34:47,130 train 200 2.869814e-02 -1.643598
2019-11-13 02:34:55,387 train 250 2.714595e-02 -1.332849
2019-11-13 02:35:03,860 train 300 2.600002e-02 -1.119776
2019-11-13 02:35:12,166 train 350 2.516102e-02 -0.965772
2019-11-13 02:35:20,367 train 400 2.445608e-02 -0.841259
2019-11-13 02:35:28,559 train 450 2.380720e-02 -0.745138
2019-11-13 02:35:36,887 train 500 2.327875e-02 -0.667148
2019-11-13 02:35:45,082 train 550 2.276440e-02 -0.599625
2019-11-13 02:35:53,224 train 600 2.233270e-02 -0.545898
2019-11-13 02:36:01,473 train 650 2.192761e-02 -0.493004
2019-11-13 02:36:09,874 train 700 2.155538e-02 -0.449493
2019-11-13 02:36:18,211 train 750 2.123612e-02 -0.410355
2019-11-13 02:36:26,615 train 800 2.091968e-02 -0.375824
2019-11-13 02:36:34,709 train 850 2.063692e-02 -0.346568
2019-11-13 02:36:38,004 training loss; R2: 2.055987e-02 -0.337738
2019-11-13 02:36:38,287 valid 000 1.494319e-02 0.252017
2019-11-13 02:36:39,957 valid 050 1.497571e-02 0.159257
2019-11-13 02:36:41,600 validation loss; R2: 1.490291e-02 0.181753
2019-11-13 02:36:41,622 epoch 1 lr 1.000000e-03
2019-11-13 02:36:42,210 train 000 1.564669e-02 0.141129
2019-11-13 02:36:50,350 train 050 1.539988e-02 0.125045
2019-11-13 02:36:58,539 train 100 1.567776e-02 0.111101
2019-11-13 02:37:06,593 train 150 1.556327e-02 0.129677
2019-11-13 02:37:14,639 train 200 1.545660e-02 0.136878
2019-11-13 02:37:22,685 train 250 1.539838e-02 0.146855
2019-11-13 02:37:30,724 train 300 1.531192e-02 0.148473
2019-11-13 02:37:38,769 train 350 1.532028e-02 0.152164
2019-11-13 02:37:46,812 train 400 1.522917e-02 0.160280
2019-11-13 02:37:54,849 train 450 1.514266e-02 0.162612
2019-11-13 02:38:02,888 train 500 1.506896e-02 0.162922
2019-11-13 02:38:10,939 train 550 1.501332e-02 0.164028
2019-11-13 02:38:19,006 train 600 1.495057e-02 0.165676
2019-11-13 02:38:27,046 train 650 1.489089e-02 0.166666
2019-11-13 02:38:35,125 train 700 1.482379e-02 0.170605
2019-11-13 02:38:43,169 train 750 1.475741e-02 0.175132
2019-11-13 02:38:51,246 train 800 1.469026e-02 0.178976
2019-11-13 02:38:59,288 train 850 1.462130e-02 0.180861
2019-11-13 02:39:01,695 training loss; R2: 1.460796e-02 0.182005
2019-11-13 02:39:01,991 valid 000 1.265848e-02 0.314095
2019-11-13 02:39:03,710 valid 050 1.243313e-02 0.029351
2019-11-13 02:39:05,266 validation loss; R2: 1.250380e-02 0.139997
2019-11-13 02:39:05,287 epoch 2 lr 1.000000e-03
2019-11-13 02:39:05,711 train 000 1.324001e-02 0.277261
2019-11-13 02:39:13,905 train 050 1.335167e-02 0.248727
2019-11-13 02:39:22,057 train 100 1.341205e-02 0.238433
2019-11-13 02:39:30,105 train 150 1.336827e-02 0.243379
2019-11-13 02:39:38,146 train 200 1.332202e-02 0.225593
2019-11-13 02:39:46,232 train 250 1.324486e-02 0.231436
2019-11-13 02:39:54,268 train 300 1.327319e-02 0.207026
2019-11-13 02:40:02,293 train 350 1.324793e-02 0.204192
2019-11-13 02:40:10,328 train 400 1.320355e-02 0.207313
2019-11-13 02:40:18,359 train 450 1.317624e-02 0.212482
2019-11-13 02:40:26,391 train 500 1.313811e-02 0.216382
2019-11-13 02:40:34,414 train 550 1.310057e-02 0.221513
2019-11-13 02:40:42,440 train 600 1.307200e-02 0.224622
2019-11-13 02:40:50,470 train 650 1.308298e-02 0.226284
2019-11-13 02:40:58,499 train 700 1.303686e-02 0.229243
2019-11-13 02:41:06,533 train 750 1.299430e-02 0.218245
2019-11-13 02:41:14,555 train 800 1.297801e-02 0.220356
2019-11-13 02:41:22,577 train 850 1.295575e-02 0.217803
2019-11-13 02:41:24,978 training loss; R2: 1.293965e-02 0.219226
2019-11-13 02:41:25,286 valid 000 1.273416e-02 0.356755
2019-11-13 02:41:26,985 valid 050 1.156664e-02 0.339526
2019-11-13 02:41:28,528 validation loss; R2: 1.172735e-02 0.302916
2019-11-13 02:41:28,549 epoch 3 lr 1.000000e-03
2019-11-13 02:41:28,999 train 000 1.104341e-02 0.367395
2019-11-13 02:41:37,191 train 050 1.246775e-02 0.302130
2019-11-13 02:41:45,320 train 100 1.235781e-02 0.289416
2019-11-13 02:41:53,351 train 150 1.239799e-02 0.269516
2019-11-13 02:42:01,385 train 200 1.227635e-02 0.272381
2019-11-13 02:42:09,414 train 250 1.233523e-02 0.272928
2019-11-13 02:42:17,454 train 300 1.230054e-02 0.276589
2019-11-13 02:42:25,476 train 350 1.235827e-02 0.274608
2019-11-13 02:42:33,500 train 400 1.233420e-02 0.276352
2019-11-13 02:42:41,526 train 450 1.231344e-02 0.273885
2019-11-13 02:42:49,549 train 500 1.232278e-02 0.270935
2019-11-13 02:42:57,571 train 550 1.229279e-02 0.272154
2019-11-13 02:43:05,594 train 600 1.225518e-02 0.272403
2019-11-13 02:43:13,618 train 650 1.220905e-02 0.274608
2019-11-13 02:43:21,637 train 700 1.218373e-02 0.268898
2019-11-13 02:43:29,656 train 750 1.215987e-02 0.264122
2019-11-13 02:43:37,682 train 800 1.215224e-02 0.265569
2019-11-13 02:43:45,703 train 850 1.212244e-02 0.265854
2019-11-13 02:43:48,101 training loss; R2: 1.211713e-02 0.266378
2019-11-13 02:43:48,403 valid 000 1.135387e-02 0.404430
2019-11-13 02:43:50,119 valid 050 1.110179e-02 0.317956
2019-11-13 02:43:51,682 validation loss; R2: 1.109985e-02 0.311916
2019-11-13 02:43:51,702 epoch 4 lr 1.000000e-03
2019-11-13 02:43:52,143 train 000 1.060589e-02 0.137158
2019-11-13 02:44:00,213 train 050 1.174129e-02 0.209011
2019-11-13 02:44:08,240 train 100 1.168924e-02 0.180165
2019-11-13 02:44:16,273 train 150 1.170107e-02 0.224214
2019-11-13 02:44:24,295 train 200 1.179394e-02 0.241560
2019-11-13 02:44:32,316 train 250 1.179640e-02 0.250128
2019-11-13 02:44:40,338 train 300 1.171620e-02 0.263513
2019-11-13 02:44:48,360 train 350 1.172389e-02 0.268893
2019-11-13 02:44:56,387 train 400 1.173087e-02 0.269874
2019-11-13 02:45:04,424 train 450 1.170553e-02 0.261720
2019-11-13 02:45:12,454 train 500 1.168162e-02 0.267855
2019-11-13 02:45:20,477 train 550 1.167290e-02 0.271175
2019-11-13 02:45:28,504 train 600 1.164868e-02 0.272948
2019-11-13 02:45:36,520 train 650 1.164559e-02 0.276620
2019-11-13 02:45:44,534 train 700 1.163383e-02 0.279034
2019-11-13 02:45:52,547 train 750 1.161393e-02 0.281196
2019-11-13 02:46:00,559 train 800 1.160494e-02 0.283239
2019-11-13 02:46:08,567 train 850 1.158370e-02 0.284675
2019-11-13 02:46:10,964 training loss; R2: 1.157663e-02 0.283769
2019-11-13 02:46:11,269 valid 000 1.138746e-02 0.431676
2019-11-13 02:46:12,988 valid 050 1.040044e-02 0.360176
2019-11-13 02:46:14,538 validation loss; R2: 1.040731e-02 0.353701
2019-11-13 02:46:14,560 epoch 5 lr 1.000000e-03
2019-11-13 02:46:14,991 train 000 1.227351e-02 0.340292
2019-11-13 02:46:23,158 train 050 1.101469e-02 0.298761
2019-11-13 02:46:31,247 train 100 1.116270e-02 0.295813
2019-11-13 02:46:39,267 train 150 1.114521e-02 0.299685
2019-11-13 02:46:47,294 train 200 1.112927e-02 0.307152
2019-11-13 02:46:55,317 train 250 1.111430e-02 0.307382
2019-11-13 02:47:03,342 train 300 1.111021e-02 0.117355
2019-11-13 02:47:11,367 train 350 1.114490e-02 0.150319
2019-11-13 02:47:19,385 train 400 1.116057e-02 0.167047
2019-11-13 02:47:27,403 train 450 1.115647e-02 0.186765
2019-11-13 02:47:35,424 train 500 1.116077e-02 0.196012
2019-11-13 02:47:43,443 train 550 1.118354e-02 0.204752
2019-11-13 02:47:51,464 train 600 1.119099e-02 0.213859
2019-11-13 02:47:59,499 train 650 1.120245e-02 0.219656
2019-11-13 02:48:07,521 train 700 1.120097e-02 0.226153
2019-11-13 02:48:15,536 train 750 1.117912e-02 0.227648
2019-11-13 02:48:23,552 train 800 1.116289e-02 0.234808
2019-11-13 02:48:31,562 train 850 1.114625e-02 0.240724
2019-11-13 02:48:33,957 training loss; R2: 1.113936e-02 0.242586
2019-11-13 02:48:34,251 valid 000 9.503754e-03 0.349709
2019-11-13 02:48:35,950 valid 050 1.046869e-02 0.322790
2019-11-13 02:48:37,512 validation loss; R2: 1.038350e-02 0.340451
2019-11-13 02:48:37,533 epoch 6 lr 1.000000e-03
2019-11-13 02:48:37,973 train 000 1.109385e-02 0.034748
2019-11-13 02:48:46,021 train 050 1.104741e-02 0.329558
2019-11-13 02:48:54,040 train 100 1.109395e-02 0.283705
2019-11-13 02:49:02,057 train 150 1.105591e-02 0.285036
2019-11-13 02:49:10,079 train 200 1.106136e-02 0.298303
2019-11-13 02:49:18,090 train 250 1.104995e-02 0.301572
2019-11-13 02:49:26,096 train 300 1.105102e-02 0.287407
2019-11-13 02:49:34,106 train 350 1.098672e-02 0.296510
2019-11-13 02:49:42,110 train 400 1.098959e-02 0.162578
2019-11-13 02:49:50,114 train 450 1.097489e-02 0.184842
2019-11-13 02:49:58,118 train 500 1.099214e-02 0.194655
2019-11-13 02:50:06,131 train 550 1.097746e-02 0.205630
2019-11-13 02:50:14,147 train 600 1.095606e-02 0.216397
2019-11-13 02:50:22,158 train 650 1.094604e-02 0.225348
2019-11-13 02:50:30,160 train 700 1.093401e-02 0.233153
2019-11-13 02:50:38,169 train 750 1.091901e-02 0.241085
2019-11-13 02:50:46,171 train 800 1.091088e-02 0.246624
2019-11-13 02:50:54,174 train 850 1.090014e-02 0.251422
2019-11-13 02:50:56,567 training loss; R2: 1.089235e-02 0.253180
2019-11-13 02:50:56,863 valid 000 3.196310e-02 -4.288482
2019-11-13 02:50:58,571 valid 050 3.204407e-02 -7.227464
2019-11-13 02:51:00,118 validation loss; R2: 3.195933e-02 -6.741194
2019-11-13 02:51:00,138 epoch 7 lr 1.000000e-03
2019-11-13 02:51:00,586 train 000 1.166123e-02 0.334634
2019-11-13 02:51:08,827 train 050 1.012149e-02 0.356171
2019-11-13 02:51:16,867 train 100 1.025751e-02 0.354969
2019-11-13 02:51:24,885 train 150 1.048508e-02 0.347221
2019-11-13 02:51:32,899 train 200 1.047456e-02 0.306570
2019-11-13 02:51:40,909 train 250 1.050424e-02 0.315130
2019-11-13 02:51:48,920 train 300 1.054408e-02 0.317185
2019-11-13 02:51:56,929 train 350 1.055034e-02 0.311470
2019-11-13 02:52:04,937 train 400 1.057603e-02 0.307987
2019-11-13 02:52:12,946 train 450 1.060614e-02 0.311321
2019-11-13 02:52:20,961 train 500 1.061877e-02 0.310408
2019-11-13 02:52:28,963 train 550 1.063786e-02 0.314335
2019-11-13 02:52:36,987 train 600 1.063505e-02 0.315654
2019-11-13 02:52:44,995 train 650 1.063121e-02 0.301625
2019-11-13 02:52:53,000 train 700 1.064447e-02 0.299605
2019-11-13 02:53:01,007 train 750 1.063533e-02 0.301278
2019-11-13 02:53:09,017 train 800 1.061008e-02 0.302193
2019-11-13 02:53:17,030 train 850 1.059782e-02 0.300850
2019-11-13 02:53:19,428 training loss; R2: 1.060259e-02 0.301323
2019-11-13 02:53:19,730 valid 000 4.245385e+00 -1356.609621
2019-11-13 02:53:21,468 valid 050 4.250595e+00 -1493.794122
2019-11-13 02:53:23,008 validation loss; R2: 4.250041e+00 -2453.526448
2019-11-13 02:53:23,036 epoch 8 lr 1.000000e-03
2019-11-13 02:53:23,443 train 000 9.326687e-03 0.456277
2019-11-13 02:53:31,497 train 050 1.061602e-02 0.291294
2019-11-13 02:53:39,507 train 100 1.061137e-02 0.286556
2019-11-13 02:53:47,521 train 150 1.060138e-02 0.287520
2019-11-13 02:53:55,538 train 200 1.055875e-02 0.304893
2019-11-13 02:54:03,551 train 250 1.055010e-02 0.314854
2019-11-13 02:54:11,589 train 300 1.051931e-02 0.310476
2019-11-13 02:54:19,601 train 350 1.048028e-02 0.304450
2019-11-13 02:54:27,611 train 400 1.048718e-02 0.310079
2019-11-13 02:54:35,626 train 450 1.046814e-02 0.313774
2019-11-13 02:54:43,634 train 500 1.046164e-02 0.308475
2019-11-13 02:54:51,659 train 550 1.046286e-02 0.311094
2019-11-13 02:54:59,674 train 600 1.045294e-02 0.311282
2019-11-13 02:55:07,683 train 650 1.045310e-02 0.306505
2019-11-13 02:55:15,691 train 700 1.043554e-02 0.308444
2019-11-13 02:55:23,705 train 750 1.042995e-02 0.310611
2019-11-13 02:55:31,716 train 800 1.042465e-02 0.303920
2019-11-13 02:55:39,730 train 850 1.043520e-02 0.306187
2019-11-13 02:55:42,126 training loss; R2: 1.044437e-02 0.306222
2019-11-13 02:55:42,426 valid 000 9.875117e-02 -9.870712
2019-11-13 02:55:44,150 valid 050 1.008895e-01 -29.665330
2019-11-13 02:55:45,711 validation loss; R2: 1.011042e-01 -36.416912
2019-11-13 02:55:45,733 epoch 9 lr 1.000000e-03
2019-11-13 02:55:46,127 train 000 9.755384e-03 0.414426
2019-11-13 02:55:54,343 train 050 1.024691e-02 0.334362
2019-11-13 02:56:02,384 train 100 1.028857e-02 0.285854
2019-11-13 02:56:10,403 train 150 1.046456e-02 0.293564
2019-11-13 02:56:18,415 train 200 1.049529e-02 0.301589
2019-11-13 02:56:26,430 train 250 1.044165e-02 0.307882
2019-11-13 02:56:34,440 train 300 1.043642e-02 0.300228
2019-11-13 02:56:42,461 train 350 1.043810e-02 0.300797
2019-11-13 02:56:50,475 train 400 1.040334e-02 0.304360
2019-11-13 02:56:58,495 train 450 1.039682e-02 0.305197
2019-11-13 02:57:06,511 train 500 1.038908e-02 0.306066
2019-11-13 02:57:14,520 train 550 1.036446e-02 0.308583
2019-11-13 02:57:22,526 train 600 1.033942e-02 0.310417
2019-11-13 02:57:30,546 train 650 1.032695e-02 0.307943
2019-11-13 02:57:38,556 train 700 1.031522e-02 0.312435
2019-11-13 02:57:46,560 train 750 1.031543e-02 -0.142726
2019-11-13 02:57:54,563 train 800 1.030484e-02 -0.112885
2019-11-13 02:58:02,566 train 850 1.028952e-02 -0.085525
2019-11-13 02:58:04,960 training loss; R2: 1.029224e-02 -0.079180
2019-11-13 02:58:05,265 valid 000 1.823725e+01 -5572.092522
2019-11-13 02:58:06,991 valid 050 1.825915e+01 -5568.437382
2019-11-13 02:58:08,542 validation loss; R2: 1.826105e+01 -5605.361490
2019-11-13 02:58:08,565 epoch 10 lr 1.000000e-03
2019-11-13 02:58:08,962 train 000 1.018525e-02 0.375835
2019-11-13 02:58:17,027 train 050 9.990864e-03 0.353152
2019-11-13 02:58:25,152 train 100 1.021334e-02 0.351430
2019-11-13 02:58:33,228 train 150 1.013470e-02 0.262225
2019-11-13 02:58:41,416 train 200 1.021197e-02 0.287322
2019-11-13 02:58:49,523 train 250 1.021358e-02 0.300850
2019-11-13 02:58:57,715 train 300 1.028112e-02 0.307016
2019-11-13 02:59:05,801 train 350 1.025251e-02 0.314586
2019-11-13 02:59:13,848 train 400 1.024143e-02 0.317745
2019-11-13 02:59:21,860 train 450 1.025537e-02 0.320709
2019-11-13 02:59:29,874 train 500 1.021501e-02 0.322568
2019-11-13 02:59:37,888 train 550 1.021348e-02 0.324922
2019-11-13 02:59:45,911 train 600 1.018513e-02 0.324288
2019-11-13 02:59:53,918 train 650 1.016534e-02 0.326925
2019-11-13 03:00:01,925 train 700 1.017162e-02 0.328992
2019-11-13 03:00:09,940 train 750 1.016425e-02 0.316065
2019-11-13 03:00:17,960 train 800 1.014032e-02 0.320116
2019-11-13 03:00:25,975 train 850 1.015256e-02 0.322438
2019-11-13 03:00:28,383 training loss; R2: 1.015789e-02 0.323018
2019-11-13 03:00:28,692 valid 000 1.092355e+00 -6404.892228
2019-11-13 03:00:30,424 valid 050 1.102331e+00 -600.102721
2019-11-13 03:00:31,982 validation loss; R2: 1.101814e+00 -548.641302
2019-11-13 03:00:32,004 epoch 11 lr 1.000000e-03
2019-11-13 03:00:32,396 train 000 9.324138e-03 0.512666
2019-11-13 03:00:40,564 train 050 9.875316e-03 0.359141
2019-11-13 03:00:48,621 train 100 9.928266e-03 0.345047
2019-11-13 03:00:56,667 train 150 9.851830e-03 0.351137
2019-11-13 03:01:04,818 train 200 9.880083e-03 0.342634
2019-11-13 03:01:12,830 train 250 9.962716e-03 0.323662
2019-11-13 03:01:20,834 train 300 9.979465e-03 0.321652
2019-11-13 03:01:28,845 train 350 1.003748e-02 0.315732
2019-11-13 03:01:36,850 train 400 1.007796e-02 0.323813
2019-11-13 03:01:44,858 train 450 1.007770e-02 0.327343
2019-11-13 03:01:52,866 train 500 1.010533e-02 0.321270
2019-11-13 03:02:00,874 train 550 1.011398e-02 0.324737
2019-11-13 03:02:08,880 train 600 1.012500e-02 0.327965
2019-11-13 03:02:16,883 train 650 1.012156e-02 0.328239
2019-11-13 03:02:24,926 train 700 1.011000e-02 0.329176
2019-11-13 03:02:32,929 train 750 1.011883e-02 0.330216
2019-11-13 03:02:40,932 train 800 1.009910e-02 0.328681
2019-11-13 03:02:48,935 train 850 1.011456e-02 0.328243
2019-11-13 03:02:51,329 training loss; R2: 1.011258e-02 0.328981
2019-11-13 03:02:51,635 valid 000 2.063676e+01 -1979.365071
2019-11-13 03:02:53,358 valid 050 2.065470e+01 -1782.881405
2019-11-13 03:02:54,912 validation loss; R2: 2.065252e+01 -2024.968967
2019-11-13 03:02:54,936 epoch 12 lr 1.000000e-03
2019-11-13 03:02:55,347 train 000 8.895484e-03 0.399432
2019-11-13 03:03:03,543 train 050 1.018465e-02 0.339055
2019-11-13 03:03:11,602 train 100 1.019519e-02 0.335305
2019-11-13 03:03:19,642 train 150 1.012685e-02 0.345683
2019-11-13 03:03:27,693 train 200 1.010670e-02 0.348223
2019-11-13 03:03:35,734 train 250 1.009634e-02 0.335045
2019-11-13 03:03:43,845 train 300 1.011756e-02 0.332067
2019-11-13 03:03:52,113 train 350 1.010404e-02 0.338458
2019-11-13 03:04:00,301 train 400 1.013611e-02 0.328485
2019-11-13 03:04:08,419 train 450 1.015137e-02 0.332055
2019-11-13 03:04:16,466 train 500 1.016669e-02 0.326053
2019-11-13 03:04:24,522 train 550 1.015670e-02 0.329054
2019-11-13 03:04:32,569 train 600 1.015060e-02 0.331424
2019-11-13 03:04:40,610 train 650 1.012647e-02 0.329908
2019-11-13 03:04:48,641 train 700 1.014152e-02 0.330602
2019-11-13 03:04:56,752 train 750 1.012780e-02 0.329000
2019-11-13 03:05:04,915 train 800 1.013115e-02 0.330801
2019-11-13 03:05:12,965 train 850 1.012479e-02 0.329892
2019-11-13 03:05:15,365 training loss; R2: 1.012071e-02 0.329826
2019-11-13 03:05:15,660 valid 000 1.923383e-02 -1.045547
2019-11-13 03:05:17,405 valid 050 2.174822e-02 -1.582205
2019-11-13 03:05:18,957 validation loss; R2: 2.142665e-02 -1.732644
2019-11-13 03:05:18,981 epoch 13 lr 1.000000e-03
2019-11-13 03:05:19,371 train 000 1.020184e-02 0.234011
2019-11-13 03:05:27,456 train 050 1.011143e-02 0.343137
2019-11-13 03:05:35,597 train 100 1.007841e-02 0.304794
2019-11-13 03:05:43,708 train 150 1.006430e-02 0.302187
2019-11-13 03:05:51,810 train 200 1.000588e-02 0.303751
2019-11-13 03:05:59,934 train 250 1.000876e-02 0.314722
2019-11-13 03:06:08,008 train 300 9.961927e-03 0.323141
2019-11-13 03:06:16,079 train 350 9.938599e-03 0.316504
2019-11-13 03:06:24,196 train 400 9.981989e-03 0.314380
2019-11-13 03:06:32,269 train 450 1.000883e-02 0.294049
2019-11-13 03:06:40,366 train 500 1.004588e-02 0.297500
2019-11-13 03:06:48,477 train 550 1.004404e-02 0.300690
2019-11-13 03:06:56,550 train 600 1.001153e-02 0.266703
2019-11-13 03:07:04,628 train 650 9.995185e-03 0.270478
2019-11-13 03:07:12,693 train 700 1.000487e-02 0.272635
2019-11-13 03:07:20,773 train 750 1.002727e-02 0.273986
2019-11-13 03:07:28,838 train 800 1.004666e-02 -0.618840
2019-11-13 03:07:36,906 train 850 1.006538e-02 -1.553411
2019-11-13 03:07:39,317 training loss; R2: 1.006018e-02 -1.519626
2019-11-13 03:07:39,614 valid 000 5.330986e+00 -244.386599
2019-11-13 03:07:41,340 valid 050 5.323812e+00 -288.689449
2019-11-13 03:07:42,890 validation loss; R2: 5.318117e+00 -426.561451
2019-11-13 03:07:42,913 epoch 14 lr 1.000000e-03
2019-11-13 03:07:43,324 train 000 9.519484e-03 0.363580
2019-11-13 03:07:51,391 train 050 1.058813e-02 0.328929
2019-11-13 03:07:59,550 train 100 1.037325e-02 0.328410
2019-11-13 03:08:07,659 train 150 1.021170e-02 0.333132
2019-11-13 03:08:15,755 train 200 1.019081e-02 0.331328
2019-11-13 03:08:23,833 train 250 1.014551e-02 0.247391
2019-11-13 03:08:31,949 train 300 1.012974e-02 0.259655
2019-11-13 03:08:40,026 train 350 1.011160e-02 0.262386
2019-11-13 03:08:48,108 train 400 1.008694e-02 0.271880
2019-11-13 03:08:56,181 train 450 1.003417e-02 0.283555
2019-11-13 03:09:04,257 train 500 1.001137e-02 0.289621
2019-11-13 03:09:12,342 train 550 1.003821e-02 0.292339
2019-11-13 03:09:20,451 train 600 1.000423e-02 0.297015
2019-11-13 03:09:28,541 train 650 9.999597e-03 0.302097
2019-11-13 03:09:36,663 train 700 9.992937e-03 0.304612
2019-11-13 03:09:44,770 train 750 9.990541e-03 0.307568
2019-11-13 03:09:52,878 train 800 9.987298e-03 0.311942
2019-11-13 03:10:00,968 train 850 9.984950e-03 0.287741
2019-11-13 03:10:03,384 training loss; R2: 9.980574e-03 0.287422
2019-11-13 03:10:03,685 valid 000 5.049852e+00 -193.138042
2019-11-13 03:10:05,421 valid 050 5.027462e+00 -298.738773
2019-11-13 03:10:06,975 validation loss; R2: 5.025014e+00 -301.453442
2019-11-13 03:10:06,995 epoch 15 lr 1.000000e-03
2019-11-13 03:10:07,429 train 000 9.537459e-03 0.473424
2019-11-13 03:10:15,566 train 050 9.811574e-03 0.361152
2019-11-13 03:10:23,642 train 100 9.872072e-03 0.357780
2019-11-13 03:10:31,709 train 150 9.869296e-03 0.358039
2019-11-13 03:10:39,836 train 200 9.859106e-03 0.349909
2019-11-13 03:10:47,965 train 250 9.807355e-03 0.357349
2019-11-13 03:10:56,058 train 300 9.888488e-03 0.318401
2019-11-13 03:11:04,137 train 350 9.868361e-03 0.327437
2019-11-13 03:11:12,210 train 400 9.852559e-03 0.332375
2019-11-13 03:11:20,275 train 450 9.846917e-03 0.335966
2019-11-13 03:11:28,354 train 500 9.857086e-03 0.332878
2019-11-13 03:11:36,459 train 550 9.868156e-03 0.333547
2019-11-13 03:11:44,560 train 600 9.884951e-03 0.334537
2019-11-13 03:11:52,665 train 650 9.894478e-03 0.327647
2019-11-13 03:12:00,739 train 700 9.915492e-03 0.327425
2019-11-13 03:12:08,805 train 750 9.904723e-03 0.329244
2019-11-13 03:12:16,870 train 800 9.896458e-03 0.331572
2019-11-13 03:12:24,937 train 850 9.921798e-03 0.332504
2019-11-13 03:12:27,345 training loss; R2: 9.927417e-03 0.332454
2019-11-13 03:12:27,639 valid 000 3.293126e-01 -136.226551
2019-11-13 03:12:29,357 valid 050 3.369387e-01 -53.954751
2019-11-13 03:12:30,905 validation loss; R2: 3.365165e-01 -53.016416
2019-11-13 03:12:30,926 epoch 16 lr 1.000000e-03
2019-11-13 03:12:31,331 train 000 9.659748e-03 0.411497
2019-11-13 03:12:39,432 train 050 9.880729e-03 0.366308
2019-11-13 03:12:47,519 train 100 9.868434e-03 0.360541
2019-11-13 03:12:55,620 train 150 9.937933e-03 0.346612
2019-11-13 03:13:03,696 train 200 9.926353e-03 0.347035
2019-11-13 03:13:11,864 train 250 9.876462e-03 0.347408
2019-11-13 03:13:19,950 train 300 9.856321e-03 0.332810
2019-11-13 03:13:28,031 train 350 9.820054e-03 0.336807
2019-11-13 03:13:36,143 train 400 9.811458e-03 0.339501
2019-11-13 03:13:44,223 train 450 9.805665e-03 0.340972
2019-11-13 03:13:52,344 train 500 9.855097e-03 0.314888
2019-11-13 03:14:00,454 train 550 9.911586e-03 0.316145
2019-11-13 03:14:08,597 train 600 9.896992e-03 0.319175
2019-11-13 03:14:16,737 train 650 9.905414e-03 0.321288
2019-11-13 03:14:24,874 train 700 9.919187e-03 0.320974
2019-11-13 03:14:32,968 train 750 9.917834e-03 0.321878
2019-11-13 03:14:41,040 train 800 9.922673e-03 0.324667
2019-11-13 03:14:49,148 train 850 9.912215e-03 0.326750
2019-11-13 03:14:51,571 training loss; R2: 9.909498e-03 0.325919
2019-11-13 03:14:51,862 valid 000 1.806927e+00 -200.169769
2019-11-13 03:14:53,599 valid 050 1.828740e+00 -983.307351
2019-11-13 03:14:55,146 validation loss; R2: 1.827285e+00 -746.131825
2019-11-13 03:14:55,166 epoch 17 lr 1.000000e-03
2019-11-13 03:14:55,575 train 000 1.031232e-02 0.337476
2019-11-13 03:15:03,696 train 050 9.659606e-03 0.341875
2019-11-13 03:15:11,810 train 100 9.719045e-03 0.359240
2019-11-13 03:15:19,923 train 150 9.658034e-03 0.354579
2019-11-13 03:15:28,230 train 200 9.722314e-03 0.356557
2019-11-13 03:15:36,367 train 250 9.756840e-03 0.348754
2019-11-13 03:15:44,579 train 300 9.778237e-03 0.345196
2019-11-13 03:15:52,789 train 350 9.779324e-03 0.350800
2019-11-13 03:16:01,011 train 400 9.790619e-03 0.350048
2019-11-13 03:16:09,325 train 450 9.780331e-03 0.353884
2019-11-13 03:16:17,532 train 500 9.812614e-03 0.342282
2019-11-13 03:16:25,721 train 550 9.808690e-03 0.346162
2019-11-13 03:16:33,798 train 600 9.810449e-03 0.347436
2019-11-13 03:16:41,909 train 650 9.835735e-03 0.348447
2019-11-13 03:16:49,997 train 700 9.824023e-03 0.350655
2019-11-13 03:16:58,074 train 750 9.833130e-03 0.349236
2019-11-13 03:17:06,200 train 800 9.837679e-03 0.339360
2019-11-13 03:17:14,307 train 850 9.833889e-03 0.341111
2019-11-13 03:17:16,722 training loss; R2: 9.838427e-03 0.341987
2019-11-13 03:17:17,030 valid 000 9.897927e+01 -5808.730269
2019-11-13 03:17:18,748 valid 050 9.875752e+01 -10525.166126
2019-11-13 03:17:20,303 validation loss; R2: 9.877661e+01 -10188.278779
2019-11-13 03:17:20,324 epoch 18 lr 1.000000e-03
2019-11-13 03:17:20,764 train 000 1.005618e-02 0.246140
2019-11-13 03:17:28,917 train 050 9.773328e-03 0.164350
2019-11-13 03:17:37,029 train 100 9.723529e-03 0.258901
2019-11-13 03:17:45,155 train 150 9.778287e-03 0.296666
2019-11-13 03:17:53,257 train 200 9.780340e-03 0.305177
2019-11-13 03:18:01,356 train 250 9.782639e-03 0.301836
2019-11-13 03:18:09,510 train 300 9.776954e-03 0.310341
2019-11-13 03:18:17,593 train 350 9.818283e-03 0.311327
2019-11-13 03:18:25,675 train 400 9.830785e-03 0.318668
2019-11-13 03:18:33,765 train 450 9.849297e-03 0.321867
2019-11-13 03:18:41,832 train 500 9.863206e-03 0.324363
2019-11-13 03:18:49,911 train 550 9.853781e-03 0.325357
2019-11-13 03:18:58,034 train 600 9.850752e-03 0.328631
2019-11-13 03:19:06,423 train 650 9.838252e-03 0.330295
2019-11-13 03:19:14,497 train 700 9.869460e-03 0.330735
2019-11-13 03:19:22,547 train 750 9.875565e-03 0.334259
2019-11-13 03:19:30,656 train 800 9.883650e-03 0.336696
2019-11-13 03:19:38,750 train 850 9.889835e-03 0.336652
2019-11-13 03:19:41,165 training loss; R2: 9.902483e-03 0.335370
2019-11-13 03:19:41,501 valid 000 1.867859e+01 -8917.808430
2019-11-13 03:19:43,150 valid 050 1.855906e+01 -21896.904685
2019-11-13 03:19:44,677 validation loss; R2: 1.856575e+01 -17146.379307
2019-11-13 03:19:44,697 epoch 19 lr 1.000000e-03
2019-11-13 03:19:45,179 train 000 9.686664e-03 0.445226
2019-11-13 03:19:53,302 train 050 1.015262e-02 0.227345
2019-11-13 03:20:01,397 train 100 1.013382e-02 0.286840
2019-11-13 03:20:09,587 train 150 9.981131e-03 0.319185
2019-11-13 03:20:17,761 train 200 9.901683e-03 0.313650
2019-11-13 03:20:25,851 train 250 9.851318e-03 0.317986
2019-11-13 03:20:33,917 train 300 9.894245e-03 0.324344
2019-11-13 03:20:41,979 train 350 9.887545e-03 0.318828
2019-11-13 03:20:50,112 train 400 9.936669e-03 0.320888
2019-11-13 03:20:58,188 train 450 9.896312e-03 0.328080
2019-11-13 03:21:06,260 train 500 9.886993e-03 0.332864
2019-11-13 03:21:14,333 train 550 9.890835e-03 0.332807
2019-11-13 03:21:22,389 train 600 9.876904e-03 0.319454
2019-11-13 03:21:30,456 train 650 9.894468e-03 0.324209
2019-11-13 03:21:38,546 train 700 9.876746e-03 0.328754
2019-11-13 03:21:46,624 train 750 9.850291e-03 0.327332
2019-11-13 03:21:54,704 train 800 9.822362e-03 0.330699
2019-11-13 03:22:02,789 train 850 9.811833e-03 0.332997
2019-11-13 03:22:05,206 training loss; R2: 9.811536e-03 0.332657
2019-11-13 03:22:05,514 valid 000 3.020922e+01 -12603.650373
2019-11-13 03:22:07,256 valid 050 3.006556e+01 -8687.037160
2019-11-13 03:22:08,800 validation loss; R2: 3.006798e+01 -7851.562332
