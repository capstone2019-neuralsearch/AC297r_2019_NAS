2019-12-03 18:20:46,937 gpu device = 1
2019-12-03 18:20:46,937 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=96, cutout=False, cutout_length=16, data='../data', dataset='graphene', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name=None, gpu=1, grad_clip=5, gz_dtree=False, init_channels=36, layers=8, learning_rate=0.002, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-graphene-EXP-20191203-182046', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-12-03 18:20:50,161 param size = 1.005085MB
2019-12-03 18:20:50,164 epoch 0 lr 2.000000e-03
2019-12-03 18:20:52,633 train 000 1.251672e+00 -46.309365
2019-12-03 18:21:03,914 train 050 1.338097e-01 -3.531396
2019-12-03 18:21:14,963 train 100 8.016157e-02 -1.692421
2019-12-03 18:21:26,003 train 150 6.331392e-02 -1.115752
2019-12-03 18:21:37,050 train 200 5.321884e-02 -0.783635
2019-12-03 18:21:43,007 training loss; R2: 4.986091e-02 -0.664091
2019-12-03 18:21:43,133 valid 000 1.724660e-02 0.464651
2019-12-03 18:21:44,647 validation loss; R2: 1.757360e-02 0.413126
2019-12-03 18:21:44,666 epoch 1 lr 2.000000e-03
2019-12-03 18:21:44,996 train 000 1.565064e-02 0.531269
2019-12-03 18:21:56,056 train 050 1.817858e-02 0.411322
2019-12-03 18:22:07,116 train 100 1.703923e-02 0.452446
2019-12-03 18:22:18,173 train 150 1.606618e-02 0.480767
2019-12-03 18:22:29,225 train 200 1.516375e-02 0.505488
2019-12-03 18:22:34,192 training loss; R2: 1.513212e-02 0.507970
2019-12-03 18:22:34,301 valid 000 7.950568e-03 0.704389
2019-12-03 18:22:35,495 validation loss; R2: 7.286653e-03 0.747065
2019-12-03 18:22:35,514 epoch 2 lr 2.000000e-03
2019-12-03 18:22:35,799 train 000 2.113443e-02 0.633715
2019-12-03 18:22:46,863 train 050 1.260963e-02 0.594454
2019-12-03 18:22:57,923 train 100 1.274226e-02 0.593901
2019-12-03 18:23:08,982 train 150 1.197715e-02 0.607476
2019-12-03 18:23:20,040 train 200 1.184353e-02 0.610897
2019-12-03 18:23:25,015 training loss; R2: 1.157605e-02 0.618152
2019-12-03 18:23:25,128 valid 000 9.336405e-03 0.784218
2019-12-03 18:23:26,323 validation loss; R2: 6.715974e-03 0.774969
2019-12-03 18:23:26,344 epoch 3 lr 2.000000e-03
2019-12-03 18:23:26,628 train 000 7.375531e-03 0.768444
2019-12-03 18:23:37,683 train 050 8.973365e-03 0.667927
2019-12-03 18:23:48,746 train 100 9.673419e-03 0.666039
2019-12-03 18:23:59,807 train 150 9.766189e-03 0.675236
2019-12-03 18:24:10,893 train 200 9.613470e-03 0.677536
2019-12-03 18:24:15,873 training loss; R2: 9.507200e-03 0.680457
2019-12-03 18:24:15,982 valid 000 8.091498e-03 0.687434
2019-12-03 18:24:17,177 validation loss; R2: 6.813695e-03 0.764302
2019-12-03 18:24:17,196 epoch 4 lr 2.000000e-03
2019-12-03 18:24:17,481 train 000 8.557920e-03 0.538007
2019-12-03 18:24:28,548 train 050 8.781809e-03 0.708459
2019-12-03 18:24:39,612 train 100 8.047266e-03 0.720798
2019-12-03 18:24:50,682 train 150 8.345599e-03 0.720364
2019-12-03 18:25:01,748 train 200 8.054283e-03 0.726464
2019-12-03 18:25:06,724 training loss; R2: 8.100326e-03 0.726940
2019-12-03 18:25:06,834 valid 000 6.673470e-03 0.902924
2019-12-03 18:25:08,030 validation loss; R2: 5.722292e-03 0.795006
2019-12-03 18:25:08,049 epoch 5 lr 2.000000e-03
2019-12-03 18:25:08,333 train 000 8.554906e-03 0.761999
2019-12-03 18:25:19,393 train 050 7.552035e-03 0.764136
2019-12-03 18:25:30,458 train 100 7.793481e-03 0.738100
2019-12-03 18:25:41,524 train 150 7.485347e-03 0.746100
2019-12-03 18:25:52,594 train 200 7.402925e-03 0.747437
2019-12-03 18:25:57,576 training loss; R2: 7.396214e-03 0.745998
2019-12-03 18:25:57,693 valid 000 4.159465e-03 0.916431
2019-12-03 18:25:58,887 validation loss; R2: 5.220291e-03 0.817362
2019-12-03 18:25:58,907 epoch 6 lr 2.000000e-03
2019-12-03 18:25:59,195 train 000 6.360188e-03 0.525822
2019-12-03 18:26:10,259 train 050 7.414883e-03 0.753017
2019-12-03 18:26:21,316 train 100 6.833877e-03 0.766527
2019-12-03 18:26:32,373 train 150 6.756822e-03 0.772363
2019-12-03 18:26:43,433 train 200 6.613891e-03 0.775480
2019-12-03 18:26:48,402 training loss; R2: 6.665262e-03 0.775515
2019-12-03 18:26:48,512 valid 000 5.958758e-03 0.813088
2019-12-03 18:26:49,708 validation loss; R2: 4.197019e-03 0.853272
2019-12-03 18:26:49,726 epoch 7 lr 2.000000e-03
2019-12-03 18:26:50,014 train 000 6.557900e-03 0.687801
2019-12-03 18:27:01,077 train 050 6.365705e-03 0.780033
2019-12-03 18:27:12,144 train 100 6.667298e-03 0.777322
2019-12-03 18:27:23,215 train 150 6.639283e-03 0.782609
2019-12-03 18:27:34,281 train 200 6.505153e-03 0.781595
2019-12-03 18:27:39,253 training loss; R2: 6.417113e-03 0.780805
2019-12-03 18:27:39,368 valid 000 3.636416e-03 0.887705
2019-12-03 18:27:40,563 validation loss; R2: 4.690010e-03 0.839374
2019-12-03 18:27:40,582 epoch 8 lr 2.000000e-03
2019-12-03 18:27:40,865 train 000 5.148582e-03 0.837279
2019-12-03 18:27:51,916 train 050 5.585756e-03 0.796371
2019-12-03 18:28:02,971 train 100 6.439593e-03 0.769268
2019-12-03 18:28:14,024 train 150 6.439435e-03 0.771251
2019-12-03 18:28:25,077 train 200 6.406962e-03 0.778375
2019-12-03 18:28:30,043 training loss; R2: 6.419484e-03 0.777744
2019-12-03 18:28:30,159 valid 000 2.869015e-03 0.902342
2019-12-03 18:28:31,355 validation loss; R2: 4.240286e-03 0.846831
2019-12-03 18:28:31,375 epoch 9 lr 2.000000e-03
2019-12-03 18:28:31,661 train 000 7.757564e-03 0.805182
2019-12-03 18:28:42,709 train 050 6.922688e-03 0.769077
2019-12-03 18:28:53,760 train 100 6.905990e-03 0.754991
2019-12-03 18:29:04,816 train 150 6.346482e-03 0.776732
2019-12-03 18:29:15,870 train 200 6.354552e-03 0.779837
2019-12-03 18:29:20,841 training loss; R2: 6.243698e-03 0.783123
2019-12-03 18:29:20,952 valid 000 3.563011e-03 0.910693
2019-12-03 18:29:22,147 validation loss; R2: 3.661418e-03 0.873450
2019-12-03 18:29:22,166 epoch 10 lr 2.000000e-03
2019-12-03 18:29:22,448 train 000 4.007231e-03 0.852411
2019-12-03 18:29:33,500 train 050 6.172295e-03 0.784444
2019-12-03 18:29:44,565 train 100 6.191029e-03 0.786475
2019-12-03 18:29:55,625 train 150 6.438899e-03 0.782349
2019-12-03 18:30:06,676 train 200 6.348219e-03 0.784317
2019-12-03 18:30:11,646 training loss; R2: 6.207332e-03 0.786517
2019-12-03 18:30:11,764 valid 000 3.419290e-03 0.820337
2019-12-03 18:30:12,958 validation loss; R2: 3.828154e-03 0.872060
2019-12-03 18:30:12,983 epoch 11 lr 2.000000e-03
2019-12-03 18:30:13,274 train 000 3.444314e-03 0.912826
2019-12-03 18:30:24,316 train 050 5.704599e-03 0.806795
2019-12-03 18:30:35,361 train 100 5.416426e-03 0.809866
2019-12-03 18:30:46,404 train 150 5.410828e-03 0.812793
2019-12-03 18:30:57,450 train 200 5.413753e-03 0.810885
2019-12-03 18:31:02,416 training loss; R2: 5.552982e-03 0.808197
2019-12-03 18:31:02,530 valid 000 2.420947e-03 0.940346
2019-12-03 18:31:03,727 validation loss; R2: 3.582425e-03 0.873154
2019-12-03 18:31:03,746 epoch 12 lr 2.000000e-03
2019-12-03 18:31:04,031 train 000 4.782090e-03 0.791092
2019-12-03 18:31:15,080 train 050 7.129470e-03 0.775242
2019-12-03 18:31:26,130 train 100 6.289985e-03 0.787174
2019-12-03 18:31:37,177 train 150 5.905710e-03 0.800816
2019-12-03 18:31:48,232 train 200 5.685308e-03 0.804501
2019-12-03 18:31:53,203 training loss; R2: 5.721547e-03 0.804414
2019-12-03 18:31:53,316 valid 000 3.465607e-03 0.769926
2019-12-03 18:31:54,512 validation loss; R2: 3.587774e-03 0.867888
2019-12-03 18:31:54,532 epoch 13 lr 2.000000e-03
2019-12-03 18:31:54,817 train 000 3.519448e-03 0.880607
2019-12-03 18:32:05,858 train 050 5.381228e-03 0.807106
2019-12-03 18:32:16,897 train 100 5.679626e-03 0.802438
2019-12-03 18:32:27,934 train 150 5.413077e-03 0.811138
2019-12-03 18:32:38,967 train 200 5.483002e-03 0.810353
2019-12-03 18:32:43,926 training loss; R2: 5.465634e-03 0.811800
2019-12-03 18:32:44,038 valid 000 3.947541e-02 -0.967246
2019-12-03 18:32:45,232 validation loss; R2: 4.249752e-02 -0.404974
2019-12-03 18:32:45,252 epoch 14 lr 2.000000e-03
2019-12-03 18:32:45,537 train 000 3.505588e-03 0.825619
2019-12-03 18:32:56,572 train 050 5.641835e-03 0.820139
2019-12-03 18:33:07,608 train 100 5.123865e-03 0.821262
2019-12-03 18:33:18,644 train 150 5.099976e-03 0.826907
2019-12-03 18:33:29,683 train 200 5.170213e-03 0.825488
2019-12-03 18:33:34,651 training loss; R2: 5.145758e-03 0.824140
2019-12-03 18:33:34,770 valid 000 7.138564e-02 -2.297571
2019-12-03 18:33:35,964 validation loss; R2: 6.822160e-02 -1.573707
2019-12-03 18:33:35,983 epoch 15 lr 2.000000e-03
2019-12-03 18:33:36,273 train 000 5.075752e-03 0.850618
2019-12-03 18:33:47,311 train 050 5.425576e-03 0.807819
2019-12-03 18:33:58,347 train 100 5.504224e-03 0.815841
2019-12-03 18:34:09,388 train 150 5.303151e-03 0.820947
2019-12-03 18:34:20,425 train 200 5.253826e-03 0.821376
2019-12-03 18:34:25,388 training loss; R2: 5.230999e-03 0.822335
2019-12-03 18:34:25,497 valid 000 3.050650e-02 -0.343778
2019-12-03 18:34:26,691 validation loss; R2: 3.805342e-02 -0.272749
2019-12-03 18:34:26,711 epoch 16 lr 2.000000e-03
2019-12-03 18:34:26,998 train 000 4.718116e-03 0.898095
2019-12-03 18:34:38,034 train 050 5.572555e-03 0.817637
2019-12-03 18:34:49,069 train 100 5.781799e-03 0.809135
2019-12-03 18:35:00,105 train 150 5.458020e-03 0.808604
2019-12-03 18:35:11,127 train 200 5.258801e-03 0.812170
2019-12-03 18:35:16,085 training loss; R2: 5.223655e-03 0.815671
2019-12-03 18:35:16,197 valid 000 3.077558e+00 -112.365752
2019-12-03 18:35:17,390 validation loss; R2: 2.993973e+00 -105.877691
2019-12-03 18:35:17,409 epoch 17 lr 2.000000e-03
2019-12-03 18:35:17,693 train 000 5.125432e-03 0.867278
2019-12-03 18:35:28,715 train 050 5.980652e-03 0.795051
2019-12-03 18:35:39,733 train 100 5.681024e-03 0.805217
2019-12-03 18:35:50,757 train 150 5.382221e-03 0.815723
2019-12-03 18:36:01,780 train 200 5.323153e-03 0.817067
2019-12-03 18:36:06,738 training loss; R2: 5.266851e-03 0.819298
2019-12-03 18:36:06,854 valid 000 6.419587e-03 0.744327
2019-12-03 18:36:08,048 validation loss; R2: 5.715077e-03 0.790642
2019-12-03 18:36:08,069 epoch 18 lr 2.000000e-03
2019-12-03 18:36:08,359 train 000 4.472457e-03 0.860921
2019-12-03 18:36:19,372 train 050 4.633760e-03 0.842695
2019-12-03 18:36:30,390 train 100 4.768159e-03 0.843418
2019-12-03 18:36:41,506 train 150 4.639155e-03 0.845617
2019-12-03 18:36:52,818 train 200 4.710219e-03 0.840417
2019-12-03 18:36:57,904 training loss; R2: 4.756679e-03 0.839956
2019-12-03 18:36:58,024 valid 000 4.362921e-03 0.800476
2019-12-03 18:36:59,218 validation loss; R2: 3.134672e-03 0.890420
2019-12-03 18:36:59,244 epoch 19 lr 2.000000e-03
2019-12-03 18:36:59,539 train 000 3.977947e-03 0.911626
2019-12-03 18:37:10,845 train 050 5.317927e-03 0.816816
2019-12-03 18:37:22,159 train 100 5.598131e-03 0.809330
2019-12-03 18:37:33,466 train 150 5.273483e-03 0.822348
2019-12-03 18:37:44,630 train 200 5.149998e-03 0.826022
2019-12-03 18:37:49,585 training loss; R2: 5.084848e-03 0.827328
2019-12-03 18:37:49,699 valid 000 1.633066e-02 0.690593
2019-12-03 18:37:50,894 validation loss; R2: 8.030568e-03 0.723105
