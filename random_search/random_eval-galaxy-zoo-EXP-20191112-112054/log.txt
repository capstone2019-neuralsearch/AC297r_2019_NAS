2019-11-12 11:20:55,168 gpu device = 1
2019-11-12 11:20:55,168 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.2, epochs=10, fc1_size=1024, fc2_size=1024, folder_name='galaxy-zoo-cleaned', gpu=1, grad_clip=5, gz_dtree=False, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-galaxy-zoo-EXP-20191112-112054', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-11-12 11:21:06,744 param size = 0.236213MB
2019-11-12 11:21:06,747 epoch 0 lr 1.000000e-03
2019-11-12 11:21:08,828 train 000 4.667366e-01 -325.826885
2019-11-12 11:21:14,198 train 050 5.012558e-02 -15.291105
2019-11-12 11:21:19,478 train 100 3.898085e-02 -8.018457
2019-11-12 11:21:24,734 train 150 3.467295e-02 -5.491301
2019-11-12 11:21:30,123 train 200 3.211625e-02 -4.183046
2019-11-12 11:21:35,598 train 250 3.057035e-02 -3.393237
2019-11-12 11:21:41,290 train 300 2.939486e-02 -2.855061
2019-11-12 11:21:46,843 train 350 2.832642e-02 -2.470546
2019-11-12 11:21:52,400 train 400 2.751659e-02 -2.177255
2019-11-12 11:21:57,863 train 450 2.685820e-02 -1.945463
2019-11-12 11:22:03,369 train 500 2.628691e-02 -1.753924
2019-11-12 11:22:08,853 train 550 2.577578e-02 -1.596584
2019-11-12 11:22:14,321 train 600 2.534969e-02 -1.464517
2019-11-12 11:22:19,789 train 650 2.490786e-02 -1.348747
2019-11-12 11:22:25,274 train 700 2.453064e-02 -1.250156
2019-11-12 11:22:30,817 train 750 2.417048e-02 -1.189296
2019-11-12 11:22:36,167 train 800 2.385725e-02 -1.117992
2019-11-12 11:22:41,709 train 850 2.352197e-02 -1.049746
2019-11-12 11:22:44,031 training loss; R2: 2.342579e-02 -1.030217
2019-11-12 11:22:44,321 valid 000 1.595874e-02 0.239309
2019-11-12 11:22:46,061 valid 050 1.833048e-02 -0.051943
2019-11-12 11:22:47,659 validation loss; R2: 1.823714e-02 0.016322
2019-11-12 11:22:47,677 epoch 1 lr 1.000000e-03
2019-11-12 11:22:48,186 train 000 1.827552e-02 -0.056826
2019-11-12 11:22:53,784 train 050 1.844669e-02 0.030616
2019-11-12 11:22:59,300 train 100 1.848400e-02 0.039162
2019-11-12 11:23:04,849 train 150 1.816157e-02 0.061872
2019-11-12 11:23:10,367 train 200 1.815637e-02 0.039052
2019-11-12 11:23:15,890 train 250 1.801992e-02 0.052921
2019-11-12 11:23:21,395 train 300 1.792890e-02 0.067160
2019-11-12 11:23:26,901 train 350 1.776001e-02 -9.834841
2019-11-12 11:23:32,425 train 400 1.764664e-02 -8.595864
2019-11-12 11:23:37,936 train 450 1.756351e-02 -7.626050
2019-11-12 11:23:43,457 train 500 1.741247e-02 -6.854393
2019-11-12 11:23:48,987 train 550 1.729449e-02 -6.223172
2019-11-12 11:23:54,553 train 600 1.721022e-02 -5.691312
2019-11-12 11:24:00,092 train 650 1.712535e-02 -5.241506
2019-11-12 11:24:05,613 train 700 1.703045e-02 -4.854063
2019-11-12 11:24:11,142 train 750 1.694439e-02 -4.519579
2019-11-12 11:24:16,687 train 800 1.688283e-02 -4.225797
2019-11-12 11:24:22,233 train 850 1.678159e-02 -3.968407
2019-11-12 11:24:23,917 training loss; R2: 1.674881e-02 -3.896078
2019-11-12 11:24:24,200 valid 000 1.828169e-02 0.163878
2019-11-12 11:24:26,173 valid 050 1.767201e-02 0.089883
2019-11-12 11:24:27,893 validation loss; R2: 1.752705e-02 0.039752
2019-11-12 11:24:27,912 epoch 2 lr 1.000000e-03
2019-11-12 11:24:28,240 train 000 1.897326e-02 0.007539
2019-11-12 11:24:33,755 train 050 1.529420e-02 0.152885
2019-11-12 11:24:39,350 train 100 1.506446e-02 0.159625
2019-11-12 11:24:44,919 train 150 1.495657e-02 0.168436
2019-11-12 11:24:50,511 train 200 1.503188e-02 0.154532
2019-11-12 11:24:56,101 train 250 1.489136e-02 0.166859
2019-11-12 11:25:01,663 train 300 1.486234e-02 0.174724
2019-11-12 11:25:07,234 train 350 1.484214e-02 0.178506
2019-11-12 11:25:12,798 train 400 1.475758e-02 0.185610
2019-11-12 11:25:18,419 train 450 1.472816e-02 0.191256
2019-11-12 11:25:24,041 train 500 1.468265e-02 0.190460
2019-11-12 11:25:29,665 train 550 1.460490e-02 0.189241
2019-11-12 11:25:35,285 train 600 1.453549e-02 0.191373
2019-11-12 11:25:40,907 train 650 1.447312e-02 0.194616
2019-11-12 11:25:46,538 train 700 1.442078e-02 0.196619
2019-11-12 11:25:52,177 train 750 1.434885e-02 -0.134577
2019-11-12 11:25:57,802 train 800 1.430423e-02 -0.114990
2019-11-12 11:26:03,433 train 850 1.425308e-02 -0.093835
2019-11-12 11:26:05,108 training loss; R2: 1.424562e-02 -0.087662
2019-11-12 11:26:05,380 valid 000 1.248658e-02 0.275685
2019-11-12 11:26:07,071 valid 050 1.425980e-02 0.020202
2019-11-12 11:26:08,605 validation loss; R2: 1.421930e-02 0.096404
2019-11-12 11:26:08,629 epoch 3 lr 1.000000e-03
2019-11-12 11:26:08,960 train 000 1.458746e-02 0.236867
2019-11-12 11:26:14,576 train 050 1.308780e-02 0.215652
2019-11-12 11:26:20,097 train 100 1.320520e-02 0.229154
2019-11-12 11:26:25,601 train 150 1.337748e-02 0.238903
2019-11-12 11:26:31,159 train 200 1.328732e-02 0.241062
2019-11-12 11:26:36,684 train 250 1.328052e-02 0.239082
2019-11-12 11:26:42,278 train 300 1.322618e-02 0.230303
2019-11-12 11:26:47,829 train 350 1.321940e-02 0.231408
2019-11-12 11:26:53,372 train 400 1.316033e-02 0.236738
2019-11-12 11:26:58,906 train 450 1.313905e-02 0.237180
2019-11-12 11:27:04,461 train 500 1.311523e-02 0.238869
2019-11-12 11:27:10,019 train 550 1.308752e-02 0.239897
2019-11-12 11:27:15,542 train 600 1.305745e-02 0.242960
2019-11-12 11:27:21,120 train 650 1.303584e-02 0.243414
2019-11-12 11:27:26,719 train 700 1.302224e-02 0.242772
2019-11-12 11:27:32,405 train 750 1.298799e-02 0.241793
2019-11-12 11:27:38,085 train 800 1.294038e-02 0.240016
2019-11-12 11:27:43,723 train 850 1.292792e-02 0.238305
2019-11-12 11:27:45,399 training loss; R2: 1.291850e-02 0.238190
2019-11-12 11:27:45,677 valid 000 1.423939e-02 0.306813
2019-11-12 11:27:47,315 valid 050 1.491303e-02 0.216590
2019-11-12 11:27:48,811 validation loss; R2: 1.469809e-02 0.205317
2019-11-12 11:27:48,824 epoch 4 lr 1.000000e-03
2019-11-12 11:27:49,152 train 000 1.416079e-02 0.319541
2019-11-12 11:27:54,794 train 050 1.237101e-02 0.276622
2019-11-12 11:28:00,411 train 100 1.241750e-02 0.260160
2019-11-12 11:28:06,021 train 150 1.251127e-02 0.267340
2019-11-12 11:28:11,667 train 200 1.244783e-02 0.264130
2019-11-12 11:28:17,462 train 250 1.241258e-02 0.264299
2019-11-12 11:28:23,123 train 300 1.241435e-02 0.262401
2019-11-12 11:28:28,797 train 350 1.243749e-02 0.263645
2019-11-12 11:28:34,447 train 400 1.240411e-02 0.249999
2019-11-12 11:28:40,008 train 450 1.234836e-02 0.250928
2019-11-12 11:28:45,585 train 500 1.233921e-02 0.247530
2019-11-12 11:28:51,158 train 550 1.231828e-02 0.246895
2019-11-12 11:28:56,747 train 600 1.228014e-02 0.250679
2019-11-12 11:29:02,299 train 650 1.225863e-02 0.254218
2019-11-12 11:29:07,851 train 700 1.222740e-02 0.257106
2019-11-12 11:29:13,387 train 750 1.222151e-02 0.258347
2019-11-12 11:29:18,962 train 800 1.220629e-02 0.259723
2019-11-12 11:29:24,484 train 850 1.218933e-02 0.261078
2019-11-12 11:29:26,180 training loss; R2: 1.218232e-02 0.261741
2019-11-12 11:29:26,462 valid 000 1.011298e-02 0.418878
2019-11-12 11:29:28,365 valid 050 1.098187e-02 0.323850
2019-11-12 11:29:30,006 validation loss; R2: 1.101722e-02 0.332141
2019-11-12 11:29:30,024 epoch 5 lr 1.000000e-03
2019-11-12 11:29:30,403 train 000 1.133660e-02 0.384676
2019-11-12 11:29:35,979 train 050 1.199351e-02 0.294422
2019-11-12 11:29:41,551 train 100 1.179549e-02 0.295647
2019-11-12 11:29:47,106 train 150 1.175653e-02 0.294234
2019-11-12 11:29:52,696 train 200 1.179154e-02 0.245119
2019-11-12 11:29:58,299 train 250 1.176622e-02 0.248288
2019-11-12 11:30:03,894 train 300 1.165096e-02 0.253239
2019-11-12 11:30:09,498 train 350 1.169790e-02 0.259304
2019-11-12 11:30:15,093 train 400 1.164913e-02 0.264136
2019-11-12 11:30:20,697 train 450 1.166210e-02 0.267245
2019-11-12 11:30:26,319 train 500 1.164831e-02 0.265203
2019-11-12 11:30:31,916 train 550 1.166163e-02 0.267185
2019-11-12 11:30:37,519 train 600 1.167613e-02 0.271820
2019-11-12 11:30:43,057 train 650 1.165892e-02 0.273044
2019-11-12 11:30:48,604 train 700 1.166320e-02 0.266793
2019-11-12 11:30:54,140 train 750 1.165513e-02 0.269683
2019-11-12 11:30:59,689 train 800 1.163647e-02 0.267847
2019-11-12 11:31:05,246 train 850 1.161245e-02 0.268611
2019-11-12 11:31:06,914 training loss; R2: 1.161369e-02 0.269410
2019-11-12 11:31:07,205 valid 000 8.793458e-03 0.438259
2019-11-12 11:31:08,912 valid 050 1.037618e-02 0.371657
2019-11-12 11:31:10,451 validation loss; R2: 1.038276e-02 0.367334
2019-11-12 11:31:10,470 epoch 6 lr 1.000000e-03
2019-11-12 11:31:10,778 train 000 1.179575e-02 0.295600
2019-11-12 11:31:16,273 train 050 1.141289e-02 0.316275
2019-11-12 11:31:22,129 train 100 1.154942e-02 0.308872
2019-11-12 11:31:27,934 train 150 1.155045e-02 0.319014
2019-11-12 11:31:33,593 train 200 1.153651e-02 0.315741
2019-11-12 11:31:39,167 train 250 1.148802e-02 0.314447
2019-11-12 11:31:44,716 train 300 1.140488e-02 0.311741
2019-11-12 11:31:50,262 train 350 1.140323e-02 0.294762
2019-11-12 11:31:55,806 train 400 1.134296e-02 0.267363
2019-11-12 11:32:01,361 train 450 1.135333e-02 0.273144
2019-11-12 11:32:06,913 train 500 1.133252e-02 0.275570
2019-11-12 11:32:12,468 train 550 1.132568e-02 0.277182
2019-11-12 11:32:18,017 train 600 1.132092e-02 0.280419
2019-11-12 11:32:23,569 train 650 1.130409e-02 0.284456
2019-11-12 11:32:29,117 train 700 1.129661e-02 0.281482
2019-11-12 11:32:34,680 train 750 1.129840e-02 0.282777
2019-11-12 11:32:40,245 train 800 1.128289e-02 0.284819
2019-11-12 11:32:45,797 train 850 1.127158e-02 0.284812
2019-11-12 11:32:47,461 training loss; R2: 1.127394e-02 0.285958
2019-11-12 11:32:47,755 valid 000 1.234589e-02 0.333783
2019-11-12 11:32:49,483 valid 050 1.114377e-02 0.155610
2019-11-12 11:32:50,997 validation loss; R2: 1.137746e-02 0.123187
2019-11-12 11:32:51,019 epoch 7 lr 1.000000e-03
2019-11-12 11:32:51,399 train 000 1.113093e-02 -0.065253
2019-11-12 11:32:57,064 train 050 1.122839e-02 0.198767
2019-11-12 11:33:02,775 train 100 1.107859e-02 0.246720
2019-11-12 11:33:08,549 train 150 1.104692e-02 0.239176
2019-11-12 11:33:14,253 train 200 1.111985e-02 0.252500
2019-11-12 11:33:20,002 train 250 1.112596e-02 0.244918
2019-11-12 11:33:25,712 train 300 1.108622e-02 0.260654
2019-11-12 11:33:31,515 train 350 1.107433e-02 0.260530
2019-11-12 11:33:37,214 train 400 1.106260e-02 0.247964
2019-11-12 11:33:42,902 train 450 1.104273e-02 0.249999
2019-11-12 11:33:48,608 train 500 1.104730e-02 0.257321
2019-11-12 11:33:54,206 train 550 1.104928e-02 0.261898
2019-11-12 11:33:59,852 train 600 1.102489e-02 0.267051
2019-11-12 11:34:05,479 train 650 1.101232e-02 0.270619
2019-11-12 11:34:11,111 train 700 1.099757e-02 0.274418
2019-11-12 11:34:16,792 train 750 1.097475e-02 0.275625
2019-11-12 11:34:22,431 train 800 1.096224e-02 0.280822
2019-11-12 11:34:28,131 train 850 1.094167e-02 0.283884
2019-11-12 11:34:29,840 training loss; R2: 1.093221e-02 0.284246
2019-11-12 11:34:30,110 valid 000 1.168808e-02 0.233665
2019-11-12 11:34:31,821 valid 050 1.041143e-02 0.372809
2019-11-12 11:34:33,350 validation loss; R2: 1.035178e-02 0.365295
2019-11-12 11:34:33,372 epoch 8 lr 1.000000e-03
2019-11-12 11:34:33,740 train 000 1.206529e-02 0.293043
2019-11-12 11:34:39,343 train 050 1.085570e-02 0.326763
2019-11-12 11:34:45,007 train 100 1.074052e-02 0.333106
2019-11-12 11:34:50,695 train 150 1.073347e-02 0.327399
2019-11-12 11:34:56,499 train 200 1.079771e-02 0.319411
2019-11-12 11:35:02,041 train 250 1.076388e-02 0.323056
2019-11-12 11:35:07,600 train 300 1.074638e-02 0.326339
2019-11-12 11:35:13,210 train 350 1.076615e-02 0.324266
2019-11-12 11:35:18,780 train 400 1.077123e-02 0.327416
2019-11-12 11:35:24,347 train 450 1.076028e-02 0.327742
2019-11-12 11:35:29,915 train 500 1.074364e-02 0.329321
2019-11-12 11:35:35,538 train 550 1.073849e-02 0.325580
2019-11-12 11:35:41,109 train 600 1.070474e-02 0.326091
2019-11-12 11:35:46,677 train 650 1.070325e-02 0.323564
2019-11-12 11:35:52,240 train 700 1.070590e-02 0.312134
2019-11-12 11:35:57,827 train 750 1.068221e-02 0.305846
2019-11-12 11:36:03,398 train 800 1.067843e-02 0.305228
2019-11-12 11:36:08,971 train 850 1.067783e-02 0.307190
2019-11-12 11:36:10,643 training loss; R2: 1.067981e-02 0.307265
2019-11-12 11:36:10,943 valid 000 8.327267e-03 0.396339
2019-11-12 11:36:12,591 valid 050 9.250554e-03 0.256671
2019-11-12 11:36:14,103 validation loss; R2: 9.237297e-03 0.086033
2019-11-12 11:36:14,116 epoch 9 lr 1.000000e-03
2019-11-12 11:36:14,482 train 000 1.117752e-02 0.407246
2019-11-12 11:36:20,100 train 050 1.051676e-02 -0.556857
2019-11-12 11:36:25,785 train 100 1.054147e-02 -0.109216
2019-11-12 11:36:31,466 train 150 1.054718e-02 0.037708
2019-11-12 11:36:37,179 train 200 1.057150e-02 0.109656
2019-11-12 11:36:42,892 train 250 1.051323e-02 0.155741
2019-11-12 11:36:48,613 train 300 1.050755e-02 0.185869
2019-11-12 11:36:54,310 train 350 1.047097e-02 0.207140
2019-11-12 11:37:00,051 train 400 1.045264e-02 0.218138
2019-11-12 11:37:05,794 train 450 1.045443e-02 0.230665
2019-11-12 11:37:11,551 train 500 1.048037e-02 0.242427
2019-11-12 11:37:17,290 train 550 1.050350e-02 0.214291
2019-11-12 11:37:23,058 train 600 1.049091e-02 0.224819
2019-11-12 11:37:28,765 train 650 1.048616e-02 0.232684
2019-11-12 11:37:34,471 train 700 1.047550e-02 0.240526
2019-11-12 11:37:40,202 train 750 1.047919e-02 0.243827
2019-11-12 11:37:45,932 train 800 1.046649e-02 0.249254
2019-11-12 11:37:51,646 train 850 1.046662e-02 0.252139
2019-11-12 11:37:53,365 training loss; R2: 1.046330e-02 0.254104
2019-11-12 11:37:53,653 valid 000 1.028869e-02 0.342850
2019-11-12 11:37:55,385 valid 050 9.440266e-03 0.292171
2019-11-12 11:37:56,926 validation loss; R2: 9.606335e-03 0.304741
