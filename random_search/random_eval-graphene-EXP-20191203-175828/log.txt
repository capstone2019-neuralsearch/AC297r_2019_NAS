2019-12-03 17:58:28,219 gpu device = 1
2019-12-03 17:58:28,219 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=96, cutout=False, cutout_length=16, data='../data', dataset='graphene', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name=None, gpu=1, grad_clip=5, gz_dtree=False, init_channels=36, layers=8, learning_rate=0.002, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-graphene-EXP-20191203-175828', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-12-03 17:58:31,435 param size = 1.293085MB
2019-12-03 17:58:31,438 epoch 0 lr 2.000000e-03
2019-12-03 17:58:34,187 train 000 6.890886e-01 -27.664747
2019-12-03 17:58:48,671 train 050 2.164018e-01 -5.360317
2019-12-03 17:59:03,082 train 100 1.260436e-01 -2.749542
2019-12-03 17:59:17,494 train 150 9.369872e-02 -1.830070
2019-12-03 17:59:31,908 train 200 7.665987e-02 -1.330050
2019-12-03 17:59:39,460 training loss; R2: 7.123345e-02 -1.161818
2019-12-03 17:59:39,593 valid 000 1.058982e-02 0.475161
2019-12-03 17:59:41,403 validation loss; R2: 1.798129e-02 0.403594
2019-12-03 17:59:41,428 epoch 1 lr 2.000000e-03
2019-12-03 17:59:41,835 train 000 2.359440e-02 0.421167
2019-12-03 17:59:56,243 train 050 2.206820e-02 0.291778
2019-12-03 18:00:10,653 train 100 2.211574e-02 0.256520
2019-12-03 18:00:25,065 train 150 2.174279e-02 0.269032
2019-12-03 18:00:39,481 train 200 2.021286e-02 0.317323
2019-12-03 18:00:45,962 training loss; R2: 1.968735e-02 0.343127
2019-12-03 18:00:46,085 valid 000 1.131818e-02 0.669411
2019-12-03 18:00:47,591 validation loss; R2: 1.281808e-02 0.541239
2019-12-03 18:00:47,623 epoch 2 lr 2.000000e-03
2019-12-03 18:00:47,974 train 000 1.359288e-02 0.258396
2019-12-03 18:01:02,396 train 050 1.294614e-02 0.606591
2019-12-03 18:01:16,824 train 100 1.257604e-02 0.599619
2019-12-03 18:01:31,248 train 150 1.183447e-02 0.611457
2019-12-03 18:01:45,674 train 200 1.156881e-02 0.607115
2019-12-03 18:01:52,169 training loss; R2: 1.143032e-02 0.613700
2019-12-03 18:01:52,295 valid 000 8.343739e-03 0.844260
2019-12-03 18:01:53,802 validation loss; R2: 5.801441e-03 0.797959
2019-12-03 18:01:53,828 epoch 3 lr 2.000000e-03
2019-12-03 18:01:54,191 train 000 8.979336e-03 0.680253
2019-12-03 18:02:08,619 train 050 9.726956e-03 0.703972
2019-12-03 18:02:23,041 train 100 9.726783e-03 0.684686
2019-12-03 18:02:37,463 train 150 9.552718e-03 0.685875
2019-12-03 18:02:51,884 train 200 9.229139e-03 0.690565
2019-12-03 18:02:58,369 training loss; R2: 9.032606e-03 0.694856
2019-12-03 18:02:58,489 valid 000 6.444700e-03 0.809337
2019-12-03 18:02:59,998 validation loss; R2: 5.806809e-03 0.796072
2019-12-03 18:03:00,025 epoch 4 lr 2.000000e-03
2019-12-03 18:03:00,376 train 000 9.042359e-03 0.728947
2019-12-03 18:03:14,802 train 050 8.264853e-03 0.726550
2019-12-03 18:03:29,230 train 100 7.833238e-03 0.731784
2019-12-03 18:03:43,661 train 150 7.657618e-03 0.736155
2019-12-03 18:03:58,085 train 200 7.688012e-03 0.734545
2019-12-03 18:04:04,574 training loss; R2: 7.602634e-03 0.738873
2019-12-03 18:04:04,698 valid 000 3.855063e-03 0.880749
2019-12-03 18:04:06,204 validation loss; R2: 4.952019e-03 0.822191
2019-12-03 18:04:06,231 epoch 5 lr 2.000000e-03
2019-12-03 18:04:06,582 train 000 1.201693e-02 0.780323
2019-12-03 18:04:21,008 train 050 7.733011e-03 0.734244
2019-12-03 18:04:35,432 train 100 7.304641e-03 0.745021
2019-12-03 18:04:49,856 train 150 7.532226e-03 0.745201
2019-12-03 18:05:04,279 train 200 7.341051e-03 0.750997
2019-12-03 18:05:10,765 training loss; R2: 7.301625e-03 0.757811
2019-12-03 18:05:10,897 valid 000 4.278693e-03 0.841564
2019-12-03 18:05:12,405 validation loss; R2: 4.808896e-03 0.834226
2019-12-03 18:05:12,440 epoch 6 lr 2.000000e-03
2019-12-03 18:05:12,809 train 000 6.490152e-03 0.753683
2019-12-03 18:05:27,234 train 050 7.323437e-03 0.745372
2019-12-03 18:05:41,657 train 100 7.283258e-03 0.756807
2019-12-03 18:05:56,083 train 150 6.935966e-03 0.766995
2019-12-03 18:06:10,511 train 200 6.781019e-03 0.767444
2019-12-03 18:06:16,999 training loss; R2: 6.738422e-03 0.769760
2019-12-03 18:06:17,122 valid 000 6.164676e-03 0.742720
2019-12-03 18:06:18,631 validation loss; R2: 3.732230e-03 0.866136
2019-12-03 18:06:18,657 epoch 7 lr 2.000000e-03
2019-12-03 18:06:19,013 train 000 2.494769e-03 0.894734
2019-12-03 18:06:33,441 train 050 5.790500e-03 0.799827
2019-12-03 18:06:47,861 train 100 6.049855e-03 0.795959
2019-12-03 18:07:02,286 train 150 5.879831e-03 0.801023
2019-12-03 18:07:16,705 train 200 5.989091e-03 0.797159
2019-12-03 18:07:23,196 training loss; R2: 5.913601e-03 0.798606
2019-12-03 18:07:23,321 valid 000 3.295102e-03 0.859336
2019-12-03 18:07:24,828 validation loss; R2: 4.334368e-03 0.847485
2019-12-03 18:07:24,854 epoch 8 lr 2.000000e-03
2019-12-03 18:07:25,208 train 000 4.537202e-03 0.833368
2019-12-03 18:07:39,623 train 050 6.633872e-03 0.786126
2019-12-03 18:07:54,058 train 100 5.951145e-03 0.805934
2019-12-03 18:08:08,484 train 150 5.721821e-03 0.816855
2019-12-03 18:08:22,915 train 200 5.668798e-03 0.812593
2019-12-03 18:08:29,403 training loss; R2: 5.674656e-03 0.812440
2019-12-03 18:08:29,534 valid 000 7.142791e-03 0.799400
2019-12-03 18:08:31,042 validation loss; R2: 3.781280e-03 0.862826
2019-12-03 18:08:31,069 epoch 9 lr 2.000000e-03
2019-12-03 18:08:31,424 train 000 7.429638e-03 0.878024
2019-12-03 18:08:45,848 train 050 4.714956e-03 0.810773
2019-12-03 18:09:00,278 train 100 4.795948e-03 0.823843
2019-12-03 18:09:14,700 train 150 4.946754e-03 0.821464
2019-12-03 18:09:29,123 train 200 5.174567e-03 0.817045
2019-12-03 18:09:35,612 training loss; R2: 5.230355e-03 0.816055
2019-12-03 18:09:35,746 valid 000 2.785514e-03 0.896013
2019-12-03 18:09:37,253 validation loss; R2: 3.553948e-03 0.882324
2019-12-03 18:09:37,284 epoch 10 lr 2.000000e-03
2019-12-03 18:09:37,647 train 000 3.810992e-03 0.825020
2019-12-03 18:09:52,073 train 050 5.214908e-03 0.818529
2019-12-03 18:10:06,499 train 100 5.191554e-03 0.821004
2019-12-03 18:10:20,936 train 150 5.100857e-03 0.827136
2019-12-03 18:10:35,355 train 200 5.026188e-03 0.832093
2019-12-03 18:10:41,842 training loss; R2: 5.067862e-03 0.831318
2019-12-03 18:10:41,965 valid 000 6.523737e-03 0.821621
2019-12-03 18:10:43,473 validation loss; R2: 3.528713e-03 0.874992
2019-12-03 18:10:43,498 epoch 11 lr 2.000000e-03
2019-12-03 18:10:43,854 train 000 5.361674e-03 0.819110
2019-12-03 18:10:58,274 train 050 5.725190e-03 0.815195
2019-12-03 18:11:12,701 train 100 5.121962e-03 0.823256
2019-12-03 18:11:27,125 train 150 5.127195e-03 0.823725
2019-12-03 18:11:41,552 train 200 4.959352e-03 0.830777
2019-12-03 18:11:48,042 training loss; R2: 4.939908e-03 0.831475
2019-12-03 18:11:48,165 valid 000 4.379440e-03 0.929141
2019-12-03 18:11:49,673 validation loss; R2: 5.042598e-03 0.814417
2019-12-03 18:11:49,702 epoch 12 lr 2.000000e-03
2019-12-03 18:11:50,060 train 000 2.599521e-03 0.905689
2019-12-03 18:12:04,485 train 050 4.159868e-03 0.852890
2019-12-03 18:12:18,905 train 100 4.306952e-03 0.851774
2019-12-03 18:12:33,328 train 150 4.608292e-03 0.842559
2019-12-03 18:12:47,750 train 200 4.561233e-03 0.844398
2019-12-03 18:12:54,232 training loss; R2: 4.564212e-03 0.845295
2019-12-03 18:12:54,352 valid 000 2.465239e-03 0.907060
2019-12-03 18:12:55,859 validation loss; R2: 3.078682e-03 0.896613
2019-12-03 18:12:55,885 epoch 13 lr 2.000000e-03
2019-12-03 18:12:56,240 train 000 6.329273e-03 0.824113
2019-12-03 18:13:10,654 train 050 4.162502e-03 0.859547
2019-12-03 18:13:25,066 train 100 4.261513e-03 0.854029
2019-12-03 18:13:39,476 train 150 4.383507e-03 0.846797
2019-12-03 18:13:53,887 train 200 4.311825e-03 0.850530
2019-12-03 18:14:00,366 training loss; R2: 4.286370e-03 0.851680
2019-12-03 18:14:00,485 valid 000 3.809809e-03 0.909615
2019-12-03 18:14:01,991 validation loss; R2: 3.066156e-03 0.898109
2019-12-03 18:14:02,018 epoch 14 lr 2.000000e-03
2019-12-03 18:14:02,373 train 000 1.196917e-02 0.770127
2019-12-03 18:14:16,778 train 050 4.986783e-03 0.830604
2019-12-03 18:14:31,183 train 100 4.656973e-03 0.844415
2019-12-03 18:14:45,589 train 150 4.619243e-03 0.844511
2019-12-03 18:14:59,993 train 200 4.578512e-03 0.845485
2019-12-03 18:15:06,474 training loss; R2: 4.524985e-03 0.846286
2019-12-03 18:15:06,599 valid 000 8.525490e-03 0.821387
2019-12-03 18:15:08,106 validation loss; R2: 6.734907e-03 0.754234
2019-12-03 18:15:08,132 epoch 15 lr 2.000000e-03
2019-12-03 18:15:08,486 train 000 5.258524e-03 0.854322
2019-12-03 18:15:22,888 train 050 4.462448e-03 0.864225
2019-12-03 18:15:37,288 train 100 4.277735e-03 0.857410
2019-12-03 18:15:51,712 train 150 4.399335e-03 0.856569
2019-12-03 18:16:06,361 train 200 4.401833e-03 0.853160
2019-12-03 18:16:13,017 training loss; R2: 4.406751e-03 0.850410
2019-12-03 18:16:13,144 valid 000 2.870087e-03 0.818601
2019-12-03 18:16:14,653 validation loss; R2: 3.835344e-03 0.865511
2019-12-03 18:16:14,688 epoch 16 lr 2.000000e-03
2019-12-03 18:16:15,053 train 000 3.876872e-03 0.888304
2019-12-03 18:16:29,851 train 050 3.955291e-03 0.868514
2019-12-03 18:16:44,647 train 100 4.005545e-03 0.866644
2019-12-03 18:16:59,438 train 150 4.197333e-03 0.862644
2019-12-03 18:17:14,229 train 200 4.403631e-03 0.853331
2019-12-03 18:17:20,885 training loss; R2: 4.370181e-03 0.853043
2019-12-03 18:17:21,012 valid 000 3.513021e-03 0.781116
2019-12-03 18:17:22,521 validation loss; R2: 4.052966e-03 0.861215
2019-12-03 18:17:22,548 epoch 17 lr 2.000000e-03
2019-12-03 18:17:22,912 train 000 3.363531e-03 0.902282
2019-12-03 18:17:37,713 train 050 4.250350e-03 0.845737
2019-12-03 18:17:52,507 train 100 4.257941e-03 0.846865
2019-12-03 18:18:07,296 train 150 4.201989e-03 0.849546
2019-12-03 18:18:22,083 train 200 4.157484e-03 0.852159
2019-12-03 18:18:28,742 training loss; R2: 4.201767e-03 0.853531
2019-12-03 18:18:28,871 valid 000 2.171730e-03 0.906722
2019-12-03 18:18:30,378 validation loss; R2: 2.756405e-03 0.904194
2019-12-03 18:18:30,405 epoch 18 lr 2.000000e-03
2019-12-03 18:18:30,767 train 000 4.428649e-03 0.887019
2019-12-03 18:18:45,548 train 050 4.122833e-03 0.854015
2019-12-03 18:19:00,331 train 100 3.993735e-03 0.858365
2019-12-03 18:19:15,117 train 150 3.998315e-03 0.859176
2019-12-03 18:19:29,899 train 200 3.932596e-03 0.862310
2019-12-03 18:19:36,550 training loss; R2: 4.051632e-03 0.858513
2019-12-03 18:19:36,670 valid 000 2.852070e-03 0.913895
2019-12-03 18:19:38,176 validation loss; R2: 3.005819e-03 0.896331
2019-12-03 18:19:38,204 epoch 19 lr 2.000000e-03
2019-12-03 18:19:38,566 train 000 3.601779e-03 0.918843
2019-12-03 18:19:53,342 train 050 4.735090e-03 0.839400
2019-12-03 18:20:08,116 train 100 4.526397e-03 0.850506
2019-12-03 18:20:22,779 train 150 4.358803e-03 0.852037
2019-12-03 18:20:37,157 train 200 4.326535e-03 0.854272
2019-12-03 18:20:43,628 training loss; R2: 4.271708e-03 0.853131
2019-12-03 18:20:43,749 valid 000 2.859834e-03 0.903441
2019-12-03 18:20:45,254 validation loss; R2: 3.266759e-03 0.884875
