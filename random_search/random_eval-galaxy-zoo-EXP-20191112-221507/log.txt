2019-11-12 22:15:07,832 gpu device = 1
2019-11-12 22:15:07,833 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name='galaxy-zoo-cleaned', gpu=1, grad_clip=5, gz_dtree=False, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-galaxy-zoo-EXP-20191112-221507', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-11-12 22:15:19,385 param size = 0.289173MB
2019-11-12 22:15:19,389 epoch 0 lr 1.000000e-03
2019-11-12 22:15:21,603 train 000 3.264931e-01 -66.602601
2019-11-12 22:15:28,704 train 050 4.950596e-02 -9.064515
2019-11-12 22:15:35,829 train 100 3.909987e-02 -4.845675
2019-11-12 22:15:42,881 train 150 3.487886e-02 -3.361807
2019-11-12 22:15:50,069 train 200 3.245286e-02 -2.585165
2019-11-12 22:15:57,382 train 250 3.096567e-02 -2.119014
2019-11-12 22:16:04,606 train 300 2.999391e-02 -1.851182
2019-11-12 22:16:11,674 train 350 2.918639e-02 -1.611484
2019-11-12 22:16:18,764 train 400 2.850856e-02 -1.435796
2019-11-12 22:16:26,060 train 450 2.803888e-02 -1.286755
2019-11-12 22:16:33,321 train 500 2.760162e-02 -1.172281
2019-11-12 22:16:40,566 train 550 2.723699e-02 -1.072206
2019-11-12 22:16:47,642 train 600 2.690391e-02 -0.988478
2019-11-12 22:16:54,886 train 650 2.662519e-02 -0.917460
2019-11-12 22:17:02,212 train 700 2.636535e-02 -0.855100
2019-11-12 22:17:09,384 train 750 2.611884e-02 -0.802765
2019-11-12 22:17:16,633 train 800 2.586129e-02 -0.755748
2019-11-12 22:17:23,785 train 850 2.562741e-02 -0.712704
2019-11-12 22:17:26,744 training loss; R2: 2.554505e-02 -0.700151
2019-11-12 22:17:27,033 valid 000 2.655707e-02 -0.738309
2019-11-12 22:17:28,715 valid 050 2.423413e-02 -0.051509
2019-11-12 22:17:30,307 validation loss; R2: 2.438673e-02 -0.096500
2019-11-12 22:17:30,323 epoch 1 lr 1.000000e-03
2019-11-12 22:17:30,874 train 000 2.697064e-02 -0.045146
2019-11-12 22:17:37,989 train 050 2.205411e-02 -0.005024
2019-11-12 22:17:45,172 train 100 2.192275e-02 -0.002304
2019-11-12 22:17:52,433 train 150 2.179876e-02 -0.009091
2019-11-12 22:17:59,564 train 200 2.155883e-02 -0.137014
2019-11-12 22:18:06,717 train 250 2.151016e-02 -0.108643
2019-11-12 22:18:13,829 train 300 2.144913e-02 -0.091159
2019-11-12 22:18:20,935 train 350 2.123269e-02 -0.075447
2019-11-12 22:18:28,199 train 400 2.113532e-02 -0.066336
2019-11-12 22:18:35,377 train 450 2.101071e-02 -0.053058
2019-11-12 22:18:42,549 train 500 2.093774e-02 -0.040146
2019-11-12 22:18:49,802 train 550 2.088965e-02 -0.030433
2019-11-12 22:18:57,107 train 600 2.083467e-02 -0.021439
2019-11-12 22:19:04,300 train 650 2.074588e-02 -0.014322
2019-11-12 22:19:11,503 train 700 2.068489e-02 -0.006432
2019-11-12 22:19:18,628 train 750 2.062296e-02 -0.000723
2019-11-12 22:19:25,776 train 800 2.055739e-02 0.002982
2019-11-12 22:19:32,942 train 850 2.049139e-02 0.006465
2019-11-12 22:19:35,040 training loss; R2: 2.046383e-02 0.008313
2019-11-12 22:19:35,310 valid 000 2.042504e-02 0.132392
2019-11-12 22:19:36,978 valid 050 1.918899e-02 0.133909
2019-11-12 22:19:38,501 validation loss; R2: 1.912238e-02 0.138346
2019-11-12 22:19:38,521 epoch 2 lr 1.000000e-03
2019-11-12 22:19:38,935 train 000 2.251954e-02 0.138105
2019-11-12 22:19:46,072 train 050 1.903679e-02 0.090731
2019-11-12 22:19:53,257 train 100 1.923223e-02 0.094947
2019-11-12 22:20:00,522 train 150 1.949416e-02 0.081121
2019-11-12 22:20:07,783 train 200 1.948880e-02 0.083192
2019-11-12 22:20:14,982 train 250 1.946768e-02 0.087384
2019-11-12 22:20:22,138 train 300 1.943336e-02 0.086725
2019-11-12 22:20:29,392 train 350 1.931546e-02 0.087022
2019-11-12 22:20:36,574 train 400 1.924849e-02 0.087072
2019-11-12 22:20:43,822 train 450 1.921434e-02 0.090584
2019-11-12 22:20:50,965 train 500 1.912014e-02 0.090599
2019-11-12 22:20:58,242 train 550 1.908012e-02 0.093517
2019-11-12 22:21:05,505 train 600 1.902493e-02 0.093802
2019-11-12 22:21:12,635 train 650 1.898427e-02 0.094627
2019-11-12 22:21:19,878 train 700 1.890958e-02 0.079660
2019-11-12 22:21:27,003 train 750 1.885277e-02 0.083738
2019-11-12 22:21:34,099 train 800 1.880110e-02 0.086842
2019-11-12 22:21:41,160 train 850 1.874902e-02 0.089126
2019-11-12 22:21:43,237 training loss; R2: 1.871383e-02 0.089991
2019-11-12 22:21:43,527 valid 000 1.844432e-02 -0.838440
2019-11-12 22:21:45,216 valid 050 1.777365e-02 0.108616
2019-11-12 22:21:46,771 validation loss; R2: 1.773594e-02 0.124823
2019-11-12 22:21:46,797 epoch 3 lr 1.000000e-03
2019-11-12 22:21:47,202 train 000 1.843184e-02 -1.869093
2019-11-12 22:21:54,222 train 050 1.818273e-02 -0.001810
2019-11-12 22:22:01,295 train 100 1.803811e-02 0.073318
2019-11-12 22:22:08,606 train 150 1.786955e-02 0.097028
2019-11-12 22:22:15,954 train 200 1.777891e-02 0.096702
2019-11-12 22:22:23,284 train 250 1.780754e-02 0.106188
2019-11-12 22:22:30,462 train 300 1.771421e-02 0.112093
2019-11-12 22:22:37,648 train 350 1.759182e-02 0.119216
2019-11-12 22:22:44,822 train 400 1.750287e-02 0.118625
2019-11-12 22:22:51,983 train 450 1.741896e-02 0.121154
2019-11-12 22:22:59,190 train 500 1.737382e-02 0.124268
2019-11-12 22:23:06,504 train 550 1.730681e-02 0.124245
2019-11-12 22:23:13,650 train 600 1.724709e-02 0.129322
2019-11-12 22:23:20,989 train 650 1.719252e-02 0.134807
2019-11-12 22:23:28,328 train 700 1.716958e-02 0.136613
2019-11-12 22:23:35,485 train 750 1.714833e-02 0.138902
2019-11-12 22:23:42,779 train 800 1.708318e-02 0.140565
2019-11-12 22:23:50,016 train 850 1.702847e-02 0.142487
2019-11-12 22:23:52,194 training loss; R2: 1.700864e-02 0.143368
2019-11-12 22:23:52,459 valid 000 1.691695e-02 0.130739
2019-11-12 22:23:54,126 valid 050 1.650112e-02 -0.063596
2019-11-12 22:23:55,644 validation loss; R2: 1.661131e-02 0.048073
2019-11-12 22:23:55,661 epoch 4 lr 1.000000e-03
2019-11-12 22:23:56,030 train 000 1.565950e-02 0.257337
2019-11-12 22:24:03,220 train 050 1.610146e-02 0.133430
2019-11-12 22:24:10,447 train 100 1.588837e-02 0.152048
2019-11-12 22:24:17,641 train 150 1.586095e-02 0.143239
2019-11-12 22:24:24,755 train 200 1.591936e-02 0.147650
2019-11-12 22:24:31,982 train 250 1.591458e-02 0.145824
2019-11-12 22:24:39,097 train 300 1.583020e-02 0.147921
2019-11-12 22:24:46,302 train 350 1.584639e-02 0.135146
2019-11-12 22:24:53,377 train 400 1.583227e-02 0.143141
2019-11-12 22:25:00,326 train 450 1.576120e-02 0.113286
2019-11-12 22:25:07,269 train 500 1.571224e-02 0.119217
2019-11-12 22:25:14,204 train 550 1.565162e-02 0.124839
2019-11-12 22:25:21,138 train 600 1.558695e-02 0.130346
2019-11-12 22:25:28,138 train 650 1.555716e-02 0.131876
2019-11-12 22:25:35,085 train 700 1.551670e-02 0.135921
2019-11-12 22:25:42,029 train 750 1.549046e-02 0.134998
2019-11-12 22:25:48,968 train 800 1.543928e-02 0.136390
2019-11-12 22:25:55,917 train 850 1.540663e-02 0.132541
2019-11-12 22:25:57,992 training loss; R2: 1.539596e-02 0.134276
2019-11-12 22:25:58,290 valid 000 1.465233e-02 0.213245
2019-11-12 22:25:59,976 valid 050 1.421428e-02 0.232849
2019-11-12 22:26:01,496 validation loss; R2: 1.412529e-02 0.235576
2019-11-12 22:26:01,523 epoch 5 lr 1.000000e-03
2019-11-12 22:26:01,967 train 000 1.370570e-02 0.203266
2019-11-12 22:26:09,206 train 050 1.473471e-02 0.218645
2019-11-12 22:26:16,242 train 100 1.468224e-02 0.225573
2019-11-12 22:26:23,207 train 150 1.476767e-02 0.225350
2019-11-12 22:26:30,155 train 200 1.480837e-02 0.215381
2019-11-12 22:26:37,136 train 250 1.481433e-02 0.219131
2019-11-12 22:26:44,130 train 300 1.475753e-02 0.217760
2019-11-12 22:26:51,077 train 350 1.474886e-02 0.211808
2019-11-12 22:26:58,032 train 400 1.469888e-02 0.211820
2019-11-12 22:27:04,980 train 450 1.465948e-02 0.210445
2019-11-12 22:27:11,926 train 500 1.465749e-02 0.211127
2019-11-12 22:27:18,876 train 550 1.461007e-02 0.212593
2019-11-12 22:27:25,825 train 600 1.456974e-02 0.211317
2019-11-12 22:27:32,766 train 650 1.452623e-02 0.212450
2019-11-12 22:27:39,708 train 700 1.447560e-02 0.208937
2019-11-12 22:27:46,650 train 750 1.446014e-02 0.210993
2019-11-12 22:27:53,595 train 800 1.444587e-02 0.211090
2019-11-12 22:28:00,533 train 850 1.440763e-02 0.209144
2019-11-12 22:28:02,612 training loss; R2: 1.439166e-02 0.209860
2019-11-12 22:28:02,913 valid 000 1.220974e-02 0.394757
2019-11-12 22:28:04,654 valid 050 1.174144e-02 0.196122
2019-11-12 22:28:06,197 validation loss; R2: 1.196498e-02 0.218172
2019-11-12 22:28:06,214 epoch 6 lr 1.000000e-03
2019-11-12 22:28:06,614 train 000 1.026467e-02 0.193832
2019-11-12 22:28:13,697 train 050 1.414403e-02 0.203800
2019-11-12 22:28:20,738 train 100 1.390970e-02 0.221341
2019-11-12 22:28:27,810 train 150 1.375500e-02 0.222694
2019-11-12 22:28:34,803 train 200 1.386875e-02 0.229494
2019-11-12 22:28:41,835 train 250 1.386423e-02 0.230910
2019-11-12 22:28:48,880 train 300 1.385129e-02 0.225609
2019-11-12 22:28:55,877 train 350 1.379729e-02 0.229492
2019-11-12 22:29:02,862 train 400 1.380844e-02 0.231954
2019-11-12 22:29:09,936 train 450 1.383372e-02 0.229705
2019-11-12 22:29:16,919 train 500 1.381187e-02 0.230840
2019-11-12 22:29:23,970 train 550 1.381777e-02 0.233375
2019-11-12 22:29:30,977 train 600 1.379031e-02 0.233942
2019-11-12 22:29:37,981 train 650 1.373191e-02 0.232216
2019-11-12 22:29:45,042 train 700 1.371799e-02 0.228754
2019-11-12 22:29:52,029 train 750 1.371480e-02 0.231471
2019-11-12 22:29:59,173 train 800 1.371594e-02 0.233320
2019-11-12 22:30:06,195 train 850 1.369359e-02 0.225717
2019-11-12 22:30:08,279 training loss; R2: 1.369584e-02 0.223955
2019-11-12 22:30:08,579 valid 000 1.389971e-02 0.299835
2019-11-12 22:30:10,304 valid 050 1.193210e-02 0.327021
2019-11-12 22:30:11,857 validation loss; R2: 1.205031e-02 0.334171
2019-11-12 22:30:11,874 epoch 7 lr 1.000000e-03
2019-11-12 22:30:12,254 train 000 1.377785e-02 0.338225
2019-11-12 22:30:19,356 train 050 1.346055e-02 0.257788
2019-11-12 22:30:26,477 train 100 1.361981e-02 0.257037
2019-11-12 22:30:33,517 train 150 1.360686e-02 0.244876
2019-11-12 22:30:40,497 train 200 1.347036e-02 0.247148
2019-11-12 22:30:47,478 train 250 1.344709e-02 0.234782
2019-11-12 22:30:54,491 train 300 1.340833e-02 0.233821
2019-11-12 22:31:01,569 train 350 1.341637e-02 0.238200
2019-11-12 22:31:08,690 train 400 1.341683e-02 0.233907
2019-11-12 22:31:15,753 train 450 1.337560e-02 0.230147
2019-11-12 22:31:22,845 train 500 1.331305e-02 0.211955
2019-11-12 22:31:29,946 train 550 1.331110e-02 0.216907
2019-11-12 22:31:37,119 train 600 1.329832e-02 0.218487
2019-11-12 22:31:44,251 train 650 1.327845e-02 0.223179
2019-11-12 22:31:51,349 train 700 1.324515e-02 0.224506
2019-11-12 22:31:58,406 train 750 1.322694e-02 0.224960
2019-11-12 22:32:05,492 train 800 1.322305e-02 0.225259
2019-11-12 22:32:12,614 train 850 1.321622e-02 -0.182547
2019-11-12 22:32:14,704 training loss; R2: 1.322006e-02 -0.174612
2019-11-12 22:32:15,007 valid 000 1.250771e-02 0.325909
2019-11-12 22:32:16,723 valid 050 1.128812e-02 0.295080
2019-11-12 22:32:18,272 validation loss; R2: 1.135747e-02 0.302796
2019-11-12 22:32:18,294 epoch 8 lr 1.000000e-03
2019-11-12 22:32:18,719 train 000 1.099441e-02 0.359431
2019-11-12 22:32:25,912 train 050 1.293621e-02 0.268786
2019-11-12 22:32:33,092 train 100 1.296511e-02 0.269561
2019-11-12 22:32:40,174 train 150 1.293301e-02 0.263302
2019-11-12 22:32:47,413 train 200 1.285411e-02 0.262249
2019-11-12 22:32:54,630 train 250 1.283884e-02 0.249668
2019-11-12 22:33:01,718 train 300 1.284022e-02 0.246319
2019-11-12 22:33:08,815 train 350 1.290535e-02 0.244248
2019-11-12 22:33:15,906 train 400 1.287447e-02 0.242906
2019-11-12 22:33:23,036 train 450 1.286474e-02 0.245853
2019-11-12 22:33:30,206 train 500 1.285138e-02 0.250021
2019-11-12 22:33:37,305 train 550 1.286197e-02 0.250263
2019-11-12 22:33:44,441 train 600 1.284487e-02 0.250301
2019-11-12 22:33:51,556 train 650 1.283664e-02 0.253638
2019-11-12 22:33:58,687 train 700 1.282649e-02 0.255800
2019-11-12 22:34:05,769 train 750 1.282613e-02 0.253352
2019-11-12 22:34:12,845 train 800 1.281863e-02 0.255542
2019-11-12 22:34:19,921 train 850 1.280051e-02 0.256657
2019-11-12 22:34:22,013 training loss; R2: 1.278952e-02 0.256438
2019-11-12 22:34:22,308 valid 000 1.103192e-02 0.329727
2019-11-12 22:34:24,045 valid 050 1.083063e-02 0.328929
2019-11-12 22:34:25,590 validation loss; R2: 1.079475e-02 0.297691
2019-11-12 22:34:25,611 epoch 9 lr 1.000000e-03
2019-11-12 22:34:26,031 train 000 1.327465e-02 0.361991
2019-11-12 22:34:33,164 train 050 1.255957e-02 0.308313
2019-11-12 22:34:40,226 train 100 1.256354e-02 0.277143
2019-11-12 22:34:47,322 train 150 1.260793e-02 0.263891
2019-11-12 22:34:54,428 train 200 1.265954e-02 0.264569
2019-11-12 22:35:01,537 train 250 1.268396e-02 -0.019516
2019-11-12 22:35:08,642 train 300 1.262306e-02 -0.169841
2019-11-12 22:35:15,795 train 350 1.261349e-02 -0.108156
2019-11-12 22:35:23,032 train 400 1.257601e-02 -0.063427
2019-11-12 22:35:30,119 train 450 1.256491e-02 -0.038541
2019-11-12 22:35:37,264 train 500 1.257728e-02 -0.017479
2019-11-12 22:35:44,511 train 550 1.256855e-02 0.008883
2019-11-12 22:35:51,767 train 600 1.256671e-02 0.033686
2019-11-12 22:35:58,873 train 650 1.255075e-02 0.053235
2019-11-12 22:36:05,944 train 700 1.255029e-02 0.071159
2019-11-12 22:36:13,057 train 750 1.255432e-02 0.083307
2019-11-12 22:36:20,137 train 800 1.253822e-02 0.095407
2019-11-12 22:36:27,216 train 850 1.253509e-02 0.107150
2019-11-12 22:36:29,386 training loss; R2: 1.252989e-02 0.110393
2019-11-12 22:36:29,687 valid 000 2.107866e+01 -1799.881062
2019-11-12 22:36:31,366 valid 050 2.103965e+01 -2260.599164
2019-11-12 22:36:32,898 validation loss; R2: 2.104483e+01 -2297.555903
2019-11-12 22:36:32,920 epoch 10 lr 1.000000e-03
2019-11-12 22:36:33,332 train 000 1.250273e-02 0.361009
2019-11-12 22:36:40,321 train 050 1.241485e-02 0.252892
2019-11-12 22:36:47,386 train 100 1.229138e-02 0.273655
2019-11-12 22:36:54,451 train 150 1.229817e-02 0.249906
2019-11-12 22:37:01,477 train 200 1.233496e-02 0.259844
2019-11-12 22:37:08,537 train 250 1.229918e-02 0.263647
2019-11-12 22:37:15,633 train 300 1.234099e-02 0.259658
2019-11-12 22:37:22,834 train 350 1.236807e-02 0.261110
2019-11-12 22:37:30,031 train 400 1.237278e-02 0.263863
2019-11-12 22:37:37,139 train 450 1.232258e-02 0.266548
2019-11-12 22:37:44,186 train 500 1.229882e-02 0.270562
2019-11-12 22:37:51,235 train 550 1.230253e-02 0.270279
2019-11-12 22:37:58,376 train 600 1.228926e-02 0.267218
2019-11-12 22:38:05,488 train 650 1.226384e-02 0.270053
2019-11-12 22:38:12,633 train 700 1.227751e-02 0.269140
2019-11-12 22:38:19,714 train 750 1.228194e-02 0.271019
2019-11-12 22:38:26,912 train 800 1.227452e-02 -0.062890
2019-11-12 22:38:34,091 train 850 1.228693e-02 -0.042518
2019-11-12 22:38:36,219 training loss; R2: 1.227612e-02 -0.047644
2019-11-12 22:38:36,517 valid 000 3.559626e+01 -9179.618743
2019-11-12 22:38:38,242 valid 050 3.567484e+01 -8400.427039
2019-11-12 22:38:39,807 validation loss; R2: 3.567310e+01 -7930.680302
2019-11-12 22:38:39,824 epoch 11 lr 1.000000e-03
2019-11-12 22:38:40,223 train 000 1.127798e-02 0.370888
2019-11-12 22:38:47,331 train 050 1.222651e-02 0.300108
2019-11-12 22:38:54,445 train 100 1.207948e-02 0.291635
2019-11-12 22:39:01,598 train 150 1.205943e-02 0.282890
2019-11-12 22:39:08,684 train 200 1.209379e-02 0.268470
2019-11-12 22:39:15,705 train 250 1.218348e-02 0.263676
2019-11-12 22:39:22,757 train 300 1.216885e-02 0.270971
2019-11-12 22:39:29,956 train 350 1.214641e-02 0.263012
2019-11-12 22:39:37,137 train 400 1.215301e-02 0.265364
2019-11-12 22:39:44,294 train 450 1.211898e-02 0.271965
2019-11-12 22:39:51,407 train 500 1.213627e-02 0.270736
2019-11-12 22:39:58,661 train 550 1.209661e-02 0.274110
2019-11-12 22:40:05,770 train 600 1.212445e-02 0.275542
2019-11-12 22:40:12,869 train 650 1.209467e-02 0.276300
2019-11-12 22:40:20,049 train 700 1.211022e-02 0.277327
2019-11-12 22:40:27,071 train 750 1.211307e-02 0.278951
2019-11-12 22:40:34,237 train 800 1.212530e-02 0.280455
2019-11-12 22:40:41,357 train 850 1.212473e-02 0.278481
2019-11-12 22:40:43,555 training loss; R2: 1.212897e-02 0.278078
2019-11-12 22:40:43,870 valid 000 2.468717e+01 -1743.025908
2019-11-12 22:40:45,563 valid 050 2.466599e+01 -2279.511537
2019-11-12 22:40:47,127 validation loss; R2: 2.467694e+01 -2301.613515
2019-11-12 22:40:47,147 epoch 12 lr 1.000000e-03
2019-11-12 22:40:47,574 train 000 1.159272e-02 0.407149
2019-11-12 22:40:54,766 train 050 1.211508e-02 0.226316
2019-11-12 22:41:01,879 train 100 1.204023e-02 0.254573
2019-11-12 22:41:09,090 train 150 1.188276e-02 0.275741
2019-11-12 22:41:16,253 train 200 1.190687e-02 0.278557
2019-11-12 22:41:23,386 train 250 1.192037e-02 0.279191
2019-11-12 22:41:30,452 train 300 1.188027e-02 0.283437
2019-11-12 22:41:37,517 train 350 1.186847e-02 0.282420
2019-11-12 22:41:44,548 train 400 1.189995e-02 0.283990
2019-11-12 22:41:51,619 train 450 1.193169e-02 0.281047
2019-11-12 22:41:58,727 train 500 1.192262e-02 0.283830
2019-11-12 22:42:05,816 train 550 1.191349e-02 0.284324
2019-11-12 22:42:12,908 train 600 1.190327e-02 0.283660
2019-11-12 22:42:20,018 train 650 1.188705e-02 0.203513
2019-11-12 22:42:27,093 train 700 1.186426e-02 0.210700
2019-11-12 22:42:34,190 train 750 1.186345e-02 0.211687
2019-11-12 22:42:41,317 train 800 1.185841e-02 0.213083
2019-11-12 22:42:48,558 train 850 1.185503e-02 0.218580
2019-11-12 22:42:50,753 training loss; R2: 1.185480e-02 0.217140
2019-11-12 22:42:51,076 valid 000 3.418048e+00 -241.751714
2019-11-12 22:42:52,714 valid 050 3.414385e+00 -219.065327
2019-11-12 22:42:54,235 validation loss; R2: 3.409012e+00 -220.328011
2019-11-12 22:42:54,252 epoch 13 lr 1.000000e-03
2019-11-12 22:42:54,661 train 000 1.129840e-02 0.382205
2019-11-12 22:43:01,733 train 050 1.150731e-02 0.315939
2019-11-12 22:43:08,700 train 100 1.197115e-02 0.304110
2019-11-12 22:43:15,652 train 150 1.186141e-02 0.293624
2019-11-12 22:43:22,602 train 200 1.181873e-02 0.236878
2019-11-12 22:43:29,557 train 250 1.178517e-02 0.245905
2019-11-12 22:43:36,504 train 300 1.178251e-02 0.242690
2019-11-12 22:43:43,499 train 350 1.177675e-02 0.245894
2019-11-12 22:43:50,447 train 400 1.177665e-02 0.246779
2019-11-12 22:43:57,399 train 450 1.177164e-02 0.246426
2019-11-12 22:44:04,375 train 500 1.176583e-02 0.253099
2019-11-12 22:44:11,343 train 550 1.176557e-02 0.256045
2019-11-12 22:44:18,302 train 600 1.177301e-02 0.261124
2019-11-12 22:44:25,254 train 650 1.178133e-02 0.261986
2019-11-12 22:44:32,200 train 700 1.178450e-02 0.264697
2019-11-12 22:44:39,138 train 750 1.179003e-02 0.263770
2019-11-12 22:44:46,077 train 800 1.180044e-02 0.251011
2019-11-12 22:44:53,011 train 850 1.179341e-02 0.254463
2019-11-12 22:44:55,086 training loss; R2: 1.178852e-02 0.255650
2019-11-12 22:44:55,396 valid 000 7.818135e+01 -8352.066653
2019-11-12 22:44:57,080 valid 050 7.857607e+01 -10792.552659
2019-11-12 22:44:58,592 validation loss; R2: 7.856216e+01 -9560.230794
2019-11-12 22:44:58,607 epoch 14 lr 1.000000e-03
2019-11-12 22:44:59,000 train 000 1.280601e-02 0.338709
2019-11-12 22:45:06,199 train 050 1.180140e-02 0.290690
2019-11-12 22:45:13,371 train 100 1.181385e-02 0.294719
2019-11-12 22:45:20,514 train 150 1.171001e-02 0.296960
2019-11-12 22:45:27,748 train 200 1.172142e-02 0.292658
2019-11-12 22:45:34,813 train 250 1.171812e-02 0.234498
2019-11-12 22:45:41,951 train 300 1.172713e-02 0.246682
2019-11-12 22:45:49,048 train 350 1.169008e-02 0.252125
2019-11-12 22:45:56,227 train 400 1.166573e-02 0.255617
2019-11-12 22:46:03,395 train 450 1.163989e-02 0.250243
2019-11-12 22:46:10,500 train 500 1.163406e-02 0.253973
2019-11-12 22:46:17,624 train 550 1.164772e-02 0.256163
2019-11-12 22:46:24,775 train 600 1.164036e-02 0.254278
2019-11-12 22:46:31,997 train 650 1.165459e-02 0.256857
2019-11-12 22:46:39,092 train 700 1.162966e-02 0.261949
2019-11-12 22:46:46,203 train 750 1.162107e-02 0.265870
2019-11-12 22:46:53,288 train 800 1.162982e-02 0.266180
2019-11-12 22:47:00,458 train 850 1.163007e-02 0.268897
2019-11-12 22:47:02,582 training loss; R2: 1.163605e-02 0.269940
2019-11-12 22:47:02,881 valid 000 6.184915e+02 -44652.922050
2019-11-12 22:47:04,610 valid 050 6.185178e+02 -52328.288212
2019-11-12 22:47:06,171 validation loss; R2: 6.184903e+02 -50970.683004
2019-11-12 22:47:06,189 epoch 15 lr 1.000000e-03
2019-11-12 22:47:06,604 train 000 1.206698e-02 0.327328
2019-11-12 22:47:13,659 train 050 1.156784e-02 0.308840
2019-11-12 22:47:20,828 train 100 1.152146e-02 0.295878
2019-11-12 22:47:28,089 train 150 1.149825e-02 0.299291
2019-11-12 22:47:35,335 train 200 1.152011e-02 0.304472
2019-11-12 22:47:42,544 train 250 1.150599e-02 0.302468
2019-11-12 22:47:49,772 train 300 1.149650e-02 0.294032
2019-11-12 22:47:56,986 train 350 1.149223e-02 0.297632
2019-11-12 22:48:04,213 train 400 1.155169e-02 0.285694
2019-11-12 22:48:11,421 train 450 1.153600e-02 0.285298
2019-11-12 22:48:18,631 train 500 1.156117e-02 0.285998
2019-11-12 22:48:25,848 train 550 1.157172e-02 0.288984
2019-11-12 22:48:33,059 train 600 1.158227e-02 0.289619
2019-11-12 22:48:40,278 train 650 1.159438e-02 0.292115
2019-11-12 22:48:47,486 train 700 1.158508e-02 0.294013
2019-11-12 22:48:54,693 train 750 1.158738e-02 0.285252
2019-11-12 22:49:01,902 train 800 1.158373e-02 0.285921
2019-11-12 22:49:09,128 train 850 1.159400e-02 0.288984
2019-11-12 22:49:11,287 training loss; R2: 1.159330e-02 0.288827
2019-11-12 22:49:11,582 valid 000 1.627668e+01 -1034.803781
2019-11-12 22:49:13,242 valid 050 1.625423e+01 -860.956533
2019-11-12 22:49:14,773 validation loss; R2: 1.625438e+01 -868.737400
2019-11-12 22:49:14,795 epoch 16 lr 1.000000e-03
2019-11-12 22:49:15,231 train 000 1.074788e-02 0.350576
2019-11-12 22:49:22,428 train 050 1.140677e-02 0.317635
2019-11-12 22:49:29,442 train 100 1.134578e-02 0.303460
2019-11-12 22:49:36,467 train 150 1.155170e-02 0.299563
2019-11-12 22:49:43,562 train 200 1.156692e-02 0.298722
2019-11-12 22:49:50,849 train 250 1.152402e-02 0.294837
2019-11-12 22:49:58,155 train 300 1.152983e-02 0.300277
2019-11-12 22:50:05,294 train 350 1.155697e-02 0.301614
2019-11-12 22:50:12,346 train 400 1.153850e-02 0.303788
2019-11-12 22:50:19,369 train 450 1.155685e-02 0.307365
2019-11-12 22:50:26,429 train 500 1.154189e-02 0.296860
2019-11-12 22:50:33,480 train 550 1.153402e-02 0.296077
2019-11-12 22:50:40,596 train 600 1.153147e-02 0.299406
2019-11-12 22:50:47,786 train 650 1.151350e-02 0.297947
2019-11-12 22:50:55,025 train 700 1.149047e-02 0.298661
2019-11-12 22:51:02,346 train 750 1.147888e-02 0.298510
2019-11-12 22:51:09,491 train 800 1.147607e-02 0.298916
2019-11-12 22:51:16,541 train 850 1.145881e-02 0.299652
2019-11-12 22:51:18,635 training loss; R2: 1.145679e-02 0.298436
2019-11-12 22:51:18,909 valid 000 7.593296e+01 -3554.690599
2019-11-12 22:51:20,571 valid 050 7.594061e+01 -2601.601912
2019-11-12 22:51:22,095 validation loss; R2: 7.593529e+01 -2604.124958
2019-11-12 22:51:22,116 epoch 17 lr 1.000000e-03
2019-11-12 22:51:22,556 train 000 1.371127e-02 0.343356
2019-11-12 22:51:29,828 train 050 1.153864e-02 0.335650
2019-11-12 22:51:36,913 train 100 1.146223e-02 0.322804
2019-11-12 22:51:43,978 train 150 1.144181e-02 0.317533
2019-11-12 22:51:51,024 train 200 1.143510e-02 0.309146
2019-11-12 22:51:58,109 train 250 1.146552e-02 0.303861
2019-11-12 22:52:05,180 train 300 1.143009e-02 0.307655
2019-11-12 22:52:12,426 train 350 1.141719e-02 0.298055
2019-11-12 22:52:19,665 train 400 1.138579e-02 0.303652
2019-11-12 22:52:26,900 train 450 1.136932e-02 0.301822
2019-11-12 22:52:33,972 train 500 1.139147e-02 0.303016
2019-11-12 22:52:41,174 train 550 1.137933e-02 0.304565
2019-11-12 22:52:48,202 train 600 1.138791e-02 0.304353
2019-11-12 22:52:55,232 train 650 1.139913e-02 0.302326
2019-11-12 22:53:02,364 train 700 1.141960e-02 0.300959
2019-11-12 22:53:09,412 train 750 1.141726e-02 0.296791
2019-11-12 22:53:16,677 train 800 1.141283e-02 0.296602
2019-11-12 22:53:23,792 train 850 1.140018e-02 0.297674
2019-11-12 22:53:25,911 training loss; R2: 1.139609e-02 0.297980
2019-11-12 22:53:26,194 valid 000 1.631537e+02 -10113.326359
2019-11-12 22:53:27,861 valid 050 1.630552e+02 -13531.239286
2019-11-12 22:53:29,398 validation loss; R2: 1.630345e+02 -13155.947403
2019-11-12 22:53:29,419 epoch 18 lr 1.000000e-03
2019-11-12 22:53:29,846 train 000 1.215610e-02 0.346215
2019-11-12 22:53:37,170 train 050 1.147146e-02 0.331244
2019-11-12 22:53:44,367 train 100 1.144006e-02 0.237800
2019-11-12 22:53:51,416 train 150 1.133456e-02 0.264518
2019-11-12 22:53:58,509 train 200 1.128399e-02 0.264642
2019-11-12 22:54:05,566 train 250 1.127539e-02 0.278527
2019-11-12 22:54:12,641 train 300 1.129611e-02 0.274484
2019-11-12 22:54:19,702 train 350 1.130247e-02 0.281555
2019-11-12 22:54:26,805 train 400 1.128426e-02 0.284944
2019-11-12 22:54:33,949 train 450 1.124548e-02 0.287607
2019-11-12 22:54:40,997 train 500 1.125640e-02 0.290951
2019-11-12 22:54:48,036 train 550 1.126957e-02 0.294438
2019-11-12 22:54:55,223 train 600 1.127401e-02 0.294103
2019-11-12 22:55:02,288 train 650 1.127802e-02 0.296733
2019-11-12 22:55:09,541 train 700 1.127206e-02 0.297769
2019-11-12 22:55:16,669 train 750 1.126078e-02 0.299978
2019-11-12 22:55:23,807 train 800 1.127473e-02 0.292531
2019-11-12 22:55:31,032 train 850 1.128721e-02 0.293895
2019-11-12 22:55:33,140 training loss; R2: 1.128976e-02 0.294542
2019-11-12 22:55:33,445 valid 000 2.193935e+01 -1241.056863
2019-11-12 22:55:35,176 valid 050 2.188693e+01 -1612.359939
2019-11-12 22:55:36,722 validation loss; R2: 2.189317e+01 -1512.715353
2019-11-12 22:55:36,745 epoch 19 lr 1.000000e-03
2019-11-12 22:55:37,136 train 000 1.146366e-02 -0.554338
2019-11-12 22:55:44,160 train 050 1.156378e-02 0.305364
2019-11-12 22:55:51,151 train 100 1.153946e-02 0.315036
2019-11-12 22:55:58,127 train 150 1.147366e-02 0.325768
2019-11-12 22:56:05,374 train 200 1.141063e-02 0.326120
2019-11-12 22:56:12,617 train 250 1.140850e-02 0.319747
2019-11-12 22:56:19,845 train 300 1.141241e-02 0.307568
2019-11-12 22:56:27,098 train 350 1.134897e-02 0.303899
2019-11-12 22:56:34,347 train 400 1.133714e-02 0.303181
2019-11-12 22:56:41,588 train 450 1.132868e-02 0.307141
2019-11-12 22:56:48,822 train 500 1.130321e-02 0.308903
2019-11-12 22:56:56,088 train 550 1.128876e-02 0.311589
2019-11-12 22:57:03,361 train 600 1.127098e-02 0.309904
2019-11-12 22:57:10,630 train 650 1.127304e-02 0.311970
2019-11-12 22:57:17,875 train 700 1.128187e-02 0.312464
2019-11-12 22:57:25,103 train 750 1.126580e-02 0.308069
2019-11-12 22:57:32,343 train 800 1.125946e-02 0.308431
2019-11-12 22:57:39,585 train 850 1.123765e-02 0.104565
2019-11-12 22:57:41,742 training loss; R2: 1.123600e-02 0.108751
2019-11-12 22:57:42,036 valid 000 9.669871e+01 -9721.109459
2019-11-12 22:57:43,696 valid 050 9.674701e+01 -11586.917483
2019-11-12 22:57:45,208 validation loss; R2: 9.674483e+01 -11387.843033
