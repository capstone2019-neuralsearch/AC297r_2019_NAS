2019-11-12 22:57:47,654 gpu device = 1
2019-11-12 22:57:47,655 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name='galaxy-zoo-cleaned', gpu=1, grad_clip=5, gz_dtree=False, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-galaxy-zoo-EXP-20191112-225747', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-11-12 22:57:59,208 param size = 0.257781MB
2019-11-12 22:57:59,212 epoch 0 lr 1.000000e-03
2019-11-12 22:58:01,377 train 000 2.759095e-01 -263.806330
2019-11-12 22:58:08,129 train 050 4.473347e-02 -7.595390
2019-11-12 22:58:14,871 train 100 3.535334e-02 -4.057264
2019-11-12 22:58:21,524 train 150 3.181071e-02 -2.796834
2019-11-12 22:58:28,192 train 200 2.971310e-02 -2.165283
2019-11-12 22:58:34,804 train 250 2.832649e-02 -1.766870
2019-11-12 22:58:41,307 train 300 2.726352e-02 -1.494983
2019-11-12 22:58:47,990 train 350 2.641432e-02 -1.282749
2019-11-12 22:58:54,492 train 400 2.568936e-02 -1.126304
2019-11-12 22:59:01,110 train 450 2.514011e-02 -1.001042
2019-11-12 22:59:07,735 train 500 2.467904e-02 -0.899311
2019-11-12 22:59:14,121 train 550 2.427401e-02 -0.813633
2019-11-12 22:59:20,501 train 600 2.385501e-02 -0.741814
2019-11-12 22:59:26,867 train 650 2.350273e-02 -0.678043
2019-11-12 22:59:33,250 train 700 2.322282e-02 -0.628397
2019-11-12 22:59:39,634 train 750 2.292908e-02 -0.580347
2019-11-12 22:59:46,027 train 800 2.267049e-02 -0.539487
2019-11-12 22:59:52,419 train 850 2.244587e-02 -0.501624
2019-11-12 22:59:55,015 training loss; R2: 2.237298e-02 -0.491282
2019-11-12 22:59:55,317 valid 000 1.752486e-02 -0.041033
2019-11-12 22:59:57,031 valid 050 1.794273e-02 0.160536
2019-11-12 22:59:58,660 validation loss; R2: 1.814747e-02 0.145505
2019-11-12 22:59:58,677 epoch 1 lr 1.000000e-03
2019-11-12 22:59:59,185 train 000 1.826305e-02 0.057256
2019-11-12 23:00:05,564 train 050 1.860628e-02 0.110294
2019-11-12 23:00:11,945 train 100 1.836612e-02 0.114272
2019-11-12 23:00:18,325 train 150 1.816925e-02 0.108818
2019-11-12 23:00:24,704 train 200 1.794522e-02 0.101257
2019-11-12 23:00:31,268 train 250 1.783047e-02 0.106057
2019-11-12 23:00:37,655 train 300 1.772799e-02 0.106484
2019-11-12 23:00:44,035 train 350 1.766410e-02 0.110176
2019-11-12 23:00:50,419 train 400 1.755002e-02 0.113099
2019-11-12 23:00:56,799 train 450 1.748516e-02 0.118450
2019-11-12 23:01:03,182 train 500 1.738518e-02 0.116960
2019-11-12 23:01:09,570 train 550 1.729047e-02 0.119898
2019-11-12 23:01:15,954 train 600 1.718382e-02 0.125460
2019-11-12 23:01:22,335 train 650 1.711488e-02 0.127954
2019-11-12 23:01:28,716 train 700 1.702448e-02 0.131522
2019-11-12 23:01:35,091 train 750 1.694159e-02 0.118901
2019-11-12 23:01:41,469 train 800 1.691220e-02 0.120787
2019-11-12 23:01:47,847 train 850 1.684990e-02 0.123625
2019-11-12 23:01:49,751 training loss; R2: 1.682431e-02 0.124765
2019-11-12 23:01:50,046 valid 000 1.563278e-02 0.201321
2019-11-12 23:01:51,772 valid 050 1.637901e-02 0.161697
2019-11-12 23:01:53,331 validation loss; R2: 1.644986e-02 0.161030
2019-11-12 23:01:53,346 epoch 2 lr 1.000000e-03
2019-11-12 23:01:53,732 train 000 1.411929e-02 0.249460
2019-11-12 23:02:00,384 train 050 1.574411e-02 0.062155
2019-11-12 23:02:06,982 train 100 1.544968e-02 0.119647
2019-11-12 23:02:13,425 train 150 1.547725e-02 0.138599
2019-11-12 23:02:19,819 train 200 1.533596e-02 0.153045
2019-11-12 23:02:26,219 train 250 1.532767e-02 0.161167
2019-11-12 23:02:32,610 train 300 1.522199e-02 0.163933
2019-11-12 23:02:38,991 train 350 1.518264e-02 0.170769
2019-11-12 23:02:45,374 train 400 1.515145e-02 0.172666
2019-11-12 23:02:51,765 train 450 1.510323e-02 0.175865
2019-11-12 23:02:58,154 train 500 1.508361e-02 0.171533
2019-11-12 23:03:04,539 train 550 1.507119e-02 0.176332
2019-11-12 23:03:10,928 train 600 1.505760e-02 0.173796
2019-11-12 23:03:17,311 train 650 1.503407e-02 0.173551
2019-11-12 23:03:23,695 train 700 1.503269e-02 0.177477
2019-11-12 23:03:30,089 train 750 1.498853e-02 0.166519
2019-11-12 23:03:36,470 train 800 1.495490e-02 0.167652
2019-11-12 23:03:42,852 train 850 1.493489e-02 0.170295
2019-11-12 23:03:44,761 training loss; R2: 1.491633e-02 0.171106
2019-11-12 23:03:45,064 valid 000 1.227832e-02 0.302817
2019-11-12 23:03:46,797 valid 050 1.286204e-02 0.126604
2019-11-12 23:03:48,359 validation loss; R2: 1.286002e-02 0.169022
2019-11-12 23:03:48,380 epoch 3 lr 1.000000e-03
2019-11-12 23:03:48,784 train 000 1.640779e-02 0.169184
2019-11-12 23:03:55,444 train 050 1.460305e-02 0.218216
2019-11-12 23:04:01,845 train 100 1.432099e-02 0.220600
2019-11-12 23:04:08,276 train 150 1.412652e-02 0.218134
2019-11-12 23:04:14,657 train 200 1.414290e-02 0.218228
2019-11-12 23:04:21,036 train 250 1.410994e-02 0.216186
2019-11-12 23:04:27,457 train 300 1.408670e-02 0.217427
2019-11-12 23:04:33,845 train 350 1.405753e-02 0.219816
2019-11-12 23:04:40,227 train 400 1.406513e-02 0.223411
2019-11-12 23:04:46,615 train 450 1.402508e-02 0.224635
2019-11-12 23:04:52,997 train 500 1.398365e-02 0.219214
2019-11-12 23:04:59,387 train 550 1.398050e-02 0.222089
2019-11-12 23:05:05,850 train 600 1.396145e-02 0.223620
2019-11-12 23:05:12,231 train 650 1.393459e-02 0.218972
2019-11-12 23:05:18,608 train 700 1.390141e-02 0.220875
2019-11-12 23:05:24,986 train 750 1.387609e-02 0.221407
2019-11-12 23:05:31,363 train 800 1.384412e-02 0.223042
2019-11-12 23:05:37,738 train 850 1.383242e-02 0.223956
2019-11-12 23:05:39,647 training loss; R2: 1.382358e-02 0.223220
2019-11-12 23:05:39,942 valid 000 2.615276e-02 -0.024007
2019-11-12 23:05:41,670 valid 050 2.558032e-02 -0.216081
2019-11-12 23:05:43,218 validation loss; R2: 2.558489e-02 -0.186461
2019-11-12 23:05:43,233 epoch 4 lr 1.000000e-03
2019-11-12 23:05:43,606 train 000 1.397729e-02 0.246969
2019-11-12 23:05:50,027 train 050 1.342354e-02 0.247733
2019-11-12 23:05:56,438 train 100 1.349314e-02 0.252504
2019-11-12 23:06:02,870 train 150 1.340530e-02 0.251271
2019-11-12 23:06:09,358 train 200 1.339916e-02 0.256152
2019-11-12 23:06:15,933 train 250 1.334662e-02 0.248295
2019-11-12 23:06:22,350 train 300 1.335936e-02 0.251020
2019-11-12 23:06:28,769 train 350 1.329157e-02 0.252289
2019-11-12 23:06:35,181 train 400 1.330551e-02 0.251939
2019-11-12 23:06:41,597 train 450 1.328223e-02 0.254337
2019-11-12 23:06:48,006 train 500 1.323148e-02 0.231010
2019-11-12 23:06:54,416 train 550 1.318908e-02 0.233312
2019-11-12 23:07:00,980 train 600 1.317259e-02 0.233764
2019-11-12 23:07:07,470 train 650 1.315397e-02 0.237141
2019-11-12 23:07:13,902 train 700 1.314703e-02 0.238553
2019-11-12 23:07:20,334 train 750 1.313648e-02 0.240926
2019-11-12 23:07:26,753 train 800 1.312134e-02 0.240919
2019-11-12 23:07:33,197 train 850 1.309976e-02 0.242269
2019-11-12 23:07:35,117 training loss; R2: 1.308625e-02 0.236055
2019-11-12 23:07:35,423 valid 000 7.056930e-02 -1.732904
2019-11-12 23:07:37,156 valid 050 6.982136e-02 -1.794424
2019-11-12 23:07:38,736 validation loss; R2: 6.991641e-02 -1.815033
2019-11-12 23:07:38,756 epoch 5 lr 1.000000e-03
2019-11-12 23:07:39,153 train 000 1.134893e-02 0.307728
2019-11-12 23:07:45,597 train 050 1.252069e-02 0.277715
2019-11-12 23:07:52,088 train 100 1.267578e-02 0.262496
2019-11-12 23:07:58,604 train 150 1.263556e-02 0.263372
2019-11-12 23:08:05,097 train 200 1.267935e-02 0.263109
2019-11-12 23:08:11,632 train 250 1.266038e-02 0.264972
2019-11-12 23:08:18,114 train 300 1.263951e-02 0.267877
2019-11-12 23:08:24,605 train 350 1.266494e-02 0.263880
2019-11-12 23:08:31,228 train 400 1.268250e-02 0.264890
2019-11-12 23:08:37,795 train 450 1.270170e-02 0.266216
2019-11-12 23:08:44,367 train 500 1.267262e-02 0.253737
2019-11-12 23:08:51,116 train 550 1.264124e-02 0.253786
2019-11-12 23:08:57,704 train 600 1.261779e-02 0.256559
2019-11-12 23:09:04,207 train 650 1.259433e-02 0.258033
2019-11-12 23:09:10,789 train 700 1.259537e-02 0.252598
2019-11-12 23:09:17,388 train 750 1.258580e-02 0.254449
2019-11-12 23:09:23,951 train 800 1.256973e-02 0.255735
2019-11-12 23:09:30,409 train 850 1.254115e-02 0.251060
2019-11-12 23:09:32,348 training loss; R2: 1.253010e-02 0.251300
2019-11-12 23:09:32,676 valid 000 4.535875e-02 -0.775170
2019-11-12 23:09:34,334 valid 050 4.908714e-02 -1.119970
2019-11-12 23:09:35,893 validation loss; R2: 4.932999e-02 -1.113773
2019-11-12 23:09:35,913 epoch 6 lr 1.000000e-03
2019-11-12 23:09:36,359 train 000 1.439701e-02 0.319633
2019-11-12 23:09:42,839 train 050 1.226012e-02 0.280924
2019-11-12 23:09:49,276 train 100 1.233785e-02 0.280617
2019-11-12 23:09:55,703 train 150 1.222788e-02 0.284307
2019-11-12 23:10:02,126 train 200 1.214943e-02 0.290250
2019-11-12 23:10:08,540 train 250 1.216770e-02 0.290568
2019-11-12 23:10:14,958 train 300 1.217604e-02 0.265843
2019-11-12 23:10:21,482 train 350 1.215582e-02 0.269349
2019-11-12 23:10:27,901 train 400 1.217295e-02 0.269834
2019-11-12 23:10:34,325 train 450 1.218999e-02 0.271562
2019-11-12 23:10:40,747 train 500 1.216577e-02 0.273097
2019-11-12 23:10:47,173 train 550 1.218564e-02 0.272624
2019-11-12 23:10:53,593 train 600 1.216511e-02 0.272635
2019-11-12 23:11:00,009 train 650 1.214829e-02 0.274233
2019-11-12 23:11:06,429 train 700 1.214631e-02 0.267062
2019-11-12 23:11:12,846 train 750 1.215202e-02 0.268319
2019-11-12 23:11:19,281 train 800 1.215710e-02 0.269493
2019-11-12 23:11:25,882 train 850 1.212849e-02 0.271214
2019-11-12 23:11:27,876 training loss; R2: 1.213023e-02 0.270433
2019-11-12 23:11:28,179 valid 000 2.468809e-02 0.064747
2019-11-12 23:11:29,846 valid 050 2.096000e-02 -0.045154
2019-11-12 23:11:31,386 validation loss; R2: 2.093023e-02 -0.043838
2019-11-12 23:11:31,405 epoch 7 lr 1.000000e-03
2019-11-12 23:11:31,843 train 000 1.339116e-02 0.369792
2019-11-12 23:11:38,336 train 050 1.207133e-02 0.266673
2019-11-12 23:11:44,922 train 100 1.202496e-02 0.265609
2019-11-12 23:11:51,492 train 150 1.189201e-02 0.277245
2019-11-12 23:11:57,974 train 200 1.187607e-02 0.282521
2019-11-12 23:12:04,499 train 250 1.187122e-02 0.277980
2019-11-12 23:12:11,209 train 300 1.187125e-02 0.281510
2019-11-12 23:12:18,016 train 350 1.187173e-02 0.272056
2019-11-12 23:12:24,507 train 400 1.186328e-02 0.271943
2019-11-12 23:12:31,114 train 450 1.187326e-02 0.273078
2019-11-12 23:12:37,682 train 500 1.190439e-02 -1.270841
2019-11-12 23:12:44,348 train 550 1.189136e-02 -1.137815
2019-11-12 23:12:51,167 train 600 1.186198e-02 -1.022980
2019-11-12 23:12:57,769 train 650 1.186554e-02 -0.921937
2019-11-12 23:13:04,371 train 700 1.186871e-02 -0.842104
2019-11-12 23:13:11,161 train 750 1.186687e-02 -0.767327
2019-11-12 23:13:17,786 train 800 1.184870e-02 -0.699803
2019-11-12 23:13:24,346 train 850 1.184341e-02 -0.640933
2019-11-12 23:13:26,312 training loss; R2: 1.184740e-02 -0.623984
2019-11-12 23:13:26,623 valid 000 2.871945e-01 -18.290994
2019-11-12 23:13:28,347 valid 050 2.905116e-01 -21.311655
2019-11-12 23:13:29,921 validation loss; R2: 2.918938e-01 -20.626413
2019-11-12 23:13:29,945 epoch 8 lr 1.000000e-03
2019-11-12 23:13:30,374 train 000 1.081159e-02 0.438858
2019-11-12 23:13:36,923 train 050 1.182084e-02 0.271861
2019-11-12 23:13:43,510 train 100 1.173390e-02 0.277708
2019-11-12 23:13:50,177 train 150 1.166106e-02 0.290694
2019-11-12 23:13:56,783 train 200 1.169260e-02 0.297597
2019-11-12 23:14:03,458 train 250 1.170312e-02 0.297891
2019-11-12 23:14:10,059 train 300 1.165032e-02 0.303998
2019-11-12 23:14:16,823 train 350 1.163621e-02 0.301468
2019-11-12 23:14:23,544 train 400 1.163174e-02 0.297054
2019-11-12 23:14:30,184 train 450 1.163401e-02 0.297435
2019-11-12 23:14:36,835 train 500 1.164376e-02 0.297669
2019-11-12 23:14:43,487 train 550 1.161111e-02 0.242674
2019-11-12 23:14:50,123 train 600 1.159208e-02 0.249367
2019-11-12 23:14:56,759 train 650 1.159544e-02 0.251640
2019-11-12 23:15:03,414 train 700 1.157042e-02 0.255974
2019-11-12 23:15:10,046 train 750 1.156122e-02 0.258029
2019-11-12 23:15:16,678 train 800 1.157296e-02 0.260683
2019-11-12 23:15:23,317 train 850 1.157031e-02 0.262250
2019-11-12 23:15:25,299 training loss; R2: 1.156206e-02 0.261615
2019-11-12 23:15:25,608 valid 000 3.315429e+00 -123.434225
2019-11-12 23:15:27,277 valid 050 3.338544e+00 -140.764924
2019-11-12 23:15:28,806 validation loss; R2: 3.340607e+00 -140.980506
2019-11-12 23:15:28,824 epoch 9 lr 1.000000e-03
2019-11-12 23:15:29,252 train 000 1.258625e-02 0.296096
2019-11-12 23:15:35,866 train 050 1.163820e-02 0.324216
2019-11-12 23:15:42,678 train 100 1.144450e-02 0.312721
2019-11-12 23:15:49,278 train 150 1.144409e-02 0.299516
2019-11-12 23:15:55,858 train 200 1.141768e-02 0.301831
2019-11-12 23:16:02,513 train 250 1.141198e-02 0.305488
2019-11-12 23:16:09,060 train 300 1.137252e-02 0.307621
2019-11-12 23:16:15,568 train 350 1.137190e-02 0.309836
2019-11-12 23:16:22,269 train 400 1.139138e-02 0.304077
2019-11-12 23:16:28,969 train 450 1.135055e-02 0.306502
2019-11-12 23:16:35,443 train 500 1.131388e-02 0.305055
2019-11-12 23:16:42,022 train 550 1.131646e-02 0.305407
2019-11-12 23:16:48,709 train 600 1.131700e-02 0.306996
2019-11-12 23:16:55,304 train 650 1.132857e-02 0.303510
2019-11-12 23:17:01,875 train 700 1.134821e-02 0.302680
2019-11-12 23:17:08,361 train 750 1.135615e-02 0.295717
2019-11-12 23:17:14,914 train 800 1.135111e-02 0.297417
2019-11-12 23:17:21,432 train 850 1.135388e-02 0.297104
2019-11-12 23:17:23,424 training loss; R2: 1.135073e-02 0.295921
2019-11-12 23:17:23,740 valid 000 1.684947e-01 -3.118670
2019-11-12 23:17:25,396 valid 050 1.583727e-01 -3.505208
2019-11-12 23:17:26,941 validation loss; R2: 1.582331e-01 -3.375340
2019-11-12 23:17:26,956 epoch 10 lr 1.000000e-03
2019-11-12 23:17:27,355 train 000 9.234235e-03 0.187465
2019-11-12 23:17:33,892 train 050 1.117210e-02 0.265990
2019-11-12 23:17:40,431 train 100 1.130523e-02 0.278557
2019-11-12 23:17:46,828 train 150 1.134567e-02 0.288618
2019-11-12 23:17:53,210 train 200 1.129587e-02 0.297110
2019-11-12 23:17:59,592 train 250 1.123442e-02 0.296679
2019-11-12 23:18:05,968 train 300 1.120428e-02 0.302144
2019-11-12 23:18:12,338 train 350 1.116296e-02 0.301231
2019-11-12 23:18:18,727 train 400 1.112923e-02 0.299570
2019-11-12 23:18:25,102 train 450 1.116155e-02 0.150222
2019-11-12 23:18:31,474 train 500 1.115834e-02 0.161922
2019-11-12 23:18:37,848 train 550 1.113504e-02 0.175484
2019-11-12 23:18:44,220 train 600 1.115372e-02 0.183934
2019-11-12 23:18:50,614 train 650 1.116475e-02 0.193088
2019-11-12 23:18:56,991 train 700 1.115284e-02 0.201904
2019-11-12 23:19:03,370 train 750 1.115278e-02 0.208857
2019-11-12 23:19:09,748 train 800 1.117986e-02 0.214756
2019-11-12 23:19:16,127 train 850 1.118837e-02 0.219813
2019-11-12 23:19:18,036 training loss; R2: 1.118058e-02 0.222123
2019-11-12 23:19:18,347 valid 000 1.433235e-02 0.193332
2019-11-12 23:19:20,023 valid 050 1.444544e-02 0.251050
2019-11-12 23:19:21,549 validation loss; R2: 1.443000e-02 0.256425
2019-11-12 23:19:21,569 epoch 11 lr 1.000000e-03
2019-11-12 23:19:21,944 train 000 1.094654e-02 0.396109
2019-11-12 23:19:28,408 train 050 1.116461e-02 0.331984
2019-11-12 23:19:35,033 train 100 1.102326e-02 0.255513
2019-11-12 23:19:41,720 train 150 1.100226e-02 0.251703
2019-11-12 23:19:48,430 train 200 1.097727e-02 0.270963
2019-11-12 23:19:55,210 train 250 1.103116e-02 0.271369
2019-11-12 23:20:01,911 train 300 1.103962e-02 0.278645
2019-11-12 23:20:08,732 train 350 1.101989e-02 0.282191
2019-11-12 23:20:15,474 train 400 1.100108e-02 0.286677
2019-11-12 23:20:22,126 train 450 1.104412e-02 0.290892
2019-11-12 23:20:28,918 train 500 1.104671e-02 0.291869
2019-11-12 23:20:35,591 train 550 1.106086e-02 0.296610
2019-11-12 23:20:42,279 train 600 1.105153e-02 0.299653
2019-11-12 23:20:49,067 train 650 1.105196e-02 0.301032
2019-11-12 23:20:55,831 train 700 1.102475e-02 0.302441
2019-11-12 23:21:02,427 train 750 1.103373e-02 0.299374
2019-11-12 23:21:09,089 train 800 1.102279e-02 0.299204
2019-11-12 23:21:15,825 train 850 1.100960e-02 0.299773
2019-11-12 23:21:17,760 training loss; R2: 1.100877e-02 0.298786
2019-11-12 23:21:18,051 valid 000 2.693734e-01 -110.179151
2019-11-12 23:21:19,724 valid 050 2.717126e-01 -316.058869
2019-11-12 23:21:21,269 validation loss; R2: 2.716162e-01 -354.264754
2019-11-12 23:21:21,288 epoch 12 lr 1.000000e-03
2019-11-12 23:21:21,686 train 000 1.047477e-02 0.383239
2019-11-12 23:21:28,194 train 050 1.069311e-02 0.322480
2019-11-12 23:21:34,733 train 100 1.090775e-02 0.319457
2019-11-12 23:21:41,260 train 150 1.093839e-02 0.281823
2019-11-12 23:21:47,888 train 200 1.092686e-02 0.278507
2019-11-12 23:21:54,472 train 250 1.095432e-02 0.285150
2019-11-12 23:22:01,217 train 300 1.095157e-02 0.288762
2019-11-12 23:22:07,974 train 350 1.089267e-02 0.291916
2019-11-12 23:22:14,512 train 400 1.090133e-02 0.296896
2019-11-12 23:22:21,036 train 450 1.089396e-02 0.300469
2019-11-12 23:22:27,448 train 500 1.091132e-02 0.298531
2019-11-12 23:22:33,861 train 550 1.091259e-02 0.299180
2019-11-12 23:22:40,426 train 600 1.092905e-02 0.301503
2019-11-12 23:22:46,903 train 650 1.094613e-02 0.303052
2019-11-12 23:22:53,388 train 700 1.094088e-02 0.303262
2019-11-12 23:22:59,793 train 750 1.094149e-02 0.302510
2019-11-12 23:23:06,195 train 800 1.093418e-02 0.300331
2019-11-12 23:23:12,595 train 850 1.091936e-02 0.294659
2019-11-12 23:23:14,508 training loss; R2: 1.092289e-02 0.294613
2019-11-12 23:23:14,835 valid 000 1.217864e+02 -23628.180721
2019-11-12 23:23:16,514 valid 050 1.218711e+02 -23751.783240
2019-11-12 23:23:18,028 validation loss; R2: 1.218863e+02 -22890.923986
2019-11-12 23:23:18,042 epoch 13 lr 1.000000e-03
2019-11-12 23:23:18,431 train 000 1.237967e-02 0.325955
2019-11-12 23:23:25,129 train 050 1.086407e-02 0.301300
2019-11-12 23:23:31,952 train 100 1.069677e-02 0.281550
2019-11-12 23:23:38,832 train 150 1.069650e-02 0.263558
2019-11-12 23:23:45,660 train 200 1.075498e-02 0.234417
2019-11-12 23:23:52,246 train 250 1.083790e-02 0.253745
2019-11-12 23:23:58,989 train 300 1.080767e-02 0.263407
2019-11-12 23:24:05,537 train 350 1.076744e-02 0.272057
2019-11-12 23:24:12,340 train 400 1.075927e-02 0.277950
2019-11-12 23:24:19,018 train 450 1.078432e-02 0.283470
2019-11-12 23:24:25,634 train 500 1.080020e-02 0.284331
2019-11-12 23:24:32,198 train 550 1.081846e-02 0.193910
2019-11-12 23:24:38,933 train 600 1.082223e-02 0.203589
2019-11-12 23:24:45,617 train 650 1.080988e-02 0.213131
2019-11-12 23:24:52,190 train 700 1.080108e-02 0.220015
2019-11-12 23:24:58,761 train 750 1.080597e-02 0.225628
2019-11-12 23:25:05,430 train 800 1.080709e-02 0.231832
2019-11-12 23:25:11,943 train 850 1.079468e-02 0.234626
2019-11-12 23:25:13,882 training loss; R2: 1.079942e-02 0.236685
2019-11-12 23:25:14,183 valid 000 1.818019e-01 -25.553077
2019-11-12 23:25:15,921 valid 050 1.824462e-01 -56.670961
2019-11-12 23:25:17,467 validation loss; R2: 1.823786e-01 -56.724036
2019-11-12 23:25:17,492 epoch 14 lr 1.000000e-03
2019-11-12 23:25:17,925 train 000 1.291362e-02 0.148738
2019-11-12 23:25:24,651 train 050 1.084354e-02 0.158308
2019-11-12 23:25:31,361 train 100 1.072397e-02 0.249574
2019-11-12 23:25:38,124 train 150 1.078380e-02 0.273553
2019-11-12 23:25:44,610 train 200 1.074733e-02 0.177487
2019-11-12 23:25:51,218 train 250 1.077186e-02 0.196029
2019-11-12 23:25:57,710 train 300 1.074676e-02 0.217218
2019-11-12 23:26:04,224 train 350 1.077061e-02 0.226514
2019-11-12 23:26:10,752 train 400 1.077066e-02 0.237332
2019-11-12 23:26:17,308 train 450 1.078796e-02 0.244914
2019-11-12 23:26:23,784 train 500 1.077169e-02 0.248375
2019-11-12 23:26:30,365 train 550 1.078947e-02 0.255926
2019-11-12 23:26:37,158 train 600 1.076313e-02 0.255958
2019-11-12 23:26:43,708 train 650 1.077564e-02 0.258750
2019-11-12 23:26:50,305 train 700 1.074863e-02 0.260768
2019-11-12 23:26:56,848 train 750 1.075356e-02 0.260458
2019-11-12 23:27:03,355 train 800 1.074674e-02 0.262131
2019-11-12 23:27:09,857 train 850 1.073446e-02 0.264969
2019-11-12 23:27:11,814 training loss; R2: 1.073823e-02 0.265722
2019-11-12 23:27:12,115 valid 000 7.468045e-02 -18.254337
2019-11-12 23:27:13,830 valid 050 7.584322e-02 -28.890276
2019-11-12 23:27:15,391 validation loss; R2: 7.586080e-02 -30.634798
2019-11-12 23:27:15,409 epoch 15 lr 1.000000e-03
2019-11-12 23:27:15,865 train 000 1.094809e-02 0.326094
2019-11-12 23:27:22,577 train 050 1.092182e-02 0.286943
2019-11-12 23:27:28,982 train 100 1.067441e-02 0.322843
2019-11-12 23:27:35,389 train 150 1.057836e-02 0.313814
2019-11-12 23:27:41,796 train 200 1.052476e-02 0.295289
2019-11-12 23:27:48,210 train 250 1.051205e-02 0.295276
2019-11-12 23:27:54,617 train 300 1.051279e-02 0.284575
2019-11-12 23:28:01,012 train 350 1.050725e-02 0.293535
2019-11-12 23:28:07,673 train 400 1.052639e-02 0.295836
2019-11-12 23:28:14,412 train 450 1.054384e-02 0.300702
2019-11-12 23:28:20,867 train 500 1.058459e-02 0.304823
2019-11-12 23:28:27,249 train 550 1.056476e-02 0.305819
2019-11-12 23:28:33,644 train 600 1.056726e-02 0.306769
2019-11-12 23:28:40,023 train 650 1.058107e-02 0.309765
2019-11-12 23:28:46,408 train 700 1.057325e-02 0.311039
2019-11-12 23:28:52,791 train 750 1.057221e-02 0.313602
2019-11-12 23:28:59,174 train 800 1.059137e-02 0.312367
2019-11-12 23:29:05,562 train 850 1.059148e-02 0.312545
2019-11-12 23:29:07,473 training loss; R2: 1.058707e-02 0.312755
2019-11-12 23:29:07,779 valid 000 1.542011e-02 -0.631295
2019-11-12 23:29:09,457 valid 050 1.729839e-02 -0.773068
2019-11-12 23:29:10,971 validation loss; R2: 1.736865e-02 -0.780405
2019-11-12 23:29:10,991 epoch 16 lr 1.000000e-03
2019-11-12 23:29:11,390 train 000 1.278710e-02 0.329552
2019-11-12 23:29:17,894 train 050 1.045761e-02 0.315117
2019-11-12 23:29:24,511 train 100 1.053915e-02 0.320690
2019-11-12 23:29:31,055 train 150 1.056588e-02 0.317569
2019-11-12 23:29:37,601 train 200 1.061373e-02 0.325756
2019-11-12 23:29:44,396 train 250 1.058590e-02 0.332326
2019-11-12 23:29:50,894 train 300 1.057982e-02 0.336352
2019-11-12 23:29:57,593 train 350 1.056809e-02 0.333186
2019-11-12 23:30:04,289 train 400 1.055215e-02 0.333628
2019-11-12 23:30:11,042 train 450 1.058030e-02 0.334510
2019-11-12 23:30:17,545 train 500 1.060129e-02 0.328644
2019-11-12 23:30:24,079 train 550 1.061914e-02 0.325934
2019-11-12 23:30:30,629 train 600 1.061498e-02 0.322201
2019-11-12 23:30:37,326 train 650 1.060455e-02 0.320988
2019-11-12 23:30:44,070 train 700 1.059055e-02 0.323829
2019-11-12 23:30:50,774 train 750 1.058782e-02 0.320694
2019-11-12 23:30:57,503 train 800 1.058033e-02 0.321780
2019-11-12 23:31:04,037 train 850 1.057968e-02 0.322772
2019-11-12 23:31:05,971 training loss; R2: 1.058246e-02 0.323036
2019-11-12 23:31:06,264 valid 000 2.322889e-02 -1.132467
2019-11-12 23:31:07,942 valid 050 2.509449e-02 -2.926199
2019-11-12 23:31:09,462 validation loss; R2: 2.522693e-02 -2.569219
2019-11-12 23:31:09,488 epoch 17 lr 1.000000e-03
2019-11-12 23:31:09,907 train 000 9.820095e-03 0.433561
2019-11-12 23:31:16,373 train 050 9.992057e-03 0.317315
2019-11-12 23:31:22,956 train 100 1.018509e-02 0.288224
2019-11-12 23:31:29,497 train 150 1.038969e-02 0.301146
2019-11-12 23:31:36,096 train 200 1.047935e-02 0.302229
2019-11-12 23:31:42,832 train 250 1.048732e-02 0.293542
2019-11-12 23:31:49,596 train 300 1.048433e-02 0.289746
2019-11-12 23:31:56,140 train 350 1.044217e-02 0.233005
2019-11-12 23:32:02,601 train 400 1.049194e-02 0.239144
2019-11-12 23:32:09,160 train 450 1.048494e-02 0.244675
2019-11-12 23:32:15,721 train 500 1.047294e-02 0.208939
2019-11-12 23:32:22,175 train 550 1.048464e-02 0.208836
2019-11-12 23:32:28,637 train 600 1.051008e-02 0.219108
2019-11-12 23:32:35,093 train 650 1.050404e-02 0.228732
2019-11-12 23:32:41,631 train 700 1.050551e-02 0.237866
2019-11-12 23:32:48,234 train 750 1.051752e-02 0.245329
2019-11-12 23:32:54,810 train 800 1.051105e-02 0.252961
2019-11-12 23:33:01,251 train 850 1.051450e-02 0.254406
2019-11-12 23:33:03,174 training loss; R2: 1.051418e-02 0.255560
2019-11-12 23:33:03,499 valid 000 1.310235e-02 -0.782817
2019-11-12 23:33:05,162 valid 050 1.418192e-02 -1.436457
2019-11-12 23:33:06,689 validation loss; R2: 1.412012e-02 -1.419932
2019-11-12 23:33:06,704 epoch 18 lr 1.000000e-03
2019-11-12 23:33:07,101 train 000 9.334174e-03 0.216850
2019-11-12 23:33:13,540 train 050 1.102333e-02 0.307717
2019-11-12 23:33:20,195 train 100 1.069323e-02 0.335369
2019-11-12 23:33:26,845 train 150 1.054640e-02 0.335121
2019-11-12 23:33:33,583 train 200 1.043354e-02 0.332103
2019-11-12 23:33:40,163 train 250 1.044082e-02 0.336668
2019-11-12 23:33:46,703 train 300 1.047176e-02 0.333303
2019-11-12 23:33:53,217 train 350 1.047986e-02 0.329318
2019-11-12 23:33:59,717 train 400 1.047491e-02 0.308931
2019-11-12 23:34:06,222 train 450 1.047697e-02 0.310681
2019-11-12 23:34:12,762 train 500 1.049572e-02 0.304410
2019-11-12 23:34:19,377 train 550 1.049176e-02 0.308901
2019-11-12 23:34:26,152 train 600 1.047848e-02 0.305564
2019-11-12 23:34:32,850 train 650 1.046329e-02 0.304317
2019-11-12 23:34:39,406 train 700 1.044757e-02 0.306501
2019-11-12 23:34:45,952 train 750 1.042471e-02 0.306912
2019-11-12 23:34:52,526 train 800 1.040214e-02 0.307750
2019-11-12 23:34:59,020 train 850 1.039577e-02 0.308183
2019-11-12 23:35:00,945 training loss; R2: 1.038474e-02 0.307925
2019-11-12 23:35:01,249 valid 000 2.987085e-01 -139.777526
2019-11-12 23:35:02,974 valid 050 3.082117e-01 -245.181709
2019-11-12 23:35:04,498 validation loss; R2: 3.084385e-01 -215.741311
2019-11-12 23:35:04,516 epoch 19 lr 1.000000e-03
2019-11-12 23:35:04,945 train 000 9.772087e-03 0.278418
2019-11-12 23:35:11,806 train 050 1.041507e-02 0.300808
2019-11-12 23:35:18,560 train 100 1.050958e-02 0.292635
2019-11-12 23:35:25,386 train 150 1.053745e-02 0.288157
2019-11-12 23:35:32,077 train 200 1.054933e-02 0.299240
2019-11-12 23:35:38,713 train 250 1.045698e-02 0.301260
2019-11-12 23:35:45,232 train 300 1.042659e-02 0.311993
2019-11-12 23:35:51,779 train 350 1.043088e-02 0.307165
2019-11-12 23:35:58,282 train 400 1.041051e-02 0.310320
2019-11-12 23:36:04,854 train 450 1.042153e-02 0.219806
2019-11-12 23:36:11,426 train 500 1.040327e-02 0.228362
2019-11-12 23:36:17,950 train 550 1.038502e-02 0.234813
2019-11-12 23:36:24,467 train 600 1.038751e-02 0.239877
2019-11-12 23:36:31,092 train 650 1.039402e-02 0.245395
2019-11-12 23:36:37,764 train 700 1.039173e-02 0.250138
2019-11-12 23:36:44,407 train 750 1.039370e-02 0.255957
2019-11-12 23:36:50,786 train 800 1.037823e-02 0.252456
2019-11-12 23:36:57,175 train 850 1.037481e-02 0.256441
2019-11-12 23:36:59,080 training loss; R2: 1.037433e-02 0.258179
2019-11-12 23:36:59,379 valid 000 8.146773e-02 -19.461418
2019-11-12 23:37:01,051 valid 050 7.918121e-02 -22.731775
2019-11-12 23:37:02,594 validation loss; R2: 7.889022e-02 -31.247643
