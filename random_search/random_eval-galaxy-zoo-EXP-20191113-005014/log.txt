2019-11-13 00:50:14,407 gpu device = 1
2019-11-13 00:50:14,407 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name='galaxy-zoo-cleaned', gpu=1, grad_clip=5, gz_dtree=False, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-galaxy-zoo-EXP-20191113-005014', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-11-13 00:50:25,877 param size = 0.421237MB
2019-11-13 00:50:25,882 epoch 0 lr 1.000000e-03
2019-11-13 00:50:28,328 train 000 1.784898e-01 -152.446907
2019-11-13 00:50:38,846 train 050 4.573274e-02 -5.537521
2019-11-13 00:50:49,056 train 100 3.558009e-02 -2.955539
2019-11-13 00:50:59,268 train 150 3.191656e-02 -2.211217
2019-11-13 00:51:09,477 train 200 2.973066e-02 -1.695315
2019-11-13 00:51:19,677 train 250 2.840385e-02 -1.389898
2019-11-13 00:51:30,136 train 300 2.732417e-02 -1.180798
2019-11-13 00:51:40,579 train 350 2.654025e-02 -1.016693
2019-11-13 00:51:50,796 train 400 2.590447e-02 -3.170007
2019-11-13 00:52:01,011 train 450 2.530795e-02 -2.815376
2019-11-13 00:52:11,606 train 500 2.480675e-02 -2.531702
2019-11-13 00:52:22,326 train 550 2.441633e-02 -2.297339
2019-11-13 00:52:32,582 train 600 2.400522e-02 -2.104767
2019-11-13 00:52:42,864 train 650 2.368843e-02 -1.941490
2019-11-13 00:52:53,166 train 700 2.338113e-02 -1.804732
2019-11-13 00:53:03,347 train 750 2.310152e-02 -1.678667
2019-11-13 00:53:13,514 train 800 2.281698e-02 -1.568094
2019-11-13 00:53:23,665 train 850 2.257752e-02 -1.468769
2019-11-13 00:53:27,744 training loss; R2: 2.253572e-02 -1.442040
2019-11-13 00:53:28,056 valid 000 1.659804e-02 -0.096069
2019-11-13 00:53:29,964 valid 050 1.788187e-02 0.098684
2019-11-13 00:53:31,768 validation loss; R2: 1.747458e-02 0.086923
2019-11-13 00:53:31,796 epoch 1 lr 1.000000e-03
2019-11-13 00:53:32,469 train 000 1.952106e-02 0.119536
2019-11-13 00:53:42,673 train 050 1.822712e-02 0.129645
2019-11-13 00:53:52,861 train 100 1.830371e-02 0.119864
2019-11-13 00:54:03,058 train 150 1.829309e-02 0.119355
2019-11-13 00:54:13,232 train 200 1.828170e-02 0.110117
2019-11-13 00:54:23,411 train 250 1.811949e-02 0.117751
2019-11-13 00:54:33,581 train 300 1.809000e-02 0.122601
2019-11-13 00:54:43,750 train 350 1.798633e-02 0.123677
2019-11-13 00:54:53,962 train 400 1.792550e-02 0.071983
2019-11-13 00:55:04,129 train 450 1.783558e-02 0.063806
2019-11-13 00:55:14,303 train 500 1.775254e-02 0.066690
2019-11-13 00:55:24,472 train 550 1.768417e-02 0.073661
2019-11-13 00:55:34,635 train 600 1.759905e-02 0.075384
2019-11-13 00:55:44,814 train 650 1.752350e-02 0.050136
2019-11-13 00:55:54,981 train 700 1.746468e-02 0.058789
2019-11-13 00:56:05,161 train 750 1.741864e-02 0.062885
2019-11-13 00:56:15,332 train 800 1.735648e-02 0.070504
2019-11-13 00:56:25,505 train 850 1.730683e-02 0.076911
2019-11-13 00:56:28,546 training loss; R2: 1.728662e-02 0.078111
2019-11-13 00:56:28,854 valid 000 2.180914e-02 -1.641536
2019-11-13 00:56:30,768 valid 050 2.344920e-02 -0.253251
2019-11-13 00:56:32,497 validation loss; R2: 2.342824e-02 -0.420992
2019-11-13 00:56:32,527 epoch 2 lr 1.000000e-03
2019-11-13 00:56:32,965 train 000 1.502564e-02 0.265638
2019-11-13 00:56:43,112 train 050 1.635771e-02 0.129508
2019-11-13 00:56:53,261 train 100 1.636236e-02 -0.039793
2019-11-13 00:57:03,402 train 150 1.626636e-02 0.026992
2019-11-13 00:57:13,545 train 200 1.613549e-02 0.073016
2019-11-13 00:57:23,698 train 250 1.601196e-02 0.097184
2019-11-13 00:57:33,846 train 300 1.589196e-02 0.106483
2019-11-13 00:57:43,992 train 350 1.588763e-02 0.118069
2019-11-13 00:57:54,143 train 400 1.589877e-02 0.112348
2019-11-13 00:58:04,292 train 450 1.588417e-02 0.122398
2019-11-13 00:58:14,434 train 500 1.579800e-02 0.133711
2019-11-13 00:58:24,591 train 550 1.573758e-02 0.139742
2019-11-13 00:58:34,733 train 600 1.568478e-02 0.137489
2019-11-13 00:58:44,879 train 650 1.565419e-02 0.142201
2019-11-13 00:58:55,018 train 700 1.563445e-02 0.147238
2019-11-13 00:59:05,164 train 750 1.560037e-02 0.147369
2019-11-13 00:59:15,315 train 800 1.553321e-02 0.151953
2019-11-13 00:59:25,463 train 850 1.547646e-02 0.156919
2019-11-13 00:59:28,499 training loss; R2: 1.547548e-02 0.155800
2019-11-13 00:59:28,800 valid 000 1.377846e-02 0.344699
2019-11-13 00:59:30,722 valid 050 1.370630e-02 0.226295
2019-11-13 00:59:32,464 validation loss; R2: 1.354340e-02 0.240586
2019-11-13 00:59:32,492 epoch 3 lr 1.000000e-03
2019-11-13 00:59:32,939 train 000 1.409372e-02 0.242899
2019-11-13 00:59:43,389 train 050 1.490996e-02 0.205805
2019-11-13 00:59:53,906 train 100 1.462223e-02 0.181470
2019-11-13 01:00:04,368 train 150 1.463965e-02 0.185229
2019-11-13 01:00:14,944 train 200 1.463585e-02 0.192250
2019-11-13 01:00:25,441 train 250 1.458969e-02 0.201908
2019-11-13 01:00:35,888 train 300 1.455556e-02 0.206719
2019-11-13 01:00:46,229 train 350 1.455528e-02 0.211823
2019-11-13 01:00:56,783 train 400 1.450758e-02 0.214839
2019-11-13 01:01:07,316 train 450 1.444880e-02 0.217959
2019-11-13 01:01:17,890 train 500 1.441426e-02 0.220232
2019-11-13 01:01:28,372 train 550 1.444210e-02 0.223314
2019-11-13 01:01:38,711 train 600 1.437827e-02 0.219065
2019-11-13 01:01:49,050 train 650 1.433659e-02 0.220267
2019-11-13 01:01:59,425 train 700 1.430829e-02 0.217771
2019-11-13 01:02:10,013 train 750 1.430035e-02 0.216818
2019-11-13 01:02:20,468 train 800 1.425594e-02 0.216996
2019-11-13 01:02:30,945 train 850 1.422733e-02 0.216824
2019-11-13 01:02:34,147 training loss; R2: 1.421946e-02 0.216662
2019-11-13 01:02:34,429 valid 000 1.287865e-02 0.078355
2019-11-13 01:02:36,345 valid 050 1.223444e-02 0.289252
2019-11-13 01:02:38,070 validation loss; R2: 1.226901e-02 0.249780
2019-11-13 01:02:38,101 epoch 4 lr 1.000000e-03
2019-11-13 01:02:38,563 train 000 1.392342e-02 0.183238
2019-11-13 01:02:49,052 train 050 1.354077e-02 0.262110
2019-11-13 01:02:59,411 train 100 1.358192e-02 0.225811
2019-11-13 01:03:09,804 train 150 1.357588e-02 0.227935
2019-11-13 01:03:20,375 train 200 1.365669e-02 0.212925
2019-11-13 01:03:30,617 train 250 1.365219e-02 0.217366
2019-11-13 01:03:41,221 train 300 1.357419e-02 0.214874
2019-11-13 01:03:51,504 train 350 1.358746e-02 0.219687
2019-11-13 01:04:01,986 train 400 1.363459e-02 0.221165
2019-11-13 01:04:12,275 train 450 1.360530e-02 0.223224
2019-11-13 01:04:22,521 train 500 1.357221e-02 0.226065
2019-11-13 01:04:32,752 train 550 1.355056e-02 0.227799
2019-11-13 01:04:42,970 train 600 1.354263e-02 0.230876
2019-11-13 01:04:53,232 train 650 1.351022e-02 0.233992
2019-11-13 01:05:03,506 train 700 1.347446e-02 0.237164
2019-11-13 01:05:14,040 train 750 1.344056e-02 0.239349
2019-11-13 01:05:24,421 train 800 1.344093e-02 0.239248
2019-11-13 01:05:34,715 train 850 1.341531e-02 0.239758
2019-11-13 01:05:37,768 training loss; R2: 1.340926e-02 0.238769
2019-11-13 01:05:38,049 valid 000 1.214419e-02 0.225143
2019-11-13 01:05:39,965 valid 050 1.201577e-02 0.248915
2019-11-13 01:05:41,698 validation loss; R2: 1.199416e-02 0.209918
2019-11-13 01:05:41,725 epoch 5 lr 1.000000e-03
2019-11-13 01:05:42,150 train 000 1.377600e-02 0.274057
2019-11-13 01:05:52,763 train 050 1.297988e-02 0.212334
2019-11-13 01:06:03,019 train 100 1.289714e-02 0.250349
2019-11-13 01:06:13,292 train 150 1.293144e-02 0.240886
2019-11-13 01:06:23,756 train 200 1.297354e-02 0.245800
2019-11-13 01:06:33,936 train 250 1.296926e-02 0.250262
2019-11-13 01:06:44,091 train 300 1.298036e-02 0.251202
2019-11-13 01:06:54,228 train 350 1.301179e-02 0.255518
2019-11-13 01:07:04,372 train 400 1.307529e-02 0.236612
2019-11-13 01:07:14,518 train 450 1.307548e-02 0.240941
2019-11-13 01:07:24,653 train 500 1.303099e-02 0.240884
2019-11-13 01:07:34,787 train 550 1.300508e-02 0.245448
2019-11-13 01:07:44,922 train 600 1.299189e-02 0.247325
2019-11-13 01:07:55,053 train 650 1.298769e-02 0.246495
2019-11-13 01:08:05,182 train 700 1.299675e-02 0.229231
2019-11-13 01:08:15,312 train 750 1.295763e-02 0.232746
2019-11-13 01:08:25,447 train 800 1.292870e-02 0.235628
2019-11-13 01:08:35,578 train 850 1.289010e-02 0.234263
2019-11-13 01:08:38,615 training loss; R2: 1.288186e-02 0.234573
2019-11-13 01:08:38,917 valid 000 6.185428e-02 -2.229754
2019-11-13 01:08:40,827 valid 050 5.886301e-02 -1.933872
2019-11-13 01:08:42,557 validation loss; R2: 5.833523e-02 -1.874579
2019-11-13 01:08:42,585 epoch 6 lr 1.000000e-03
2019-11-13 01:08:43,008 train 000 1.264761e-02 0.152841
2019-11-13 01:08:53,155 train 050 1.243741e-02 0.286332
2019-11-13 01:09:03,297 train 100 1.261780e-02 0.282638
2019-11-13 01:09:13,440 train 150 1.270940e-02 0.285329
2019-11-13 01:09:23,580 train 200 1.270713e-02 0.265644
2019-11-13 01:09:33,718 train 250 1.268327e-02 0.268845
2019-11-13 01:09:43,858 train 300 1.269420e-02 0.263557
2019-11-13 01:09:53,996 train 350 1.265689e-02 0.268858
2019-11-13 01:10:04,136 train 400 1.266382e-02 0.267986
2019-11-13 01:10:14,284 train 450 1.264087e-02 0.268603
2019-11-13 01:10:24,422 train 500 1.262621e-02 0.271753
2019-11-13 01:10:34,554 train 550 1.263312e-02 0.266534
2019-11-13 01:10:44,699 train 600 1.260250e-02 0.268946
2019-11-13 01:10:54,839 train 650 1.258913e-02 0.267089
2019-11-13 01:11:04,978 train 700 1.256302e-02 0.270289
2019-11-13 01:11:15,118 train 750 1.252635e-02 0.271845
2019-11-13 01:11:25,252 train 800 1.249651e-02 0.272095
2019-11-13 01:11:35,385 train 850 1.250590e-02 0.274063
2019-11-13 01:11:38,425 training loss; R2: 1.249882e-02 0.274168
2019-11-13 01:11:38,724 valid 000 1.047281e+00 -396.028595
2019-11-13 01:11:40,644 valid 050 1.038093e+00 -119.776092
2019-11-13 01:11:42,369 validation loss; R2: 1.037339e+00 -122.277723
2019-11-13 01:11:42,396 epoch 7 lr 1.000000e-03
2019-11-13 01:11:42,840 train 000 1.302714e-02 0.255969
2019-11-13 01:11:53,107 train 050 1.249092e-02 0.271444
2019-11-13 01:12:03,276 train 100 1.232156e-02 0.288951
2019-11-13 01:12:13,441 train 150 1.234261e-02 0.288506
2019-11-13 01:12:23,587 train 200 1.229889e-02 0.284775
2019-11-13 01:12:33,746 train 250 1.222608e-02 0.288049
2019-11-13 01:12:43,888 train 300 1.224551e-02 0.290482
2019-11-13 01:12:54,026 train 350 1.225790e-02 0.287575
2019-11-13 01:13:04,158 train 400 1.223870e-02 0.288234
2019-11-13 01:13:14,285 train 450 1.223970e-02 0.286804
2019-11-13 01:13:24,407 train 500 1.221508e-02 0.282994
2019-11-13 01:13:34,536 train 550 1.220322e-02 0.279833
2019-11-13 01:13:44,665 train 600 1.218801e-02 0.279519
2019-11-13 01:13:54,789 train 650 1.220418e-02 0.281362
2019-11-13 01:14:04,926 train 700 1.217245e-02 0.282970
2019-11-13 01:14:15,063 train 750 1.216683e-02 0.284367
2019-11-13 01:14:25,193 train 800 1.215504e-02 0.286242
2019-11-13 01:14:35,331 train 850 1.213851e-02 0.288467
2019-11-13 01:14:38,362 training loss; R2: 1.214448e-02 0.288170
2019-11-13 01:14:38,698 valid 000 1.164467e+01 -1092.519081
2019-11-13 01:14:40,652 valid 050 1.165195e+01 -1295.092967
2019-11-13 01:14:42,377 validation loss; R2: 1.164978e+01 -3206.914556
2019-11-13 01:14:42,407 epoch 8 lr 1.000000e-03
2019-11-13 01:14:42,824 train 000 1.054580e-02 -1.456460
2019-11-13 01:14:53,012 train 050 1.200877e-02 0.281210
2019-11-13 01:15:03,168 train 100 1.202996e-02 0.285484
2019-11-13 01:15:13,299 train 150 1.201221e-02 0.280044
2019-11-13 01:15:23,435 train 200 1.199420e-02 0.289389
2019-11-13 01:15:33,565 train 250 1.196929e-02 0.272369
2019-11-13 01:15:43,693 train 300 1.197328e-02 0.277758
2019-11-13 01:15:53,819 train 350 1.193239e-02 0.282676
2019-11-13 01:16:03,950 train 400 1.192634e-02 0.279962
2019-11-13 01:16:14,089 train 450 1.197935e-02 0.282851
2019-11-13 01:16:24,227 train 500 1.196036e-02 0.285534
2019-11-13 01:16:34,359 train 550 1.195482e-02 0.287213
2019-11-13 01:16:44,498 train 600 1.193793e-02 0.287965
2019-11-13 01:16:54,635 train 650 1.190425e-02 0.287751
2019-11-13 01:17:04,766 train 700 1.191466e-02 0.288640
2019-11-13 01:17:14,910 train 750 1.190731e-02 0.285889
2019-11-13 01:17:25,045 train 800 1.190140e-02 0.285563
2019-11-13 01:17:35,179 train 850 1.188931e-02 0.284963
2019-11-13 01:17:38,213 training loss; R2: 1.189392e-02 0.285931
2019-11-13 01:17:38,533 valid 000 2.773915e+01 -2452.987518
2019-11-13 01:17:40,448 valid 050 2.782443e+01 -3362.576037
2019-11-13 01:17:42,176 validation loss; R2: 2.783773e+01 -3190.612300
2019-11-13 01:17:42,204 epoch 9 lr 1.000000e-03
2019-11-13 01:17:42,625 train 000 1.209922e-02 0.337022
2019-11-13 01:17:53,071 train 050 1.166790e-02 0.300842
2019-11-13 01:18:03,328 train 100 1.159198e-02 0.306361
2019-11-13 01:18:14,024 train 150 1.168196e-02 0.309865
2019-11-13 01:18:24,426 train 200 1.181710e-02 0.297656
2019-11-13 01:18:34,681 train 250 1.176504e-02 0.299799
2019-11-13 01:18:45,182 train 300 1.173324e-02 0.297537
2019-11-13 01:18:55,540 train 350 1.176720e-02 0.299781
2019-11-13 01:19:05,818 train 400 1.173012e-02 0.303207
2019-11-13 01:19:16,196 train 450 1.177332e-02 0.304767
2019-11-13 01:19:26,616 train 500 1.173012e-02 0.306172
2019-11-13 01:19:36,896 train 550 1.172263e-02 0.304592
2019-11-13 01:19:47,072 train 600 1.172917e-02 0.304846
2019-11-13 01:19:57,221 train 650 1.171805e-02 0.305434
2019-11-13 01:20:07,355 train 700 1.172139e-02 0.306437
2019-11-13 01:20:17,496 train 750 1.171935e-02 0.304899
2019-11-13 01:20:27,649 train 800 1.171891e-02 0.303731
2019-11-13 01:20:37,781 train 850 1.172749e-02 0.304768
2019-11-13 01:20:40,813 training loss; R2: 1.171936e-02 0.305225
2019-11-13 01:20:41,129 valid 000 2.361017e+01 -2714.357532
2019-11-13 01:20:43,037 valid 050 2.366222e+01 -4533.547373
2019-11-13 01:20:44,761 validation loss; R2: 2.365882e+01 -3647.978207
2019-11-13 01:20:44,790 epoch 10 lr 1.000000e-03
2019-11-13 01:20:45,213 train 000 1.004221e-02 0.379418
2019-11-13 01:20:55,347 train 050 1.151237e-02 0.309441
2019-11-13 01:21:05,478 train 100 1.169821e-02 0.283563
2019-11-13 01:21:15,600 train 150 1.170942e-02 0.293006
2019-11-13 01:21:25,720 train 200 1.167863e-02 0.294452
2019-11-13 01:21:35,845 train 250 1.169836e-02 0.295341
2019-11-13 01:21:45,964 train 300 1.168458e-02 0.298243
2019-11-13 01:21:56,085 train 350 1.165481e-02 0.301409
2019-11-13 01:22:06,214 train 400 1.169344e-02 0.297418
2019-11-13 01:22:16,323 train 450 1.165746e-02 0.299504
2019-11-13 01:22:26,467 train 500 1.164621e-02 0.301277
2019-11-13 01:22:36,593 train 550 1.163531e-02 0.296523
2019-11-13 01:22:46,717 train 600 1.160052e-02 0.299636
2019-11-13 01:22:56,837 train 650 1.160836e-02 0.299300
2019-11-13 01:23:06,957 train 700 1.161241e-02 0.298542
2019-11-13 01:23:17,076 train 750 1.159248e-02 0.298415
2019-11-13 01:23:27,191 train 800 1.159185e-02 0.297065
2019-11-13 01:23:37,311 train 850 1.159181e-02 0.297552
2019-11-13 01:23:40,340 training loss; R2: 1.158252e-02 0.297902
2019-11-13 01:23:40,636 valid 000 2.440777e+01 -3915.451405
2019-11-13 01:23:42,544 valid 050 2.437232e+01 -1989.915334
2019-11-13 01:23:44,270 validation loss; R2: 2.437861e+01 -1955.109175
2019-11-13 01:23:44,300 epoch 11 lr 1.000000e-03
2019-11-13 01:23:44,744 train 000 1.005583e-02 0.440479
2019-11-13 01:23:54,903 train 050 1.147740e-02 0.311341
2019-11-13 01:24:05,052 train 100 1.154908e-02 0.249429
2019-11-13 01:24:15,188 train 150 1.156754e-02 0.270936
2019-11-13 01:24:25,325 train 200 1.148918e-02 0.282814
2019-11-13 01:24:35,467 train 250 1.151158e-02 0.293059
2019-11-13 01:24:45,598 train 300 1.150004e-02 0.294944
2019-11-13 01:24:55,737 train 350 1.143951e-02 0.297643
2019-11-13 01:25:05,894 train 400 1.143946e-02 0.301252
2019-11-13 01:25:16,024 train 450 1.144480e-02 0.307109
2019-11-13 01:25:26,155 train 500 1.142965e-02 0.307865
2019-11-13 01:25:36,295 train 550 1.139691e-02 0.310878
2019-11-13 01:25:46,424 train 600 1.141366e-02 0.311627
2019-11-13 01:25:56,556 train 650 1.138344e-02 0.311760
2019-11-13 01:26:06,689 train 700 1.139097e-02 0.310338
2019-11-13 01:26:16,813 train 750 1.138924e-02 0.310764
2019-11-13 01:26:26,941 train 800 1.137871e-02 0.311656
2019-11-13 01:26:37,070 train 850 1.138090e-02 0.311145
2019-11-13 01:26:40,098 training loss; R2: 1.137743e-02 0.311929
2019-11-13 01:26:40,409 valid 000 1.762261e+02 -13009.171604
2019-11-13 01:26:42,312 valid 050 1.763580e+02 -13377.470405
2019-11-13 01:26:44,038 validation loss; R2: 1.763842e+02 -13209.252833
2019-11-13 01:26:44,065 epoch 12 lr 1.000000e-03
2019-11-13 01:26:44,488 train 000 1.181885e-02 0.324017
2019-11-13 01:26:54,711 train 050 1.138545e-02 0.329505
2019-11-13 01:27:04,961 train 100 1.132910e-02 0.313604
2019-11-13 01:27:15,361 train 150 1.129022e-02 0.311247
2019-11-13 01:27:25,678 train 200 1.135409e-02 0.301263
2019-11-13 01:27:36,234 train 250 1.131208e-02 0.309774
2019-11-13 01:27:46,561 train 300 1.131565e-02 0.309439
2019-11-13 01:27:56,816 train 350 1.130112e-02 0.304660
2019-11-13 01:28:07,328 train 400 1.129830e-02 0.304707
2019-11-13 01:28:17,593 train 450 1.128765e-02 0.305706
2019-11-13 01:28:27,970 train 500 1.128913e-02 0.305548
2019-11-13 01:28:38,575 train 550 1.128168e-02 0.307563
2019-11-13 01:28:48,850 train 600 1.130892e-02 0.306644
2019-11-13 01:28:59,561 train 650 1.131078e-02 0.301336
2019-11-13 01:29:10,212 train 700 1.131972e-02 0.303068
2019-11-13 01:29:20,422 train 750 1.132865e-02 0.302232
2019-11-13 01:29:30,685 train 800 1.132847e-02 0.303654
2019-11-13 01:29:41,330 train 850 1.132703e-02 0.306057
2019-11-13 01:29:44,542 training loss; R2: 1.132323e-02 0.306843
2019-11-13 01:29:44,827 valid 000 6.534102e+00 -538.666702
2019-11-13 01:29:46,724 valid 050 6.529333e+00 -415.281475
2019-11-13 01:29:48,451 validation loss; R2: 6.535387e+00 -401.861423
2019-11-13 01:29:48,477 epoch 13 lr 1.000000e-03
2019-11-13 01:29:48,898 train 000 1.005119e-02 0.309453
2019-11-13 01:29:59,084 train 050 1.151037e-02 0.274110
2019-11-13 01:30:09,674 train 100 1.149307e-02 0.289106
2019-11-13 01:30:20,108 train 150 1.130858e-02 0.292366
2019-11-13 01:30:30,277 train 200 1.129017e-02 0.295841
2019-11-13 01:30:40,438 train 250 1.135088e-02 0.305288
2019-11-13 01:30:50,607 train 300 1.125610e-02 0.311164
2019-11-13 01:31:00,743 train 350 1.127243e-02 0.286451
2019-11-13 01:31:10,873 train 400 1.125353e-02 0.289688
2019-11-13 01:31:20,985 train 450 1.125392e-02 0.293279
2019-11-13 01:31:31,121 train 500 1.123515e-02 0.298314
2019-11-13 01:31:41,250 train 550 1.123640e-02 0.301920
2019-11-13 01:31:51,396 train 600 1.124557e-02 0.303367
2019-11-13 01:32:01,540 train 650 1.123395e-02 0.304936
2019-11-13 01:32:11,654 train 700 1.124699e-02 0.299253
2019-11-13 01:32:21,761 train 750 1.125685e-02 0.298800
2019-11-13 01:32:31,905 train 800 1.126505e-02 0.299492
2019-11-13 01:32:42,011 train 850 1.125349e-02 0.300783
2019-11-13 01:32:45,034 training loss; R2: 1.124509e-02 0.301939
2019-11-13 01:32:45,351 valid 000 2.746408e+02 -74663.945020
2019-11-13 01:32:47,257 valid 050 2.746356e+02 -181680.474806
2019-11-13 01:32:48,979 validation loss; R2: 2.746215e+02 -157612.102406
2019-11-13 01:32:49,009 epoch 14 lr 1.000000e-03
2019-11-13 01:32:49,432 train 000 1.092052e-02 0.375596
2019-11-13 01:32:59,541 train 050 1.097506e-02 0.307300
2019-11-13 01:33:09,697 train 100 1.098557e-02 0.318803
2019-11-13 01:33:19,834 train 150 1.100538e-02 0.326895
2019-11-13 01:33:30,026 train 200 1.104215e-02 0.322495
2019-11-13 01:33:40,195 train 250 1.106324e-02 0.324159
2019-11-13 01:33:50,375 train 300 1.108366e-02 0.326114
2019-11-13 01:34:00,544 train 350 1.109825e-02 0.316240
2019-11-13 01:34:10,711 train 400 1.105528e-02 0.310006
2019-11-13 01:34:20,865 train 450 1.104442e-02 0.309677
2019-11-13 01:34:31,055 train 500 1.106300e-02 0.310131
2019-11-13 01:34:41,251 train 550 1.105960e-02 0.313901
2019-11-13 01:34:51,439 train 600 1.108424e-02 0.314644
2019-11-13 01:35:01,652 train 650 1.109890e-02 0.315639
2019-11-13 01:35:11,861 train 700 1.109875e-02 0.315789
2019-11-13 01:35:22,039 train 750 1.110300e-02 0.315000
2019-11-13 01:35:32,230 train 800 1.110064e-02 0.310702
2019-11-13 01:35:42,419 train 850 1.110132e-02 0.312424
2019-11-13 01:35:45,454 training loss; R2: 1.110012e-02 0.312503
2019-11-13 01:35:45,751 valid 000 6.821475e+02 -148975.100939
2019-11-13 01:35:47,653 valid 050 6.822264e+02 -116015.600365
2019-11-13 01:35:49,379 validation loss; R2: 6.822661e+02 -137937.780104
2019-11-13 01:35:49,407 epoch 15 lr 1.000000e-03
2019-11-13 01:35:49,858 train 000 9.352675e-03 0.384195
2019-11-13 01:36:00,065 train 050 1.086377e-02 0.320514
2019-11-13 01:36:10,268 train 100 1.091122e-02 0.335807
2019-11-13 01:36:20,412 train 150 1.105572e-02 0.317601
2019-11-13 01:36:30,596 train 200 1.118812e-02 0.317222
2019-11-13 01:36:40,810 train 250 1.117361e-02 0.321765
2019-11-13 01:36:50,994 train 300 1.115093e-02 0.320922
2019-11-13 01:37:01,230 train 350 1.115919e-02 0.312642
2019-11-13 01:37:11,448 train 400 1.122144e-02 0.309806
2019-11-13 01:37:21,647 train 450 1.122902e-02 0.305486
2019-11-13 01:37:31,829 train 500 1.122009e-02 0.307624
2019-11-13 01:37:42,045 train 550 1.122222e-02 0.309666
2019-11-13 01:37:52,275 train 600 1.118637e-02 0.309993
2019-11-13 01:38:02,453 train 650 1.117974e-02 0.277333
2019-11-13 01:38:12,674 train 700 1.115228e-02 0.280195
2019-11-13 01:38:22,905 train 750 1.114039e-02 0.278657
2019-11-13 01:38:33,131 train 800 1.113340e-02 0.280803
2019-11-13 01:38:43,325 train 850 1.113472e-02 0.284857
2019-11-13 01:38:46,375 training loss; R2: 1.112848e-02 0.286205
2019-11-13 01:38:46,682 valid 000 4.784737e+01 -1991.151569
2019-11-13 01:38:48,585 valid 050 4.784755e+01 -2597.373598
2019-11-13 01:38:50,304 validation loss; R2: 4.783196e+01 -2660.873534
2019-11-13 01:38:50,332 epoch 16 lr 1.000000e-03
2019-11-13 01:38:50,779 train 000 1.129315e-02 0.251697
2019-11-13 01:39:01,125 train 050 1.126616e-02 0.295527
2019-11-13 01:39:11,382 train 100 1.099534e-02 0.308259
2019-11-13 01:39:21,615 train 150 1.097684e-02 0.300721
2019-11-13 01:39:31,840 train 200 1.099158e-02 0.285495
2019-11-13 01:39:42,091 train 250 1.098001e-02 0.278062
2019-11-13 01:39:52,336 train 300 1.094337e-02 0.292145
2019-11-13 01:40:02,575 train 350 1.093642e-02 0.298253
2019-11-13 01:40:12,815 train 400 1.094952e-02 0.305329
2019-11-13 01:40:23,059 train 450 1.093237e-02 0.304461
2019-11-13 01:40:33,289 train 500 1.090750e-02 0.298784
2019-11-13 01:40:43,537 train 550 1.089760e-02 0.300644
2019-11-13 01:40:53,775 train 600 1.089026e-02 0.303775
2019-11-13 01:41:04,021 train 650 1.088568e-02 0.305952
2019-11-13 01:41:14,259 train 700 1.088613e-02 0.307768
2019-11-13 01:41:24,498 train 750 1.089069e-02 0.309629
2019-11-13 01:41:34,766 train 800 1.088925e-02 0.311652
2019-11-13 01:41:45,003 train 850 1.090815e-02 0.313170
2019-11-13 01:41:48,069 training loss; R2: 1.091779e-02 0.313440
2019-11-13 01:41:48,381 valid 000 3.221771e+04 -3439315.900381
2019-11-13 01:41:50,287 valid 050 3.221880e+04 -4260047.383313
2019-11-13 01:41:52,011 validation loss; R2: 3.221886e+04 -4276277.501530
2019-11-13 01:41:52,121 epoch 17 lr 1.000000e-03
2019-11-13 01:41:52,550 train 000 8.319000e-03 0.465007
2019-11-13 01:42:02,769 train 050 1.070520e-02 0.339817
2019-11-13 01:42:12,984 train 100 1.079340e-02 0.338507
2019-11-13 01:42:23,207 train 150 1.072721e-02 0.315762
2019-11-13 01:42:33,426 train 200 1.071785e-02 0.199718
2019-11-13 01:42:43,642 train 250 1.078448e-02 0.228168
2019-11-13 01:42:53,865 train 300 1.080097e-02 0.226336
2019-11-13 01:43:04,087 train 350 1.089715e-02 0.224727
2019-11-13 01:43:14,312 train 400 1.092018e-02 0.235352
2019-11-13 01:43:24,539 train 450 1.089002e-02 0.241684
2019-11-13 01:43:34,759 train 500 1.088452e-02 0.252085
2019-11-13 01:43:44,971 train 550 1.091174e-02 0.255618
2019-11-13 01:43:55,191 train 600 1.091698e-02 0.255371
2019-11-13 01:44:05,406 train 650 1.091161e-02 0.257934
2019-11-13 01:44:15,628 train 700 1.092832e-02 0.262387
2019-11-13 01:44:25,855 train 750 1.094804e-02 0.265646
2019-11-13 01:44:36,071 train 800 1.094266e-02 0.270400
2019-11-13 01:44:46,286 train 850 1.095057e-02 0.275352
2019-11-13 01:44:49,342 training loss; R2: 1.094832e-02 0.276159
2019-11-13 01:44:49,644 valid 000 1.252657e+03 -1456010.640080
2019-11-13 01:44:51,544 valid 050 1.252264e+03 -1515932.863193
2019-11-13 01:44:53,259 validation loss; R2: 1.252330e+03 -1407567.136167
2019-11-13 01:44:53,285 epoch 18 lr 1.000000e-03
2019-11-13 01:44:53,728 train 000 1.029116e-02 0.392886
2019-11-13 01:45:04,005 train 050 1.106451e-02 0.310904
2019-11-13 01:45:14,254 train 100 1.093436e-02 0.327945
2019-11-13 01:45:24,489 train 150 1.102330e-02 0.039934
2019-11-13 01:45:34,715 train 200 1.103199e-02 0.112133
2019-11-13 01:45:44,949 train 250 1.106349e-02 0.152117
2019-11-13 01:45:55,181 train 300 1.100587e-02 0.183149
2019-11-13 01:46:05,403 train 350 1.098475e-02 0.209478
2019-11-13 01:46:15,633 train 400 1.096059e-02 0.224166
2019-11-13 01:46:25,857 train 450 1.096219e-02 0.234613
2019-11-13 01:46:36,077 train 500 1.094421e-02 0.242881
2019-11-13 01:46:46,350 train 550 1.093210e-02 0.240422
2019-11-13 01:46:56,605 train 600 1.093728e-02 0.246129
2019-11-13 01:47:06,864 train 650 1.096900e-02 0.251045
2019-11-13 01:47:17,101 train 700 1.099074e-02 0.254098
2019-11-13 01:47:27,325 train 750 1.098327e-02 0.256545
2019-11-13 01:47:37,552 train 800 1.098152e-02 0.259678
2019-11-13 01:47:47,794 train 850 1.097525e-02 0.263844
2019-11-13 01:47:50,853 training loss; R2: 1.099567e-02 0.263909
2019-11-13 01:47:51,173 valid 000 3.157792e+03 -388921.711884
2019-11-13 01:47:53,078 valid 050 3.158916e+03 -650724.597754
2019-11-13 01:47:54,801 validation loss; R2: 3.158713e+03 -712573.672275
2019-11-13 01:47:54,829 epoch 19 lr 1.000000e-03
2019-11-13 01:47:55,276 train 000 9.996301e-03 0.285504
2019-11-13 01:48:05,471 train 050 1.185540e-02 0.179885
2019-11-13 01:48:15,681 train 100 1.194237e-02 0.141137
2019-11-13 01:48:25,897 train 150 1.230882e-02 -0.025439
2019-11-13 01:48:36,116 train 200 1.217786e-02 0.036536
2019-11-13 01:48:46,335 train 250 1.196894e-02 0.096542
2019-11-13 01:48:56,561 train 300 1.186512e-02 0.127511
2019-11-13 01:49:06,777 train 350 1.179556e-02 0.149665
2019-11-13 01:49:16,993 train 400 1.173425e-02 0.168474
2019-11-13 01:49:27,201 train 450 1.169790e-02 0.185808
2019-11-13 01:49:37,361 train 500 1.174077e-02 0.198147
2019-11-13 01:49:47,545 train 550 1.173683e-02 0.208776
2019-11-13 01:49:57,735 train 600 1.172211e-02 0.220293
2019-11-13 01:50:07,951 train 650 1.169233e-02 0.223745
2019-11-13 01:50:18,171 train 700 1.165209e-02 0.229234
2019-11-13 01:50:28,402 train 750 1.161151e-02 0.235882
2019-11-13 01:50:38,625 train 800 1.158596e-02 0.189777
2019-11-13 01:50:48,853 train 850 1.155524e-02 0.196220
2019-11-13 01:50:51,910 training loss; R2: 1.156603e-02 0.198492
2019-11-13 01:50:52,212 valid 000 5.393759e+03 -647461.621929
2019-11-13 01:50:54,123 valid 050 5.394053e+03 -749847.784051
2019-11-13 01:50:55,846 validation loss; R2: 5.393813e+03 -884001.096804
