2019-12-04 00:44:26,280 gpu device = 1
2019-12-04 00:44:26,280 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=96, cutout=False, cutout_length=16, data='../data', dataset='graphene', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name=None, gpu=1, grad_clip=5, gz_dtree=False, init_channels=36, layers=8, learning_rate=0.002, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-graphene-EXP-20191204-004426', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-12-04 00:44:29,496 param size = 1.090693MB
2019-12-04 00:44:29,498 epoch 0 lr 2.000000e-03
2019-12-04 00:44:32,061 train 000 4.243426e-01 -13.445859
2019-12-04 00:44:44,170 train 050 1.761812e-01 -5.572117
2019-12-04 00:44:56,315 train 100 1.032214e-01 -2.813115
2019-12-04 00:45:08,649 train 150 7.792704e-02 -1.808886
2019-12-04 00:45:20,980 train 200 6.451470e-02 -1.300432
2019-12-04 00:45:27,588 training loss; R2: 6.015626e-02 -1.137356
2019-12-04 00:45:27,728 valid 000 1.563249e-02 0.316331
2019-12-04 00:45:29,349 validation loss; R2: 1.953276e-02 0.357143
2019-12-04 00:45:29,369 epoch 1 lr 2.000000e-03
2019-12-04 00:45:29,764 train 000 1.924930e-02 0.398539
2019-12-04 00:45:42,097 train 050 1.986787e-02 0.317427
2019-12-04 00:45:54,438 train 100 2.004676e-02 0.350399
2019-12-04 00:46:06,775 train 150 1.970077e-02 0.381434
2019-12-04 00:46:19,113 train 200 1.852179e-02 0.411626
2019-12-04 00:46:24,666 training loss; R2: 1.816677e-02 0.420052
2019-12-04 00:46:24,803 valid 000 1.352268e-02 0.593385
2019-12-04 00:46:26,102 validation loss; R2: 1.328916e-02 0.536942
2019-12-04 00:46:26,129 epoch 2 lr 2.000000e-03
2019-12-04 00:46:26,462 train 000 1.246954e-02 0.494323
2019-12-04 00:46:38,804 train 050 1.350365e-02 0.541093
2019-12-04 00:46:51,144 train 100 1.354430e-02 0.554748
2019-12-04 00:47:03,483 train 150 1.299979e-02 0.572682
2019-12-04 00:47:15,823 train 200 1.266022e-02 0.586435
2019-12-04 00:47:21,376 training loss; R2: 1.263246e-02 0.591951
2019-12-04 00:47:21,524 valid 000 5.926453e-03 0.786593
2019-12-04 00:47:22,823 validation loss; R2: 6.883482e-03 0.780447
2019-12-04 00:47:22,843 epoch 3 lr 2.000000e-03
2019-12-04 00:47:23,167 train 000 8.148081e-03 0.592887
2019-12-04 00:47:35,221 train 050 1.168471e-02 0.636347
2019-12-04 00:47:47,273 train 100 1.144848e-02 0.629341
2019-12-04 00:47:59,324 train 150 1.094161e-02 0.641221
2019-12-04 00:48:11,373 train 200 1.054093e-02 0.654608
2019-12-04 00:48:16,790 training loss; R2: 1.036563e-02 0.657196
2019-12-04 00:48:16,935 valid 000 9.812213e-03 0.821220
2019-12-04 00:48:18,233 validation loss; R2: 5.373121e-03 0.819577
2019-12-04 00:48:18,254 epoch 4 lr 2.000000e-03
2019-12-04 00:48:18,584 train 000 8.531502e-03 0.831448
2019-12-04 00:48:30,638 train 050 8.897219e-03 0.689106
2019-12-04 00:48:42,691 train 100 9.195171e-03 0.683756
2019-12-04 00:48:54,747 train 150 9.379595e-03 0.683783
2019-12-04 00:49:06,806 train 200 9.406617e-03 0.685397
2019-12-04 00:49:12,229 training loss; R2: 9.408011e-03 0.682988
2019-12-04 00:49:12,362 valid 000 7.731327e-03 0.455257
2019-12-04 00:49:13,661 validation loss; R2: 1.080284e-02 0.646484
2019-12-04 00:49:13,683 epoch 5 lr 2.000000e-03
2019-12-04 00:49:14,021 train 000 4.855754e-03 0.567542
2019-12-04 00:49:26,090 train 050 8.652441e-03 0.718004
2019-12-04 00:49:38,162 train 100 8.305417e-03 0.724355
2019-12-04 00:49:50,229 train 150 8.220237e-03 0.733701
2019-12-04 00:50:02,294 train 200 7.947638e-03 0.734323
2019-12-04 00:50:07,721 training loss; R2: 7.957091e-03 0.733647
2019-12-04 00:50:07,856 valid 000 4.069113e-03 0.857864
2019-12-04 00:50:09,155 validation loss; R2: 5.866174e-03 0.793686
2019-12-04 00:50:09,176 epoch 6 lr 2.000000e-03
2019-12-04 00:50:09,508 train 000 4.853245e-03 0.745797
2019-12-04 00:50:21,574 train 050 6.846183e-03 0.761454
2019-12-04 00:50:33,644 train 100 7.319373e-03 0.752351
2019-12-04 00:50:45,709 train 150 7.470916e-03 0.743318
2019-12-04 00:50:57,768 train 200 7.296524e-03 0.748556
2019-12-04 00:51:03,190 training loss; R2: 7.327687e-03 0.751000
2019-12-04 00:51:03,325 valid 000 7.144503e-03 0.850071
2019-12-04 00:51:04,623 validation loss; R2: 5.191457e-03 0.825087
2019-12-04 00:51:04,644 epoch 7 lr 2.000000e-03
2019-12-04 00:51:04,967 train 000 8.068419e-03 0.661025
2019-12-04 00:51:17,026 train 050 7.707072e-03 0.750597
2019-12-04 00:51:29,084 train 100 7.439793e-03 0.755615
2019-12-04 00:51:41,144 train 150 7.534628e-03 0.751776
2019-12-04 00:51:53,206 train 200 7.251296e-03 0.760763
2019-12-04 00:51:58,629 training loss; R2: 7.297393e-03 0.759292
2019-12-04 00:51:58,764 valid 000 8.970288e-03 0.794205
2019-12-04 00:52:00,063 validation loss; R2: 6.586424e-03 0.773651
2019-12-04 00:52:00,083 epoch 8 lr 2.000000e-03
2019-12-04 00:52:00,413 train 000 7.248630e-03 0.759394
2019-12-04 00:52:12,472 train 050 6.527601e-03 0.763403
2019-12-04 00:52:24,530 train 100 6.717186e-03 0.770055
2019-12-04 00:52:36,595 train 150 6.504307e-03 0.777016
2019-12-04 00:52:48,654 train 200 6.472784e-03 0.779378
2019-12-04 00:52:54,072 training loss; R2: 6.388621e-03 0.780921
2019-12-04 00:52:54,214 valid 000 5.408390e-03 0.791672
2019-12-04 00:52:55,512 validation loss; R2: 6.202069e-03 0.802466
2019-12-04 00:52:55,534 epoch 9 lr 2.000000e-03
2019-12-04 00:52:55,858 train 000 3.881100e-03 0.811863
2019-12-04 00:53:07,912 train 050 6.677975e-03 0.784355
2019-12-04 00:53:19,964 train 100 6.601649e-03 0.781364
2019-12-04 00:53:32,018 train 150 6.407862e-03 0.782926
2019-12-04 00:53:44,072 train 200 6.335566e-03 0.786258
2019-12-04 00:53:49,493 training loss; R2: 6.291558e-03 0.786386
2019-12-04 00:53:49,630 valid 000 5.045729e-03 0.882177
2019-12-04 00:53:50,928 validation loss; R2: 4.278267e-03 0.853523
2019-12-04 00:53:50,950 epoch 10 lr 2.000000e-03
2019-12-04 00:53:51,283 train 000 1.601913e-02 0.761549
2019-12-04 00:54:03,352 train 050 5.871762e-03 0.800700
2019-12-04 00:54:15,423 train 100 5.852096e-03 0.806434
2019-12-04 00:54:27,503 train 150 5.929371e-03 0.798175
2019-12-04 00:54:39,569 train 200 5.932788e-03 0.797813
2019-12-04 00:54:44,993 training loss; R2: 5.936217e-03 0.796044
2019-12-04 00:54:45,129 valid 000 5.858524e-03 0.777722
2019-12-04 00:54:46,428 validation loss; R2: 3.482538e-03 0.877143
2019-12-04 00:54:46,452 epoch 11 lr 2.000000e-03
2019-12-04 00:54:46,779 train 000 5.148592e-03 0.742074
2019-12-04 00:54:58,837 train 050 5.651320e-03 0.809655
2019-12-04 00:55:10,894 train 100 5.818679e-03 0.805548
2019-12-04 00:55:22,949 train 150 5.973866e-03 0.800422
2019-12-04 00:55:35,002 train 200 5.913287e-03 0.800288
2019-12-04 00:55:40,419 training loss; R2: 5.957055e-03 0.801099
2019-12-04 00:55:40,554 valid 000 2.758184e-03 0.859829
2019-12-04 00:55:41,852 validation loss; R2: 3.162481e-03 0.894657
2019-12-04 00:55:41,874 epoch 12 lr 2.000000e-03
2019-12-04 00:55:42,199 train 000 9.868162e-03 0.833957
2019-12-04 00:55:54,245 train 050 6.694191e-03 0.787574
2019-12-04 00:56:06,291 train 100 6.206020e-03 0.794144
2019-12-04 00:56:18,340 train 150 6.092834e-03 0.797253
2019-12-04 00:56:30,390 train 200 6.110591e-03 0.796378
2019-12-04 00:56:35,807 training loss; R2: 6.014537e-03 0.797845
2019-12-04 00:56:35,942 valid 000 3.000908e-03 0.869111
2019-12-04 00:56:37,240 validation loss; R2: 3.832482e-03 0.869543
2019-12-04 00:56:37,262 epoch 13 lr 2.000000e-03
2019-12-04 00:56:37,588 train 000 4.437171e-03 0.826356
2019-12-04 00:56:49,641 train 050 5.696937e-03 0.810171
2019-12-04 00:57:01,690 train 100 5.382421e-03 0.816130
2019-12-04 00:57:13,740 train 150 5.302203e-03 0.818900
2019-12-04 00:57:25,799 train 200 5.346658e-03 0.820905
2019-12-04 00:57:31,217 training loss; R2: 5.281884e-03 0.822168
2019-12-04 00:57:31,353 valid 000 2.652418e-03 0.867360
2019-12-04 00:57:32,652 validation loss; R2: 3.232505e-03 0.885034
2019-12-04 00:57:32,674 epoch 14 lr 2.000000e-03
2019-12-04 00:57:33,002 train 000 2.636574e-03 0.929075
2019-12-04 00:57:45,055 train 050 4.643416e-03 0.843036
2019-12-04 00:57:57,106 train 100 4.908359e-03 0.831339
2019-12-04 00:58:09,152 train 150 5.297754e-03 0.821506
2019-12-04 00:58:21,195 train 200 5.336020e-03 0.818574
2019-12-04 00:58:26,609 training loss; R2: 5.316180e-03 0.818202
2019-12-04 00:58:26,743 valid 000 5.205538e-03 0.903628
2019-12-04 00:58:28,041 validation loss; R2: 3.467834e-03 0.882765
2019-12-04 00:58:28,062 epoch 15 lr 2.000000e-03
2019-12-04 00:58:28,392 train 000 4.183305e-03 0.797798
2019-12-04 00:58:40,434 train 050 4.857186e-03 0.830339
2019-12-04 00:58:52,475 train 100 5.041163e-03 0.826133
2019-12-04 00:59:04,519 train 150 5.142956e-03 0.826605
2019-12-04 00:59:16,559 train 200 5.261628e-03 0.822880
2019-12-04 00:59:21,972 training loss; R2: 5.307419e-03 0.822018
2019-12-04 00:59:22,127 valid 000 1.752784e+00 -137.051636
2019-12-04 00:59:23,425 validation loss; R2: 1.698844e+00 -59.652166
2019-12-04 00:59:23,446 epoch 16 lr 2.000000e-03
2019-12-04 00:59:23,769 train 000 4.761342e-03 0.909214
2019-12-04 00:59:35,821 train 050 5.474500e-03 0.829152
2019-12-04 00:59:47,874 train 100 5.445428e-03 0.824560
2019-12-04 00:59:59,923 train 150 5.180296e-03 0.826309
2019-12-04 01:00:11,974 train 200 5.244713e-03 0.822658
2019-12-04 01:00:17,391 training loss; R2: 5.184101e-03 0.823639
2019-12-04 01:00:17,528 valid 000 8.853526e-01 -15.637956
2019-12-04 01:00:18,825 validation loss; R2: 8.676599e-01 -31.733794
2019-12-04 01:00:18,847 epoch 17 lr 2.000000e-03
2019-12-04 01:00:19,176 train 000 3.906057e-03 0.873773
2019-12-04 01:00:31,217 train 050 4.908656e-03 0.831919
2019-12-04 01:00:43,259 train 100 5.012043e-03 0.830630
2019-12-04 01:00:55,293 train 150 4.861885e-03 0.838491
2019-12-04 01:01:07,331 train 200 4.760863e-03 0.838301
2019-12-04 01:01:12,743 training loss; R2: 4.707145e-03 0.841028
2019-12-04 01:01:12,879 valid 000 2.199080e+00 -55.516719
2019-12-04 01:01:14,176 validation loss; R2: 2.196482e+00 -77.086637
2019-12-04 01:01:14,198 epoch 18 lr 2.000000e-03
2019-12-04 01:01:14,524 train 000 3.529263e-03 0.886733
2019-12-04 01:01:26,563 train 050 4.381661e-03 0.834456
2019-12-04 01:01:38,604 train 100 4.544050e-03 0.836096
2019-12-04 01:01:50,646 train 150 4.753811e-03 0.835894
2019-12-04 01:02:02,685 train 200 4.962403e-03 0.825710
2019-12-04 01:02:08,097 training loss; R2: 5.018329e-03 0.826241
2019-12-04 01:02:08,237 valid 000 2.627426e+00 -79.275374
2019-12-04 01:02:09,534 validation loss; R2: 2.634944e+00 -96.515199
2019-12-04 01:02:09,558 epoch 19 lr 2.000000e-03
2019-12-04 01:02:09,884 train 000 5.512284e-03 0.768380
2019-12-04 01:02:21,916 train 050 5.088563e-03 0.835504
2019-12-04 01:02:33,947 train 100 5.537925e-03 0.820936
2019-12-04 01:02:45,978 train 150 5.409520e-03 0.821938
2019-12-04 01:02:58,003 train 200 5.177071e-03 0.826295
2019-12-04 01:03:03,412 training loss; R2: 5.081031e-03 0.826129
2019-12-04 01:03:03,550 valid 000 5.980726e-03 0.743694
2019-12-04 01:03:04,848 validation loss; R2: 3.731008e-03 0.872112
