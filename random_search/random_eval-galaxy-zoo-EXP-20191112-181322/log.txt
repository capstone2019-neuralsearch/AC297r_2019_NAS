2019-11-12 18:13:22,693 gpu device = 1
2019-11-12 18:13:22,693 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name='galaxy-zoo-cleaned', gpu=1, grad_clip=5, gz_dtree=False, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-galaxy-zoo-EXP-20191112-181322', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-11-12 18:13:34,122 param size = 0.258325MB
2019-11-12 18:13:34,125 epoch 0 lr 1.000000e-03
2019-11-12 18:13:36,248 train 000 6.842257e-01 -261.148435
2019-11-12 18:13:43,034 train 050 6.349367e-02 -16.704256
2019-11-12 18:13:49,840 train 100 4.654254e-02 -9.053845
2019-11-12 18:13:56,290 train 150 3.992304e-02 -6.332117
2019-11-12 18:14:02,681 train 200 3.656657e-02 -4.888363
2019-11-12 18:14:09,084 train 250 3.436873e-02 -4.046626
2019-11-12 18:14:15,480 train 300 3.287250e-02 -3.426045
2019-11-12 18:14:21,877 train 350 3.162306e-02 -2.980536
2019-11-12 18:14:28,271 train 400 3.067005e-02 -2.640087
2019-11-12 18:14:34,664 train 450 2.988009e-02 -2.375300
2019-11-12 18:14:41,057 train 500 2.923421e-02 -2.162651
2019-11-12 18:14:47,453 train 550 2.867148e-02 -1.982166
2019-11-12 18:14:53,844 train 600 2.817787e-02 -1.835952
2019-11-12 18:15:00,236 train 650 2.771176e-02 -1.700125
2019-11-12 18:15:06,632 train 700 2.724389e-02 -1.589205
2019-11-12 18:15:13,022 train 750 2.684602e-02 -1.491263
2019-11-12 18:15:19,409 train 800 2.647822e-02 -1.409586
2019-11-12 18:15:25,801 train 850 2.611534e-02 -1.335216
2019-11-12 18:15:28,436 training loss; R2: 2.602154e-02 -1.311937
2019-11-12 18:15:28,734 valid 000 1.786043e-02 0.099138
2019-11-12 18:15:30,437 valid 050 1.825256e-02 0.072794
2019-11-12 18:15:32,067 validation loss; R2: 1.817329e-02 0.049025
2019-11-12 18:15:32,088 epoch 1 lr 1.000000e-03
2019-11-12 18:15:32,665 train 000 2.004633e-02 -0.025279
2019-11-12 18:15:39,080 train 050 1.972974e-02 -0.080597
2019-11-12 18:15:45,454 train 100 1.963469e-02 -0.025180
2019-11-12 18:15:51,858 train 150 1.943508e-02 -0.027212
2019-11-12 18:15:58,347 train 200 1.952616e-02 -0.029738
2019-11-12 18:16:04,722 train 250 1.935499e-02 -0.023776
2019-11-12 18:16:11,091 train 300 1.930951e-02 -0.025593
2019-11-12 18:16:17,466 train 350 1.916525e-02 -0.015560
2019-11-12 18:16:23,835 train 400 1.905170e-02 -0.018059
2019-11-12 18:16:30,209 train 450 1.901867e-02 -0.012360
2019-11-12 18:16:36,577 train 500 1.892424e-02 -0.003965
2019-11-12 18:16:42,946 train 550 1.884371e-02 -0.003211
2019-11-12 18:16:49,313 train 600 1.874838e-02 0.004909
2019-11-12 18:16:55,688 train 650 1.866929e-02 0.011728
2019-11-12 18:17:02,061 train 700 1.860905e-02 0.018175
2019-11-12 18:17:08,428 train 750 1.852941e-02 0.023582
2019-11-12 18:17:14,800 train 800 1.845043e-02 0.027228
2019-11-12 18:17:21,171 train 850 1.837408e-02 0.030105
2019-11-12 18:17:23,077 training loss; R2: 1.835685e-02 0.030887
2019-11-12 18:17:23,360 valid 000 1.598909e-02 0.202958
2019-11-12 18:17:25,096 valid 050 1.692598e-02 0.084502
2019-11-12 18:17:26,666 validation loss; R2: 1.684845e-02 0.115944
2019-11-12 18:17:26,681 epoch 2 lr 1.000000e-03
2019-11-12 18:17:27,050 train 000 1.711776e-02 0.014498
2019-11-12 18:17:33,519 train 050 1.671098e-02 0.080308
2019-11-12 18:17:39,910 train 100 1.662580e-02 0.074950
2019-11-12 18:17:46,284 train 150 1.665446e-02 0.048601
2019-11-12 18:17:52,675 train 200 1.681064e-02 0.064612
2019-11-12 18:17:59,044 train 250 1.673557e-02 0.052036
2019-11-12 18:18:05,418 train 300 1.666950e-02 0.060658
2019-11-12 18:18:11,787 train 350 1.660274e-02 -0.046296
2019-11-12 18:18:18,161 train 400 1.649382e-02 -0.022614
2019-11-12 18:18:24,536 train 450 1.642519e-02 -0.004574
2019-11-12 18:18:30,906 train 500 1.637368e-02 0.010696
2019-11-12 18:18:37,284 train 550 1.631544e-02 0.015648
2019-11-12 18:18:43,660 train 600 1.627185e-02 0.021970
2019-11-12 18:18:50,035 train 650 1.617613e-02 0.028655
2019-11-12 18:18:56,405 train 700 1.609617e-02 0.037780
2019-11-12 18:19:02,777 train 750 1.607065e-02 0.045620
2019-11-12 18:19:09,153 train 800 1.601325e-02 0.054133
2019-11-12 18:19:15,523 train 850 1.595764e-02 0.062558
2019-11-12 18:19:17,429 training loss; R2: 1.596261e-02 0.064755
2019-11-12 18:19:17,719 valid 000 1.277516e-02 0.205298
2019-11-12 18:19:19,435 valid 050 1.444531e-02 0.197704
2019-11-12 18:19:21,001 validation loss; R2: 1.453431e-02 0.205399
2019-11-12 18:19:21,017 epoch 3 lr 1.000000e-03
2019-11-12 18:19:21,381 train 000 1.527157e-02 0.224630
2019-11-12 18:19:27,964 train 050 1.494946e-02 0.176879
2019-11-12 18:19:34,376 train 100 1.499167e-02 0.163827
2019-11-12 18:19:40,832 train 150 1.511835e-02 0.165879
2019-11-12 18:19:47,332 train 200 1.497317e-02 0.169836
2019-11-12 18:19:53,758 train 250 1.488715e-02 0.174114
2019-11-12 18:20:00,173 train 300 1.483643e-02 0.174997
2019-11-12 18:20:06,584 train 350 1.471946e-02 0.174044
2019-11-12 18:20:12,999 train 400 1.473387e-02 0.176047
2019-11-12 18:20:19,450 train 450 1.469441e-02 0.173257
2019-11-12 18:20:26,074 train 500 1.470566e-02 0.176145
2019-11-12 18:20:32,710 train 550 1.467578e-02 0.177656
2019-11-12 18:20:39,198 train 600 1.465430e-02 0.179756
2019-11-12 18:20:45,638 train 650 1.460723e-02 0.180995
2019-11-12 18:20:52,149 train 700 1.461813e-02 0.172253
2019-11-12 18:20:58,780 train 750 1.460052e-02 0.173242
2019-11-12 18:21:05,259 train 800 1.454249e-02 0.175906
2019-11-12 18:21:11,861 train 850 1.451646e-02 0.177054
2019-11-12 18:21:13,776 training loss; R2: 1.450528e-02 0.178009
2019-11-12 18:21:14,064 valid 000 1.208101e-02 0.313648
2019-11-12 18:21:15,783 valid 050 1.279262e-02 0.286146
2019-11-12 18:21:17,341 validation loss; R2: 1.278713e-02 0.286449
2019-11-12 18:21:17,366 epoch 4 lr 1.000000e-03
2019-11-12 18:21:17,774 train 000 1.399072e-02 0.341344
2019-11-12 18:21:24,239 train 050 1.396239e-02 0.213266
2019-11-12 18:21:30,618 train 100 1.389625e-02 0.227764
2019-11-12 18:21:36,999 train 150 1.385943e-02 0.226065
2019-11-12 18:21:43,375 train 200 1.376055e-02 0.219182
2019-11-12 18:21:49,760 train 250 1.377258e-02 0.220949
2019-11-12 18:21:56,141 train 300 1.376595e-02 0.223463
2019-11-12 18:22:02,527 train 350 1.370931e-02 0.224447
2019-11-12 18:22:08,909 train 400 1.375827e-02 0.220671
2019-11-12 18:22:15,291 train 450 1.376722e-02 0.218848
2019-11-12 18:22:21,676 train 500 1.375358e-02 0.220522
2019-11-12 18:22:28,053 train 550 1.371831e-02 0.219735
2019-11-12 18:22:34,431 train 600 1.368138e-02 0.215573
2019-11-12 18:22:40,812 train 650 1.366291e-02 0.216655
2019-11-12 18:22:47,191 train 700 1.363930e-02 0.218884
2019-11-12 18:22:53,575 train 750 1.361954e-02 0.220366
2019-11-12 18:22:59,951 train 800 1.358788e-02 0.222661
2019-11-12 18:23:06,328 train 850 1.355791e-02 0.224556
2019-11-12 18:23:08,238 training loss; R2: 1.355396e-02 0.224802
2019-11-12 18:23:08,516 valid 000 1.252669e-02 0.375309
2019-11-12 18:23:10,271 valid 050 1.217574e-02 0.321768
2019-11-12 18:23:11,818 validation loss; R2: 1.225395e-02 0.328281
2019-11-12 18:23:11,839 epoch 5 lr 1.000000e-03
2019-11-12 18:23:12,215 train 000 1.284853e-02 0.242060
2019-11-12 18:23:18,634 train 050 1.318314e-02 0.009383
2019-11-12 18:23:25,022 train 100 1.317129e-02 0.082299
2019-11-12 18:23:31,407 train 150 1.300697e-02 0.134138
2019-11-12 18:23:37,796 train 200 1.309824e-02 0.160156
2019-11-12 18:23:44,179 train 250 1.304600e-02 0.175991
2019-11-12 18:23:50,575 train 300 1.304551e-02 0.176098
2019-11-12 18:23:56,959 train 350 1.300763e-02 0.189017
2019-11-12 18:24:03,344 train 400 1.299219e-02 0.195499
2019-11-12 18:24:09,725 train 450 1.296958e-02 0.191305
2019-11-12 18:24:16,235 train 500 1.294678e-02 0.197607
2019-11-12 18:24:22,621 train 550 1.295233e-02 0.205619
2019-11-12 18:24:29,037 train 600 1.295147e-02 0.203399
2019-11-12 18:24:35,620 train 650 1.294662e-02 0.207921
2019-11-12 18:24:41,993 train 700 1.293427e-02 0.210966
2019-11-12 18:24:48,375 train 750 1.292106e-02 0.212518
2019-11-12 18:24:54,760 train 800 1.289156e-02 0.215360
2019-11-12 18:25:01,131 train 850 1.288846e-02 0.211270
2019-11-12 18:25:03,036 training loss; R2: 1.288721e-02 0.211273
2019-11-12 18:25:03,319 valid 000 1.127559e-02 0.390522
2019-11-12 18:25:05,062 valid 050 1.098550e-02 0.356537
2019-11-12 18:25:06,617 validation loss; R2: 1.124136e-02 0.355639
2019-11-12 18:25:06,631 epoch 6 lr 1.000000e-03
2019-11-12 18:25:07,014 train 000 1.166288e-02 0.261132
2019-11-12 18:25:13,705 train 050 1.237133e-02 0.207702
2019-11-12 18:25:20,362 train 100 1.232813e-02 0.219676
2019-11-12 18:25:26,932 train 150 1.235862e-02 0.237399
2019-11-12 18:25:33,355 train 200 1.246885e-02 0.234971
2019-11-12 18:25:39,764 train 250 1.244190e-02 0.244166
2019-11-12 18:25:46,177 train 300 1.244545e-02 0.133063
2019-11-12 18:25:52,592 train 350 1.242398e-02 0.146784
2019-11-12 18:25:59,000 train 400 1.242591e-02 0.150949
2019-11-12 18:26:05,413 train 450 1.246313e-02 0.163233
2019-11-12 18:26:11,824 train 500 1.248762e-02 0.170699
2019-11-12 18:26:18,238 train 550 1.246419e-02 0.178219
2019-11-12 18:26:24,652 train 600 1.245565e-02 0.184514
2019-11-12 18:26:31,073 train 650 1.244044e-02 0.191372
2019-11-12 18:26:37,582 train 700 1.241449e-02 0.193490
2019-11-12 18:26:44,013 train 750 1.238283e-02 0.199349
2019-11-12 18:26:50,428 train 800 1.236435e-02 0.180587
2019-11-12 18:26:56,840 train 850 1.236994e-02 0.185998
2019-11-12 18:26:58,754 training loss; R2: 1.235243e-02 0.185627
2019-11-12 18:26:59,036 valid 000 9.945295e-03 0.007589
2019-11-12 18:27:00,794 valid 050 1.111856e-02 0.320481
2019-11-12 18:27:02,399 validation loss; R2: 1.104071e-02 0.311820
2019-11-12 18:27:02,418 epoch 7 lr 1.000000e-03
2019-11-12 18:27:02,812 train 000 1.340829e-02 0.120618
2019-11-12 18:27:09,407 train 050 1.254829e-02 0.255496
2019-11-12 18:27:16,042 train 100 1.236775e-02 0.257529
2019-11-12 18:27:22,657 train 150 1.224499e-02 0.263696
2019-11-12 18:27:29,231 train 200 1.221236e-02 0.267707
2019-11-12 18:27:35,765 train 250 1.211153e-02 0.272102
2019-11-12 18:27:42,328 train 300 1.211547e-02 0.271520
2019-11-12 18:27:48,939 train 350 1.212052e-02 0.274632
2019-11-12 18:27:55,732 train 400 1.213253e-02 0.272416
2019-11-12 18:28:02,277 train 450 1.210997e-02 0.270322
2019-11-12 18:28:08,876 train 500 1.211310e-02 0.267572
2019-11-12 18:28:15,568 train 550 1.209661e-02 0.270717
2019-11-12 18:28:22,028 train 600 1.206033e-02 0.272219
2019-11-12 18:28:28,518 train 650 1.205328e-02 0.273232
2019-11-12 18:28:35,337 train 700 1.204872e-02 0.269871
2019-11-12 18:28:42,190 train 750 1.205830e-02 0.267037
2019-11-12 18:28:48,695 train 800 1.203221e-02 0.269190
2019-11-12 18:28:55,220 train 850 1.203299e-02 0.267884
2019-11-12 18:28:57,162 training loss; R2: 1.202170e-02 0.267936
2019-11-12 18:28:57,447 valid 000 1.151697e-02 -0.924459
2019-11-12 18:28:59,153 valid 050 1.061719e-02 0.290926
2019-11-12 18:29:00,716 validation loss; R2: 1.054963e-02 0.291381
2019-11-12 18:29:00,740 epoch 8 lr 1.000000e-03
2019-11-12 18:29:01,145 train 000 1.077969e-02 0.284189
2019-11-12 18:29:07,859 train 050 1.168304e-02 0.224784
2019-11-12 18:29:14,550 train 100 1.163028e-02 0.272426
2019-11-12 18:29:21,055 train 150 1.170871e-02 0.281171
2019-11-12 18:29:27,698 train 200 1.175673e-02 0.279989
2019-11-12 18:29:34,193 train 250 1.182707e-02 0.275794
2019-11-12 18:29:40,840 train 300 1.183973e-02 0.277436
2019-11-12 18:29:47,319 train 350 1.182071e-02 0.280507
2019-11-12 18:29:53,781 train 400 1.180906e-02 0.281229
2019-11-12 18:30:00,276 train 450 1.175666e-02 0.282956
2019-11-12 18:30:06,761 train 500 1.174712e-02 0.284947
2019-11-12 18:30:13,323 train 550 1.171819e-02 0.285061
2019-11-12 18:30:19,867 train 600 1.171232e-02 0.286102
2019-11-12 18:30:26,376 train 650 1.170614e-02 0.287403
2019-11-12 18:30:32,895 train 700 1.169980e-02 0.283727
2019-11-12 18:30:39,457 train 750 1.167629e-02 0.279849
2019-11-12 18:30:45,945 train 800 1.167249e-02 0.279217
2019-11-12 18:30:52,438 train 850 1.167340e-02 0.279779
2019-11-12 18:30:54,419 training loss; R2: 1.167327e-02 0.278863
2019-11-12 18:30:54,714 valid 000 9.354494e-03 0.459865
2019-11-12 18:30:56,433 valid 050 1.024610e-02 0.398216
2019-11-12 18:30:58,003 validation loss; R2: 1.032087e-02 0.327357
2019-11-12 18:30:58,024 epoch 9 lr 1.000000e-03
2019-11-12 18:30:58,411 train 000 1.444798e-02 0.355196
2019-11-12 18:31:05,041 train 050 1.176627e-02 0.292670
2019-11-12 18:31:11,590 train 100 1.170065e-02 0.302055
2019-11-12 18:31:18,073 train 150 1.150160e-02 0.296827
2019-11-12 18:31:24,732 train 200 1.146742e-02 0.296677
2019-11-12 18:31:31,578 train 250 1.147859e-02 0.299362
2019-11-12 18:31:38,261 train 300 1.154330e-02 0.297379
2019-11-12 18:31:44,774 train 350 1.150079e-02 0.301820
2019-11-12 18:31:51,254 train 400 1.150242e-02 0.297126
2019-11-12 18:31:57,822 train 450 1.150520e-02 0.295030
2019-11-12 18:32:04,454 train 500 1.150441e-02 0.295100
2019-11-12 18:32:11,203 train 550 1.151381e-02 0.246540
2019-11-12 18:32:17,966 train 600 1.148036e-02 0.249221
2019-11-12 18:32:24,493 train 650 1.148820e-02 0.252558
2019-11-12 18:32:31,005 train 700 1.149137e-02 0.256865
2019-11-12 18:32:37,616 train 750 1.147514e-02 0.207037
2019-11-12 18:32:44,216 train 800 1.147561e-02 0.214632
2019-11-12 18:32:50,806 train 850 1.146804e-02 0.216480
2019-11-12 18:32:52,738 training loss; R2: 1.145957e-02 0.218413
2019-11-12 18:32:53,029 valid 000 1.082776e-02 0.426067
2019-11-12 18:32:54,735 valid 050 1.040448e-02 0.382243
2019-11-12 18:32:56,299 validation loss; R2: 1.021145e-02 0.373562
2019-11-12 18:32:56,320 epoch 10 lr 1.000000e-03
2019-11-12 18:32:56,747 train 000 1.251483e-02 0.331409
2019-11-12 18:33:03,408 train 050 1.150738e-02 0.294200
2019-11-12 18:33:09,971 train 100 1.134348e-02 0.264541
2019-11-12 18:33:16,482 train 150 1.140438e-02 0.271592
2019-11-12 18:33:23,067 train 200 1.138008e-02 0.276428
2019-11-12 18:33:29,661 train 250 1.135117e-02 0.270227
2019-11-12 18:33:36,137 train 300 1.135180e-02 0.279096
2019-11-12 18:33:42,691 train 350 1.137683e-02 0.253659
2019-11-12 18:33:49,289 train 400 1.134931e-02 0.262229
2019-11-12 18:33:55,793 train 450 1.130130e-02 0.267466
2019-11-12 18:34:02,477 train 500 1.127635e-02 0.271245
2019-11-12 18:34:08,976 train 550 1.123891e-02 0.261104
2019-11-12 18:34:15,643 train 600 1.122272e-02 0.266822
2019-11-12 18:34:22,356 train 650 1.121629e-02 0.269038
2019-11-12 18:34:29,176 train 700 1.121777e-02 0.273517
2019-11-12 18:34:35,854 train 750 1.122079e-02 0.276722
2019-11-12 18:34:42,596 train 800 1.122181e-02 0.276714
2019-11-12 18:34:49,225 train 850 1.121526e-02 0.277162
2019-11-12 18:34:51,212 training loss; R2: 1.120920e-02 0.278103
2019-11-12 18:34:51,507 valid 000 1.339153e-01 -7.612099
2019-11-12 18:34:53,195 valid 050 1.349501e-01 -8.265707
2019-11-12 18:34:54,722 validation loss; R2: 1.348160e-01 -9.744666
2019-11-12 18:34:54,740 epoch 11 lr 1.000000e-03
2019-11-12 18:34:55,148 train 000 1.075931e-02 0.400626
2019-11-12 18:35:01,618 train 050 1.121309e-02 0.329991
2019-11-12 18:35:08,074 train 100 1.116490e-02 0.305154
2019-11-12 18:35:14,502 train 150 1.107112e-02 0.278997
2019-11-12 18:35:20,912 train 200 1.113416e-02 0.284779
2019-11-12 18:35:27,323 train 250 1.112471e-02 0.289437
2019-11-12 18:35:33,736 train 300 1.109114e-02 0.298276
2019-11-12 18:35:40,146 train 350 1.108290e-02 0.296948
2019-11-12 18:35:46,549 train 400 1.106422e-02 0.298508
2019-11-12 18:35:52,968 train 450 1.104225e-02 0.297806
2019-11-12 18:35:59,374 train 500 1.102831e-02 0.299182
2019-11-12 18:36:05,783 train 550 1.102036e-02 0.300480
2019-11-12 18:36:12,189 train 600 1.103901e-02 0.299595
2019-11-12 18:36:18,597 train 650 1.104928e-02 0.299827
2019-11-12 18:36:25,010 train 700 1.102916e-02 0.269187
2019-11-12 18:36:31,418 train 750 1.100811e-02 0.270626
2019-11-12 18:36:37,920 train 800 1.100831e-02 0.274392
2019-11-12 18:36:44,390 train 850 1.100551e-02 0.276356
2019-11-12 18:36:46,302 training loss; R2: 1.101792e-02 0.277111
2019-11-12 18:36:46,606 valid 000 1.017833e-02 0.290052
2019-11-12 18:36:48,329 valid 050 1.080297e-02 0.336600
2019-11-12 18:36:49,873 validation loss; R2: 1.080901e-02 0.334956
2019-11-12 18:36:49,891 epoch 12 lr 1.000000e-03
2019-11-12 18:36:50,336 train 000 1.361235e-02 0.380179
2019-11-12 18:36:57,017 train 050 1.090256e-02 -1.147118
2019-11-12 18:37:03,597 train 100 1.112476e-02 -0.423359
2019-11-12 18:37:09,995 train 150 1.115737e-02 -0.178245
2019-11-12 18:37:16,369 train 200 1.114213e-02 -0.053051
2019-11-12 18:37:22,742 train 250 1.104168e-02 0.020384
2019-11-12 18:37:29,195 train 300 1.101823e-02 0.070705
2019-11-12 18:37:35,572 train 350 1.100855e-02 0.099424
2019-11-12 18:37:41,952 train 400 1.096003e-02 0.118342
2019-11-12 18:37:48,331 train 450 1.093270e-02 0.143147
2019-11-12 18:37:54,704 train 500 1.093107e-02 0.157844
2019-11-12 18:38:01,075 train 550 1.093463e-02 0.174023
2019-11-12 18:38:07,450 train 600 1.093243e-02 0.180169
2019-11-12 18:38:13,826 train 650 1.094175e-02 0.193338
2019-11-12 18:38:20,199 train 700 1.095660e-02 0.201848
2019-11-12 18:38:26,578 train 750 1.095803e-02 0.211034
2019-11-12 18:38:32,951 train 800 1.094299e-02 0.216588
2019-11-12 18:38:39,323 train 850 1.094498e-02 0.214552
2019-11-12 18:38:41,225 training loss; R2: 1.094251e-02 0.216851
2019-11-12 18:38:41,514 valid 000 8.916235e-03 0.347142
2019-11-12 18:38:43,180 valid 050 1.026937e-02 0.399940
2019-11-12 18:38:44,700 validation loss; R2: 1.022839e-02 0.396443
2019-11-12 18:38:44,720 epoch 13 lr 1.000000e-03
2019-11-12 18:38:45,084 train 000 1.169086e-02 0.371340
2019-11-12 18:38:51,639 train 050 1.055304e-02 0.247072
2019-11-12 18:38:58,228 train 100 1.059092e-02 0.288473
2019-11-12 18:39:04,704 train 150 1.060336e-02 0.294314
2019-11-12 18:39:11,221 train 200 1.064941e-02 0.292057
2019-11-12 18:39:17,731 train 250 1.065087e-02 0.304966
2019-11-12 18:39:24,143 train 300 1.069720e-02 0.305704
2019-11-12 18:39:30,537 train 350 1.069799e-02 0.310989
2019-11-12 18:39:36,928 train 400 1.074626e-02 0.302415
2019-11-12 18:39:43,313 train 450 1.078606e-02 0.305026
2019-11-12 18:39:49,705 train 500 1.080048e-02 0.306321
2019-11-12 18:39:56,100 train 550 1.080736e-02 0.306372
2019-11-12 18:40:02,489 train 600 1.080315e-02 0.308116
2019-11-12 18:40:08,881 train 650 1.080248e-02 0.308877
2019-11-12 18:40:15,271 train 700 1.079592e-02 0.312059
2019-11-12 18:40:21,666 train 750 1.077789e-02 0.310409
2019-11-12 18:40:28,056 train 800 1.077464e-02 0.309672
2019-11-12 18:40:34,442 train 850 1.078345e-02 0.308542
2019-11-12 18:40:36,348 training loss; R2: 1.077597e-02 0.309413
2019-11-12 18:40:36,652 valid 000 8.989023e-03 0.401650
2019-11-12 18:40:38,325 valid 050 9.597019e-03 0.409920
2019-11-12 18:40:39,858 validation loss; R2: 9.601995e-03 0.403132
2019-11-12 18:40:39,872 epoch 14 lr 1.000000e-03
2019-11-12 18:40:40,253 train 000 1.239963e-02 0.411135
2019-11-12 18:40:47,003 train 050 1.083681e-02 0.297053
2019-11-12 18:40:53,718 train 100 1.063948e-02 0.322065
2019-11-12 18:41:00,259 train 150 1.074294e-02 0.324240
2019-11-12 18:41:06,883 train 200 1.077474e-02 0.317718
2019-11-12 18:41:13,452 train 250 1.068318e-02 0.322130
2019-11-12 18:41:20,055 train 300 1.064992e-02 0.323342
2019-11-12 18:41:26,608 train 350 1.072309e-02 0.321137
2019-11-12 18:41:33,330 train 400 1.076092e-02 0.315989
2019-11-12 18:41:40,008 train 450 1.075541e-02 0.314567
2019-11-12 18:41:46,580 train 500 1.076401e-02 0.317492
2019-11-12 18:41:53,121 train 550 1.075803e-02 0.315934
2019-11-12 18:41:59,645 train 600 1.076932e-02 0.316202
2019-11-12 18:42:06,166 train 650 1.076134e-02 0.317749
2019-11-12 18:42:12,975 train 700 1.073830e-02 0.317948
2019-11-12 18:42:19,568 train 750 1.071690e-02 0.318258
2019-11-12 18:42:26,133 train 800 1.070966e-02 0.316589
2019-11-12 18:42:32,759 train 850 1.071794e-02 0.317069
2019-11-12 18:42:34,744 training loss; R2: 1.071635e-02 0.317451
2019-11-12 18:42:35,008 valid 000 1.298946e+00 -110.564445
2019-11-12 18:42:36,733 valid 050 1.304849e+00 -361.398806
2019-11-12 18:42:38,372 validation loss; R2: 1.306904e+00 -328.458204
2019-11-12 18:42:38,397 epoch 15 lr 1.000000e-03
2019-11-12 18:42:38,797 train 000 1.057272e-02 0.240039
2019-11-12 18:42:45,466 train 050 1.084649e-02 0.290531
2019-11-12 18:42:52,077 train 100 1.080909e-02 0.303798
2019-11-12 18:42:58,660 train 150 1.072669e-02 0.307340
2019-11-12 18:43:05,197 train 200 1.070505e-02 0.311003
2019-11-12 18:43:11,785 train 250 1.071518e-02 0.307011
2019-11-12 18:43:18,252 train 300 1.072394e-02 0.253082
2019-11-12 18:43:24,785 train 350 1.072511e-02 0.259991
2019-11-12 18:43:31,260 train 400 1.073391e-02 0.268819
2019-11-12 18:43:37,721 train 450 1.071723e-02 0.276804
2019-11-12 18:43:44,460 train 500 1.067783e-02 0.281755
2019-11-12 18:43:51,222 train 550 1.066450e-02 0.283764
2019-11-12 18:43:57,835 train 600 1.066940e-02 0.285568
2019-11-12 18:44:04,351 train 650 1.066979e-02 0.286279
2019-11-12 18:44:11,020 train 700 1.068804e-02 0.283305
2019-11-12 18:44:17,587 train 750 1.069947e-02 0.280322
2019-11-12 18:44:24,142 train 800 1.071601e-02 0.278337
2019-11-12 18:44:30,718 train 850 1.071520e-02 0.280615
2019-11-12 18:44:32,728 training loss; R2: 1.071858e-02 0.279567
2019-11-12 18:44:33,020 valid 000 2.781223e+00 -261.424215
2019-11-12 18:44:34,720 valid 050 2.779648e+00 -504.459873
2019-11-12 18:44:36,220 validation loss; R2: 2.778832e+00 -487.329085
2019-11-12 18:44:36,237 epoch 16 lr 1.000000e-03
2019-11-12 18:44:36,611 train 000 9.000463e-03 0.264948
2019-11-12 18:44:43,055 train 050 1.033763e-02 0.350727
2019-11-12 18:44:49,790 train 100 1.044192e-02 0.347073
2019-11-12 18:44:56,534 train 150 1.054314e-02 0.314771
2019-11-12 18:45:03,161 train 200 1.060523e-02 0.308365
2019-11-12 18:45:09,923 train 250 1.066584e-02 0.309174
2019-11-12 18:45:16,570 train 300 1.060420e-02 0.315492
2019-11-12 18:45:23,168 train 350 1.059508e-02 0.315209
2019-11-12 18:45:29,836 train 400 1.058893e-02 0.312554
2019-11-12 18:45:36,323 train 450 1.064335e-02 0.309786
2019-11-12 18:45:42,926 train 500 1.066317e-02 0.309663
2019-11-12 18:45:49,593 train 550 1.065722e-02 0.314586
2019-11-12 18:45:56,117 train 600 1.064892e-02 0.310828
2019-11-12 18:46:02,853 train 650 1.065007e-02 0.313113
2019-11-12 18:46:09,404 train 700 1.064345e-02 0.309730
2019-11-12 18:46:15,998 train 750 1.063038e-02 0.308679
2019-11-12 18:46:22,778 train 800 1.063255e-02 0.305215
2019-11-12 18:46:29,414 train 850 1.064203e-02 0.304160
2019-11-12 18:46:31,410 training loss; R2: 1.065004e-02 0.303428
2019-11-12 18:46:31,717 valid 000 2.896132e+00 -305.395870
2019-11-12 18:46:33,413 valid 050 2.894911e+00 -297.681458
2019-11-12 18:46:34,924 validation loss; R2: 2.895739e+00 -308.859145
2019-11-12 18:46:34,938 epoch 17 lr 1.000000e-03
2019-11-12 18:46:35,318 train 000 1.034619e-02 0.065759
2019-11-12 18:46:42,031 train 050 1.072126e-02 0.286098
2019-11-12 18:46:48,612 train 100 1.071882e-02 0.263827
2019-11-12 18:46:55,238 train 150 1.067729e-02 0.291366
2019-11-12 18:47:01,731 train 200 1.070156e-02 0.296149
2019-11-12 18:47:08,424 train 250 1.067337e-02 0.278142
2019-11-12 18:47:14,962 train 300 1.065965e-02 0.236075
2019-11-12 18:47:21,543 train 350 1.060074e-02 0.249883
2019-11-12 18:47:28,030 train 400 1.058785e-02 0.255248
2019-11-12 18:47:34,531 train 450 1.060238e-02 0.246203
2019-11-12 18:47:41,098 train 500 1.059566e-02 0.254278
2019-11-12 18:47:47,577 train 550 1.060933e-02 0.262848
2019-11-12 18:47:54,397 train 600 1.060281e-02 0.261219
2019-11-12 18:48:01,073 train 650 1.059570e-02 0.265915
2019-11-12 18:48:07,654 train 700 1.059693e-02 0.270215
2019-11-12 18:48:14,077 train 750 1.057728e-02 0.267741
2019-11-12 18:48:20,721 train 800 1.057443e-02 0.271157
2019-11-12 18:48:27,413 train 850 1.058187e-02 0.268545
2019-11-12 18:48:29,323 training loss; R2: 1.057699e-02 0.269568
2019-11-12 18:48:29,611 valid 000 2.570927e-01 -31.606820
2019-11-12 18:48:31,285 valid 050 2.558929e-01 -38.562474
2019-11-12 18:48:32,820 validation loss; R2: 2.562624e-01 -39.479086
2019-11-12 18:48:32,844 epoch 18 lr 1.000000e-03
2019-11-12 18:48:33,242 train 000 1.098003e-02 0.301506
2019-11-12 18:48:40,028 train 050 1.052666e-02 0.145866
2019-11-12 18:48:46,823 train 100 1.050969e-02 0.220292
2019-11-12 18:48:53,546 train 150 1.058990e-02 0.229618
2019-11-12 18:49:00,091 train 200 1.061918e-02 0.198139
2019-11-12 18:49:06,641 train 250 1.064225e-02 0.208930
2019-11-12 18:49:13,147 train 300 1.062307e-02 0.228481
2019-11-12 18:49:19,742 train 350 1.064797e-02 0.194740
2019-11-12 18:49:26,465 train 400 1.064031e-02 0.200470
2019-11-12 18:49:33,125 train 450 1.062704e-02 0.206235
2019-11-12 18:49:39,834 train 500 1.061701e-02 0.211711
2019-11-12 18:49:46,621 train 550 1.060864e-02 0.189106
2019-11-12 18:49:53,408 train 600 1.061565e-02 0.197659
2019-11-12 18:50:00,009 train 650 1.060308e-02 0.204799
2019-11-12 18:50:06,498 train 700 1.059981e-02 0.206710
2019-11-12 18:50:12,900 train 750 1.060357e-02 0.203483
2019-11-12 18:50:19,297 train 800 1.060582e-02 0.207302
2019-11-12 18:50:25,690 train 850 1.062034e-02 0.213410
2019-11-12 18:50:27,622 training loss; R2: 1.062399e-02 0.214750
2019-11-12 18:50:27,955 valid 000 2.189508e+00 -122.797367
2019-11-12 18:50:29,626 valid 050 2.183887e+00 -320.553147
2019-11-12 18:50:31,138 validation loss; R2: 2.185370e+00 -304.727163
2019-11-12 18:50:31,156 epoch 19 lr 1.000000e-03
2019-11-12 18:50:31,593 train 000 8.749464e-03 0.473166
2019-11-12 18:50:38,460 train 050 1.067366e-02 0.269203
2019-11-12 18:50:45,124 train 100 1.061509e-02 0.283639
2019-11-12 18:50:51,654 train 150 1.050620e-02 0.301044
2019-11-12 18:50:58,286 train 200 1.046396e-02 0.305989
2019-11-12 18:51:04,834 train 250 1.049737e-02 0.305044
2019-11-12 18:51:11,405 train 300 1.055210e-02 0.295477
2019-11-12 18:51:18,117 train 350 1.052697e-02 0.298829
2019-11-12 18:51:24,940 train 400 1.052100e-02 0.298651
2019-11-12 18:51:31,782 train 450 1.053834e-02 0.297839
2019-11-12 18:51:38,396 train 500 1.055451e-02 0.298695
2019-11-12 18:51:44,899 train 550 1.055908e-02 0.297725
2019-11-12 18:51:51,420 train 600 1.056522e-02 0.286300
2019-11-12 18:51:57,877 train 650 1.057455e-02 0.287792
2019-11-12 18:52:04,459 train 700 1.058568e-02 0.289731
2019-11-12 18:52:10,954 train 750 1.056777e-02 0.286986
2019-11-12 18:52:17,440 train 800 1.058174e-02 0.286614
2019-11-12 18:52:24,065 train 850 1.059987e-02 0.287959
2019-11-12 18:52:26,123 training loss; R2: 1.060175e-02 0.287158
2019-11-12 18:52:26,424 valid 000 1.115975e-01 -27.902847
2019-11-12 18:52:28,125 valid 050 1.096554e-01 -11.629465
2019-11-12 18:52:29,694 validation loss; R2: 1.097318e-01 -11.569655
