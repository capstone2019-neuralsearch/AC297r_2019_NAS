2019-11-13 18:52:43,745 gpu device = 1
2019-11-13 18:52:43,745 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name='galaxy-zoo-cleaned', gpu=1, grad_clip=5, gz_dtree=False, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-galaxy-zoo-EXP-20191113-185243', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-11-13 18:52:55,313 param size = 0.214869MB
2019-11-13 18:52:55,317 epoch 0 lr 1.000000e-03
2019-11-13 18:52:57,449 train 000 2.363317e-01 -116.397420
2019-11-13 18:53:03,379 train 050 4.448955e-02 -4.688819
2019-11-13 18:53:09,360 train 100 3.544816e-02 -2.581777
2019-11-13 18:53:15,220 train 150 3.195489e-02 -1.825340
2019-11-13 18:53:21,057 train 200 2.998595e-02 -1.482765
2019-11-13 18:53:26,864 train 250 2.884389e-02 -1.223862
2019-11-13 18:53:32,697 train 300 2.784731e-02 -1.640380
2019-11-13 18:53:38,513 train 350 2.705207e-02 -1.418937
2019-11-13 18:53:44,316 train 400 2.644955e-02 -1.249246
2019-11-13 18:53:50,095 train 450 2.589768e-02 -1.109958
2019-11-13 18:53:55,833 train 500 2.540451e-02 -0.999232
2019-11-13 18:54:01,566 train 550 2.494807e-02 -0.906915
2019-11-13 18:54:07,300 train 600 2.459195e-02 -0.828319
2019-11-13 18:54:13,048 train 650 2.424974e-02 -0.759531
2019-11-13 18:54:18,780 train 700 2.396634e-02 -0.702520
2019-11-13 18:54:24,515 train 750 2.367369e-02 -0.650465
2019-11-13 18:54:30,245 train 800 2.340022e-02 -0.605225
2019-11-13 18:54:35,988 train 850 2.318101e-02 -0.566578
2019-11-13 18:54:38,390 training loss; R2: 2.310838e-02 -0.556834
2019-11-13 18:54:38,738 valid 000 1.717116e-02 0.072658
2019-11-13 18:54:40,453 valid 050 1.745076e-02 0.130453
2019-11-13 18:54:42,085 validation loss; R2: 1.759428e-02 0.129620
2019-11-13 18:54:42,099 epoch 1 lr 1.000000e-03
2019-11-13 18:54:42,612 train 000 1.724790e-02 0.003868
2019-11-13 18:54:48,288 train 050 1.934653e-02 0.094286
2019-11-13 18:54:53,840 train 100 1.945978e-02 0.086739
2019-11-13 18:54:59,388 train 150 1.910256e-02 0.087452
2019-11-13 18:55:04,917 train 200 1.879150e-02 0.086192
2019-11-13 18:55:10,608 train 250 1.871295e-02 0.087719
2019-11-13 18:55:16,413 train 300 1.867450e-02 0.092321
2019-11-13 18:55:22,208 train 350 1.855919e-02 0.088522
2019-11-13 18:55:27,951 train 400 1.840608e-02 0.095396
2019-11-13 18:55:33,703 train 450 1.829314e-02 0.097741
2019-11-13 18:55:39,450 train 500 1.817534e-02 0.094792
2019-11-13 18:55:45,194 train 550 1.803416e-02 0.097445
2019-11-13 18:55:50,923 train 600 1.796754e-02 0.101113
2019-11-13 18:55:56,658 train 650 1.785704e-02 0.103019
2019-11-13 18:56:02,391 train 700 1.774516e-02 0.107651
2019-11-13 18:56:08,127 train 750 1.762686e-02 0.111274
2019-11-13 18:56:13,874 train 800 1.750555e-02 0.115678
2019-11-13 18:56:19,609 train 850 1.741555e-02 0.118804
2019-11-13 18:56:21,329 training loss; R2: 1.738343e-02 0.120201
2019-11-13 18:56:21,695 valid 000 1.222607e-02 0.243964
2019-11-13 18:56:23,412 valid 050 1.400996e-02 0.249635
2019-11-13 18:56:24,986 validation loss; R2: 1.391730e-02 0.221921
2019-11-13 18:56:24,999 epoch 2 lr 1.000000e-03
2019-11-13 18:56:25,389 train 000 1.670549e-02 0.168259
2019-11-13 18:56:31,159 train 050 1.584630e-02 0.158391
2019-11-13 18:56:36,985 train 100 1.573424e-02 0.174103
2019-11-13 18:56:42,815 train 150 1.560089e-02 0.176862
2019-11-13 18:56:48,621 train 200 1.557422e-02 0.181261
2019-11-13 18:56:54,435 train 250 1.550703e-02 0.186136
2019-11-13 18:57:00,245 train 300 1.547383e-02 0.187818
2019-11-13 18:57:06,093 train 350 1.541630e-02 0.177559
2019-11-13 18:57:11,893 train 400 1.532956e-02 0.179004
2019-11-13 18:57:17,643 train 450 1.525106e-02 0.150338
2019-11-13 18:57:23,400 train 500 1.523288e-02 0.155119
2019-11-13 18:57:29,143 train 550 1.517052e-02 0.142687
2019-11-13 18:57:34,894 train 600 1.511855e-02 0.144933
2019-11-13 18:57:40,632 train 650 1.510912e-02 0.149587
2019-11-13 18:57:46,369 train 700 1.505426e-02 0.153253
2019-11-13 18:57:52,102 train 750 1.500365e-02 0.158586
2019-11-13 18:57:57,837 train 800 1.497511e-02 0.162428
2019-11-13 18:58:03,578 train 850 1.494175e-02 0.166472
2019-11-13 18:58:05,308 training loss; R2: 1.494269e-02 0.167891
2019-11-13 18:58:05,686 valid 000 1.473959e-02 0.246970
2019-11-13 18:58:07,402 valid 050 1.228949e-02 0.310248
2019-11-13 18:58:08,974 validation loss; R2: 1.235160e-02 0.312351
2019-11-13 18:58:08,988 epoch 3 lr 1.000000e-03
2019-11-13 18:58:09,379 train 000 1.305378e-02 0.193068
2019-11-13 18:58:15,191 train 050 1.412619e-02 0.248350
2019-11-13 18:58:21,140 train 100 1.401287e-02 0.252194
2019-11-13 18:58:27,066 train 150 1.401489e-02 0.244342
2019-11-13 18:58:32,991 train 200 1.397257e-02 0.200918
2019-11-13 18:58:38,945 train 250 1.399863e-02 0.204626
2019-11-13 18:58:44,879 train 300 1.398512e-02 0.192410
2019-11-13 18:58:50,841 train 350 1.393738e-02 0.189151
2019-11-13 18:58:56,805 train 400 1.389133e-02 0.199768
2019-11-13 18:59:02,918 train 450 1.381486e-02 0.207322
2019-11-13 18:59:09,056 train 500 1.377755e-02 0.212476
2019-11-13 18:59:15,022 train 550 1.376709e-02 0.217921
2019-11-13 18:59:20,835 train 600 1.374768e-02 0.222806
2019-11-13 18:59:26,567 train 650 1.372518e-02 0.225090
2019-11-13 18:59:32,311 train 700 1.371767e-02 0.225461
2019-11-13 18:59:38,034 train 750 1.368961e-02 0.228547
2019-11-13 18:59:43,766 train 800 1.365556e-02 0.227375
2019-11-13 18:59:49,520 train 850 1.364676e-02 0.224014
2019-11-13 18:59:51,232 training loss; R2: 1.364726e-02 0.224132
2019-11-13 18:59:51,575 valid 000 1.151591e-02 0.351620
2019-11-13 18:59:53,239 valid 050 1.149754e-02 0.329352
2019-11-13 18:59:54,814 validation loss; R2: 1.161056e-02 0.280767
2019-11-13 18:59:54,833 epoch 4 lr 1.000000e-03
2019-11-13 18:59:55,232 train 000 1.214963e-02 0.271955
2019-11-13 19:00:01,007 train 050 1.303911e-02 0.209348
2019-11-13 19:00:06,854 train 100 1.298437e-02 0.141839
2019-11-13 19:00:12,675 train 150 1.298882e-02 0.183816
2019-11-13 19:00:18,481 train 200 1.298169e-02 0.205485
2019-11-13 19:00:24,294 train 250 1.306282e-02 0.205118
2019-11-13 19:00:30,145 train 300 1.306257e-02 0.212526
2019-11-13 19:00:35,994 train 350 1.306990e-02 0.208896
2019-11-13 19:00:41,837 train 400 1.306641e-02 0.215557
2019-11-13 19:00:47,659 train 450 1.304531e-02 0.222502
2019-11-13 19:00:53,481 train 500 1.306365e-02 0.227335
2019-11-13 19:00:59,330 train 550 1.301996e-02 0.226824
2019-11-13 19:01:05,155 train 600 1.297927e-02 0.230556
2019-11-13 19:01:10,985 train 650 1.296340e-02 0.232724
2019-11-13 19:01:16,812 train 700 1.293620e-02 -0.366689
2019-11-13 19:01:22,639 train 750 1.292125e-02 -0.324490
2019-11-13 19:01:28,462 train 800 1.291959e-02 -0.287899
2019-11-13 19:01:34,281 train 850 1.291284e-02 -0.256567
2019-11-13 19:01:36,017 training loss; R2: 1.290233e-02 -0.256256
2019-11-13 19:01:36,385 valid 000 1.450101e-02 0.278156
2019-11-13 19:01:38,059 valid 050 1.092377e-02 0.339660
2019-11-13 19:01:39,602 validation loss; R2: 1.085199e-02 0.326649
2019-11-13 19:01:39,620 epoch 5 lr 1.000000e-03
2019-11-13 19:01:40,047 train 000 1.277747e-02 0.349069
2019-11-13 19:01:45,907 train 050 1.253947e-02 0.274145
2019-11-13 19:01:51,915 train 100 1.262133e-02 0.265536
2019-11-13 19:01:57,832 train 150 1.268499e-02 0.267528
2019-11-13 19:02:03,828 train 200 1.272878e-02 0.270778
2019-11-13 19:02:09,698 train 250 1.267501e-02 0.266886
2019-11-13 19:02:15,708 train 300 1.262209e-02 0.263294
2019-11-13 19:02:21,597 train 350 1.259419e-02 0.263102
2019-11-13 19:02:27,502 train 400 1.257602e-02 0.262807
2019-11-13 19:02:33,437 train 450 1.251994e-02 0.266357
2019-11-13 19:02:39,358 train 500 1.252712e-02 0.268473
2019-11-13 19:02:45,314 train 550 1.253056e-02 0.269945
2019-11-13 19:02:51,166 train 600 1.249644e-02 0.271667
2019-11-13 19:02:57,084 train 650 1.246401e-02 0.271738
2019-11-13 19:03:03,084 train 700 1.247096e-02 0.272435
2019-11-13 19:03:09,025 train 750 1.247764e-02 0.271618
2019-11-13 19:03:14,901 train 800 1.245269e-02 0.274543
2019-11-13 19:03:20,811 train 850 1.245066e-02 0.273749
2019-11-13 19:03:22,592 training loss; R2: 1.243927e-02 0.274476
2019-11-13 19:03:22,946 valid 000 1.052161e-02 0.307070
2019-11-13 19:03:24,670 valid 050 1.103625e-02 0.343387
2019-11-13 19:03:26,227 validation loss; R2: 1.128333e-02 0.348977
2019-11-13 19:03:26,241 epoch 6 lr 1.000000e-03
2019-11-13 19:03:26,671 train 000 9.953835e-03 0.370378
2019-11-13 19:03:32,751 train 050 1.202320e-02 0.238763
2019-11-13 19:03:38,878 train 100 1.218259e-02 0.244931
2019-11-13 19:03:44,994 train 150 1.218481e-02 0.249005
2019-11-13 19:03:51,117 train 200 1.219652e-02 0.258528
2019-11-13 19:03:57,228 train 250 1.216816e-02 0.267745
2019-11-13 19:04:03,343 train 300 1.216096e-02 0.260219
2019-11-13 19:04:09,435 train 350 1.212435e-02 0.266380
2019-11-13 19:04:15,479 train 400 1.212834e-02 0.273880
2019-11-13 19:04:21,525 train 450 1.212570e-02 0.277641
2019-11-13 19:04:27,577 train 500 1.210639e-02 0.280974
2019-11-13 19:04:33,618 train 550 1.210193e-02 0.280894
2019-11-13 19:04:39,659 train 600 1.209365e-02 0.257933
2019-11-13 19:04:45,684 train 650 1.209059e-02 0.259202
2019-11-13 19:04:51,706 train 700 1.208030e-02 0.263255
2019-11-13 19:04:57,538 train 750 1.207140e-02 0.267453
2019-11-13 19:05:03,341 train 800 1.206088e-02 0.267898
2019-11-13 19:05:09,147 train 850 1.203510e-02 0.270936
2019-11-13 19:05:10,871 training loss; R2: 1.203361e-02 0.271225
2019-11-13 19:05:11,226 valid 000 1.047236e-02 -4.676291
2019-11-13 19:05:12,920 valid 050 1.054607e-02 0.239413
2019-11-13 19:05:14,448 validation loss; R2: 1.039869e-02 0.282582
2019-11-13 19:05:14,467 epoch 7 lr 1.000000e-03
2019-11-13 19:05:14,851 train 000 1.124981e-02 0.327645
2019-11-13 19:05:20,657 train 050 1.188763e-02 0.246192
2019-11-13 19:05:26,366 train 100 1.186750e-02 0.253411
2019-11-13 19:05:32,004 train 150 1.194372e-02 0.276421
2019-11-13 19:05:37,907 train 200 1.184595e-02 0.282124
2019-11-13 19:05:43,775 train 250 1.181155e-02 0.287069
2019-11-13 19:05:49,687 train 300 1.181327e-02 0.288288
2019-11-13 19:05:55,564 train 350 1.181080e-02 0.291891
2019-11-13 19:06:01,545 train 400 1.181007e-02 0.291962
2019-11-13 19:06:07,422 train 450 1.180911e-02 0.289463
2019-11-13 19:06:13,374 train 500 1.178722e-02 0.291286
2019-11-13 19:06:19,268 train 550 1.176038e-02 0.286555
2019-11-13 19:06:25,164 train 600 1.177954e-02 0.284214
2019-11-13 19:06:31,038 train 650 1.176570e-02 0.286144
2019-11-13 19:06:36,925 train 700 1.177195e-02 0.287271
2019-11-13 19:06:42,830 train 750 1.176317e-02 0.290072
2019-11-13 19:06:48,683 train 800 1.176280e-02 0.289586
2019-11-13 19:06:54,555 train 850 1.176279e-02 0.291693
2019-11-13 19:06:56,301 training loss; R2: 1.177002e-02 0.291449
2019-11-13 19:06:56,626 valid 000 1.009169e-02 0.391577
2019-11-13 19:06:58,306 valid 050 1.076757e-02 0.200874
2019-11-13 19:06:59,825 validation loss; R2: 1.080406e-02 0.197278
2019-11-13 19:06:59,841 epoch 8 lr 1.000000e-03
2019-11-13 19:07:00,263 train 000 1.223896e-02 0.184104
2019-11-13 19:07:06,113 train 050 1.169077e-02 0.263280
2019-11-13 19:07:12,025 train 100 1.169093e-02 0.265069
2019-11-13 19:07:17,887 train 150 1.178881e-02 0.256397
2019-11-13 19:07:23,742 train 200 1.181999e-02 0.266079
2019-11-13 19:07:29,600 train 250 1.178238e-02 0.278599
2019-11-13 19:07:35,587 train 300 1.178032e-02 0.269512
2019-11-13 19:07:41,427 train 350 1.179795e-02 0.273308
2019-11-13 19:07:47,265 train 400 1.175313e-02 0.279688
2019-11-13 19:07:53,119 train 450 1.174656e-02 0.282042
2019-11-13 19:07:58,978 train 500 1.172673e-02 0.285084
2019-11-13 19:08:04,806 train 550 1.170032e-02 0.288238
2019-11-13 19:08:10,635 train 600 1.166239e-02 0.287253
2019-11-13 19:08:16,488 train 650 1.166230e-02 0.280394
2019-11-13 19:08:22,309 train 700 1.164461e-02 0.281744
2019-11-13 19:08:28,132 train 750 1.163543e-02 0.284564
2019-11-13 19:08:33,978 train 800 1.162079e-02 0.286595
2019-11-13 19:08:39,828 train 850 1.159872e-02 0.288576
2019-11-13 19:08:41,569 training loss; R2: 1.159926e-02 0.288272
2019-11-13 19:08:41,959 valid 000 2.947862e-02 -0.688732
2019-11-13 19:08:43,662 valid 050 3.041908e-02 -0.921092
2019-11-13 19:08:45,183 validation loss; R2: 3.041276e-02 -0.906249
2019-11-13 19:08:45,207 epoch 9 lr 1.000000e-03
2019-11-13 19:08:45,633 train 000 1.094304e-02 0.390140
2019-11-13 19:08:51,386 train 050 1.172255e-02 0.314354
2019-11-13 19:08:57,075 train 100 1.142557e-02 0.316509
2019-11-13 19:09:02,909 train 150 1.138825e-02 0.326406
2019-11-13 19:09:08,602 train 200 1.144931e-02 0.309890
2019-11-13 19:09:14,504 train 250 1.142781e-02 0.313009
2019-11-13 19:09:20,445 train 300 1.141039e-02 -2.054422
2019-11-13 19:09:26,361 train 350 1.141407e-02 -1.716604
2019-11-13 19:09:32,278 train 400 1.137884e-02 -1.460419
2019-11-13 19:09:38,177 train 450 1.137928e-02 -1.265022
2019-11-13 19:09:44,076 train 500 1.140196e-02 -1.109517
2019-11-13 19:09:49,956 train 550 1.140478e-02 -0.979620
2019-11-13 19:09:55,874 train 600 1.142076e-02 -0.872447
2019-11-13 19:10:01,820 train 650 1.140900e-02 -0.783816
2019-11-13 19:10:07,716 train 700 1.143199e-02 -0.705497
2019-11-13 19:10:13,629 train 750 1.141507e-02 -0.636223
2019-11-13 19:10:19,549 train 800 1.143097e-02 -0.579289
2019-11-13 19:10:25,455 train 850 1.144904e-02 -0.536651
2019-11-13 19:10:27,222 training loss; R2: 1.144211e-02 -0.522492
2019-11-13 19:10:27,572 valid 000 2.241200e-01 -16.993124
2019-11-13 19:10:29,310 valid 050 2.392726e-01 -18.507634
2019-11-13 19:10:30,882 validation loss; R2: 2.400485e-01 -17.467121
2019-11-13 19:10:30,899 epoch 10 lr 1.000000e-03
2019-11-13 19:10:31,348 train 000 1.243299e-02 0.332916
2019-11-13 19:10:36,961 train 050 1.087184e-02 0.324907
2019-11-13 19:10:42,611 train 100 1.104010e-02 0.271604
2019-11-13 19:10:48,234 train 150 1.121680e-02 0.276839
2019-11-13 19:10:54,128 train 200 1.124978e-02 0.289330
2019-11-13 19:11:00,165 train 250 1.126268e-02 0.301388
2019-11-13 19:11:06,305 train 300 1.125115e-02 0.286104
2019-11-13 19:11:12,451 train 350 1.124139e-02 0.291790
2019-11-13 19:11:18,616 train 400 1.124974e-02 0.299047
2019-11-13 19:11:24,761 train 450 1.123932e-02 0.297355
2019-11-13 19:11:30,916 train 500 1.125677e-02 0.293895
2019-11-13 19:11:37,058 train 550 1.126916e-02 0.297504
2019-11-13 19:11:43,034 train 600 1.130130e-02 0.297430
2019-11-13 19:11:48,842 train 650 1.128050e-02 0.300890
2019-11-13 19:11:54,656 train 700 1.129700e-02 0.302045
2019-11-13 19:12:00,477 train 750 1.129517e-02 0.288962
2019-11-13 19:12:06,296 train 800 1.128491e-02 0.291534
2019-11-13 19:12:12,123 train 850 1.128993e-02 0.293452
2019-11-13 19:12:13,856 training loss; R2: 1.129748e-02 0.294407
2019-11-13 19:12:14,214 valid 000 2.858514e-01 -17.082251
2019-11-13 19:12:15,869 valid 050 2.833949e-01 -16.635528
2019-11-13 19:12:17,394 validation loss; R2: 2.827210e-01 -16.626799
2019-11-13 19:12:17,412 epoch 11 lr 1.000000e-03
2019-11-13 19:12:17,826 train 000 1.075418e-02 0.238418
2019-11-13 19:12:23,673 train 050 1.087185e-02 0.175060
2019-11-13 19:12:29,758 train 100 1.099274e-02 0.246945
2019-11-13 19:12:35,684 train 150 1.104352e-02 0.265129
2019-11-13 19:12:41,551 train 200 1.110665e-02 0.269771
2019-11-13 19:12:47,543 train 250 1.111025e-02 0.248307
2019-11-13 19:12:53,651 train 300 1.111514e-02 0.260947
2019-11-13 19:12:59,596 train 350 1.112264e-02 0.270694
2019-11-13 19:13:05,649 train 400 1.112747e-02 0.277046
2019-11-13 19:13:11,526 train 450 1.110940e-02 0.284182
2019-11-13 19:13:17,427 train 500 1.112598e-02 0.251981
2019-11-13 19:13:23,353 train 550 1.112489e-02 0.252566
2019-11-13 19:13:29,289 train 600 1.110864e-02 0.259040
2019-11-13 19:13:35,207 train 650 1.109832e-02 0.263082
2019-11-13 19:13:41,207 train 700 1.110060e-02 0.268662
2019-11-13 19:13:47,129 train 750 1.108663e-02 0.264673
2019-11-13 19:13:53,062 train 800 1.109356e-02 0.266519
2019-11-13 19:13:58,987 train 850 1.109131e-02 0.269081
2019-11-13 19:14:00,750 training loss; R2: 1.109018e-02 0.270200
2019-11-13 19:14:01,122 valid 000 2.783088e-01 -56.065792
2019-11-13 19:14:02,805 valid 050 2.774014e-01 -71.439383
2019-11-13 19:14:04,321 validation loss; R2: 2.770308e-01 -67.733561
2019-11-13 19:14:04,344 epoch 12 lr 1.000000e-03
2019-11-13 19:14:04,760 train 000 9.564618e-03 0.169648
2019-11-13 19:14:10,671 train 050 1.098936e-02 0.111157
2019-11-13 19:14:16,610 train 100 1.090605e-02 0.217706
2019-11-13 19:14:22,678 train 150 1.087614e-02 0.258177
2019-11-13 19:14:28,606 train 200 1.099789e-02 0.274555
2019-11-13 19:14:34,713 train 250 1.100381e-02 0.267239
2019-11-13 19:14:40,561 train 300 1.095954e-02 0.277425
2019-11-13 19:14:46,496 train 350 1.096676e-02 0.286726
2019-11-13 19:14:52,436 train 400 1.097592e-02 0.292030
2019-11-13 19:14:58,290 train 450 1.095280e-02 0.296242
2019-11-13 19:15:04,226 train 500 1.094091e-02 0.293771
2019-11-13 19:15:10,111 train 550 1.097658e-02 0.294997
2019-11-13 19:15:16,072 train 600 1.097498e-02 0.299680
2019-11-13 19:15:21,987 train 650 1.097601e-02 0.303838
2019-11-13 19:15:27,915 train 700 1.097238e-02 0.305982
2019-11-13 19:15:33,808 train 750 1.097716e-02 0.307340
2019-11-13 19:15:39,712 train 800 1.098234e-02 0.307101
2019-11-13 19:15:45,598 train 850 1.099025e-02 0.308313
2019-11-13 19:15:47,362 training loss; R2: 1.098803e-02 0.308525
2019-11-13 19:15:47,739 valid 000 7.536311e-02 -7.327938
2019-11-13 19:15:49,422 valid 050 7.839055e-02 -4.813824
2019-11-13 19:15:50,939 validation loss; R2: 7.818024e-02 -4.966563
2019-11-13 19:15:50,952 epoch 13 lr 1.000000e-03
2019-11-13 19:15:51,369 train 000 1.095326e-02 0.380718
2019-11-13 19:15:57,173 train 050 1.076843e-02 0.265607
2019-11-13 19:16:03,112 train 100 1.089802e-02 0.296587
2019-11-13 19:16:09,056 train 150 1.100922e-02 0.292876
2019-11-13 19:16:14,984 train 200 1.103789e-02 0.299399
2019-11-13 19:16:20,919 train 250 1.108759e-02 0.298370
2019-11-13 19:16:26,890 train 300 1.114154e-02 0.300209
2019-11-13 19:16:32,824 train 350 1.109884e-02 0.304185
2019-11-13 19:16:38,770 train 400 1.112967e-02 0.304691
2019-11-13 19:16:44,749 train 450 1.107605e-02 0.298693
2019-11-13 19:16:50,717 train 500 1.108789e-02 0.301662
2019-11-13 19:16:56,690 train 550 1.106072e-02 0.304966
2019-11-13 19:17:02,623 train 600 1.107829e-02 0.306169
2019-11-13 19:17:08,561 train 650 1.109343e-02 0.307038
2019-11-13 19:17:14,528 train 700 1.110202e-02 0.306535
2019-11-13 19:17:20,480 train 750 1.110339e-02 0.307383
2019-11-13 19:17:26,394 train 800 1.109664e-02 0.306432
2019-11-13 19:17:32,314 train 850 1.110713e-02 0.305500
2019-11-13 19:17:34,110 training loss; R2: 1.110439e-02 0.306477
2019-11-13 19:17:34,479 valid 000 6.376539e-01 -108.298671
2019-11-13 19:17:36,204 valid 050 6.217360e-01 -76.859886
2019-11-13 19:17:37,760 validation loss; R2: 6.209719e-01 -73.417515
2019-11-13 19:17:37,784 epoch 14 lr 1.000000e-03
2019-11-13 19:17:38,233 train 000 1.255643e-02 0.362282
2019-11-13 19:17:44,184 train 050 1.072415e-02 0.326911
2019-11-13 19:17:50,169 train 100 1.094695e-02 0.276186
2019-11-13 19:17:56,135 train 150 1.105095e-02 0.286911
2019-11-13 19:18:01,969 train 200 1.099367e-02 0.298494
2019-11-13 19:18:07,914 train 250 1.104681e-02 0.302675
2019-11-13 19:18:13,869 train 300 1.106089e-02 0.192259
2019-11-13 19:18:19,692 train 350 1.104369e-02 0.201307
2019-11-13 19:18:25,539 train 400 1.109155e-02 0.217827
2019-11-13 19:18:31,368 train 450 1.110665e-02 0.231097
2019-11-13 19:18:37,210 train 500 1.108644e-02 0.243194
2019-11-13 19:18:43,026 train 550 1.109970e-02 0.249701
2019-11-13 19:18:48,839 train 600 1.111343e-02 0.251218
2019-11-13 19:18:54,653 train 650 1.110825e-02 0.258635
2019-11-13 19:19:00,483 train 700 1.108893e-02 0.265986
2019-11-13 19:19:06,297 train 750 1.109806e-02 0.269957
2019-11-13 19:19:12,118 train 800 1.107836e-02 0.273519
2019-11-13 19:19:17,940 train 850 1.107241e-02 0.271288
2019-11-13 19:19:19,677 training loss; R2: 1.106535e-02 0.272974
2019-11-13 19:19:20,022 valid 000 3.817876e-01 -27.277659
2019-11-13 19:19:21,692 valid 050 3.816909e-01 -44.010458
2019-11-13 19:19:23,205 validation loss; R2: 3.823691e-01 -44.711878
2019-11-13 19:19:23,223 epoch 15 lr 1.000000e-03
2019-11-13 19:19:23,660 train 000 1.142578e-02 0.400600
2019-11-13 19:19:29,426 train 050 1.087386e-02 0.265324
2019-11-13 19:19:35,355 train 100 1.085263e-02 0.205003
2019-11-13 19:19:41,266 train 150 1.083485e-02 0.249836
2019-11-13 19:19:47,284 train 200 1.087932e-02 0.227646
2019-11-13 19:19:53,152 train 250 1.086828e-02 0.250507
2019-11-13 19:19:58,999 train 300 1.087248e-02 0.257452
2019-11-13 19:20:04,989 train 350 1.091261e-02 0.272330
2019-11-13 19:20:10,860 train 400 1.090909e-02 0.274588
2019-11-13 19:20:16,752 train 450 1.090953e-02 0.271340
2019-11-13 19:20:22,678 train 500 1.088089e-02 0.278467
2019-11-13 19:20:28,566 train 550 1.087829e-02 0.283983
2019-11-13 19:20:34,457 train 600 1.089017e-02 0.287573
2019-11-13 19:20:40,346 train 650 1.088527e-02 0.277446
2019-11-13 19:20:46,249 train 700 1.087769e-02 0.280850
2019-11-13 19:20:52,151 train 750 1.088362e-02 0.283682
2019-11-13 19:20:58,035 train 800 1.089065e-02 0.288019
2019-11-13 19:21:03,943 train 850 1.087696e-02 0.291554
2019-11-13 19:21:05,680 training loss; R2: 1.087518e-02 0.291830
2019-11-13 19:21:06,053 valid 000 4.905913e-01 -60.746969
2019-11-13 19:21:07,747 valid 050 4.730158e-01 -79.040359
2019-11-13 19:21:09,321 validation loss; R2: 4.744573e-01 -77.470827
2019-11-13 19:21:09,334 epoch 16 lr 1.000000e-03
2019-11-13 19:21:09,819 train 000 1.132372e-02 0.436466
2019-11-13 19:21:15,626 train 050 1.078831e-02 0.337734
2019-11-13 19:21:21,617 train 100 1.076478e-02 0.322409
2019-11-13 19:21:27,749 train 150 1.066249e-02 0.332714
2019-11-13 19:21:33,899 train 200 1.072311e-02 0.333772
2019-11-13 19:21:40,032 train 250 1.079462e-02 0.334945
2019-11-13 19:21:46,159 train 300 1.080608e-02 0.334109
2019-11-13 19:21:52,275 train 350 1.079101e-02 0.334607
2019-11-13 19:21:58,396 train 400 1.080975e-02 0.330830
2019-11-13 19:22:04,523 train 450 1.080630e-02 0.332066
2019-11-13 19:22:10,664 train 500 1.081501e-02 0.329992
2019-11-13 19:22:16,776 train 550 1.079859e-02 0.332169
2019-11-13 19:22:22,887 train 600 1.077905e-02 0.332817
2019-11-13 19:22:28,997 train 650 1.077661e-02 0.331854
2019-11-13 19:22:35,108 train 700 1.078032e-02 0.333258
2019-11-13 19:22:41,230 train 750 1.079579e-02 0.329939
2019-11-13 19:22:47,340 train 800 1.080605e-02 0.331550
2019-11-13 19:22:53,444 train 850 1.081085e-02 0.328792
2019-11-13 19:22:55,269 training loss; R2: 1.081098e-02 0.327567
2019-11-13 19:22:55,607 valid 000 6.041272e-02 -3.193004
2019-11-13 19:22:57,281 valid 050 6.277113e-02 -4.324329
2019-11-13 19:22:58,803 validation loss; R2: 6.291321e-02 -4.368707
2019-11-13 19:22:58,816 epoch 17 lr 1.000000e-03
2019-11-13 19:22:59,283 train 000 1.008186e-02 0.437228
2019-11-13 19:23:05,124 train 050 1.050781e-02 0.337628
2019-11-13 19:23:10,979 train 100 1.073800e-02 0.341936
2019-11-13 19:23:16,855 train 150 1.067659e-02 0.338588
2019-11-13 19:23:22,761 train 200 1.071034e-02 0.342572
2019-11-13 19:23:28,671 train 250 1.066072e-02 0.339515
2019-11-13 19:23:34,552 train 300 1.063441e-02 -0.402939
2019-11-13 19:23:40,422 train 350 1.063787e-02 -0.305979
2019-11-13 19:23:46,296 train 400 1.062822e-02 -0.225274
2019-11-13 19:23:52,180 train 450 1.062913e-02 -0.164418
2019-11-13 19:23:58,076 train 500 1.062420e-02 -0.114321
2019-11-13 19:24:03,963 train 550 1.064351e-02 -0.072863
2019-11-13 19:24:09,779 train 600 1.064200e-02 -0.041236
2019-11-13 19:24:15,656 train 650 1.066540e-02 -0.012305
2019-11-13 19:24:21,559 train 700 1.068681e-02 0.011531
2019-11-13 19:24:27,438 train 750 1.069113e-02 0.034009
2019-11-13 19:24:33,299 train 800 1.068198e-02 0.051403
2019-11-13 19:24:39,133 train 850 1.068867e-02 0.052218
2019-11-13 19:24:40,881 training loss; R2: 1.069046e-02 0.057679
2019-11-13 19:24:41,264 valid 000 5.195805e-01 -44.640699
2019-11-13 19:24:42,936 valid 050 5.333716e-01 -67.891379
2019-11-13 19:24:44,432 validation loss; R2: 5.352455e-01 -66.822339
2019-11-13 19:24:44,446 epoch 18 lr 1.000000e-03
2019-11-13 19:24:44,861 train 000 8.993244e-03 0.450243
2019-11-13 19:24:50,704 train 050 1.074557e-02 0.351483
2019-11-13 19:24:56,726 train 100 1.068671e-02 0.350641
2019-11-13 19:25:02,848 train 150 1.069794e-02 0.337261
2019-11-13 19:25:08,901 train 200 1.067900e-02 0.331860
2019-11-13 19:25:14,849 train 250 1.064576e-02 0.336902
2019-11-13 19:25:20,916 train 300 1.063412e-02 0.331703
2019-11-13 19:25:26,900 train 350 1.063940e-02 0.331166
2019-11-13 19:25:32,841 train 400 1.067391e-02 0.325292
2019-11-13 19:25:38,822 train 450 1.064129e-02 0.301819
2019-11-13 19:25:44,744 train 500 1.062999e-02 0.298853
2019-11-13 19:25:50,701 train 550 1.062270e-02 0.303370
2019-11-13 19:25:56,628 train 600 1.062016e-02 0.285528
2019-11-13 19:26:02,498 train 650 1.062472e-02 0.279905
2019-11-13 19:26:08,387 train 700 1.062285e-02 0.283247
2019-11-13 19:26:14,276 train 750 1.063342e-02 0.286673
2019-11-13 19:26:20,260 train 800 1.063499e-02 0.288758
2019-11-13 19:26:26,194 train 850 1.063475e-02 0.293170
2019-11-13 19:26:27,985 training loss; R2: 1.063793e-02 0.293827
2019-11-13 19:26:28,339 valid 000 7.503046e-01 -209.072274
2019-11-13 19:26:30,087 valid 050 7.456408e-01 -159.309367
2019-11-13 19:26:31,640 validation loss; R2: 7.447358e-01 -164.062279
2019-11-13 19:26:31,667 epoch 19 lr 1.000000e-03
2019-11-13 19:26:32,120 train 000 1.047533e-02 0.254715
2019-11-13 19:26:37,931 train 050 1.046099e-02 0.342305
2019-11-13 19:26:43,950 train 100 1.056952e-02 0.330651
2019-11-13 19:26:50,117 train 150 1.057293e-02 0.321734
2019-11-13 19:26:56,204 train 200 1.051006e-02 0.331072
2019-11-13 19:27:02,046 train 250 1.051170e-02 0.306490
2019-11-13 19:27:07,863 train 300 1.049934e-02 0.300932
2019-11-13 19:27:13,701 train 350 1.050745e-02 0.307678
2019-11-13 19:27:19,520 train 400 1.053839e-02 0.298058
2019-11-13 19:27:25,340 train 450 1.056488e-02 0.301882
2019-11-13 19:27:31,156 train 500 1.055881e-02 0.303297
2019-11-13 19:27:36,967 train 550 1.055308e-02 0.305343
2019-11-13 19:27:42,782 train 600 1.054646e-02 0.307240
2019-11-13 19:27:48,608 train 650 1.054918e-02 0.310579
2019-11-13 19:27:54,420 train 700 1.055299e-02 0.309579
2019-11-13 19:28:00,230 train 750 1.055826e-02 0.312429
2019-11-13 19:28:06,050 train 800 1.056450e-02 0.313443
2019-11-13 19:28:11,874 train 850 1.057734e-02 0.315566
2019-11-13 19:28:13,610 training loss; R2: 1.057936e-02 0.316199
2019-11-13 19:28:13,957 valid 000 2.152048e+01 -780.954961
2019-11-13 19:28:15,625 valid 050 2.153345e+01 -1547.952895
2019-11-13 19:28:17,198 validation loss; R2: 2.152563e+01 -1690.721558
