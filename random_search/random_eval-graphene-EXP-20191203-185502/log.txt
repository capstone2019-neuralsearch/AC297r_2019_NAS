2019-12-03 18:55:02,795 gpu device = 1
2019-12-03 18:55:02,795 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=96, cutout=False, cutout_length=16, data='../data', dataset='graphene', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name=None, gpu=1, grad_clip=5, gz_dtree=False, init_channels=36, layers=8, learning_rate=0.002, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-graphene-EXP-20191203-185502', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-12-03 18:55:06,013 param size = 1.055557MB
2019-12-03 18:55:06,016 epoch 0 lr 2.000000e-03
2019-12-03 18:55:08,640 train 000 2.544279e+00 -46.849411
2019-12-03 18:55:21,105 train 050 2.657509e-01 -6.791804
2019-12-03 18:55:33,499 train 100 1.536463e-01 -3.572780
2019-12-03 18:55:45,891 train 150 1.162300e-01 -2.483190
2019-12-03 18:55:58,278 train 200 9.658126e-02 -1.905282
2019-12-03 18:56:04,854 training loss; R2: 8.988403e-02 -1.723769
2019-12-03 18:56:04,980 valid 000 3.270395e-02 0.274472
2019-12-03 18:56:06,617 validation loss; R2: 2.449459e-02 0.143091
2019-12-03 18:56:06,637 epoch 1 lr 2.000000e-03
2019-12-03 18:56:06,984 train 000 5.026324e-02 0.260961
2019-12-03 18:56:19,357 train 050 2.808189e-02 0.110936
2019-12-03 18:56:32,065 train 100 2.738098e-02 0.128037
2019-12-03 18:56:44,765 train 150 2.672947e-02 0.159938
2019-12-03 18:56:57,467 train 200 2.552265e-02 0.177509
2019-12-03 18:57:03,182 training loss; R2: 2.526807e-02 0.182789
2019-12-03 18:57:03,300 valid 000 1.685899e-02 0.331281
2019-12-03 18:57:04,632 validation loss; R2: 2.007250e-02 0.353423
2019-12-03 18:57:04,655 epoch 2 lr 2.000000e-03
2019-12-03 18:57:04,982 train 000 2.040806e-02 0.506081
2019-12-03 18:57:17,664 train 050 1.828094e-02 0.398300
2019-12-03 18:57:30,329 train 100 1.883627e-02 0.400235
2019-12-03 18:57:42,996 train 150 1.726090e-02 0.437723
2019-12-03 18:57:55,664 train 200 1.649604e-02 0.451785
2019-12-03 18:58:01,364 training loss; R2: 1.661810e-02 0.452807
2019-12-03 18:58:01,487 valid 000 2.239638e-02 0.622712
2019-12-03 18:58:02,818 validation loss; R2: 1.058353e-02 0.658360
2019-12-03 18:58:02,841 epoch 3 lr 2.000000e-03
2019-12-03 18:58:03,165 train 000 7.484617e-03 0.535987
2019-12-03 18:58:15,838 train 050 1.502054e-02 0.494841
2019-12-03 18:58:28,503 train 100 1.443978e-02 0.511789
2019-12-03 18:58:41,165 train 150 1.349785e-02 0.547533
2019-12-03 18:58:53,832 train 200 1.375937e-02 0.548337
2019-12-03 18:58:59,526 training loss; R2: 1.338820e-02 0.552018
2019-12-03 18:58:59,645 valid 000 3.219147e-02 0.570401
2019-12-03 18:59:00,976 validation loss; R2: 9.254100e-03 0.717072
2019-12-03 18:59:00,998 epoch 4 lr 2.000000e-03
2019-12-03 18:59:01,319 train 000 3.845477e-02 0.532787
2019-12-03 18:59:13,956 train 050 1.455853e-02 0.598797
2019-12-03 18:59:26,601 train 100 1.203451e-02 0.629821
2019-12-03 18:59:39,248 train 150 1.121706e-02 0.638081
2019-12-03 18:59:51,909 train 200 1.091455e-02 0.646565
2019-12-03 18:59:57,608 training loss; R2: 1.080239e-02 0.648668
2019-12-03 18:59:57,729 valid 000 1.034024e-02 0.737870
2019-12-03 18:59:59,061 validation loss; R2: 8.697635e-03 0.723912
2019-12-03 18:59:59,083 epoch 5 lr 2.000000e-03
2019-12-03 18:59:59,411 train 000 8.063419e-03 0.684482
2019-12-03 19:00:12,079 train 050 1.071573e-02 0.671946
2019-12-03 19:00:24,743 train 100 1.042534e-02 0.670658
2019-12-03 19:00:37,334 train 150 1.004822e-02 0.672977
2019-12-03 19:00:49,700 train 200 9.528701e-03 0.680248
2019-12-03 19:00:55,263 training loss; R2: 9.483198e-03 0.681513
2019-12-03 19:00:55,381 valid 000 5.424006e-03 0.773903
2019-12-03 19:00:56,710 validation loss; R2: 5.408121e-03 0.828158
2019-12-03 19:00:56,732 epoch 6 lr 2.000000e-03
2019-12-03 19:00:57,047 train 000 5.917683e-03 0.814858
2019-12-03 19:01:09,412 train 050 8.532565e-03 0.707655
2019-12-03 19:01:21,776 train 100 8.783590e-03 0.706541
2019-12-03 19:01:34,135 train 150 8.843339e-03 0.698107
2019-12-03 19:01:46,500 train 200 8.866682e-03 0.701871
2019-12-03 19:01:52,061 training loss; R2: 8.842410e-03 0.701449
2019-12-03 19:01:52,185 valid 000 6.231036e-03 0.757662
2019-12-03 19:01:53,514 validation loss; R2: 5.625825e-03 0.816433
2019-12-03 19:01:53,537 epoch 7 lr 2.000000e-03
2019-12-03 19:01:53,848 train 000 1.268022e-02 0.361626
2019-12-03 19:02:06,207 train 050 9.390249e-03 0.689860
2019-12-03 19:02:18,565 train 100 9.330093e-03 0.683913
2019-12-03 19:02:30,927 train 150 8.943074e-03 0.691111
2019-12-03 19:02:43,400 train 200 9.001529e-03 0.693325
2019-12-03 19:02:49,100 training loss; R2: 9.120431e-03 0.695649
2019-12-03 19:02:49,217 valid 000 3.837290e-03 0.864472
2019-12-03 19:02:50,548 validation loss; R2: 4.519445e-03 0.839931
2019-12-03 19:02:50,569 epoch 8 lr 2.000000e-03
2019-12-03 19:02:50,894 train 000 8.393560e-03 0.601505
2019-12-03 19:03:03,552 train 050 8.225850e-03 0.710437
2019-12-03 19:03:16,203 train 100 7.984413e-03 0.731304
2019-12-03 19:03:28,861 train 150 7.902880e-03 0.732635
2019-12-03 19:03:41,310 train 200 7.880513e-03 0.736018
2019-12-03 19:03:46,870 training loss; R2: 7.850502e-03 0.735843
2019-12-03 19:03:46,988 valid 000 4.020357e-03 0.800590
2019-12-03 19:03:48,317 validation loss; R2: 5.316424e-03 0.819646
2019-12-03 19:03:48,347 epoch 9 lr 2.000000e-03
2019-12-03 19:03:48,665 train 000 7.462345e-03 0.807699
2019-12-03 19:04:01,030 train 050 7.256510e-03 0.736525
2019-12-03 19:04:13,390 train 100 7.452511e-03 0.737567
2019-12-03 19:04:25,748 train 150 7.785217e-03 0.730629
2019-12-03 19:04:38,110 train 200 7.751554e-03 0.735773
2019-12-03 19:04:43,670 training loss; R2: 7.809310e-03 0.734518
2019-12-03 19:04:43,795 valid 000 4.720343e-03 0.826714
2019-12-03 19:04:45,126 validation loss; R2: 3.764713e-03 0.867102
2019-12-03 19:04:45,152 epoch 10 lr 2.000000e-03
2019-12-03 19:04:45,476 train 000 9.661064e-03 0.656885
2019-12-03 19:04:57,824 train 050 8.379184e-03 0.704275
2019-12-03 19:05:10,173 train 100 7.611169e-03 0.736888
2019-12-03 19:05:22,513 train 150 7.723171e-03 0.733522
2019-12-03 19:05:34,848 train 200 7.763895e-03 0.733501
2019-12-03 19:05:40,397 training loss; R2: 7.742414e-03 0.734074
2019-12-03 19:05:40,517 valid 000 1.062709e-02 0.865234
2019-12-03 19:05:41,847 validation loss; R2: 4.406136e-03 0.851283
2019-12-03 19:05:41,869 epoch 11 lr 2.000000e-03
2019-12-03 19:05:42,184 train 000 1.073941e-02 0.212981
2019-12-03 19:05:54,513 train 050 7.160051e-03 0.773383
2019-12-03 19:06:06,850 train 100 6.872612e-03 0.763852
2019-12-03 19:06:19,178 train 150 6.975104e-03 0.759739
2019-12-03 19:06:31,505 train 200 7.031113e-03 0.761365
2019-12-03 19:06:37,050 training loss; R2: 7.022604e-03 0.761153
2019-12-03 19:06:37,173 valid 000 4.241410e-03 0.872181
2019-12-03 19:06:38,504 validation loss; R2: 3.955600e-03 0.867655
2019-12-03 19:06:38,526 epoch 12 lr 2.000000e-03
2019-12-03 19:06:38,840 train 000 3.705787e-03 0.897230
2019-12-03 19:06:51,161 train 050 7.023262e-03 0.757560
2019-12-03 19:07:03,483 train 100 6.413439e-03 0.774006
2019-12-03 19:07:15,804 train 150 6.638548e-03 0.768286
2019-12-03 19:07:28,132 train 200 6.930280e-03 0.761754
2019-12-03 19:07:33,672 training loss; R2: 6.870730e-03 0.763044
2019-12-03 19:07:33,793 valid 000 3.937362e-03 0.816805
2019-12-03 19:07:35,121 validation loss; R2: 5.010440e-03 0.834105
2019-12-03 19:07:35,143 epoch 13 lr 2.000000e-03
2019-12-03 19:07:35,470 train 000 3.824805e-03 0.851885
2019-12-03 19:07:47,788 train 050 6.455346e-03 0.770480
2019-12-03 19:08:00,106 train 100 6.614830e-03 0.768022
2019-12-03 19:08:12,423 train 150 6.745834e-03 0.770707
2019-12-03 19:08:24,740 train 200 6.716436e-03 0.770626
2019-12-03 19:08:30,275 training loss; R2: 6.744287e-03 0.773712
2019-12-03 19:08:30,393 valid 000 6.258268e-03 0.778652
2019-12-03 19:08:31,722 validation loss; R2: 5.729734e-03 0.797410
2019-12-03 19:08:31,744 epoch 14 lr 2.000000e-03
2019-12-03 19:08:32,055 train 000 6.889466e-03 0.621888
2019-12-03 19:08:44,369 train 050 5.956947e-03 0.790938
2019-12-03 19:08:56,684 train 100 6.064823e-03 0.789136
2019-12-03 19:09:08,995 train 150 6.109951e-03 0.789709
2019-12-03 19:09:21,303 train 200 6.025602e-03 0.794445
2019-12-03 19:09:26,836 training loss; R2: 6.036157e-03 0.794549
2019-12-03 19:09:26,957 valid 000 1.738857e-03 0.945591
2019-12-03 19:09:28,287 validation loss; R2: 3.408485e-03 0.884587
2019-12-03 19:09:28,307 epoch 15 lr 2.000000e-03
2019-12-03 19:09:28,623 train 000 5.720831e-03 0.791788
2019-12-03 19:09:40,926 train 050 5.824493e-03 0.789429
2019-12-03 19:09:53,227 train 100 5.989750e-03 0.792447
2019-12-03 19:10:05,530 train 150 6.119441e-03 0.796593
2019-12-03 19:10:17,832 train 200 5.895106e-03 0.803095
2019-12-03 19:10:23,365 training loss; R2: 5.887908e-03 0.803800
2019-12-03 19:10:23,486 valid 000 7.297699e-02 -0.287612
2019-12-03 19:10:24,815 validation loss; R2: 5.390999e-02 -0.926207
2019-12-03 19:10:24,838 epoch 16 lr 2.000000e-03
2019-12-03 19:10:25,153 train 000 6.918103e-03 0.762027
2019-12-03 19:10:37,456 train 050 5.418534e-03 0.817625
2019-12-03 19:10:49,762 train 100 5.598174e-03 0.812244
2019-12-03 19:11:02,074 train 150 5.978701e-03 0.802587
2019-12-03 19:11:14,375 train 200 6.036092e-03 0.795502
2019-12-03 19:11:19,903 training loss; R2: 6.019559e-03 0.796677
2019-12-03 19:11:20,024 valid 000 3.179548e-02 -0.391217
2019-12-03 19:11:21,352 validation loss; R2: 4.067547e-02 -0.432363
2019-12-03 19:11:21,374 epoch 17 lr 2.000000e-03
2019-12-03 19:11:21,692 train 000 3.671027e-03 0.863124
2019-12-03 19:11:33,981 train 050 6.154538e-03 0.803731
2019-12-03 19:11:46,276 train 100 5.822160e-03 0.805905
2019-12-03 19:11:58,573 train 150 5.935278e-03 0.804873
2019-12-03 19:12:10,867 train 200 5.982103e-03 0.799752
2019-12-03 19:12:16,395 training loss; R2: 6.019394e-03 0.800899
2019-12-03 19:12:16,514 valid 000 6.575940e-02 -3.539383
2019-12-03 19:12:17,841 validation loss; R2: 6.870028e-02 -1.422251
2019-12-03 19:12:17,863 epoch 18 lr 2.000000e-03
2019-12-03 19:12:18,178 train 000 1.034877e-02 0.825935
2019-12-03 19:12:30,465 train 050 5.501298e-03 0.800529
2019-12-03 19:12:42,752 train 100 5.468606e-03 0.815648
2019-12-03 19:12:55,038 train 150 5.436767e-03 0.809484
2019-12-03 19:13:07,321 train 200 5.553750e-03 0.811569
2019-12-03 19:13:12,843 training loss; R2: 5.589820e-03 0.811734
2019-12-03 19:13:12,963 valid 000 9.057179e-02 -2.020791
2019-12-03 19:13:14,290 validation loss; R2: 8.550197e-02 -2.043644
2019-12-03 19:13:14,311 epoch 19 lr 2.000000e-03
2019-12-03 19:13:14,626 train 000 4.011305e-03 0.766569
2019-12-03 19:13:26,901 train 050 7.401817e-03 0.777882
2019-12-03 19:13:39,180 train 100 6.400600e-03 0.789295
2019-12-03 19:13:51,458 train 150 5.913538e-03 0.804141
2019-12-03 19:14:03,735 train 200 5.951648e-03 0.804490
2019-12-03 19:14:09,257 training loss; R2: 5.988026e-03 0.804378
2019-12-03 19:14:09,377 valid 000 6.516310e-02 -1.679446
2019-12-03 19:14:10,705 validation loss; R2: 7.286270e-02 -1.591146
