2019-12-04 00:25:14,919 gpu device = 1
2019-12-04 00:25:14,919 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=96, cutout=False, cutout_length=16, data='../data', dataset='graphene', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name=None, gpu=1, grad_clip=5, gz_dtree=False, init_channels=36, layers=8, learning_rate=0.002, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-graphene-EXP-20191204-002514', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-12-04 00:25:18,120 param size = 1.158805MB
2019-12-04 00:25:18,123 epoch 0 lr 2.000000e-03
2019-12-04 00:25:20,798 train 000 2.903228e+00 -100.097927
2019-12-04 00:25:33,326 train 050 2.149929e-01 -6.742113
2019-12-04 00:25:45,759 train 100 1.295182e-01 -3.585151
2019-12-04 00:25:58,187 train 150 9.700486e-02 -2.406767
2019-12-04 00:26:10,613 train 200 8.097837e-02 -1.802544
2019-12-04 00:26:17,307 training loss; R2: 7.532935e-02 -1.602538
2019-12-04 00:26:17,454 valid 000 4.544945e-02 0.366715
2019-12-04 00:26:19,090 validation loss; R2: 1.732371e-02 0.459375
2019-12-04 00:26:19,116 epoch 1 lr 2.000000e-03
2019-12-04 00:26:19,571 train 000 2.020112e-02 0.387728
2019-12-04 00:26:32,006 train 050 2.811338e-02 0.142194
2019-12-04 00:26:44,437 train 100 2.491694e-02 0.217707
2019-12-04 00:26:56,876 train 150 2.360247e-02 0.250307
2019-12-04 00:27:09,311 train 200 2.281889e-02 0.280314
2019-12-04 00:27:14,905 training loss; R2: 2.239565e-02 0.288796
2019-12-04 00:27:15,043 valid 000 2.503644e-02 0.534358
2019-12-04 00:27:16,359 validation loss; R2: 1.561158e-02 0.507188
2019-12-04 00:27:16,381 epoch 2 lr 2.000000e-03
2019-12-04 00:27:16,713 train 000 1.278328e-02 0.388605
2019-12-04 00:27:29,144 train 050 1.648458e-02 0.417002
2019-12-04 00:27:41,572 train 100 1.684055e-02 0.436371
2019-12-04 00:27:54,009 train 150 1.587249e-02 0.464705
2019-12-04 00:28:06,441 train 200 1.611291e-02 0.470763
2019-12-04 00:28:12,029 training loss; R2: 1.581341e-02 0.482980
2019-12-04 00:28:12,168 valid 000 1.414246e-02 0.751570
2019-12-04 00:28:13,484 validation loss; R2: 9.559029e-03 0.683714
2019-12-04 00:28:13,507 epoch 3 lr 2.000000e-03
2019-12-04 00:28:13,840 train 000 1.028720e-02 0.590605
2019-12-04 00:28:26,275 train 050 1.404983e-02 0.547875
2019-12-04 00:28:38,714 train 100 1.335506e-02 0.566462
2019-12-04 00:28:51,142 train 150 1.310770e-02 0.573581
2019-12-04 00:29:03,571 train 200 1.289181e-02 0.582420
2019-12-04 00:29:09,161 training loss; R2: 1.267642e-02 0.592150
2019-12-04 00:29:09,293 valid 000 6.228736e-03 0.651198
2019-12-04 00:29:10,609 validation loss; R2: 1.137209e-02 0.632478
2019-12-04 00:29:10,631 epoch 4 lr 2.000000e-03
2019-12-04 00:29:10,967 train 000 6.279757e-03 0.551405
2019-12-04 00:29:23,393 train 050 1.171894e-02 0.612343
2019-12-04 00:29:35,822 train 100 1.184601e-02 0.612722
2019-12-04 00:29:48,256 train 150 1.136921e-02 0.624048
2019-12-04 00:30:00,689 train 200 1.091486e-02 0.631001
2019-12-04 00:30:06,280 training loss; R2: 1.075860e-02 0.635613
2019-12-04 00:30:06,413 valid 000 4.432022e-03 0.815653
2019-12-04 00:30:07,729 validation loss; R2: 5.336448e-03 0.827242
2019-12-04 00:30:07,751 epoch 5 lr 2.000000e-03
2019-12-04 00:30:08,084 train 000 2.787048e-02 0.421032
2019-12-04 00:30:20,520 train 050 9.747313e-03 0.662942
2019-12-04 00:30:32,959 train 100 1.038029e-02 0.665537
2019-12-04 00:30:45,394 train 150 1.053056e-02 0.657808
2019-12-04 00:30:57,831 train 200 1.016155e-02 0.663721
2019-12-04 00:31:03,424 training loss; R2: 1.003850e-02 0.668026
2019-12-04 00:31:03,562 valid 000 1.107508e-02 0.761120
2019-12-04 00:31:04,878 validation loss; R2: 5.992276e-03 0.805297
2019-12-04 00:31:04,900 epoch 6 lr 2.000000e-03
2019-12-04 00:31:05,231 train 000 8.088510e-03 0.556588
2019-12-04 00:31:17,672 train 050 9.730293e-03 0.709520
2019-12-04 00:31:30,114 train 100 9.254511e-03 0.692735
2019-12-04 00:31:42,555 train 150 9.028454e-03 0.700199
2019-12-04 00:31:54,994 train 200 8.895184e-03 0.696812
2019-12-04 00:32:00,588 training loss; R2: 8.872714e-03 0.699119
2019-12-04 00:32:00,720 valid 000 3.249640e-03 0.721184
2019-12-04 00:32:02,036 validation loss; R2: 4.978683e-03 0.831926
2019-12-04 00:32:02,066 epoch 7 lr 2.000000e-03
2019-12-04 00:32:02,398 train 000 5.046024e-03 0.668006
2019-12-04 00:32:14,835 train 050 8.435587e-03 0.709084
2019-12-04 00:32:27,262 train 100 8.241730e-03 0.716002
2019-12-04 00:32:39,689 train 150 8.856924e-03 0.702593
2019-12-04 00:32:52,121 train 200 8.834155e-03 0.707154
2019-12-04 00:32:57,708 training loss; R2: 8.795981e-03 0.704687
2019-12-04 00:32:57,842 valid 000 4.794082e-03 0.840397
2019-12-04 00:32:59,161 validation loss; R2: 6.107775e-03 0.798045
2019-12-04 00:32:59,184 epoch 8 lr 2.000000e-03
2019-12-04 00:32:59,516 train 000 1.069479e-02 0.644836
2019-12-04 00:33:11,947 train 050 8.949011e-03 0.683049
2019-12-04 00:33:24,385 train 100 8.393949e-03 0.694587
2019-12-04 00:33:36,821 train 150 8.439654e-03 0.706978
2019-12-04 00:33:49,254 train 200 8.174492e-03 0.721178
2019-12-04 00:33:54,843 training loss; R2: 8.353537e-03 0.717876
2019-12-04 00:33:54,981 valid 000 1.245165e-02 0.685126
2019-12-04 00:33:56,297 validation loss; R2: 7.105824e-03 0.764879
2019-12-04 00:33:56,320 epoch 9 lr 2.000000e-03
2019-12-04 00:33:56,652 train 000 6.645478e-03 0.677380
2019-12-04 00:34:09,087 train 050 7.705852e-03 0.752343
2019-12-04 00:34:21,525 train 100 8.100062e-03 0.737508
2019-12-04 00:34:33,967 train 150 8.069510e-03 0.729453
2019-12-04 00:34:46,413 train 200 7.944246e-03 0.734995
2019-12-04 00:34:52,010 training loss; R2: 8.034797e-03 0.734054
2019-12-04 00:34:52,143 valid 000 6.645337e-03 0.749559
2019-12-04 00:34:53,459 validation loss; R2: 7.520868e-03 0.753783
2019-12-04 00:34:53,482 epoch 10 lr 2.000000e-03
2019-12-04 00:34:53,811 train 000 3.815280e-03 0.681966
2019-12-04 00:35:06,245 train 050 7.948015e-03 0.738965
2019-12-04 00:35:18,677 train 100 8.945230e-03 0.720523
2019-12-04 00:35:31,105 train 150 8.578573e-03 0.721924
2019-12-04 00:35:43,531 train 200 8.214457e-03 0.731634
2019-12-04 00:35:49,122 training loss; R2: 8.085216e-03 0.735811
2019-12-04 00:35:49,262 valid 000 5.306995e-03 0.872769
2019-12-04 00:35:50,578 validation loss; R2: 5.105230e-03 0.826102
2019-12-04 00:35:50,600 epoch 11 lr 2.000000e-03
2019-12-04 00:35:50,931 train 000 5.364471e-03 0.822854
2019-12-04 00:36:03,363 train 050 6.855222e-03 0.748292
2019-12-04 00:36:15,799 train 100 6.587994e-03 0.768569
2019-12-04 00:36:28,228 train 150 6.541270e-03 0.772594
2019-12-04 00:36:40,665 train 200 6.898378e-03 0.767494
2019-12-04 00:36:46,265 training loss; R2: 6.948391e-03 0.768582
2019-12-04 00:36:46,397 valid 000 5.971068e-03 0.783834
2019-12-04 00:36:47,713 validation loss; R2: 7.341381e-03 0.758088
2019-12-04 00:36:47,736 epoch 12 lr 2.000000e-03
2019-12-04 00:36:48,067 train 000 6.871695e-03 0.702526
2019-12-04 00:37:00,506 train 050 7.783856e-03 0.735001
2019-12-04 00:37:12,947 train 100 7.423537e-03 0.751316
2019-12-04 00:37:25,384 train 150 7.665878e-03 0.748385
2019-12-04 00:37:37,824 train 200 7.744505e-03 0.746073
2019-12-04 00:37:43,419 training loss; R2: 7.627436e-03 0.745903
2019-12-04 00:37:43,552 valid 000 3.854857e-03 0.900987
2019-12-04 00:37:44,869 validation loss; R2: 3.877587e-03 0.868692
2019-12-04 00:37:44,892 epoch 13 lr 2.000000e-03
2019-12-04 00:37:45,225 train 000 4.981915e-03 0.857817
2019-12-04 00:37:57,660 train 050 6.323332e-03 0.765355
2019-12-04 00:38:10,096 train 100 6.969316e-03 0.758628
2019-12-04 00:38:22,531 train 150 7.269367e-03 0.753701
2019-12-04 00:38:34,970 train 200 7.199359e-03 0.754918
2019-12-04 00:38:40,567 training loss; R2: 7.168784e-03 0.758106
2019-12-04 00:38:40,699 valid 000 8.175726e-03 0.793306
2019-12-04 00:38:42,015 validation loss; R2: 6.541142e-03 0.791989
2019-12-04 00:38:42,039 epoch 14 lr 2.000000e-03
2019-12-04 00:38:42,371 train 000 7.164242e-03 0.621718
2019-12-04 00:38:54,811 train 050 6.656536e-03 0.782963
2019-12-04 00:39:07,240 train 100 6.561595e-03 0.784639
2019-12-04 00:39:19,669 train 150 6.368396e-03 0.786045
2019-12-04 00:39:32,095 train 200 6.546236e-03 0.776272
2019-12-04 00:39:37,683 training loss; R2: 6.533749e-03 0.777843
2019-12-04 00:39:37,820 valid 000 4.413706e-03 0.807511
2019-12-04 00:39:39,136 validation loss; R2: 4.328086e-03 0.849192
2019-12-04 00:39:39,159 epoch 15 lr 2.000000e-03
2019-12-04 00:39:39,494 train 000 7.225832e-03 0.872391
2019-12-04 00:39:51,922 train 050 7.066816e-03 0.795932
2019-12-04 00:40:04,355 train 100 6.578559e-03 0.783214
2019-12-04 00:40:16,792 train 150 6.382362e-03 0.789644
2019-12-04 00:40:29,233 train 200 6.262547e-03 0.791284
2019-12-04 00:40:34,824 training loss; R2: 6.203628e-03 0.795063
2019-12-04 00:40:34,964 valid 000 4.239071e-03 0.840664
2019-12-04 00:40:36,280 validation loss; R2: 4.935988e-03 0.840698
2019-12-04 00:40:36,304 epoch 16 lr 2.000000e-03
2019-12-04 00:40:36,636 train 000 6.039600e-03 0.743516
2019-12-04 00:40:49,062 train 050 6.893960e-03 0.784143
2019-12-04 00:41:01,493 train 100 6.680784e-03 0.778096
2019-12-04 00:41:13,919 train 150 6.984515e-03 0.777114
2019-12-04 00:41:26,343 train 200 6.509352e-03 0.786934
2019-12-04 00:41:31,930 training loss; R2: 6.416107e-03 0.789570
2019-12-04 00:41:32,071 valid 000 6.882265e-03 0.887318
2019-12-04 00:41:33,387 validation loss; R2: 3.494231e-03 0.885469
2019-12-04 00:41:33,411 epoch 17 lr 2.000000e-03
2019-12-04 00:41:33,746 train 000 4.783201e-03 0.770845
2019-12-04 00:41:46,167 train 050 6.198640e-03 0.781650
2019-12-04 00:41:58,582 train 100 6.403853e-03 0.779755
2019-12-04 00:42:10,996 train 150 6.471466e-03 0.786348
2019-12-04 00:42:23,407 train 200 6.425775e-03 0.784028
2019-12-04 00:42:28,989 training loss; R2: 6.411906e-03 0.783322
2019-12-04 00:42:29,126 valid 000 7.062934e-03 0.894719
2019-12-04 00:42:30,441 validation loss; R2: 4.787903e-03 0.847291
2019-12-04 00:42:30,465 epoch 18 lr 2.000000e-03
2019-12-04 00:42:30,796 train 000 5.184139e-03 0.618751
2019-12-04 00:42:43,222 train 050 6.223772e-03 0.769896
2019-12-04 00:42:55,659 train 100 6.137534e-03 0.775368
2019-12-04 00:43:08,096 train 150 6.342162e-03 0.778010
2019-12-04 00:43:20,529 train 200 6.261928e-03 0.780949
2019-12-04 00:43:26,125 training loss; R2: 6.342785e-03 0.781910
2019-12-04 00:43:26,269 valid 000 2.608921e-03 0.919906
2019-12-04 00:43:27,585 validation loss; R2: 3.761472e-03 0.871983
2019-12-04 00:43:27,614 epoch 19 lr 2.000000e-03
2019-12-04 00:43:27,947 train 000 5.201695e-03 0.832573
2019-12-04 00:43:40,357 train 050 7.027276e-03 0.768575
2019-12-04 00:43:52,764 train 100 7.025855e-03 0.767830
2019-12-04 00:44:05,169 train 150 7.079412e-03 0.766494
2019-12-04 00:44:17,571 train 200 6.730178e-03 0.776266
2019-12-04 00:44:23,157 training loss; R2: 6.638243e-03 0.777227
2019-12-04 00:44:23,297 valid 000 3.655480e-03 0.841441
2019-12-04 00:44:24,611 validation loss; R2: 4.617036e-03 0.837957
