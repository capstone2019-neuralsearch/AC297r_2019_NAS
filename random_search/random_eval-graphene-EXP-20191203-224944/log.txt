2019-12-03 22:49:44,637 gpu device = 1
2019-12-03 22:49:44,637 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=96, cutout=False, cutout_length=16, data='../data', dataset='graphene', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name=None, gpu=1, grad_clip=5, gz_dtree=False, init_channels=36, layers=8, learning_rate=0.002, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-graphene-EXP-20191203-224944', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-12-03 22:49:47,862 param size = 0.900181MB
2019-12-03 22:49:47,865 epoch 0 lr 2.000000e-03
2019-12-03 22:49:50,336 train 000 2.140835e+00 -58.271113
2019-12-03 22:50:01,028 train 050 3.816713e-01 -9.091000
2019-12-03 22:50:11,648 train 100 2.188608e-01 -4.933020
2019-12-03 22:50:22,068 train 150 1.580888e-01 -3.371583
2019-12-03 22:50:32,487 train 200 1.274361e-01 -2.553382
2019-12-03 22:50:38,189 training loss; R2: 1.172849e-01 -2.283709
2019-12-03 22:50:38,323 valid 000 1.646275e-02 -0.419181
2019-12-03 22:50:39,808 validation loss; R2: 2.691716e-02 0.094898
2019-12-03 22:50:39,823 epoch 1 lr 2.000000e-03
2019-12-03 22:50:40,167 train 000 1.436935e-02 -0.037227
2019-12-03 22:50:50,587 train 050 2.658334e-02 0.108309
2019-12-03 22:51:01,011 train 100 2.590329e-02 0.152757
2019-12-03 22:51:11,437 train 150 2.653282e-02 0.134850
2019-12-03 22:51:21,861 train 200 2.574413e-02 0.165292
2019-12-03 22:51:26,547 training loss; R2: 2.550545e-02 0.170723
2019-12-03 22:51:26,679 valid 000 1.065466e-02 0.260647
2019-12-03 22:51:27,834 validation loss; R2: 1.616142e-02 0.471186
2019-12-03 22:51:27,850 epoch 2 lr 2.000000e-03
2019-12-03 22:51:28,136 train 000 1.829783e-02 0.283953
2019-12-03 22:51:38,563 train 050 2.344397e-02 0.298064
2019-12-03 22:51:48,990 train 100 2.109162e-02 0.349092
2019-12-03 22:51:59,413 train 150 1.920607e-02 0.404068
2019-12-03 22:52:09,845 train 200 1.831741e-02 0.415745
2019-12-03 22:52:14,534 training loss; R2: 1.816098e-02 0.415865
2019-12-03 22:52:14,660 valid 000 4.846325e-03 0.713235
2019-12-03 22:52:15,816 validation loss; R2: 8.288742e-03 0.732293
2019-12-03 22:52:15,831 epoch 3 lr 2.000000e-03
2019-12-03 22:52:16,120 train 000 1.109977e-02 0.599108
2019-12-03 22:52:26,540 train 050 1.544549e-02 0.516237
2019-12-03 22:52:36,960 train 100 1.525161e-02 0.519870
2019-12-03 22:52:47,379 train 150 1.488970e-02 0.522623
2019-12-03 22:52:57,799 train 200 1.411641e-02 0.543704
2019-12-03 22:53:02,482 training loss; R2: 1.393716e-02 0.544533
2019-12-03 22:53:02,608 valid 000 1.346693e-02 0.678150
2019-12-03 22:53:03,764 validation loss; R2: 9.765651e-03 0.650303
2019-12-03 22:53:03,780 epoch 4 lr 2.000000e-03
2019-12-03 22:53:04,072 train 000 1.255360e-02 0.528167
2019-12-03 22:53:14,500 train 050 1.256175e-02 0.550760
2019-12-03 22:53:24,925 train 100 1.217932e-02 0.574215
2019-12-03 22:53:35,348 train 150 1.218334e-02 0.591611
2019-12-03 22:53:45,774 train 200 1.188052e-02 0.604031
2019-12-03 22:53:50,460 training loss; R2: 1.174223e-02 0.609182
2019-12-03 22:53:50,587 valid 000 3.640280e-03 0.887203
2019-12-03 22:53:51,743 validation loss; R2: 5.374868e-03 0.821979
2019-12-03 22:53:51,758 epoch 5 lr 2.000000e-03
2019-12-03 22:53:52,046 train 000 8.548073e-03 0.692968
2019-12-03 22:54:02,477 train 050 1.040769e-02 0.654744
2019-12-03 22:54:12,914 train 100 1.054492e-02 0.647974
2019-12-03 22:54:23,349 train 150 1.105453e-02 0.647230
2019-12-03 22:54:33,789 train 200 1.065239e-02 0.652189
2019-12-03 22:54:38,479 training loss; R2: 1.033689e-02 0.657753
2019-12-03 22:54:38,613 valid 000 3.323951e-03 0.870398
2019-12-03 22:54:39,769 validation loss; R2: 5.498159e-03 0.822466
2019-12-03 22:54:39,785 epoch 6 lr 2.000000e-03
2019-12-03 22:54:40,071 train 000 8.107889e-03 0.732830
2019-12-03 22:54:50,503 train 050 9.451113e-03 0.683089
2019-12-03 22:55:00,939 train 100 9.523740e-03 0.684212
2019-12-03 22:55:11,368 train 150 9.719899e-03 0.672196
2019-12-03 22:55:21,868 train 200 9.518177e-03 0.674768
2019-12-03 22:55:26,649 training loss; R2: 9.520080e-03 0.678254
2019-12-03 22:55:26,781 valid 000 4.486511e-03 0.774223
2019-12-03 22:55:27,938 validation loss; R2: 5.023441e-03 0.828157
2019-12-03 22:55:27,954 epoch 7 lr 2.000000e-03
2019-12-03 22:55:28,247 train 000 1.132077e-02 0.824610
2019-12-03 22:55:38,872 train 050 1.010519e-02 0.649619
2019-12-03 22:55:49,493 train 100 9.238588e-03 0.676435
2019-12-03 22:56:00,118 train 150 8.793876e-03 0.695085
2019-12-03 22:56:10,741 train 200 8.772856e-03 0.694473
2019-12-03 22:56:15,518 training loss; R2: 8.809569e-03 0.695582
2019-12-03 22:56:15,661 valid 000 5.229322e-03 0.857641
2019-12-03 22:56:16,817 validation loss; R2: 5.164394e-03 0.819540
2019-12-03 22:56:16,833 epoch 8 lr 2.000000e-03
2019-12-03 22:56:17,120 train 000 9.343971e-03 0.758265
2019-12-03 22:56:27,546 train 050 9.302224e-03 0.690001
2019-12-03 22:56:37,973 train 100 8.576614e-03 0.707672
2019-12-03 22:56:48,394 train 150 8.048774e-03 0.723674
2019-12-03 22:56:58,815 train 200 8.207054e-03 0.720726
2019-12-03 22:57:03,500 training loss; R2: 8.249266e-03 0.720276
2019-12-03 22:57:03,630 valid 000 5.797835e-03 0.873239
2019-12-03 22:57:04,786 validation loss; R2: 4.077628e-03 0.857340
2019-12-03 22:57:04,802 epoch 9 lr 2.000000e-03
2019-12-03 22:57:05,092 train 000 6.978583e-03 0.796841
2019-12-03 22:57:15,518 train 050 8.157038e-03 0.730842
2019-12-03 22:57:25,940 train 100 7.559731e-03 0.743410
2019-12-03 22:57:36,363 train 150 7.687020e-03 0.731483
2019-12-03 22:57:46,782 train 200 7.609487e-03 0.738148
2019-12-03 22:57:51,466 training loss; R2: 7.535455e-03 0.738538
2019-12-03 22:57:51,598 valid 000 9.265305e-03 0.832710
2019-12-03 22:57:52,754 validation loss; R2: 5.781772e-03 0.807399
2019-12-03 22:57:52,770 epoch 10 lr 2.000000e-03
2019-12-03 22:57:53,058 train 000 3.679899e-03 0.921501
2019-12-03 22:58:03,484 train 050 7.469403e-03 0.761482
2019-12-03 22:58:13,904 train 100 7.185740e-03 0.763566
2019-12-03 22:58:24,327 train 150 7.801832e-03 0.746153
2019-12-03 22:58:34,746 train 200 7.726044e-03 0.742274
2019-12-03 22:58:39,428 training loss; R2: 7.591417e-03 0.744008
2019-12-03 22:58:39,561 valid 000 2.449899e-03 0.901004
2019-12-03 22:58:40,716 validation loss; R2: 4.609185e-03 0.848838
2019-12-03 22:58:40,731 epoch 11 lr 2.000000e-03
2019-12-03 22:58:41,018 train 000 5.024256e-03 0.832687
2019-12-03 22:58:51,436 train 050 6.600582e-03 0.769866
2019-12-03 22:59:01,856 train 100 6.505600e-03 0.786278
2019-12-03 22:59:12,273 train 150 6.831679e-03 0.775798
2019-12-03 22:59:22,694 train 200 6.932177e-03 0.765099
2019-12-03 22:59:27,378 training loss; R2: 7.077058e-03 0.761302
2019-12-03 22:59:27,502 valid 000 2.582730e-03 0.844767
2019-12-03 22:59:28,658 validation loss; R2: 4.654302e-03 0.817964
2019-12-03 22:59:28,675 epoch 12 lr 2.000000e-03
2019-12-03 22:59:28,966 train 000 6.548929e-03 0.747374
2019-12-03 22:59:39,383 train 050 7.133426e-03 0.733249
2019-12-03 22:59:49,796 train 100 7.004598e-03 0.751113
2019-12-03 23:00:00,211 train 150 7.067571e-03 0.750797
2019-12-03 23:00:10,623 train 200 7.013058e-03 0.759336
2019-12-03 23:00:15,303 training loss; R2: 7.200142e-03 0.756296
2019-12-03 23:00:15,430 valid 000 5.540982e-03 0.830245
2019-12-03 23:00:16,586 validation loss; R2: 6.712766e-03 0.767168
2019-12-03 23:00:16,602 epoch 13 lr 2.000000e-03
2019-12-03 23:00:16,890 train 000 5.172803e-03 0.835934
2019-12-03 23:00:27,304 train 050 6.944247e-03 0.775507
2019-12-03 23:00:37,717 train 100 6.565801e-03 0.774892
2019-12-03 23:00:48,133 train 150 6.476208e-03 0.776421
2019-12-03 23:00:58,549 train 200 6.450228e-03 0.777541
2019-12-03 23:01:03,230 training loss; R2: 6.548018e-03 0.777253
2019-12-03 23:01:03,365 valid 000 3.153529e-03 0.844892
2019-12-03 23:01:04,522 validation loss; R2: 3.887447e-03 0.868426
2019-12-03 23:01:04,538 epoch 14 lr 2.000000e-03
2019-12-03 23:01:04,831 train 000 8.281623e-03 0.852408
2019-12-03 23:01:15,263 train 050 6.040686e-03 0.804057
2019-12-03 23:01:25,870 train 100 6.073000e-03 0.802671
2019-12-03 23:01:36,478 train 150 6.176811e-03 0.791843
2019-12-03 23:01:47,091 train 200 6.431882e-03 0.781641
2019-12-03 23:01:51,862 training loss; R2: 6.460334e-03 0.781738
2019-12-03 23:01:51,995 valid 000 2.984073e-03 0.888269
2019-12-03 23:01:53,152 validation loss; R2: 4.447562e-03 0.858639
2019-12-03 23:01:53,169 epoch 15 lr 2.000000e-03
2019-12-03 23:01:53,461 train 000 5.082182e-03 0.794413
2019-12-03 23:02:04,076 train 050 6.523908e-03 0.781488
2019-12-03 23:02:14,685 train 100 6.210091e-03 0.796827
2019-12-03 23:02:25,296 train 150 5.908848e-03 0.802503
2019-12-03 23:02:35,901 train 200 5.919939e-03 0.800972
2019-12-03 23:02:40,669 training loss; R2: 5.918974e-03 0.801532
2019-12-03 23:02:40,794 valid 000 3.528488e-03 0.771158
2019-12-03 23:02:41,951 validation loss; R2: 6.393822e-03 0.782171
2019-12-03 23:02:41,967 epoch 16 lr 2.000000e-03
2019-12-03 23:02:42,262 train 000 5.366937e-03 0.851569
2019-12-03 23:02:52,868 train 050 6.395873e-03 0.782881
2019-12-03 23:03:03,476 train 100 5.939051e-03 0.795531
2019-12-03 23:03:14,082 train 150 5.961523e-03 0.795551
2019-12-03 23:03:24,683 train 200 6.037201e-03 0.795577
2019-12-03 23:03:29,450 training loss; R2: 6.176620e-03 0.790658
2019-12-03 23:03:29,581 valid 000 3.608205e-03 0.807891
2019-12-03 23:03:30,737 validation loss; R2: 3.328263e-03 0.879524
2019-12-03 23:03:30,753 epoch 17 lr 2.000000e-03
2019-12-03 23:03:31,044 train 000 6.700078e-03 0.829021
2019-12-03 23:03:41,646 train 050 7.612946e-03 0.760406
2019-12-03 23:03:52,248 train 100 7.072270e-03 0.777846
2019-12-03 23:04:02,852 train 150 6.755661e-03 0.781070
2019-12-03 23:04:13,396 train 200 6.382447e-03 0.788497
2019-12-03 23:04:18,069 training loss; R2: 6.262193e-03 0.787901
2019-12-03 23:04:18,195 valid 000 5.487324e-03 0.802241
2019-12-03 23:04:19,350 validation loss; R2: 4.037085e-03 0.862360
2019-12-03 23:04:19,365 epoch 18 lr 2.000000e-03
2019-12-03 23:04:19,652 train 000 5.614964e-03 0.765726
2019-12-03 23:04:30,047 train 050 5.349803e-03 0.818349
2019-12-03 23:04:40,445 train 100 5.670421e-03 0.817325
2019-12-03 23:04:50,840 train 150 5.927952e-03 0.801185
2019-12-03 23:05:01,234 train 200 6.165063e-03 0.800251
2019-12-03 23:05:05,905 training loss; R2: 6.050361e-03 0.800183
2019-12-03 23:05:06,030 valid 000 2.880750e-03 0.892642
2019-12-03 23:05:07,185 validation loss; R2: 3.149991e-03 0.887502
2019-12-03 23:05:07,200 epoch 19 lr 2.000000e-03
2019-12-03 23:05:07,485 train 000 7.491695e-03 0.863549
2019-12-03 23:05:17,878 train 050 6.001011e-03 0.806440
2019-12-03 23:05:28,271 train 100 5.980824e-03 0.792736
2019-12-03 23:05:38,663 train 150 5.777483e-03 0.795406
2019-12-03 23:05:49,057 train 200 5.714334e-03 0.801924
2019-12-03 23:05:53,729 training loss; R2: 5.730299e-03 0.802182
2019-12-03 23:05:53,853 valid 000 2.777133e-03 0.901440
2019-12-03 23:05:55,008 validation loss; R2: 4.721539e-03 0.845001
