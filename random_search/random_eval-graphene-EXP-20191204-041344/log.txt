2019-12-04 04:13:44,666 gpu device = 1
2019-12-04 04:13:44,666 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=96, cutout=False, cutout_length=16, data='../data', dataset='graphene', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name=None, gpu=1, grad_clip=5, gz_dtree=False, init_channels=36, layers=8, learning_rate=0.002, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-graphene-EXP-20191204-041344', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-12-04 04:13:47,870 param size = 0.978805MB
2019-12-04 04:13:47,873 epoch 0 lr 2.000000e-03
2019-12-04 04:13:50,471 train 000 4.542179e-01 -22.244650
2019-12-04 04:14:01,495 train 050 1.369051e-01 -3.370607
2019-12-04 04:14:12,473 train 100 8.471214e-02 -1.718730
2019-12-04 04:14:23,460 train 150 6.571224e-02 -1.125330
2019-12-04 04:14:34,449 train 200 5.575497e-02 -0.807697
2019-12-04 04:14:40,435 training loss; R2: 5.308051e-02 -0.713696
2019-12-04 04:14:40,569 valid 000 1.406826e-02 0.472831
2019-12-04 04:14:42,111 validation loss; R2: 1.775853e-02 0.416990
2019-12-04 04:14:42,129 epoch 1 lr 2.000000e-03
2019-12-04 04:14:42,520 train 000 2.443433e-02 0.312057
2019-12-04 04:14:53,504 train 050 2.624471e-02 0.169453
2019-12-04 04:15:04,494 train 100 2.508923e-02 0.201404
2019-12-04 04:15:15,485 train 150 2.390227e-02 0.237527
2019-12-04 04:15:26,477 train 200 2.304355e-02 0.261161
2019-12-04 04:15:31,421 training loss; R2: 2.259749e-02 0.270611
2019-12-04 04:15:31,555 valid 000 1.568957e-02 0.586293
2019-12-04 04:15:32,748 validation loss; R2: 1.304381e-02 0.584725
2019-12-04 04:15:32,766 epoch 2 lr 2.000000e-03
2019-12-04 04:15:33,066 train 000 1.993909e-02 0.290040
2019-12-04 04:15:44,056 train 050 1.830337e-02 0.383399
2019-12-04 04:15:55,048 train 100 1.655475e-02 0.435834
2019-12-04 04:16:06,044 train 150 1.573517e-02 0.480097
2019-12-04 04:16:17,038 train 200 1.510714e-02 0.500531
2019-12-04 04:16:21,980 training loss; R2: 1.478921e-02 0.512252
2019-12-04 04:16:22,107 valid 000 1.357082e-02 0.620020
2019-12-04 04:16:23,300 validation loss; R2: 7.191360e-03 0.757173
2019-12-04 04:16:23,317 epoch 3 lr 2.000000e-03
2019-12-04 04:16:23,616 train 000 8.312691e-03 0.517886
2019-12-04 04:16:34,614 train 050 1.219673e-02 0.578634
2019-12-04 04:16:45,611 train 100 1.115351e-02 0.603297
2019-12-04 04:16:56,609 train 150 1.153698e-02 0.609714
2019-12-04 04:17:07,612 train 200 1.143034e-02 0.617322
2019-12-04 04:17:12,557 training loss; R2: 1.139940e-02 0.618723
2019-12-04 04:17:12,683 valid 000 5.385398e-03 0.833400
2019-12-04 04:17:13,877 validation loss; R2: 6.286813e-03 0.788751
2019-12-04 04:17:13,895 epoch 4 lr 2.000000e-03
2019-12-04 04:17:14,194 train 000 1.186748e-02 0.519005
2019-12-04 04:17:25,196 train 050 9.199470e-03 0.678765
2019-12-04 04:17:36,197 train 100 9.369035e-03 0.670512
2019-12-04 04:17:47,196 train 150 1.006099e-02 0.650813
2019-12-04 04:17:58,198 train 200 1.008426e-02 0.663602
2019-12-04 04:18:03,145 training loss; R2: 1.007524e-02 0.660046
2019-12-04 04:18:03,278 valid 000 7.987503e-03 0.779470
2019-12-04 04:18:04,471 validation loss; R2: 8.666188e-03 0.684709
2019-12-04 04:18:04,489 epoch 5 lr 2.000000e-03
2019-12-04 04:18:04,786 train 000 8.988864e-03 0.776821
2019-12-04 04:18:15,785 train 050 9.526122e-03 0.672491
2019-12-04 04:18:26,785 train 100 9.158217e-03 0.677974
2019-12-04 04:18:37,783 train 150 9.269229e-03 0.674694
2019-12-04 04:18:48,785 train 200 9.086123e-03 0.688522
2019-12-04 04:18:53,730 training loss; R2: 9.206677e-03 0.688477
2019-12-04 04:18:53,863 valid 000 1.548915e-02 0.433320
2019-12-04 04:18:55,056 validation loss; R2: 1.209238e-02 0.590342
2019-12-04 04:18:55,074 epoch 6 lr 2.000000e-03
2019-12-04 04:18:55,374 train 000 1.879818e-02 0.227835
2019-12-04 04:19:06,371 train 050 1.207139e-02 0.593752
2019-12-04 04:19:17,368 train 100 1.035117e-02 0.646561
2019-12-04 04:19:28,366 train 150 9.413013e-03 0.679861
2019-12-04 04:19:39,362 train 200 8.805912e-03 0.704513
2019-12-04 04:19:44,305 training loss; R2: 8.533451e-03 0.710095
2019-12-04 04:19:44,444 valid 000 3.941808e-03 0.870151
2019-12-04 04:19:45,637 validation loss; R2: 4.676064e-03 0.844865
2019-12-04 04:19:45,655 epoch 7 lr 2.000000e-03
2019-12-04 04:19:45,951 train 000 8.797761e-03 0.707264
2019-12-04 04:19:56,950 train 050 7.425699e-03 0.721393
2019-12-04 04:20:07,945 train 100 7.159413e-03 0.749173
2019-12-04 04:20:18,941 train 150 7.845976e-03 0.743471
2019-12-04 04:20:29,936 train 200 7.708555e-03 0.745874
2019-12-04 04:20:34,880 training loss; R2: 7.606223e-03 0.746938
2019-12-04 04:20:35,008 valid 000 6.640845e-03 0.793378
2019-12-04 04:20:36,202 validation loss; R2: 7.013437e-03 0.752819
2019-12-04 04:20:36,219 epoch 8 lr 2.000000e-03
2019-12-04 04:20:36,521 train 000 9.056367e-03 0.846669
2019-12-04 04:20:47,518 train 050 7.222281e-03 0.755169
2019-12-04 04:20:58,515 train 100 7.361096e-03 0.758090
2019-12-04 04:21:09,511 train 150 7.365335e-03 0.751562
2019-12-04 04:21:20,507 train 200 7.273973e-03 0.751189
2019-12-04 04:21:25,451 training loss; R2: 7.231740e-03 0.753246
2019-12-04 04:21:25,579 valid 000 6.021888e-03 0.610223
2019-12-04 04:21:26,773 validation loss; R2: 6.287200e-03 0.784193
2019-12-04 04:21:26,791 epoch 9 lr 2.000000e-03
2019-12-04 04:21:27,087 train 000 5.117124e-03 0.842105
2019-12-04 04:21:38,080 train 050 8.360587e-03 0.724825
2019-12-04 04:21:49,075 train 100 7.893738e-03 0.735137
2019-12-04 04:22:00,071 train 150 7.624313e-03 0.734584
2019-12-04 04:22:11,065 train 200 7.424365e-03 0.740087
2019-12-04 04:22:16,009 training loss; R2: 7.330367e-03 0.744506
2019-12-04 04:22:16,142 valid 000 9.716784e-03 0.834326
2019-12-04 04:22:17,335 validation loss; R2: 5.756332e-03 0.803721
2019-12-04 04:22:17,353 epoch 10 lr 2.000000e-03
2019-12-04 04:22:17,652 train 000 7.567085e-03 0.740354
2019-12-04 04:22:28,648 train 050 6.400480e-03 0.784531
2019-12-04 04:22:39,645 train 100 6.692375e-03 0.765734
2019-12-04 04:22:50,644 train 150 6.455049e-03 0.769391
2019-12-04 04:23:01,638 train 200 6.737306e-03 0.764711
2019-12-04 04:23:06,580 training loss; R2: 6.701536e-03 0.765880
2019-12-04 04:23:06,707 valid 000 4.524663e-03 0.827842
2019-12-04 04:23:07,901 validation loss; R2: 5.140461e-03 0.819130
2019-12-04 04:23:07,917 epoch 11 lr 2.000000e-03
2019-12-04 04:23:08,215 train 000 4.609263e-03 0.796542
2019-12-04 04:23:19,206 train 050 6.868261e-03 0.777360
2019-12-04 04:23:30,197 train 100 6.538963e-03 0.780462
2019-12-04 04:23:41,187 train 150 6.715000e-03 0.770068
2019-12-04 04:23:52,185 train 200 6.444123e-03 0.771245
2019-12-04 04:23:57,129 training loss; R2: 6.421110e-03 0.773629
2019-12-04 04:23:57,254 valid 000 4.104090e-03 0.877146
2019-12-04 04:23:58,448 validation loss; R2: 3.805793e-03 0.864236
2019-12-04 04:23:58,465 epoch 12 lr 2.000000e-03
2019-12-04 04:23:58,763 train 000 6.811326e-03 0.874543
2019-12-04 04:24:09,948 train 050 5.933174e-03 0.797353
2019-12-04 04:24:21,210 train 100 5.827997e-03 0.799327
2019-12-04 04:24:32,455 train 150 6.098930e-03 0.787826
2019-12-04 04:24:43,689 train 200 5.964201e-03 0.795009
2019-12-04 04:24:48,741 training loss; R2: 5.977603e-03 0.792113
2019-12-04 04:24:48,871 valid 000 4.307577e-03 0.856123
2019-12-04 04:24:50,065 validation loss; R2: 3.555162e-03 0.874978
2019-12-04 04:24:50,091 epoch 13 lr 2.000000e-03
2019-12-04 04:24:50,396 train 000 5.075159e-03 0.556979
2019-12-04 04:25:01,631 train 050 6.366765e-03 0.763961
2019-12-04 04:25:12,865 train 100 5.798231e-03 0.782454
2019-12-04 04:25:24,099 train 150 5.728300e-03 0.794401
2019-12-04 04:25:35,331 train 200 5.876711e-03 0.789399
2019-12-04 04:25:40,384 training loss; R2: 5.998863e-03 0.789828
2019-12-04 04:25:40,514 valid 000 3.064588e-03 0.863879
2019-12-04 04:25:41,709 validation loss; R2: 3.842640e-03 0.868811
2019-12-04 04:25:41,728 epoch 14 lr 2.000000e-03
2019-12-04 04:25:42,035 train 000 1.014519e-02 0.532744
2019-12-04 04:25:53,275 train 050 5.721714e-03 0.802765
2019-12-04 04:26:04,512 train 100 5.674611e-03 0.809996
2019-12-04 04:26:15,749 train 150 5.716779e-03 0.811431
2019-12-04 04:26:26,985 train 200 5.836305e-03 0.807515
2019-12-04 04:26:32,037 training loss; R2: 5.744449e-03 0.807520
2019-12-04 04:26:32,177 valid 000 1.979243e-02 0.459159
2019-12-04 04:26:33,371 validation loss; R2: 1.831114e-02 0.314710
2019-12-04 04:26:33,389 epoch 15 lr 2.000000e-03
2019-12-04 04:26:33,695 train 000 5.730001e-03 0.570714
2019-12-04 04:26:44,930 train 050 5.141023e-03 0.824948
2019-12-04 04:26:56,159 train 100 5.557709e-03 0.815392
2019-12-04 04:27:07,393 train 150 5.873039e-03 0.797918
2019-12-04 04:27:18,627 train 200 6.055722e-03 0.796178
2019-12-04 04:27:23,679 training loss; R2: 5.889625e-03 0.800280
2019-12-04 04:27:23,809 valid 000 3.143949e-02 -0.725782
2019-12-04 04:27:25,002 validation loss; R2: 4.328576e-02 -0.506123
2019-12-04 04:27:25,022 epoch 16 lr 2.000000e-03
2019-12-04 04:27:25,327 train 000 1.165117e-02 0.805935
2019-12-04 04:27:36,554 train 050 5.790926e-03 0.809829
2019-12-04 04:27:47,780 train 100 5.351458e-03 0.825787
2019-12-04 04:27:59,003 train 150 5.162699e-03 0.825756
2019-12-04 04:28:10,229 train 200 5.356997e-03 0.822805
2019-12-04 04:28:15,277 training loss; R2: 5.393906e-03 0.818110
2019-12-04 04:28:15,405 valid 000 7.345033e-02 -1.326280
2019-12-04 04:28:16,597 validation loss; R2: 7.651590e-02 -1.621865
2019-12-04 04:28:16,621 epoch 17 lr 2.000000e-03
2019-12-04 04:28:16,925 train 000 9.701205e-03 0.779031
2019-12-04 04:28:28,147 train 050 5.422164e-03 0.813073
2019-12-04 04:28:39,366 train 100 5.473803e-03 0.813976
2019-12-04 04:28:50,588 train 150 5.351687e-03 0.821470
2019-12-04 04:29:01,806 train 200 5.293929e-03 0.818622
2019-12-04 04:29:06,851 training loss; R2: 5.228159e-03 0.820361
2019-12-04 04:29:06,981 valid 000 7.392200e-02 -0.649090
2019-12-04 04:29:08,173 validation loss; R2: 7.499322e-02 -1.673978
2019-12-04 04:29:08,192 epoch 18 lr 2.000000e-03
2019-12-04 04:29:08,497 train 000 5.067882e-03 0.865683
2019-12-04 04:29:19,668 train 050 5.635701e-03 0.803083
2019-12-04 04:29:30,628 train 100 5.393889e-03 0.809765
2019-12-04 04:29:41,591 train 150 5.385250e-03 0.815187
2019-12-04 04:29:52,550 train 200 5.320467e-03 0.819343
2019-12-04 04:29:57,475 training loss; R2: 5.356240e-03 0.817925
2019-12-04 04:29:57,603 valid 000 8.932238e-02 -1.474306
2019-12-04 04:29:58,795 validation loss; R2: 8.491437e-02 -2.129840
2019-12-04 04:29:58,813 epoch 19 lr 2.000000e-03
2019-12-04 04:29:59,111 train 000 4.307354e-03 0.864182
2019-12-04 04:30:10,063 train 050 4.853541e-03 0.829608
2019-12-04 04:30:21,012 train 100 5.180256e-03 0.813569
2019-12-04 04:30:31,960 train 150 5.114914e-03 0.816945
2019-12-04 04:30:42,908 train 200 5.149081e-03 0.821589
2019-12-04 04:30:47,829 training loss; R2: 5.163198e-03 0.822861
2019-12-04 04:30:47,963 valid 000 3.689094e-03 0.807284
2019-12-04 04:30:49,155 validation loss; R2: 3.853542e-03 0.862721
