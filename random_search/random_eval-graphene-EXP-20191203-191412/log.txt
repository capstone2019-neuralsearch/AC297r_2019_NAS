2019-12-03 19:14:12,340 gpu device = 1
2019-12-03 19:14:12,341 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=96, cutout=False, cutout_length=16, data='../data', dataset='graphene', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name=None, gpu=1, grad_clip=5, gz_dtree=False, init_channels=36, layers=8, learning_rate=0.002, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-graphene-EXP-20191203-191412', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-12-03 19:14:15,541 param size = 0.850861MB
2019-12-03 19:14:15,544 epoch 0 lr 2.000000e-03
2019-12-03 19:14:18,012 train 000 1.643105e+00 -69.797162
2019-12-03 19:14:27,492 train 050 3.153269e-01 -8.167170
2019-12-03 19:14:36,938 train 100 1.745908e-01 -4.142094
2019-12-03 19:14:46,380 train 150 1.260418e-01 -2.741183
2019-12-03 19:14:55,831 train 200 1.021004e-01 -2.032178
2019-12-03 19:15:01,015 training loss; R2: 9.462749e-02 -1.812830
2019-12-03 19:15:01,133 valid 000 3.764983e-02 0.403690
2019-12-03 19:15:02,575 validation loss; R2: 1.585623e-02 0.503034
2019-12-03 19:15:02,593 epoch 1 lr 2.000000e-03
2019-12-03 19:15:02,907 train 000 2.346051e-02 0.356913
2019-12-03 19:15:12,372 train 050 2.606038e-02 0.159540
2019-12-03 19:15:21,821 train 100 2.475532e-02 0.214655
2019-12-03 19:15:31,277 train 150 2.369347e-02 0.246924
2019-12-03 19:15:40,733 train 200 2.206647e-02 0.287423
2019-12-03 19:15:44,985 training loss; R2: 2.146455e-02 0.300253
2019-12-03 19:15:45,097 valid 000 5.810631e-03 0.712955
2019-12-03 19:15:46,135 validation loss; R2: 9.285988e-03 0.716919
2019-12-03 19:15:46,150 epoch 2 lr 2.000000e-03
2019-12-03 19:15:46,406 train 000 8.643593e-03 0.628594
2019-12-03 19:15:55,857 train 050 1.618701e-02 0.430477
2019-12-03 19:16:05,310 train 100 1.635271e-02 0.458477
2019-12-03 19:16:14,755 train 150 1.652584e-02 0.454569
2019-12-03 19:16:24,203 train 200 1.626010e-02 0.470124
2019-12-03 19:16:28,454 training loss; R2: 1.602540e-02 0.478187
2019-12-03 19:16:28,568 valid 000 4.974726e-03 0.727205
2019-12-03 19:16:29,605 validation loss; R2: 8.119015e-03 0.741002
2019-12-03 19:16:29,620 epoch 3 lr 2.000000e-03
2019-12-03 19:16:29,876 train 000 6.179772e-03 0.647076
2019-12-03 19:16:39,325 train 050 1.329568e-02 0.562058
2019-12-03 19:16:48,773 train 100 1.359913e-02 0.561069
2019-12-03 19:16:58,222 train 150 1.289670e-02 0.577897
2019-12-03 19:17:07,671 train 200 1.225027e-02 0.597863
2019-12-03 19:17:11,919 training loss; R2: 1.219963e-02 0.601901
2019-12-03 19:17:12,037 valid 000 4.184404e-03 0.889497
2019-12-03 19:17:13,076 validation loss; R2: 5.608741e-03 0.821420
2019-12-03 19:17:13,091 epoch 4 lr 2.000000e-03
2019-12-03 19:17:13,352 train 000 1.024655e-02 0.610803
2019-12-03 19:17:22,800 train 050 1.243791e-02 0.613152
2019-12-03 19:17:32,250 train 100 1.190225e-02 0.614198
2019-12-03 19:17:41,700 train 150 1.148396e-02 0.625614
2019-12-03 19:17:51,147 train 200 1.118116e-02 0.634830
2019-12-03 19:17:55,395 training loss; R2: 1.109934e-02 0.640249
2019-12-03 19:17:55,503 valid 000 6.451343e-03 0.721401
2019-12-03 19:17:56,542 validation loss; R2: 7.177570e-03 0.768805
2019-12-03 19:17:56,556 epoch 5 lr 2.000000e-03
2019-12-03 19:17:56,816 train 000 1.050921e-02 0.826855
2019-12-03 19:18:06,261 train 050 9.270264e-03 0.676841
2019-12-03 19:18:15,706 train 100 9.671335e-03 0.652856
2019-12-03 19:18:25,148 train 150 9.569287e-03 0.673331
2019-12-03 19:18:34,588 train 200 9.269425e-03 0.679822
2019-12-03 19:18:38,832 training loss; R2: 9.462999e-03 0.675995
2019-12-03 19:18:38,950 valid 000 7.458004e-03 0.798806
2019-12-03 19:18:39,987 validation loss; R2: 6.758984e-03 0.784852
2019-12-03 19:18:40,002 epoch 6 lr 2.000000e-03
2019-12-03 19:18:40,259 train 000 1.157599e-02 0.637257
2019-12-03 19:18:49,707 train 050 9.741611e-03 0.665353
2019-12-03 19:18:59,162 train 100 9.976508e-03 0.665941
2019-12-03 19:19:08,618 train 150 1.011540e-02 0.671170
2019-12-03 19:19:18,066 train 200 9.746678e-03 0.680045
2019-12-03 19:19:22,312 training loss; R2: 9.501555e-03 0.684188
2019-12-03 19:19:22,426 valid 000 9.741762e-03 0.580076
2019-12-03 19:19:23,465 validation loss; R2: 6.805093e-03 0.776176
2019-12-03 19:19:23,481 epoch 7 lr 2.000000e-03
2019-12-03 19:19:23,740 train 000 9.200205e-03 0.777253
2019-12-03 19:19:33,190 train 050 8.978025e-03 0.726092
2019-12-03 19:19:42,636 train 100 8.347227e-03 0.723633
2019-12-03 19:19:52,087 train 150 8.090731e-03 0.732040
2019-12-03 19:20:01,536 train 200 8.039840e-03 0.731402
2019-12-03 19:20:05,782 training loss; R2: 7.990520e-03 0.734591
2019-12-03 19:20:05,891 valid 000 6.275896e-03 0.883130
2019-12-03 19:20:06,930 validation loss; R2: 4.554167e-03 0.847241
2019-12-03 19:20:06,951 epoch 8 lr 2.000000e-03
2019-12-03 19:20:07,212 train 000 8.376875e-03 0.836032
2019-12-03 19:20:16,659 train 050 9.095239e-03 0.721897
2019-12-03 19:20:26,112 train 100 8.103292e-03 0.739298
2019-12-03 19:20:35,552 train 150 7.863997e-03 0.742337
2019-12-03 19:20:44,998 train 200 7.759670e-03 0.741610
2019-12-03 19:20:49,243 training loss; R2: 7.744428e-03 0.743703
2019-12-03 19:20:49,355 valid 000 1.086146e-02 0.826515
2019-12-03 19:20:50,392 validation loss; R2: 4.088118e-03 0.868970
2019-12-03 19:20:50,407 epoch 9 lr 2.000000e-03
2019-12-03 19:20:50,663 train 000 6.277315e-03 0.639441
2019-12-03 19:21:00,110 train 050 7.356840e-03 0.732370
2019-12-03 19:21:09,551 train 100 7.399832e-03 0.733245
2019-12-03 19:21:18,992 train 150 7.477825e-03 0.735397
2019-12-03 19:21:28,438 train 200 7.465432e-03 0.741375
2019-12-03 19:21:32,684 training loss; R2: 7.605752e-03 0.743118
2019-12-03 19:21:32,796 valid 000 2.914709e-03 0.834703
2019-12-03 19:21:33,834 validation loss; R2: 3.936913e-03 0.868721
2019-12-03 19:21:33,849 epoch 10 lr 2.000000e-03
2019-12-03 19:21:34,105 train 000 7.369083e-03 0.821651
2019-12-03 19:21:43,547 train 050 7.115934e-03 0.754589
2019-12-03 19:21:52,992 train 100 7.063486e-03 0.748710
2019-12-03 19:22:02,433 train 150 7.146018e-03 0.756840
2019-12-03 19:22:11,873 train 200 7.308728e-03 0.752157
2019-12-03 19:22:16,115 training loss; R2: 7.269731e-03 0.751329
2019-12-03 19:22:16,227 valid 000 5.690156e-03 0.751204
2019-12-03 19:22:17,264 validation loss; R2: 3.942073e-03 0.868104
2019-12-03 19:22:17,280 epoch 11 lr 2.000000e-03
2019-12-03 19:22:17,542 train 000 1.174297e-02 0.606769
2019-12-03 19:22:26,979 train 050 7.420712e-03 0.749751
2019-12-03 19:22:36,425 train 100 7.259518e-03 0.765188
2019-12-03 19:22:45,867 train 150 7.483783e-03 0.757433
2019-12-03 19:22:55,310 train 200 7.320054e-03 0.760026
2019-12-03 19:22:59,552 training loss; R2: 7.318494e-03 0.757414
2019-12-03 19:22:59,662 valid 000 5.010359e-03 0.890122
2019-12-03 19:23:00,701 validation loss; R2: 4.854839e-03 0.838937
2019-12-03 19:23:00,715 epoch 12 lr 2.000000e-03
2019-12-03 19:23:00,972 train 000 5.027772e-03 0.790385
2019-12-03 19:23:10,418 train 050 7.258535e-03 0.745691
2019-12-03 19:23:19,856 train 100 7.049187e-03 0.754509
2019-12-03 19:23:29,292 train 150 6.985139e-03 0.759842
2019-12-03 19:23:38,730 train 200 6.860535e-03 0.762477
2019-12-03 19:23:42,971 training loss; R2: 6.910040e-03 0.763246
2019-12-03 19:23:43,094 valid 000 4.158951e-03 0.847371
2019-12-03 19:23:44,133 validation loss; R2: 5.077844e-03 0.829220
2019-12-03 19:23:44,148 epoch 13 lr 2.000000e-03
2019-12-03 19:23:44,405 train 000 1.188672e-02 0.791353
2019-12-03 19:23:53,845 train 050 7.604808e-03 0.769066
2019-12-03 19:24:03,284 train 100 7.625199e-03 0.752849
2019-12-03 19:24:12,719 train 150 7.174086e-03 0.761385
2019-12-03 19:24:22,157 train 200 7.003529e-03 0.764769
2019-12-03 19:24:26,399 training loss; R2: 6.865258e-03 0.767559
2019-12-03 19:24:26,509 valid 000 1.152503e-02 0.657898
2019-12-03 19:24:27,548 validation loss; R2: 1.159024e-02 0.621198
2019-12-03 19:24:27,563 epoch 14 lr 2.000000e-03
2019-12-03 19:24:27,822 train 000 4.816021e-03 0.752226
2019-12-03 19:24:37,256 train 050 6.907079e-03 0.757761
2019-12-03 19:24:46,689 train 100 7.820555e-03 0.750902
2019-12-03 19:24:56,116 train 150 7.948090e-03 0.746946
2019-12-03 19:25:05,546 train 200 7.642162e-03 0.754572
2019-12-03 19:25:09,784 training loss; R2: 7.430151e-03 0.757221
2019-12-03 19:25:09,893 valid 000 8.170365e-03 0.740016
2019-12-03 19:25:10,930 validation loss; R2: 8.381825e-03 0.715389
2019-12-03 19:25:10,945 epoch 15 lr 2.000000e-03
2019-12-03 19:25:11,221 train 000 5.131090e-03 0.743636
2019-12-03 19:25:20,648 train 050 6.740385e-03 0.759959
2019-12-03 19:25:30,078 train 100 6.582041e-03 0.779604
2019-12-03 19:25:39,508 train 150 6.843904e-03 0.774562
2019-12-03 19:25:48,940 train 200 6.668598e-03 0.775851
2019-12-03 19:25:53,180 training loss; R2: 6.769362e-03 0.772084
2019-12-03 19:25:53,285 valid 000 3.973380e-03 0.761741
2019-12-03 19:25:54,322 validation loss; R2: 7.081455e-03 0.771092
2019-12-03 19:25:54,337 epoch 16 lr 2.000000e-03
2019-12-03 19:25:54,592 train 000 1.372035e-02 0.769236
2019-12-03 19:26:04,019 train 050 7.885473e-03 0.771230
2019-12-03 19:26:13,452 train 100 7.108428e-03 0.772052
2019-12-03 19:26:22,882 train 150 6.864389e-03 0.778921
2019-12-03 19:26:32,302 train 200 6.565697e-03 0.785427
2019-12-03 19:26:36,536 training loss; R2: 6.411075e-03 0.788907
2019-12-03 19:26:36,644 valid 000 1.213843e-02 0.514774
2019-12-03 19:26:37,680 validation loss; R2: 1.611335e-02 0.488072
2019-12-03 19:26:37,695 epoch 17 lr 2.000000e-03
2019-12-03 19:26:37,950 train 000 5.109933e-03 0.803070
2019-12-03 19:26:47,367 train 050 6.402516e-03 0.783876
2019-12-03 19:26:56,793 train 100 6.095945e-03 0.796571
2019-12-03 19:27:06,212 train 150 6.176755e-03 0.797546
2019-12-03 19:27:15,634 train 200 6.258223e-03 0.794810
2019-12-03 19:27:19,873 training loss; R2: 6.285639e-03 0.792973
2019-12-03 19:27:19,985 valid 000 9.235546e-03 0.726658
2019-12-03 19:27:21,022 validation loss; R2: 8.260451e-03 0.735618
2019-12-03 19:27:21,037 epoch 18 lr 2.000000e-03
2019-12-03 19:27:21,295 train 000 8.356175e-03 0.709492
2019-12-03 19:27:30,721 train 050 6.159510e-03 0.777491
2019-12-03 19:27:40,147 train 100 6.049268e-03 0.786014
2019-12-03 19:27:49,570 train 150 6.246334e-03 0.787295
2019-12-03 19:27:58,994 train 200 6.304416e-03 0.791229
2019-12-03 19:28:03,228 training loss; R2: 6.308792e-03 0.786872
2019-12-03 19:28:03,335 valid 000 7.335149e-03 0.594073
2019-12-03 19:28:04,372 validation loss; R2: 9.729496e-03 0.676347
2019-12-03 19:28:04,387 epoch 19 lr 2.000000e-03
2019-12-03 19:28:04,642 train 000 5.135753e-03 0.774390
2019-12-03 19:28:14,063 train 050 7.135196e-03 0.781023
2019-12-03 19:28:23,481 train 100 6.641364e-03 0.787811
2019-12-03 19:28:32,898 train 150 6.444826e-03 0.791888
2019-12-03 19:28:42,313 train 200 6.188539e-03 0.793130
2019-12-03 19:28:46,547 training loss; R2: 6.109060e-03 0.795082
2019-12-03 19:28:46,658 valid 000 5.147538e-03 0.719248
2019-12-03 19:28:47,695 validation loss; R2: 6.673253e-03 0.769505
