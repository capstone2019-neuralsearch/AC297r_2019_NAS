2019-11-13 11:36:08,372 gpu device = 1
2019-11-13 11:36:08,372 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name='galaxy-zoo-cleaned', gpu=1, grad_clip=5, gz_dtree=False, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-galaxy-zoo-EXP-20191113-113608', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-11-13 11:36:19,955 param size = 0.321045MB
2019-11-13 11:36:19,959 epoch 0 lr 1.000000e-03
2019-11-13 11:36:22,248 train 000 4.878837e-01 -298.078045
2019-11-13 11:36:30,345 train 050 5.121996e-02 -10.799679
2019-11-13 11:36:38,270 train 100 3.947665e-02 -5.790001
2019-11-13 11:36:46,145 train 150 3.489115e-02 -4.056479
2019-11-13 11:36:54,017 train 200 3.225319e-02 -3.229113
2019-11-13 11:37:01,890 train 250 3.062312e-02 -2.667381
2019-11-13 11:37:09,768 train 300 2.938787e-02 -2.281744
2019-11-13 11:37:17,744 train 350 2.842654e-02 -2.009036
2019-11-13 11:37:25,668 train 400 2.763164e-02 -1.817409
2019-11-13 11:37:33,570 train 450 2.692071e-02 -1.625787
2019-11-13 11:37:41,499 train 500 2.632192e-02 -1.476614
2019-11-13 11:37:49,693 train 550 2.581082e-02 -1.355285
2019-11-13 11:37:57,941 train 600 2.536305e-02 -1.245566
2019-11-13 11:38:06,163 train 650 2.496239e-02 -1.152595
2019-11-13 11:38:14,382 train 700 2.460285e-02 -1.075722
2019-11-13 11:38:22,555 train 750 2.428276e-02 -1.006562
2019-11-13 11:38:30,476 train 800 2.400070e-02 -0.943193
2019-11-13 11:38:38,403 train 850 2.374671e-02 -0.886554
2019-11-13 11:38:41,629 training loss; R2: 2.368051e-02 -0.871206
2019-11-13 11:38:41,915 valid 000 1.608716e-02 0.143569
2019-11-13 11:38:43,616 valid 050 1.774561e-02 0.159684
2019-11-13 11:38:45,230 validation loss; R2: 1.770527e-02 0.165251
2019-11-13 11:38:45,248 epoch 1 lr 1.000000e-03
2019-11-13 11:38:45,829 train 000 2.081105e-02 -0.027153
2019-11-13 11:38:53,933 train 050 1.925066e-02 0.036159
2019-11-13 11:39:01,821 train 100 1.901045e-02 0.047281
2019-11-13 11:39:09,684 train 150 1.885651e-02 0.014165
2019-11-13 11:39:17,544 train 200 1.878584e-02 0.027042
2019-11-13 11:39:25,401 train 250 1.875311e-02 0.039239
2019-11-13 11:39:33,257 train 300 1.863280e-02 0.043095
2019-11-13 11:39:41,119 train 350 1.856102e-02 0.050318
2019-11-13 11:39:48,969 train 400 1.847691e-02 0.051285
2019-11-13 11:39:56,827 train 450 1.845189e-02 0.056802
2019-11-13 11:40:04,687 train 500 1.836695e-02 0.059773
2019-11-13 11:40:12,545 train 550 1.832643e-02 0.061865
2019-11-13 11:40:20,398 train 600 1.822372e-02 0.065488
2019-11-13 11:40:28,265 train 650 1.814586e-02 0.067354
2019-11-13 11:40:36,117 train 700 1.810040e-02 0.069713
2019-11-13 11:40:43,974 train 750 1.805788e-02 0.070252
2019-11-13 11:40:51,832 train 800 1.801597e-02 0.073435
2019-11-13 11:40:59,750 train 850 1.797479e-02 0.076882
2019-11-13 11:41:02,104 training loss; R2: 1.796064e-02 0.077505
2019-11-13 11:41:02,426 valid 000 1.406841e-02 0.297472
2019-11-13 11:41:04,123 valid 050 1.540842e-02 0.181243
2019-11-13 11:41:05,655 validation loss; R2: 1.543468e-02 0.143328
2019-11-13 11:41:05,675 epoch 2 lr 1.000000e-03
2019-11-13 11:41:06,111 train 000 1.746397e-02 0.197113
2019-11-13 11:41:13,974 train 050 1.674765e-02 -0.152262
2019-11-13 11:41:21,830 train 100 1.701850e-02 -0.033413
2019-11-13 11:41:29,680 train 150 1.689338e-02 0.017963
2019-11-13 11:41:37,528 train 200 1.689563e-02 0.043153
2019-11-13 11:41:45,377 train 250 1.680965e-02 0.058764
2019-11-13 11:41:53,225 train 300 1.676744e-02 0.076325
2019-11-13 11:42:01,074 train 350 1.672886e-02 0.081438
2019-11-13 11:42:08,919 train 400 1.669792e-02 0.088856
2019-11-13 11:42:16,762 train 450 1.663869e-02 0.067279
2019-11-13 11:42:24,605 train 500 1.657027e-02 0.077131
2019-11-13 11:42:32,450 train 550 1.651016e-02 0.081257
2019-11-13 11:42:40,299 train 600 1.646026e-02 0.087765
2019-11-13 11:42:48,138 train 650 1.639661e-02 0.093523
2019-11-13 11:42:55,983 train 700 1.639925e-02 0.094789
2019-11-13 11:43:03,834 train 750 1.636815e-02 0.101640
2019-11-13 11:43:11,677 train 800 1.634352e-02 0.104589
2019-11-13 11:43:19,523 train 850 1.630559e-02 0.108933
2019-11-13 11:43:21,871 training loss; R2: 1.630299e-02 0.110304
2019-11-13 11:43:22,196 valid 000 1.844471e-02 0.252602
2019-11-13 11:43:23,929 valid 050 1.580989e-02 0.015834
2019-11-13 11:43:25,505 validation loss; R2: 1.580013e-02 0.109745
2019-11-13 11:43:25,524 epoch 3 lr 1.000000e-03
2019-11-13 11:43:25,927 train 000 1.696603e-02 0.194285
2019-11-13 11:43:33,920 train 050 1.577014e-02 0.175352
2019-11-13 11:43:41,851 train 100 1.565293e-02 0.122131
2019-11-13 11:43:49,731 train 150 1.544648e-02 0.062441
2019-11-13 11:43:57,601 train 200 1.550136e-02 0.093838
2019-11-13 11:44:05,455 train 250 1.548888e-02 0.110507
2019-11-13 11:44:13,308 train 300 1.556762e-02 0.118768
2019-11-13 11:44:21,170 train 350 1.553162e-02 0.127585
2019-11-13 11:44:29,090 train 400 1.543590e-02 0.114531
2019-11-13 11:44:36,937 train 450 1.538089e-02 0.126590
2019-11-13 11:44:44,790 train 500 1.534489e-02 0.134668
2019-11-13 11:44:52,649 train 550 1.530955e-02 0.141274
2019-11-13 11:45:00,496 train 600 1.528556e-02 0.147126
2019-11-13 11:45:08,346 train 650 1.528472e-02 0.143033
2019-11-13 11:45:16,198 train 700 1.523900e-02 0.147840
2019-11-13 11:45:24,050 train 750 1.517689e-02 0.145553
2019-11-13 11:45:31,894 train 800 1.512509e-02 0.149196
2019-11-13 11:45:39,739 train 850 1.507406e-02 0.152003
2019-11-13 11:45:42,087 training loss; R2: 1.506446e-02 0.152694
2019-11-13 11:45:42,403 valid 000 1.507415e-02 0.274001
2019-11-13 11:45:44,122 valid 050 1.509425e-02 0.214440
2019-11-13 11:45:45,689 validation loss; R2: 1.514962e-02 0.204771
2019-11-13 11:45:45,709 epoch 4 lr 1.000000e-03
2019-11-13 11:45:46,132 train 000 1.576448e-02 0.301417
2019-11-13 11:45:54,010 train 050 1.467459e-02 0.234757
2019-11-13 11:46:01,859 train 100 1.458328e-02 0.226605
2019-11-13 11:46:09,712 train 150 1.443365e-02 0.209926
2019-11-13 11:46:17,569 train 200 1.451904e-02 0.213223
2019-11-13 11:46:25,421 train 250 1.440818e-02 0.212558
2019-11-13 11:46:33,269 train 300 1.433666e-02 0.208311
2019-11-13 11:46:41,116 train 350 1.425454e-02 0.205848
2019-11-13 11:46:48,975 train 400 1.423790e-02 0.209460
2019-11-13 11:46:56,822 train 450 1.421300e-02 0.214664
2019-11-13 11:47:04,671 train 500 1.418129e-02 0.211250
2019-11-13 11:47:12,524 train 550 1.415946e-02 0.212278
2019-11-13 11:47:20,372 train 600 1.411989e-02 0.213692
2019-11-13 11:47:28,220 train 650 1.407710e-02 0.215716
2019-11-13 11:47:36,070 train 700 1.408153e-02 0.215716
2019-11-13 11:47:43,918 train 750 1.406082e-02 0.214697
2019-11-13 11:47:51,772 train 800 1.404369e-02 0.215272
2019-11-13 11:47:59,625 train 850 1.401829e-02 0.205720
2019-11-13 11:48:01,972 training loss; R2: 1.400902e-02 0.205429
2019-11-13 11:48:02,278 valid 000 1.280842e-02 0.222047
2019-11-13 11:48:04,004 valid 050 1.476544e-02 0.215548
2019-11-13 11:48:05,563 validation loss; R2: 1.469810e-02 0.219069
2019-11-13 11:48:05,582 epoch 5 lr 1.000000e-03
2019-11-13 11:48:06,012 train 000 1.396727e-02 0.348208
2019-11-13 11:48:13,964 train 050 1.354877e-02 0.259282
2019-11-13 11:48:21,878 train 100 1.348404e-02 0.252511
2019-11-13 11:48:29,846 train 150 1.350218e-02 0.257434
2019-11-13 11:48:37,754 train 200 1.353974e-02 0.251569
2019-11-13 11:48:45,658 train 250 1.354363e-02 0.250788
2019-11-13 11:48:53,562 train 300 1.357009e-02 0.244008
2019-11-13 11:49:01,455 train 350 1.353771e-02 0.240042
2019-11-13 11:49:09,346 train 400 1.349259e-02 0.242656
2019-11-13 11:49:17,512 train 450 1.345333e-02 0.238772
2019-11-13 11:49:25,456 train 500 1.347936e-02 0.232929
2019-11-13 11:49:33,431 train 550 1.343193e-02 0.235297
2019-11-13 11:49:41,327 train 600 1.344906e-02 0.235909
2019-11-13 11:49:49,319 train 650 1.342280e-02 0.238183
2019-11-13 11:49:57,247 train 700 1.342010e-02 0.239666
2019-11-13 11:50:05,176 train 750 1.340244e-02 0.241484
2019-11-13 11:50:13,151 train 800 1.340342e-02 0.241279
2019-11-13 11:50:21,122 train 850 1.340751e-02 0.241438
2019-11-13 11:50:23,523 training loss; R2: 1.340647e-02 0.238970
2019-11-13 11:50:23,842 valid 000 1.572430e-02 -0.025618
2019-11-13 11:50:25,582 valid 050 1.736244e-02 0.030971
2019-11-13 11:50:27,153 validation loss; R2: 1.729678e-02 0.069231
2019-11-13 11:50:27,173 epoch 6 lr 1.000000e-03
2019-11-13 11:50:27,577 train 000 1.385922e-02 0.293686
2019-11-13 11:50:35,492 train 050 1.327767e-02 0.292182
2019-11-13 11:50:43,420 train 100 1.317662e-02 0.275830
2019-11-13 11:50:51,399 train 150 1.308836e-02 0.208502
2019-11-13 11:50:59,356 train 200 1.308980e-02 0.212066
2019-11-13 11:51:07,304 train 250 1.304494e-02 0.220302
2019-11-13 11:51:15,252 train 300 1.307429e-02 0.231609
2019-11-13 11:51:23,172 train 350 1.305619e-02 0.233519
2019-11-13 11:51:31,098 train 400 1.305962e-02 0.224714
2019-11-13 11:51:39,034 train 450 1.305188e-02 0.226460
2019-11-13 11:51:47,044 train 500 1.302692e-02 0.232841
2019-11-13 11:51:54,996 train 550 1.302218e-02 0.233389
2019-11-13 11:52:02,934 train 600 1.300412e-02 0.235505
2019-11-13 11:52:10,907 train 650 1.301819e-02 0.237395
2019-11-13 11:52:18,950 train 700 1.297091e-02 0.229277
2019-11-13 11:52:26,963 train 750 1.295569e-02 0.231519
2019-11-13 11:52:35,037 train 800 1.295471e-02 0.232063
2019-11-13 11:52:43,107 train 850 1.292559e-02 0.234173
2019-11-13 11:52:45,483 training loss; R2: 1.293018e-02 0.234776
2019-11-13 11:52:45,819 valid 000 1.694342e-02 0.208850
2019-11-13 11:52:47,571 valid 050 1.943934e-02 -0.025717
2019-11-13 11:52:49,159 validation loss; R2: 1.939558e-02 0.038122
2019-11-13 11:52:49,183 epoch 7 lr 1.000000e-03
2019-11-13 11:52:49,616 train 000 1.163429e-02 0.326316
2019-11-13 11:52:57,766 train 050 1.230977e-02 0.148430
2019-11-13 11:53:05,781 train 100 1.253238e-02 0.004862
2019-11-13 11:53:13,862 train 150 1.249391e-02 0.073196
2019-11-13 11:53:21,862 train 200 1.253251e-02 0.124457
2019-11-13 11:53:29,826 train 250 1.259214e-02 0.145047
2019-11-13 11:53:37,751 train 300 1.261782e-02 0.145074
2019-11-13 11:53:45,733 train 350 1.263960e-02 0.161674
2019-11-13 11:53:53,641 train 400 1.269878e-02 0.165781
2019-11-13 11:54:01,679 train 450 1.268921e-02 0.176210
2019-11-13 11:54:09,684 train 500 1.266928e-02 0.186521
2019-11-13 11:54:17,851 train 550 1.265503e-02 0.195668
2019-11-13 11:54:25,836 train 600 1.267387e-02 0.203101
2019-11-13 11:54:33,884 train 650 1.263701e-02 0.208370
2019-11-13 11:54:42,094 train 700 1.264172e-02 0.210469
2019-11-13 11:54:50,272 train 750 1.261455e-02 0.214501
2019-11-13 11:54:58,212 train 800 1.258777e-02 0.219253
2019-11-13 11:55:06,280 train 850 1.259669e-02 0.218297
2019-11-13 11:55:08,743 training loss; R2: 1.259487e-02 0.219561
2019-11-13 11:55:09,038 valid 000 1.653910e-02 0.190877
2019-11-13 11:55:10,751 valid 050 1.587712e-02 0.150736
2019-11-13 11:55:12,324 validation loss; R2: 1.590453e-02 0.159273
2019-11-13 11:55:12,342 epoch 8 lr 1.000000e-03
2019-11-13 11:55:12,756 train 000 1.332891e-02 0.183036
2019-11-13 11:55:20,630 train 050 1.261124e-02 0.267517
2019-11-13 11:55:28,511 train 100 1.247389e-02 0.246941
2019-11-13 11:55:36,395 train 150 1.260335e-02 0.263006
2019-11-13 11:55:44,318 train 200 1.256571e-02 0.266945
2019-11-13 11:55:52,360 train 250 1.246579e-02 0.270344
2019-11-13 11:56:00,582 train 300 1.239192e-02 0.272844
2019-11-13 11:56:08,803 train 350 1.235859e-02 0.272384
2019-11-13 11:56:17,033 train 400 1.238869e-02 0.275123
2019-11-13 11:56:25,247 train 450 1.240454e-02 0.271542
2019-11-13 11:56:33,200 train 500 1.240391e-02 0.269492
2019-11-13 11:56:41,343 train 550 1.239119e-02 0.265802
2019-11-13 11:56:49,352 train 600 1.236997e-02 0.268001
2019-11-13 11:56:57,263 train 650 1.235661e-02 0.269834
2019-11-13 11:57:05,194 train 700 1.236779e-02 0.269857
2019-11-13 11:57:13,220 train 750 1.237391e-02 0.271828
2019-11-13 11:57:21,195 train 800 1.235971e-02 0.252346
2019-11-13 11:57:29,335 train 850 1.236538e-02 0.248927
2019-11-13 11:57:31,764 training loss; R2: 1.236175e-02 0.236875
2019-11-13 11:57:32,073 valid 000 1.571042e-02 0.272500
2019-11-13 11:57:33,723 valid 050 1.212128e-02 0.330983
2019-11-13 11:57:35,246 validation loss; R2: 1.212312e-02 0.329968
2019-11-13 11:57:35,264 epoch 9 lr 1.000000e-03
2019-11-13 11:57:35,676 train 000 1.528944e-02 0.215022
2019-11-13 11:57:43,844 train 050 1.202719e-02 0.280311
2019-11-13 11:57:52,106 train 100 1.215661e-02 -0.228113
2019-11-13 11:58:00,245 train 150 1.217489e-02 -0.047933
2019-11-13 11:58:08,159 train 200 1.218507e-02 0.033428
2019-11-13 11:58:16,206 train 250 1.218661e-02 0.062136
2019-11-13 11:58:24,172 train 300 1.216937e-02 0.101058
2019-11-13 11:58:32,034 train 350 1.214966e-02 0.127857
2019-11-13 11:58:39,942 train 400 1.213856e-02 0.151734
2019-11-13 11:58:47,787 train 450 1.215477e-02 0.164820
2019-11-13 11:58:55,662 train 500 1.215994e-02 0.147266
2019-11-13 11:59:03,536 train 550 1.215667e-02 0.161460
2019-11-13 11:59:11,397 train 600 1.214524e-02 0.172789
2019-11-13 11:59:19,233 train 650 1.212869e-02 0.179100
2019-11-13 11:59:27,067 train 700 1.211084e-02 0.187630
2019-11-13 11:59:34,904 train 750 1.212409e-02 0.193442
2019-11-13 11:59:42,738 train 800 1.211335e-02 0.200785
2019-11-13 11:59:50,572 train 850 1.211683e-02 0.203708
2019-11-13 11:59:52,915 training loss; R2: 1.211580e-02 0.204813
2019-11-13 11:59:53,251 valid 000 1.425253e-01 -4.677083
2019-11-13 11:59:54,939 valid 050 1.414597e-01 -11.052287
2019-11-13 11:59:56,461 validation loss; R2: 1.420543e-01 -11.025663
2019-11-13 11:59:56,480 epoch 10 lr 1.000000e-03
2019-11-13 11:59:56,943 train 000 1.292119e-02 0.409583
2019-11-13 12:00:04,815 train 050 1.166289e-02 0.233479
2019-11-13 12:00:12,684 train 100 1.174360e-02 0.224428
2019-11-13 12:00:20,739 train 150 1.179327e-02 0.253211
2019-11-13 12:00:28,750 train 200 1.186348e-02 0.263929
2019-11-13 12:00:36,776 train 250 1.185857e-02 0.273232
2019-11-13 12:00:44,702 train 300 1.188038e-02 0.260741
2019-11-13 12:00:52,586 train 350 1.183962e-02 0.246075
2019-11-13 12:01:00,478 train 400 1.184013e-02 0.250748
2019-11-13 12:01:08,344 train 450 1.185985e-02 0.238823
2019-11-13 12:01:16,230 train 500 1.186997e-02 0.243412
2019-11-13 12:01:24,196 train 550 1.189385e-02 0.164026
2019-11-13 12:01:32,223 train 600 1.188934e-02 0.177488
2019-11-13 12:01:40,181 train 650 1.189932e-02 0.186082
2019-11-13 12:01:48,260 train 700 1.188835e-02 0.194994
2019-11-13 12:01:56,311 train 750 1.187262e-02 0.195785
2019-11-13 12:02:04,362 train 800 1.185488e-02 0.200606
2019-11-13 12:02:12,462 train 850 1.184823e-02 0.203340
2019-11-13 12:02:14,825 training loss; R2: 1.183769e-02 0.204277
2019-11-13 12:02:15,145 valid 000 1.237931e-02 0.399105
2019-11-13 12:02:16,925 valid 050 1.078332e-02 0.375690
2019-11-13 12:02:18,519 validation loss; R2: 1.089605e-02 0.377162
2019-11-13 12:02:18,538 epoch 11 lr 1.000000e-03
2019-11-13 12:02:18,969 train 000 1.174366e-02 0.345925
2019-11-13 12:02:27,171 train 050 1.178328e-02 0.302113
2019-11-13 12:02:35,121 train 100 1.171217e-02 0.302475
2019-11-13 12:02:43,069 train 150 1.165710e-02 0.302862
2019-11-13 12:02:51,018 train 200 1.169887e-02 0.288564
2019-11-13 12:02:59,017 train 250 1.164945e-02 0.285086
2019-11-13 12:03:06,999 train 300 1.168531e-02 0.281430
2019-11-13 12:03:14,921 train 350 1.168858e-02 0.279819
2019-11-13 12:03:22,837 train 400 1.170823e-02 0.274356
2019-11-13 12:03:30,769 train 450 1.171969e-02 0.265859
2019-11-13 12:03:38,679 train 500 1.170369e-02 0.267311
2019-11-13 12:03:46,598 train 550 1.169652e-02 0.258268
2019-11-13 12:03:54,619 train 600 1.171044e-02 0.261668
2019-11-13 12:04:02,570 train 650 1.168723e-02 0.265232
2019-11-13 12:04:10,561 train 700 1.167475e-02 0.265751
2019-11-13 12:04:18,507 train 750 1.168719e-02 0.269309
2019-11-13 12:04:26,679 train 800 1.167754e-02 0.268786
2019-11-13 12:04:34,629 train 850 1.167408e-02 0.271986
2019-11-13 12:04:36,990 training loss; R2: 1.167474e-02 0.269765
2019-11-13 12:04:37,295 valid 000 1.306309e-02 -0.140866
2019-11-13 12:04:39,032 valid 050 1.413697e-02 -0.439187
2019-11-13 12:04:40,592 validation loss; R2: 1.427880e-02 -0.314731
2019-11-13 12:04:40,611 epoch 12 lr 1.000000e-03
2019-11-13 12:04:41,057 train 000 1.171638e-02 0.056738
2019-11-13 12:04:49,075 train 050 1.164953e-02 0.302107
2019-11-13 12:04:57,059 train 100 1.162976e-02 0.299047
2019-11-13 12:05:04,990 train 150 1.167126e-02 0.291466
2019-11-13 12:05:12,945 train 200 1.171201e-02 0.280537
2019-11-13 12:05:20,924 train 250 1.165447e-02 0.262902
2019-11-13 12:05:28,849 train 300 1.162996e-02 0.270267
2019-11-13 12:05:36,770 train 350 1.162501e-02 0.279549
2019-11-13 12:05:44,681 train 400 1.162952e-02 0.261454
2019-11-13 12:05:52,592 train 450 1.159634e-02 0.254322
2019-11-13 12:06:00,501 train 500 1.159299e-02 0.259759
2019-11-13 12:06:08,501 train 550 1.157553e-02 0.259652
2019-11-13 12:06:16,438 train 600 1.157480e-02 0.261514
2019-11-13 12:06:24,353 train 650 1.157451e-02 0.259957
2019-11-13 12:06:32,260 train 700 1.155126e-02 0.263761
2019-11-13 12:06:40,168 train 750 1.153556e-02 0.268160
2019-11-13 12:06:48,102 train 800 1.154972e-02 0.268811
2019-11-13 12:06:56,065 train 850 1.156216e-02 0.271762
2019-11-13 12:06:58,430 training loss; R2: 1.155511e-02 0.271946
2019-11-13 12:06:58,764 valid 000 1.142981e-02 0.303847
2019-11-13 12:07:00,505 valid 050 1.239473e-02 0.225106
2019-11-13 12:07:02,085 validation loss; R2: 1.241023e-02 0.211126
2019-11-13 12:07:02,104 epoch 13 lr 1.000000e-03
2019-11-13 12:07:02,543 train 000 1.518460e-02 0.339498
2019-11-13 12:07:10,651 train 050 1.134493e-02 0.188155
2019-11-13 12:07:18,579 train 100 1.138447e-02 0.233988
2019-11-13 12:07:26,589 train 150 1.138374e-02 0.258398
2019-11-13 12:07:34,518 train 200 1.138677e-02 0.274731
2019-11-13 12:07:42,466 train 250 1.139932e-02 0.277276
2019-11-13 12:07:50,423 train 300 1.140613e-02 0.271440
2019-11-13 12:07:58,443 train 350 1.140925e-02 0.276270
2019-11-13 12:08:06,508 train 400 1.143096e-02 0.276380
2019-11-13 12:08:14,507 train 450 1.142043e-02 0.278024
2019-11-13 12:08:22,456 train 500 1.141420e-02 0.281203
2019-11-13 12:08:30,386 train 550 1.140665e-02 0.285884
2019-11-13 12:08:38,298 train 600 1.139883e-02 0.289815
2019-11-13 12:08:46,197 train 650 1.138688e-02 0.287980
2019-11-13 12:08:54,091 train 700 1.140497e-02 0.286325
2019-11-13 12:09:02,103 train 750 1.139256e-02 0.288876
2019-11-13 12:09:10,029 train 800 1.138557e-02 0.290724
2019-11-13 12:09:18,002 train 850 1.138546e-02 0.290959
2019-11-13 12:09:20,384 training loss; R2: 1.138591e-02 0.291649
2019-11-13 12:09:20,688 valid 000 5.180483e-01 -41.578475
2019-11-13 12:09:22,402 valid 050 5.108186e-01 -46.194659
2019-11-13 12:09:23,960 validation loss; R2: 5.118420e-01 -46.799086
2019-11-13 12:09:23,978 epoch 14 lr 1.000000e-03
2019-11-13 12:09:24,393 train 000 1.170467e-02 0.272543
2019-11-13 12:09:32,466 train 050 1.149329e-02 0.317278
2019-11-13 12:09:40,636 train 100 1.162974e-02 0.307742
2019-11-13 12:09:48,800 train 150 1.137523e-02 0.320512
2019-11-13 12:09:56,956 train 200 1.133673e-02 0.320479
2019-11-13 12:10:05,107 train 250 1.134092e-02 0.316346
2019-11-13 12:10:13,257 train 300 1.141873e-02 0.319372
2019-11-13 12:10:21,421 train 350 1.139984e-02 0.322468
2019-11-13 12:10:29,561 train 400 1.141459e-02 0.316196
2019-11-13 12:10:37,581 train 450 1.137170e-02 0.317716
2019-11-13 12:10:45,459 train 500 1.135748e-02 0.316307
2019-11-13 12:10:53,498 train 550 1.135160e-02 0.309259
2019-11-13 12:11:01,663 train 600 1.132613e-02 0.305620
2019-11-13 12:11:09,837 train 650 1.130889e-02 0.306863
2019-11-13 12:11:18,034 train 700 1.131162e-02 0.309041
2019-11-13 12:11:26,191 train 750 1.131937e-02 0.309152
2019-11-13 12:11:34,407 train 800 1.133175e-02 0.310898
2019-11-13 12:11:42,645 train 850 1.133688e-02 0.308992
2019-11-13 12:11:44,997 training loss; R2: 1.133924e-02 0.307690
2019-11-13 12:11:45,317 valid 000 1.216807e-02 0.330721
2019-11-13 12:11:47,094 valid 050 1.037651e-02 0.361581
2019-11-13 12:11:48,615 validation loss; R2: 1.040060e-02 0.367600
2019-11-13 12:11:48,633 epoch 15 lr 1.000000e-03
2019-11-13 12:11:49,075 train 000 1.006335e-02 0.368086
2019-11-13 12:11:56,934 train 050 1.091186e-02 0.103979
2019-11-13 12:12:04,802 train 100 1.106487e-02 0.105744
2019-11-13 12:12:12,675 train 150 1.109200e-02 0.181213
2019-11-13 12:12:20,634 train 200 1.108778e-02 0.186150
2019-11-13 12:12:28,603 train 250 1.108768e-02 0.203185
2019-11-13 12:12:36,724 train 300 1.112075e-02 0.205968
2019-11-13 12:12:44,635 train 350 1.110930e-02 0.226291
2019-11-13 12:12:52,624 train 400 1.113905e-02 0.239909
2019-11-13 12:13:00,600 train 450 1.113773e-02 0.244549
2019-11-13 12:13:08,801 train 500 1.114229e-02 0.251600
2019-11-13 12:13:16,885 train 550 1.117192e-02 0.256110
2019-11-13 12:13:24,816 train 600 1.117653e-02 0.240897
2019-11-13 12:13:32,725 train 650 1.119117e-02 0.236149
2019-11-13 12:13:40,610 train 700 1.121059e-02 0.240188
2019-11-13 12:13:48,492 train 750 1.121777e-02 0.242369
2019-11-13 12:13:56,374 train 800 1.123974e-02 0.248898
2019-11-13 12:14:04,254 train 850 1.127172e-02 0.251506
2019-11-13 12:14:06,609 training loss; R2: 1.128398e-02 0.252854
2019-11-13 12:14:06,885 valid 000 4.309477e-02 -2.873664
2019-11-13 12:14:08,584 valid 050 4.552029e-02 -3.324729
2019-11-13 12:14:10,096 validation loss; R2: 4.531185e-02 -3.111752
2019-11-13 12:14:10,114 epoch 16 lr 1.000000e-03
2019-11-13 12:14:10,506 train 000 1.429021e-02 0.293475
2019-11-13 12:14:18,523 train 050 1.175742e-02 0.269163
2019-11-13 12:14:26,761 train 100 1.176710e-02 0.255352
2019-11-13 12:14:34,745 train 150 1.161854e-02 0.253536
2019-11-13 12:14:42,683 train 200 1.157667e-02 0.190557
2019-11-13 12:14:50,598 train 250 1.143446e-02 0.212877
2019-11-13 12:14:58,554 train 300 1.139593e-02 0.226014
2019-11-13 12:15:06,848 train 350 1.137788e-02 0.238538
2019-11-13 12:15:15,093 train 400 1.136856e-02 0.249691
2019-11-13 12:15:23,086 train 450 1.134427e-02 0.257069
2019-11-13 12:15:31,263 train 500 1.133557e-02 0.264916
2019-11-13 12:15:39,398 train 550 1.133260e-02 0.265107
2019-11-13 12:15:47,445 train 600 1.133207e-02 0.268420
2019-11-13 12:15:55,708 train 650 1.130657e-02 0.274442
2019-11-13 12:16:03,859 train 700 1.129182e-02 0.277335
2019-11-13 12:16:11,899 train 750 1.126908e-02 0.281311
2019-11-13 12:16:20,023 train 800 1.125355e-02 0.282984
2019-11-13 12:16:28,152 train 850 1.123869e-02 0.285692
2019-11-13 12:16:30,564 training loss; R2: 1.123773e-02 0.286041
2019-11-13 12:16:30,882 valid 000 1.280181e-02 0.254263
2019-11-13 12:16:32,798 valid 050 1.190614e-02 0.201806
2019-11-13 12:16:34,496 validation loss; R2: 1.193343e-02 0.180169
2019-11-13 12:16:34,520 epoch 17 lr 1.000000e-03
2019-11-13 12:16:34,977 train 000 1.080767e-02 0.367574
2019-11-13 12:16:43,421 train 050 1.132959e-02 0.319319
2019-11-13 12:16:51,720 train 100 1.115875e-02 0.323959
2019-11-13 12:17:00,237 train 150 1.111342e-02 0.309588
2019-11-13 12:17:08,335 train 200 1.109394e-02 0.315200
2019-11-13 12:17:16,353 train 250 1.110288e-02 0.307601
2019-11-13 12:17:24,367 train 300 1.108422e-02 0.312959
2019-11-13 12:17:32,354 train 350 1.109700e-02 0.309298
2019-11-13 12:17:40,386 train 400 1.109270e-02 0.314261
2019-11-13 12:17:48,489 train 450 1.111548e-02 0.313111
2019-11-13 12:17:56,543 train 500 1.110949e-02 0.315687
2019-11-13 12:18:04,570 train 550 1.110672e-02 0.317217
2019-11-13 12:18:12,561 train 600 1.112234e-02 0.318070
2019-11-13 12:18:20,586 train 650 1.110723e-02 0.319189
2019-11-13 12:18:28,663 train 700 1.111743e-02 0.319925
2019-11-13 12:18:36,654 train 750 1.110742e-02 0.317757
2019-11-13 12:18:44,651 train 800 1.111735e-02 0.319466
2019-11-13 12:18:52,646 train 850 1.111988e-02 0.318974
2019-11-13 12:18:55,029 training loss; R2: 1.112364e-02 0.319210
2019-11-13 12:18:55,330 valid 000 3.532223e-02 -1.202534
2019-11-13 12:18:57,204 valid 050 3.578582e-02 -1.037182
2019-11-13 12:18:58,869 validation loss; R2: 3.570506e-02 -1.033220
2019-11-13 12:18:58,889 epoch 18 lr 1.000000e-03
2019-11-13 12:18:59,330 train 000 8.894585e-03 0.477150
2019-11-13 12:19:07,363 train 050 1.128561e-02 0.316030
2019-11-13 12:19:15,309 train 100 1.128038e-02 0.285893
2019-11-13 12:19:23,420 train 150 1.130363e-02 0.293346
2019-11-13 12:19:31,441 train 200 1.128649e-02 0.299086
2019-11-13 12:19:39,588 train 250 1.120539e-02 0.305418
2019-11-13 12:19:47,643 train 300 1.118940e-02 0.305811
2019-11-13 12:19:55,906 train 350 1.114302e-02 0.307655
2019-11-13 12:20:04,154 train 400 1.110276e-02 0.308126
2019-11-13 12:20:12,404 train 450 1.111907e-02 0.310379
2019-11-13 12:20:20,638 train 500 1.111396e-02 0.309748
2019-11-13 12:20:28,875 train 550 1.109663e-02 0.309236
2019-11-13 12:20:37,128 train 600 1.109562e-02 0.309821
2019-11-13 12:20:45,364 train 650 1.108064e-02 0.310624
2019-11-13 12:20:53,599 train 700 1.108559e-02 0.311675
2019-11-13 12:21:01,836 train 750 1.106457e-02 0.315472
2019-11-13 12:21:10,084 train 800 1.105491e-02 0.317452
2019-11-13 12:21:18,239 train 850 1.104628e-02 0.316381
2019-11-13 12:21:20,655 training loss; R2: 1.104573e-02 0.317049
2019-11-13 12:21:20,968 valid 000 5.651960e-02 -1.066560
2019-11-13 12:21:22,709 valid 050 5.603188e-02 -1.986253
2019-11-13 12:21:24,248 validation loss; R2: 5.573017e-02 -2.139517
2019-11-13 12:21:24,266 epoch 19 lr 1.000000e-03
2019-11-13 12:21:24,697 train 000 1.144721e-02 0.111261
2019-11-13 12:21:32,674 train 050 1.122527e-02 0.302519
2019-11-13 12:21:40,605 train 100 1.111181e-02 0.311605
2019-11-13 12:21:48,512 train 150 1.103913e-02 0.316798
2019-11-13 12:21:56,439 train 200 1.100265e-02 0.291082
2019-11-13 12:22:04,335 train 250 1.093701e-02 0.301377
2019-11-13 12:22:12,258 train 300 1.095286e-02 0.309333
2019-11-13 12:22:20,161 train 350 1.095130e-02 0.310587
2019-11-13 12:22:28,081 train 400 1.096223e-02 0.314888
2019-11-13 12:22:36,340 train 450 1.095719e-02 0.308233
2019-11-13 12:22:44,572 train 500 1.093694e-02 0.312164
2019-11-13 12:22:52,575 train 550 1.095980e-02 0.316244
2019-11-13 12:23:00,497 train 600 1.095018e-02 0.320300
2019-11-13 12:23:08,391 train 650 1.093731e-02 0.321854
2019-11-13 12:23:16,295 train 700 1.091793e-02 0.323442
2019-11-13 12:23:24,191 train 750 1.094804e-02 0.322176
2019-11-13 12:23:32,090 train 800 1.094129e-02 0.310186
2019-11-13 12:23:39,989 train 850 1.094375e-02 0.311330
2019-11-13 12:23:42,347 training loss; R2: 1.094304e-02 0.310865
2019-11-13 12:23:42,637 valid 000 2.157218e-02 -0.394294
2019-11-13 12:23:44,330 valid 050 2.326536e-02 -0.523031
2019-11-13 12:23:45,844 validation loss; R2: 2.328022e-02 -0.500055
