2019-11-13 04:09:34,815 gpu device = 1
2019-11-13 04:09:34,815 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name='galaxy-zoo-cleaned', gpu=1, grad_clip=5, gz_dtree=False, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-galaxy-zoo-EXP-20191113-040934', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-11-13 04:09:46,654 param size = 0.195637MB
2019-11-13 04:09:46,658 epoch 0 lr 1.000000e-03
2019-11-13 04:09:48,734 train 000 1.642051e+00 -1357.502687
2019-11-13 04:09:54,010 train 050 9.340876e-02 -43.277275
2019-11-13 04:09:59,514 train 100 6.349105e-02 -22.806938
2019-11-13 04:10:05,029 train 150 5.254736e-02 -15.751059
2019-11-13 04:10:10,534 train 200 4.669523e-02 -12.017889
2019-11-13 04:10:16,029 train 250 4.297096e-02 -9.738076
2019-11-13 04:10:21,440 train 300 4.035806e-02 -8.214980
2019-11-13 04:10:26,728 train 350 3.842394e-02 -11.033348
2019-11-13 04:10:31,972 train 400 3.688075e-02 -9.716737
2019-11-13 04:10:37,218 train 450 3.564213e-02 -8.670892
2019-11-13 04:10:42,500 train 500 3.459341e-02 -7.829070
2019-11-13 04:10:47,770 train 550 3.371381e-02 -7.141269
2019-11-13 04:10:53,061 train 600 3.302333e-02 -6.582222
2019-11-13 04:10:58,332 train 650 3.237479e-02 -6.089635
2019-11-13 04:11:03,598 train 700 3.177504e-02 -5.668552
2019-11-13 04:11:08,856 train 750 3.121463e-02 -5.311404
2019-11-13 04:11:14,141 train 800 3.071480e-02 -4.986662
2019-11-13 04:11:19,402 train 850 3.029819e-02 -4.700278
2019-11-13 04:11:21,677 training loss; R2: 3.015320e-02 -4.620477
2019-11-13 04:11:21,981 valid 000 2.088838e-02 0.100243
2019-11-13 04:11:23,601 valid 050 2.089828e-02 0.084092
2019-11-13 04:11:25,185 validation loss; R2: 2.068119e-02 0.044383
2019-11-13 04:11:25,196 epoch 1 lr 1.000000e-03
2019-11-13 04:11:25,654 train 000 2.087679e-02 -0.019002
2019-11-13 04:11:30,947 train 050 2.312292e-02 -0.331989
2019-11-13 04:11:36,349 train 100 2.281095e-02 -0.208094
2019-11-13 04:11:41,689 train 150 2.258286e-02 -0.164483
2019-11-13 04:11:46,939 train 200 2.257167e-02 -0.135722
2019-11-13 04:11:52,192 train 250 2.247101e-02 -0.120791
2019-11-13 04:11:57,454 train 300 2.232686e-02 -0.110506
2019-11-13 04:12:02,711 train 350 2.220266e-02 -0.097656
2019-11-13 04:12:07,997 train 400 2.209184e-02 -0.103724
2019-11-13 04:12:13,258 train 450 2.198874e-02 -0.102706
2019-11-13 04:12:18,525 train 500 2.191288e-02 -0.093394
2019-11-13 04:12:23,769 train 550 2.185145e-02 -0.083641
2019-11-13 04:12:29,029 train 600 2.172542e-02 -0.075007
2019-11-13 04:12:34,273 train 650 2.162575e-02 -0.067409
2019-11-13 04:12:39,573 train 700 2.151773e-02 -0.130209
2019-11-13 04:12:44,854 train 750 2.142330e-02 -0.120804
2019-11-13 04:12:50,238 train 800 2.135588e-02 -0.111368
2019-11-13 04:12:55,549 train 850 2.127167e-02 -0.103035
2019-11-13 04:12:57,165 training loss; R2: 2.126898e-02 -0.100706
2019-11-13 04:12:57,460 valid 000 1.838242e-02 0.195163
2019-11-13 04:12:59,116 valid 050 1.897184e-02 0.121380
2019-11-13 04:13:00,635 validation loss; R2: 1.920327e-02 0.030686
2019-11-13 04:13:00,646 epoch 2 lr 1.000000e-03
2019-11-13 04:13:00,998 train 000 2.010051e-02 0.150036
2019-11-13 04:13:06,106 train 050 1.977807e-02 0.013682
2019-11-13 04:13:11,440 train 100 1.979300e-02 0.020927
2019-11-13 04:13:16,700 train 150 1.964899e-02 0.036118
2019-11-13 04:13:21,927 train 200 1.957557e-02 0.041624
2019-11-13 04:13:27,166 train 250 1.944179e-02 0.045057
2019-11-13 04:13:32,295 train 300 1.943558e-02 0.042459
2019-11-13 04:13:37,385 train 350 1.933295e-02 0.041298
2019-11-13 04:13:42,490 train 400 1.930255e-02 0.043961
2019-11-13 04:13:47,607 train 450 1.922594e-02 0.049138
2019-11-13 04:13:52,708 train 500 1.913940e-02 0.050892
2019-11-13 04:13:57,826 train 550 1.915040e-02 0.053314
2019-11-13 04:14:02,943 train 600 1.908106e-02 0.050909
2019-11-13 04:14:08,056 train 650 1.902566e-02 0.055882
2019-11-13 04:14:13,170 train 700 1.892009e-02 0.043738
2019-11-13 04:14:18,287 train 750 1.886289e-02 0.045735
2019-11-13 04:14:23,448 train 800 1.877633e-02 0.050756
2019-11-13 04:14:28,613 train 850 1.870117e-02 0.053499
2019-11-13 04:14:30,159 training loss; R2: 1.868095e-02 0.054551
2019-11-13 04:14:30,478 valid 000 1.333378e-02 0.218315
2019-11-13 04:14:32,162 valid 050 1.558392e-02 0.231536
2019-11-13 04:14:33,678 validation loss; R2: 1.572824e-02 0.136075
2019-11-13 04:14:33,688 epoch 3 lr 1.000000e-03
2019-11-13 04:14:34,050 train 000 1.841414e-02 0.101498
2019-11-13 04:14:39,406 train 050 1.745052e-02 0.089543
2019-11-13 04:14:44,565 train 100 1.739161e-02 0.091431
2019-11-13 04:14:49,765 train 150 1.734067e-02 0.086157
2019-11-13 04:14:55,055 train 200 1.711995e-02 0.096128
2019-11-13 04:15:00,147 train 250 1.709735e-02 0.086576
2019-11-13 04:15:05,500 train 300 1.707993e-02 0.091593
2019-11-13 04:15:10,744 train 350 1.701087e-02 0.097353
2019-11-13 04:15:15,978 train 400 1.700039e-02 0.102416
2019-11-13 04:15:21,249 train 450 1.694252e-02 0.109737
2019-11-13 04:15:26,525 train 500 1.688316e-02 0.107354
2019-11-13 04:15:31,861 train 550 1.684312e-02 0.110612
2019-11-13 04:15:37,312 train 600 1.679085e-02 0.112877
2019-11-13 04:15:42,750 train 650 1.673474e-02 0.117373
2019-11-13 04:15:48,245 train 700 1.668196e-02 0.120850
2019-11-13 04:15:53,738 train 750 1.667107e-02 0.119861
2019-11-13 04:15:59,187 train 800 1.664946e-02 0.121782
2019-11-13 04:16:04,631 train 850 1.660359e-02 0.122731
2019-11-13 04:16:06,260 training loss; R2: 1.659158e-02 0.123858
2019-11-13 04:16:06,581 valid 000 1.280444e-02 0.239757
2019-11-13 04:16:08,225 valid 050 1.352583e-02 0.261013
2019-11-13 04:16:09,737 validation loss; R2: 1.379058e-02 0.261448
2019-11-13 04:16:09,754 epoch 4 lr 1.000000e-03
2019-11-13 04:16:10,107 train 000 1.358742e-02 0.179953
2019-11-13 04:16:15,344 train 050 1.559807e-02 0.141437
2019-11-13 04:16:20,631 train 100 1.548284e-02 0.134847
2019-11-13 04:16:25,903 train 150 1.561491e-02 0.138588
2019-11-13 04:16:31,223 train 200 1.564855e-02 0.056944
2019-11-13 04:16:36,608 train 250 1.564519e-02 0.078738
2019-11-13 04:16:41,924 train 300 1.557192e-02 0.096716
2019-11-13 04:16:47,200 train 350 1.558548e-02 0.103475
2019-11-13 04:16:52,446 train 400 1.553042e-02 0.104660
2019-11-13 04:16:57,685 train 450 1.550651e-02 0.113595
2019-11-13 04:17:02,942 train 500 1.549371e-02 -0.469205
2019-11-13 04:17:08,203 train 550 1.548168e-02 -0.411592
2019-11-13 04:17:13,496 train 600 1.547662e-02 -0.360677
2019-11-13 04:17:18,721 train 650 1.544436e-02 -0.318050
2019-11-13 04:17:24,011 train 700 1.540194e-02 -0.283431
2019-11-13 04:17:29,268 train 750 1.539978e-02 -0.251816
2019-11-13 04:17:34,487 train 800 1.537564e-02 -0.223550
2019-11-13 04:17:39,717 train 850 1.536063e-02 -0.200326
2019-11-13 04:17:41,226 training loss; R2: 1.534795e-02 -0.193347
2019-11-13 04:17:41,524 valid 000 1.153577e-02 0.348471
2019-11-13 04:17:43,183 valid 050 1.229816e-02 0.239417
2019-11-13 04:17:44,713 validation loss; R2: 1.256889e-02 0.263188
2019-11-13 04:17:44,735 epoch 5 lr 1.000000e-03
2019-11-13 04:17:45,098 train 000 1.472440e-02 0.258043
2019-11-13 04:17:50,080 train 050 1.492348e-02 0.195785
2019-11-13 04:17:55,056 train 100 1.486680e-02 0.208827
2019-11-13 04:18:00,028 train 150 1.484481e-02 0.206301
2019-11-13 04:18:05,430 train 200 1.485259e-02 0.207511
2019-11-13 04:18:10,669 train 250 1.477160e-02 0.211682
2019-11-13 04:18:15,900 train 300 1.476731e-02 0.204074
2019-11-13 04:18:21,105 train 350 1.471062e-02 0.206266
2019-11-13 04:18:26,335 train 400 1.472599e-02 0.205695
2019-11-13 04:18:31,541 train 450 1.470391e-02 0.201103
2019-11-13 04:18:36,744 train 500 1.469338e-02 0.200949
2019-11-13 04:18:41,968 train 550 1.466386e-02 0.192655
2019-11-13 04:18:47,184 train 600 1.463888e-02 0.194438
2019-11-13 04:18:52,407 train 650 1.463800e-02 0.195927
2019-11-13 04:18:57,617 train 700 1.464657e-02 0.198625
2019-11-13 04:19:02,822 train 750 1.460031e-02 0.201086
2019-11-13 04:19:08,054 train 800 1.459024e-02 0.197853
2019-11-13 04:19:13,261 train 850 1.453797e-02 0.198550
2019-11-13 04:19:14,815 training loss; R2: 1.453880e-02 0.199705
2019-11-13 04:19:15,139 valid 000 1.186880e-02 0.327038
2019-11-13 04:19:16,857 valid 050 1.179461e-02 0.312758
2019-11-13 04:19:18,445 validation loss; R2: 1.191734e-02 0.317055
2019-11-13 04:19:18,459 epoch 6 lr 1.000000e-03
2019-11-13 04:19:18,837 train 000 1.493249e-02 0.107938
2019-11-13 04:19:24,033 train 050 1.391155e-02 0.178811
2019-11-13 04:19:29,298 train 100 1.399095e-02 0.203177
2019-11-13 04:19:34,585 train 150 1.397172e-02 0.212298
2019-11-13 04:19:39,949 train 200 1.405252e-02 0.196975
2019-11-13 04:19:45,282 train 250 1.401458e-02 0.206570
2019-11-13 04:19:50,542 train 300 1.396177e-02 0.213514
2019-11-13 04:19:55,794 train 350 1.394931e-02 0.214892
2019-11-13 04:20:01,027 train 400 1.397501e-02 0.216843
2019-11-13 04:20:06,276 train 450 1.396188e-02 0.220603
2019-11-13 04:20:11,519 train 500 1.395799e-02 0.223040
2019-11-13 04:20:16,770 train 550 1.397822e-02 0.220793
2019-11-13 04:20:21,999 train 600 1.396987e-02 0.220819
2019-11-13 04:20:27,234 train 650 1.393869e-02 0.211533
2019-11-13 04:20:32,475 train 700 1.393312e-02 0.209021
2019-11-13 04:20:37,747 train 750 1.392724e-02 0.209217
2019-11-13 04:20:43,005 train 800 1.393463e-02 0.209631
2019-11-13 04:20:48,266 train 850 1.395373e-02 0.210956
2019-11-13 04:20:49,855 training loss; R2: 1.394909e-02 0.211362
2019-11-13 04:20:50,185 valid 000 1.054701e-02 0.317874
2019-11-13 04:20:51,871 valid 050 1.144266e-02 0.356927
2019-11-13 04:20:53,362 validation loss; R2: 1.148884e-02 0.352869
2019-11-13 04:20:53,379 epoch 7 lr 1.000000e-03
2019-11-13 04:20:53,709 train 000 1.295428e-02 -0.086944
2019-11-13 04:20:59,064 train 050 1.361559e-02 0.202652
2019-11-13 04:21:04,429 train 100 1.377692e-02 0.209821
2019-11-13 04:21:09,827 train 150 1.383112e-02 0.209562
2019-11-13 04:21:15,204 train 200 1.381712e-02 0.204004
2019-11-13 04:21:20,549 train 250 1.379548e-02 0.187430
2019-11-13 04:21:25,942 train 300 1.376148e-02 0.187958
2019-11-13 04:21:31,275 train 350 1.376135e-02 0.186234
2019-11-13 04:21:36,470 train 400 1.367376e-02 0.188352
2019-11-13 04:21:41,716 train 450 1.365621e-02 0.194432
2019-11-13 04:21:46,984 train 500 1.365753e-02 0.198565
2019-11-13 04:21:52,218 train 550 1.364606e-02 0.200916
2019-11-13 04:21:57,436 train 600 1.362375e-02 0.203805
2019-11-13 04:22:02,660 train 650 1.360015e-02 0.208002
2019-11-13 04:22:07,884 train 700 1.359929e-02 0.209830
2019-11-13 04:22:13,142 train 750 1.358693e-02 0.211416
2019-11-13 04:22:18,352 train 800 1.359021e-02 0.214449
2019-11-13 04:22:23,560 train 850 1.357874e-02 0.216501
2019-11-13 04:22:25,122 training loss; R2: 1.357433e-02 0.217052
2019-11-13 04:22:25,425 valid 000 7.143845e-02 -1.796800
2019-11-13 04:22:27,100 valid 050 7.456197e-02 -1.928076
2019-11-13 04:22:28,682 validation loss; R2: 7.507792e-02 -1.977012
2019-11-13 04:22:28,696 epoch 8 lr 1.000000e-03
2019-11-13 04:22:29,082 train 000 1.307332e-02 0.087052
2019-11-13 04:22:34,341 train 050 1.310000e-02 0.095235
2019-11-13 04:22:39,562 train 100 1.304413e-02 0.160301
2019-11-13 04:22:44,799 train 150 1.321633e-02 0.149043
2019-11-13 04:22:50,028 train 200 1.326346e-02 0.150182
2019-11-13 04:22:55,292 train 250 1.323818e-02 0.139449
2019-11-13 04:23:00,541 train 300 1.327976e-02 0.157512
2019-11-13 04:23:06,052 train 350 1.328463e-02 0.172596
2019-11-13 04:23:11,356 train 400 1.326419e-02 0.187571
2019-11-13 04:23:16,585 train 450 1.322712e-02 0.193288
2019-11-13 04:23:21,841 train 500 1.322527e-02 0.198467
2019-11-13 04:23:27,076 train 550 1.323605e-02 0.204988
2019-11-13 04:23:32,316 train 600 1.322987e-02 0.202461
2019-11-13 04:23:37,757 train 650 1.323754e-02 0.203852
2019-11-13 04:23:43,021 train 700 1.322197e-02 0.201261
2019-11-13 04:23:48,285 train 750 1.321107e-02 0.205303
2019-11-13 04:23:53,558 train 800 1.319847e-02 0.209644
2019-11-13 04:23:58,833 train 850 1.318345e-02 0.212225
2019-11-13 04:24:00,394 training loss; R2: 1.318092e-02 0.212162
2019-11-13 04:24:00,731 valid 000 2.300772e+01 -743.517942
2019-11-13 04:24:02,431 valid 050 2.297086e+01 -681.603402
2019-11-13 04:24:03,922 validation loss; R2: 2.297408e+01 -681.866188
2019-11-13 04:24:03,939 epoch 9 lr 1.000000e-03
2019-11-13 04:24:04,288 train 000 1.250242e-02 0.281090
2019-11-13 04:24:09,626 train 050 1.294001e-02 0.272989
2019-11-13 04:24:15,124 train 100 1.297899e-02 0.221566
2019-11-13 04:24:20,632 train 150 1.299378e-02 0.230405
2019-11-13 04:24:25,965 train 200 1.298039e-02 0.220672
2019-11-13 04:24:31,388 train 250 1.298894e-02 0.226037
2019-11-13 04:24:36,644 train 300 1.288830e-02 0.236736
2019-11-13 04:24:41,970 train 350 1.289858e-02 0.235929
2019-11-13 04:24:47,284 train 400 1.297562e-02 0.238539
2019-11-13 04:24:52,602 train 450 1.299133e-02 0.242816
2019-11-13 04:24:57,825 train 500 1.298592e-02 0.247606
2019-11-13 04:25:03,145 train 550 1.296237e-02 0.250070
2019-11-13 04:25:08,380 train 600 1.295516e-02 0.245223
2019-11-13 04:25:13,623 train 650 1.293825e-02 0.246760
2019-11-13 04:25:18,838 train 700 1.293176e-02 0.235220
2019-11-13 04:25:24,073 train 750 1.293672e-02 0.232705
2019-11-13 04:25:29,267 train 800 1.294432e-02 0.232430
2019-11-13 04:25:34,464 train 850 1.293137e-02 0.234662
2019-11-13 04:25:36,027 training loss; R2: 1.292328e-02 0.235610
2019-11-13 04:25:36,340 valid 000 1.360460e+00 -71.641603
2019-11-13 04:25:38,060 valid 050 1.364855e+00 -101.357748
2019-11-13 04:25:39,615 validation loss; R2: 1.365451e+00 -106.213404
2019-11-13 04:25:39,629 epoch 10 lr 1.000000e-03
2019-11-13 04:25:39,981 train 000 1.271157e-02 0.338870
2019-11-13 04:25:45,198 train 050 1.273776e-02 0.265818
2019-11-13 04:25:50,448 train 100 1.283451e-02 0.244040
2019-11-13 04:25:55,905 train 150 1.276232e-02 0.257006
2019-11-13 04:26:01,189 train 200 1.271009e-02 0.262798
2019-11-13 04:26:06,432 train 250 1.275820e-02 0.251577
2019-11-13 04:26:11,681 train 300 1.275020e-02 0.255936
2019-11-13 04:26:16,939 train 350 1.275539e-02 0.260194
2019-11-13 04:26:22,172 train 400 1.278527e-02 0.258198
2019-11-13 04:26:27,414 train 450 1.279877e-02 0.258651
2019-11-13 04:26:32,649 train 500 1.278389e-02 0.260636
2019-11-13 04:26:37,872 train 550 1.276364e-02 0.262214
2019-11-13 04:26:43,100 train 600 1.272259e-02 0.208616
2019-11-13 04:26:48,336 train 650 1.269354e-02 0.193247
2019-11-13 04:26:53,562 train 700 1.269221e-02 0.199276
2019-11-13 04:26:58,798 train 750 1.269740e-02 0.201616
2019-11-13 04:27:04,028 train 800 1.270874e-02 0.206224
2019-11-13 04:27:09,269 train 850 1.269907e-02 0.208709
2019-11-13 04:27:10,827 training loss; R2: 1.268927e-02 0.210768
2019-11-13 04:27:11,144 valid 000 1.150874e+00 -64.103034
2019-11-13 04:27:12,797 valid 050 1.154514e+00 -62.675117
2019-11-13 04:27:14,335 validation loss; R2: 1.154208e+00 -67.829788
2019-11-13 04:27:14,349 epoch 11 lr 1.000000e-03
2019-11-13 04:27:14,712 train 000 1.228495e-02 0.300493
2019-11-13 04:27:19,931 train 050 1.233395e-02 0.296457
2019-11-13 04:27:25,223 train 100 1.246343e-02 0.297568
2019-11-13 04:27:30,521 train 150 1.243158e-02 0.291321
2019-11-13 04:27:35,787 train 200 1.243906e-02 0.284228
2019-11-13 04:27:41,056 train 250 1.242426e-02 0.284690
2019-11-13 04:27:46,313 train 300 1.246222e-02 0.284556
2019-11-13 04:27:51,526 train 350 1.246770e-02 0.284211
2019-11-13 04:27:56,743 train 400 1.248803e-02 0.284095
2019-11-13 04:28:01,970 train 450 1.247278e-02 0.284695
2019-11-13 04:28:07,180 train 500 1.247948e-02 0.286860
2019-11-13 04:28:12,393 train 550 1.249770e-02 0.286426
2019-11-13 04:28:17,592 train 600 1.249520e-02 0.280294
2019-11-13 04:28:22,807 train 650 1.249534e-02 0.274521
2019-11-13 04:28:28,006 train 700 1.250522e-02 0.269004
2019-11-13 04:28:33,182 train 750 1.250362e-02 0.268878
2019-11-13 04:28:38,375 train 800 1.251373e-02 0.269298
2019-11-13 04:28:43,576 train 850 1.252302e-02 0.265824
2019-11-13 04:28:45,147 training loss; R2: 1.251576e-02 0.265896
2019-11-13 04:28:45,458 valid 000 6.806239e-01 -52.732938
2019-11-13 04:28:47,158 valid 050 6.765871e-01 -67.948583
2019-11-13 04:28:48,680 validation loss; R2: 6.751124e-01 -65.191015
2019-11-13 04:28:48,694 epoch 12 lr 1.000000e-03
2019-11-13 04:28:49,075 train 000 1.213627e-02 0.347333
2019-11-13 04:28:54,421 train 050 1.250854e-02 0.286658
2019-11-13 04:28:59,687 train 100 1.242218e-02 0.251314
2019-11-13 04:29:04,955 train 150 1.237921e-02 0.248947
2019-11-13 04:29:10,210 train 200 1.236033e-02 0.245361
2019-11-13 04:29:15,696 train 250 1.235160e-02 0.253014
2019-11-13 04:29:21,070 train 300 1.237050e-02 0.251841
2019-11-13 04:29:26,428 train 350 1.239874e-02 0.236372
2019-11-13 04:29:31,947 train 400 1.239215e-02 0.239123
2019-11-13 04:29:37,200 train 450 1.239437e-02 0.242584
2019-11-13 04:29:42,418 train 500 1.237903e-02 0.241983
2019-11-13 04:29:47,593 train 550 1.237815e-02 0.244864
2019-11-13 04:29:52,762 train 600 1.237886e-02 0.243801
2019-11-13 04:29:58,015 train 650 1.237034e-02 0.247219
2019-11-13 04:30:03,471 train 700 1.237087e-02 0.244761
2019-11-13 04:30:08,613 train 750 1.234846e-02 0.248279
2019-11-13 04:30:13,935 train 800 1.234892e-02 0.248095
2019-11-13 04:30:19,126 train 850 1.235101e-02 0.249461
2019-11-13 04:30:20,673 training loss; R2: 1.235149e-02 0.247693
2019-11-13 04:30:20,979 valid 000 9.256985e-01 -109.210250
2019-11-13 04:30:22,686 valid 050 9.179552e-01 -104.486480
2019-11-13 04:30:24,239 validation loss; R2: 9.176821e-01 -101.748271
2019-11-13 04:30:24,258 epoch 13 lr 1.000000e-03
2019-11-13 04:30:24,684 train 000 1.056782e-02 0.287220
2019-11-13 04:30:29,924 train 050 1.227654e-02 0.277426
2019-11-13 04:30:35,147 train 100 1.237623e-02 0.255855
2019-11-13 04:30:40,369 train 150 1.223764e-02 0.272745
2019-11-13 04:30:45,659 train 200 1.218027e-02 0.276527
2019-11-13 04:30:50,957 train 250 1.220111e-02 0.248481
2019-11-13 04:30:56,255 train 300 1.224138e-02 0.251354
2019-11-13 04:31:01,627 train 350 1.224573e-02 0.255724
2019-11-13 04:31:06,889 train 400 1.222730e-02 0.259656
2019-11-13 04:31:12,110 train 450 1.220918e-02 0.262507
2019-11-13 04:31:17,344 train 500 1.220044e-02 0.260888
2019-11-13 04:31:22,586 train 550 1.223447e-02 0.254070
2019-11-13 04:31:27,810 train 600 1.223524e-02 0.249530
2019-11-13 04:31:33,033 train 650 1.224839e-02 0.248414
2019-11-13 04:31:38,020 train 700 1.223207e-02 0.252517
2019-11-13 04:31:43,060 train 750 1.224182e-02 0.252411
2019-11-13 04:31:48,271 train 800 1.221709e-02 0.254467
2019-11-13 04:31:53,506 train 850 1.221390e-02 0.250468
2019-11-13 04:31:55,054 training loss; R2: 1.222151e-02 0.250247
2019-11-13 04:31:55,357 valid 000 3.320950e+00 -108.213875
2019-11-13 04:31:57,079 valid 050 3.340636e+00 -167.955625
2019-11-13 04:31:58,641 validation loss; R2: 3.338934e+00 -162.461893
2019-11-13 04:31:58,654 epoch 14 lr 1.000000e-03
2019-11-13 04:31:59,016 train 000 1.481510e-02 0.272884
2019-11-13 04:32:04,215 train 050 1.234249e-02 0.267185
2019-11-13 04:32:09,426 train 100 1.229670e-02 0.273775
2019-11-13 04:32:14,734 train 150 1.222448e-02 0.286830
2019-11-13 04:32:20,087 train 200 1.219085e-02 0.239069
2019-11-13 04:32:25,501 train 250 1.222543e-02 0.244631
2019-11-13 04:32:31,044 train 300 1.218399e-02 0.251135
2019-11-13 04:32:36,446 train 350 1.216842e-02 0.258339
2019-11-13 04:32:41,867 train 400 1.221390e-02 0.262735
2019-11-13 04:32:47,380 train 450 1.217783e-02 0.267126
2019-11-13 04:32:52,852 train 500 1.218952e-02 0.257332
2019-11-13 04:32:58,233 train 550 1.218397e-02 0.257230
2019-11-13 04:33:03,487 train 600 1.215759e-02 0.259725
2019-11-13 04:33:08,730 train 650 1.212329e-02 0.263637
2019-11-13 04:33:14,000 train 700 1.213930e-02 0.264128
2019-11-13 04:33:19,253 train 750 1.211928e-02 0.267956
2019-11-13 04:33:24,495 train 800 1.211507e-02 0.270753
2019-11-13 04:33:29,720 train 850 1.212963e-02 0.273496
2019-11-13 04:33:31,279 training loss; R2: 1.213216e-02 0.273804
2019-11-13 04:33:31,608 valid 000 3.061218e-01 -9.455856
2019-11-13 04:33:33,282 valid 050 3.091064e-01 -14.973724
2019-11-13 04:33:34,801 validation loss; R2: 3.091715e-01 -16.504811
2019-11-13 04:33:34,812 epoch 15 lr 1.000000e-03
2019-11-13 04:33:35,197 train 000 1.027476e-02 0.332400
2019-11-13 04:33:40,410 train 050 1.195866e-02 0.290446
2019-11-13 04:33:45,694 train 100 1.206848e-02 0.270613
2019-11-13 04:33:51,023 train 150 1.199101e-02 0.274758
2019-11-13 04:33:56,323 train 200 1.203510e-02 0.280285
2019-11-13 04:34:01,712 train 250 1.205129e-02 0.285254
2019-11-13 04:34:06,997 train 300 1.201391e-02 0.279419
2019-11-13 04:34:12,282 train 350 1.205615e-02 0.282255
2019-11-13 04:34:17,552 train 400 1.203436e-02 0.236015
2019-11-13 04:34:22,797 train 450 1.203466e-02 0.241218
2019-11-13 04:34:28,071 train 500 1.205496e-02 0.246545
2019-11-13 04:34:33,424 train 550 1.207911e-02 0.249626
2019-11-13 04:34:38,690 train 600 1.208910e-02 0.250152
2019-11-13 04:34:43,931 train 650 1.209112e-02 0.252173
2019-11-13 04:34:49,274 train 700 1.206775e-02 0.255543
2019-11-13 04:34:54,666 train 750 1.207042e-02 0.254114
2019-11-13 04:34:59,929 train 800 1.202300e-02 0.254072
2019-11-13 04:35:05,134 train 850 1.200786e-02 0.254881
2019-11-13 04:35:06,680 training loss; R2: 1.201692e-02 0.255434
2019-11-13 04:35:06,999 valid 000 2.514919e+00 -89.926112
2019-11-13 04:35:08,698 valid 050 2.537394e+00 -291.156904
2019-11-13 04:35:10,209 validation loss; R2: 2.537155e+00 -212.692159
2019-11-13 04:35:10,228 epoch 16 lr 1.000000e-03
2019-11-13 04:35:10,604 train 000 1.319689e-02 0.334023
2019-11-13 04:35:15,907 train 050 1.163459e-02 0.325321
2019-11-13 04:35:21,205 train 100 1.190569e-02 0.305704
2019-11-13 04:35:26,476 train 150 1.186860e-02 0.288766
2019-11-13 04:35:31,754 train 200 1.190305e-02 0.293825
2019-11-13 04:35:37,028 train 250 1.195365e-02 0.294541
2019-11-13 04:35:42,283 train 300 1.197480e-02 0.294818
2019-11-13 04:35:47,562 train 350 1.196836e-02 0.293397
2019-11-13 04:35:52,896 train 400 1.197720e-02 0.291248
2019-11-13 04:35:58,291 train 450 1.197982e-02 0.294408
2019-11-13 04:36:03,775 train 500 1.199179e-02 0.295036
2019-11-13 04:36:09,142 train 550 1.194790e-02 0.290556
2019-11-13 04:36:14,378 train 600 1.193361e-02 0.292854
2019-11-13 04:36:19,626 train 650 1.192362e-02 0.293217
2019-11-13 04:36:24,864 train 700 1.192390e-02 0.289865
2019-11-13 04:36:30,239 train 750 1.192799e-02 0.289028
2019-11-13 04:36:35,759 train 800 1.192621e-02 0.289634
2019-11-13 04:36:41,143 train 850 1.192109e-02 0.288005
2019-11-13 04:36:42,714 training loss; R2: 1.191923e-02 0.288229
2019-11-13 04:36:43,048 valid 000 1.845465e+00 -798.760307
2019-11-13 04:36:44,700 valid 050 1.844812e+00 -545.818346
2019-11-13 04:36:46,236 validation loss; R2: 1.842467e+00 -682.268593
2019-11-13 04:36:46,253 epoch 17 lr 1.000000e-03
2019-11-13 04:36:46,598 train 000 1.178471e-02 -0.847834
2019-11-13 04:36:51,694 train 050 1.195566e-02 0.233245
2019-11-13 04:36:56,985 train 100 1.206894e-02 0.235575
2019-11-13 04:37:02,235 train 150 1.201149e-02 0.264956
2019-11-13 04:37:07,440 train 200 1.198291e-02 0.278733
2019-11-13 04:37:12,802 train 250 1.195684e-02 0.288711
2019-11-13 04:37:18,094 train 300 1.190610e-02 0.294924
2019-11-13 04:37:23,448 train 350 1.190140e-02 0.297347
2019-11-13 04:37:28,636 train 400 1.187642e-02 0.296558
2019-11-13 04:37:33,904 train 450 1.184432e-02 0.297352
2019-11-13 04:37:39,098 train 500 1.185687e-02 0.281961
2019-11-13 04:37:44,362 train 550 1.183950e-02 0.281030
2019-11-13 04:37:49,564 train 600 1.184068e-02 0.282903
2019-11-13 04:37:54,759 train 650 1.185144e-02 0.280389
2019-11-13 04:38:00,032 train 700 1.185336e-02 0.282336
2019-11-13 04:38:05,246 train 750 1.187742e-02 -0.377116
2019-11-13 04:38:10,456 train 800 1.187622e-02 -0.333934
2019-11-13 04:38:15,785 train 850 1.185831e-02 -0.297048
2019-11-13 04:38:17,439 training loss; R2: 1.186179e-02 -0.286445
2019-11-13 04:38:17,748 valid 000 1.201117e+00 -51.515653
2019-11-13 04:38:19,438 valid 050 1.227438e+00 -57.149478
2019-11-13 04:38:20,945 validation loss; R2: 1.226700e+00 -58.432989
2019-11-13 04:38:20,960 epoch 18 lr 1.000000e-03
2019-11-13 04:38:21,386 train 000 1.096811e-02 0.397052
2019-11-13 04:38:26,568 train 050 1.166901e-02 0.170438
2019-11-13 04:38:31,795 train 100 1.171028e-02 0.239165
2019-11-13 04:38:37,030 train 150 1.175675e-02 0.270033
2019-11-13 04:38:42,235 train 200 1.177943e-02 0.247080
2019-11-13 04:38:47,433 train 250 1.187338e-02 0.256947
2019-11-13 04:38:52,662 train 300 1.184481e-02 0.262601
2019-11-13 04:38:57,852 train 350 1.182045e-02 0.266895
2019-11-13 04:39:03,050 train 400 1.177852e-02 0.273078
2019-11-13 04:39:08,245 train 450 1.180407e-02 0.272664
2019-11-13 04:39:13,459 train 500 1.179947e-02 0.278329
2019-11-13 04:39:18,672 train 550 1.177674e-02 0.282028
2019-11-13 04:39:23,859 train 600 1.177721e-02 0.276125
2019-11-13 04:39:29,054 train 650 1.179618e-02 0.274092
2019-11-13 04:39:34,244 train 700 1.176723e-02 0.278883
2019-11-13 04:39:39,460 train 750 1.177727e-02 0.278156
2019-11-13 04:39:44,544 train 800 1.177436e-02 0.279099
2019-11-13 04:39:49,543 train 850 1.177310e-02 0.280491
2019-11-13 04:39:51,023 training loss; R2: 1.176776e-02 0.280168
2019-11-13 04:39:51,306 valid 000 2.153774e+01 -911.002275
2019-11-13 04:39:53,007 valid 050 2.147957e+01 -688.149978
2019-11-13 04:39:54,562 validation loss; R2: 2.149225e+01 -713.847996
2019-11-13 04:39:54,581 epoch 19 lr 1.000000e-03
2019-11-13 04:39:54,926 train 000 1.337666e-02 0.373374
2019-11-13 04:40:00,162 train 050 1.152733e-02 0.336490
2019-11-13 04:40:05,428 train 100 1.173477e-02 0.289848
2019-11-13 04:40:10,792 train 150 1.165286e-02 -0.753713
2019-11-13 04:40:16,340 train 200 1.165606e-02 -0.500571
2019-11-13 04:40:21,687 train 250 1.166971e-02 -0.395844
2019-11-13 04:40:26,933 train 300 1.166216e-02 -0.282050
2019-11-13 04:40:32,173 train 350 1.168473e-02 -0.201537
2019-11-13 04:40:37,430 train 400 1.168088e-02 -0.139350
2019-11-13 04:40:42,665 train 450 1.167937e-02 -0.091335
2019-11-13 04:40:47,895 train 500 1.167875e-02 -0.054827
2019-11-13 04:40:53,399 train 550 1.169169e-02 -0.021314
2019-11-13 04:40:58,923 train 600 1.168802e-02 0.006013
2019-11-13 04:41:04,444 train 650 1.168129e-02 0.029599
2019-11-13 04:41:09,957 train 700 1.169170e-02 0.050518
2019-11-13 04:41:15,462 train 750 1.171774e-02 0.067221
2019-11-13 04:41:20,980 train 800 1.171037e-02 0.082178
2019-11-13 04:41:26,485 train 850 1.170351e-02 0.094438
2019-11-13 04:41:28,125 training loss; R2: 1.170562e-02 0.097678
2019-11-13 04:41:28,454 valid 000 4.121386e+00 -536.486436
2019-11-13 04:41:30,115 valid 050 4.089289e+00 -523.029445
2019-11-13 04:41:31,617 validation loss; R2: 4.090526e+00 -513.057524
