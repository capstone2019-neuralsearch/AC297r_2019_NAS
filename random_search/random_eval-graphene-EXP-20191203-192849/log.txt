2019-12-03 19:28:49,296 gpu device = 1
2019-12-03 19:28:49,296 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=96, cutout=False, cutout_length=16, data='../data', dataset='graphene', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name=None, gpu=1, grad_clip=5, gz_dtree=False, init_channels=36, layers=8, learning_rate=0.002, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-graphene-EXP-20191203-192849', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-12-03 19:28:52,604 param size = 1.168381MB
2019-12-03 19:28:52,607 epoch 0 lr 2.000000e-03
2019-12-03 19:28:55,195 train 000 3.696482e-01 -8.911794
2019-12-03 19:29:07,863 train 050 1.604084e-01 -6.448011
2019-12-03 19:29:20,501 train 100 9.300204e-02 -3.167070
2019-12-03 19:29:33,138 train 150 6.971807e-02 -2.008968
2019-12-03 19:29:45,781 train 200 5.642153e-02 -1.394456
2019-12-03 19:29:52,468 training loss; R2: 5.237523e-02 -1.207154
2019-12-03 19:29:52,605 valid 000 1.026223e-02 0.711221
2019-12-03 19:29:54,258 validation loss; R2: 9.522625e-03 0.688804
2019-12-03 19:29:54,280 epoch 1 lr 2.000000e-03
2019-12-03 19:29:54,637 train 000 1.504480e-02 -0.184223
2019-12-03 19:30:07,282 train 050 1.293910e-02 0.567527
2019-12-03 19:30:19,930 train 100 1.207584e-02 0.599087
2019-12-03 19:30:32,580 train 150 1.168987e-02 0.606282
2019-12-03 19:30:45,231 train 200 1.128130e-02 0.618479
2019-12-03 19:30:50,918 training loss; R2: 1.118165e-02 0.623501
2019-12-03 19:30:51,046 valid 000 5.227160e-03 0.864317
2019-12-03 19:30:52,381 validation loss; R2: 6.085023e-03 0.779695
2019-12-03 19:30:52,417 epoch 2 lr 2.000000e-03
2019-12-03 19:30:52,732 train 000 6.593635e-03 0.806913
2019-12-03 19:31:05,379 train 050 9.300005e-03 0.682892
2019-12-03 19:31:18,029 train 100 8.935595e-03 0.709676
2019-12-03 19:31:30,681 train 150 9.184086e-03 0.694374
2019-12-03 19:31:43,342 train 200 9.120930e-03 0.697245
2019-12-03 19:31:49,033 training loss; R2: 8.977247e-03 0.701636
2019-12-03 19:31:49,154 valid 000 6.061297e-03 0.857804
2019-12-03 19:31:50,488 validation loss; R2: 5.003199e-03 0.829980
2019-12-03 19:31:50,512 epoch 3 lr 2.000000e-03
2019-12-03 19:31:50,832 train 000 6.228764e-03 0.637895
2019-12-03 19:32:03,486 train 050 7.567438e-03 0.738223
2019-12-03 19:32:16,143 train 100 8.083055e-03 0.732463
2019-12-03 19:32:28,796 train 150 7.641016e-03 0.745623
2019-12-03 19:32:41,432 train 200 7.637044e-03 0.745236
2019-12-03 19:32:47,113 training loss; R2: 7.739298e-03 0.741085
2019-12-03 19:32:47,236 valid 000 8.942124e-03 0.766234
2019-12-03 19:32:48,569 validation loss; R2: 8.929952e-03 0.695131
2019-12-03 19:32:48,592 epoch 4 lr 2.000000e-03
2019-12-03 19:32:48,906 train 000 9.733391e-03 0.720184
2019-12-03 19:33:01,542 train 050 7.404623e-03 0.758908
2019-12-03 19:33:14,187 train 100 7.179686e-03 0.752979
2019-12-03 19:33:26,830 train 150 7.072678e-03 0.766575
2019-12-03 19:33:39,473 train 200 6.935967e-03 0.769542
2019-12-03 19:33:45,162 training loss; R2: 6.874699e-03 0.770717
2019-12-03 19:33:45,281 valid 000 2.074298e-03 0.883161
2019-12-03 19:33:46,616 validation loss; R2: 3.707910e-03 0.871911
2019-12-03 19:33:46,641 epoch 5 lr 2.000000e-03
2019-12-03 19:33:46,962 train 000 9.484253e-03 0.831758
2019-12-03 19:33:59,608 train 050 6.914786e-03 0.759820
2019-12-03 19:34:12,256 train 100 7.098725e-03 0.756069
2019-12-03 19:34:24,899 train 150 7.154853e-03 0.757315
2019-12-03 19:34:37,545 train 200 6.930434e-03 0.767215
2019-12-03 19:34:43,233 training loss; R2: 6.834552e-03 0.770501
2019-12-03 19:34:43,350 valid 000 5.737016e-03 0.902998
2019-12-03 19:34:44,685 validation loss; R2: 3.525497e-03 0.881784
2019-12-03 19:34:44,710 epoch 6 lr 2.000000e-03
2019-12-03 19:34:45,031 train 000 5.940031e-03 0.517307
2019-12-03 19:34:57,669 train 050 5.494345e-03 0.792962
2019-12-03 19:35:10,310 train 100 5.509361e-03 0.803949
2019-12-03 19:35:22,961 train 150 5.702890e-03 0.800659
2019-12-03 19:35:35,617 train 200 5.986087e-03 0.795836
2019-12-03 19:35:41,310 training loss; R2: 5.976406e-03 0.799744
2019-12-03 19:35:41,439 valid 000 4.790985e-03 0.802440
2019-12-03 19:35:42,772 validation loss; R2: 4.786026e-03 0.827722
2019-12-03 19:35:42,797 epoch 7 lr 2.000000e-03
2019-12-03 19:35:43,114 train 000 6.766178e-03 0.876572
2019-12-03 19:35:55,759 train 050 6.138469e-03 0.782139
2019-12-03 19:36:08,403 train 100 5.697133e-03 0.795236
2019-12-03 19:36:21,042 train 150 5.465451e-03 0.808124
2019-12-03 19:36:33,684 train 200 5.495345e-03 0.813890
2019-12-03 19:36:39,371 training loss; R2: 5.451312e-03 0.814722
2019-12-03 19:36:39,488 valid 000 2.909252e-03 0.910900
2019-12-03 19:36:40,823 validation loss; R2: 4.177769e-03 0.858764
2019-12-03 19:36:40,847 epoch 8 lr 2.000000e-03
2019-12-03 19:36:41,163 train 000 5.100019e-03 0.885292
2019-12-03 19:36:53,799 train 050 4.911339e-03 0.821566
2019-12-03 19:37:06,428 train 100 5.572086e-03 0.805047
2019-12-03 19:37:19,067 train 150 5.655573e-03 0.810026
2019-12-03 19:37:31,700 train 200 5.494346e-03 0.813812
2019-12-03 19:37:37,385 training loss; R2: 5.463443e-03 0.813532
2019-12-03 19:37:37,506 valid 000 2.052118e-03 0.926089
2019-12-03 19:37:38,841 validation loss; R2: 3.396186e-03 0.887361
2019-12-03 19:37:38,869 epoch 9 lr 2.000000e-03
2019-12-03 19:37:39,192 train 000 3.307800e-03 0.882947
2019-12-03 19:37:51,813 train 050 5.624723e-03 0.795928
2019-12-03 19:38:04,445 train 100 5.270076e-03 0.815349
2019-12-03 19:38:17,070 train 150 5.110616e-03 0.822493
2019-12-03 19:38:29,691 train 200 5.154138e-03 0.821966
2019-12-03 19:38:35,365 training loss; R2: 5.174281e-03 0.822960
2019-12-03 19:38:35,491 valid 000 4.698445e-03 0.879041
2019-12-03 19:38:36,825 validation loss; R2: 4.834917e-03 0.831830
2019-12-03 19:38:36,857 epoch 10 lr 2.000000e-03
2019-12-03 19:38:37,180 train 000 4.815015e-03 0.822105
2019-12-03 19:38:49,791 train 050 5.017175e-03 0.832557
2019-12-03 19:39:02,401 train 100 5.008984e-03 0.827261
2019-12-03 19:39:15,008 train 150 5.063237e-03 0.825960
2019-12-03 19:39:27,617 train 200 5.043551e-03 0.829475
2019-12-03 19:39:33,285 training loss; R2: 5.112543e-03 0.827064
2019-12-03 19:39:33,406 valid 000 9.357681e-03 0.710887
2019-12-03 19:39:34,740 validation loss; R2: 9.177456e-03 0.692868
2019-12-03 19:39:34,765 epoch 11 lr 2.000000e-03
2019-12-03 19:39:35,083 train 000 5.800094e-03 0.807001
2019-12-03 19:39:47,685 train 050 5.332811e-03 0.839746
2019-12-03 19:40:00,298 train 100 5.249657e-03 0.838561
2019-12-03 19:40:12,908 train 150 5.064642e-03 0.833099
2019-12-03 19:40:25,506 train 200 5.206061e-03 0.825993
2019-12-03 19:40:31,172 training loss; R2: 5.187601e-03 0.826035
2019-12-03 19:40:31,287 valid 000 2.835443e-03 0.893111
2019-12-03 19:40:32,620 validation loss; R2: 3.644683e-03 0.872418
2019-12-03 19:40:32,644 epoch 12 lr 2.000000e-03
2019-12-03 19:40:32,960 train 000 3.673350e-03 0.811578
2019-12-03 19:40:45,560 train 050 4.119388e-03 0.848104
2019-12-03 19:40:58,154 train 100 4.604513e-03 0.839491
2019-12-03 19:41:10,745 train 150 4.715830e-03 0.838547
2019-12-03 19:41:23,335 train 200 4.769258e-03 0.834485
2019-12-03 19:41:28,997 training loss; R2: 4.787975e-03 0.834478
2019-12-03 19:41:29,118 valid 000 3.791958e-03 0.889435
2019-12-03 19:41:30,452 validation loss; R2: 2.723308e-03 0.907548
2019-12-03 19:41:30,474 epoch 13 lr 2.000000e-03
2019-12-03 19:41:30,792 train 000 3.848855e-03 0.830809
2019-12-03 19:41:43,372 train 050 5.045640e-03 0.839102
2019-12-03 19:41:55,950 train 100 5.094295e-03 0.830953
2019-12-03 19:42:08,529 train 150 4.938523e-03 0.836351
2019-12-03 19:42:21,105 train 200 5.047978e-03 0.833613
2019-12-03 19:42:26,758 training loss; R2: 4.919849e-03 0.833223
2019-12-03 19:42:26,883 valid 000 1.861199e-02 0.270321
2019-12-03 19:42:28,217 validation loss; R2: 1.828991e-02 0.322774
2019-12-03 19:42:28,249 epoch 14 lr 2.000000e-03
2019-12-03 19:42:28,568 train 000 2.362364e-03 0.811245
2019-12-03 19:42:41,146 train 050 4.655822e-03 0.832643
2019-12-03 19:42:53,715 train 100 4.751069e-03 0.834245
2019-12-03 19:43:06,292 train 150 4.748984e-03 0.837816
2019-12-03 19:43:18,892 train 200 4.709721e-03 0.833995
2019-12-03 19:43:24,542 training loss; R2: 4.825214e-03 0.833007
2019-12-03 19:43:24,663 valid 000 2.296798e-02 -0.004549
2019-12-03 19:43:25,996 validation loss; R2: 2.015900e-02 0.249487
2019-12-03 19:43:26,020 epoch 15 lr 2.000000e-03
2019-12-03 19:43:26,340 train 000 2.838753e-03 0.943862
2019-12-03 19:43:38,908 train 050 4.596566e-03 0.846094
2019-12-03 19:43:51,465 train 100 4.585364e-03 0.845361
2019-12-03 19:44:04,028 train 150 4.515056e-03 0.850537
2019-12-03 19:44:16,588 train 200 4.365330e-03 0.853892
2019-12-03 19:44:22,227 training loss; R2: 4.355357e-03 0.855071
2019-12-03 19:44:22,352 valid 000 1.217123e-01 -4.145423
2019-12-03 19:44:23,685 validation loss; R2: 1.167927e-01 -3.080243
2019-12-03 19:44:23,713 epoch 16 lr 2.000000e-03
2019-12-03 19:44:24,037 train 000 3.540190e-03 0.762491
2019-12-03 19:44:36,586 train 050 4.518915e-03 0.861728
2019-12-03 19:44:49,145 train 100 4.307863e-03 0.857039
2019-12-03 19:45:01,704 train 150 4.326108e-03 0.853564
2019-12-03 19:45:14,261 train 200 4.536497e-03 0.846789
2019-12-03 19:45:19,914 training loss; R2: 4.549563e-03 0.845569
2019-12-03 19:45:20,030 valid 000 1.615468e-02 -0.168525
2019-12-03 19:45:21,361 validation loss; R2: 3.519412e-02 -0.119702
2019-12-03 19:45:21,394 epoch 17 lr 2.000000e-03
2019-12-03 19:45:21,708 train 000 6.537850e-03 0.805296
2019-12-03 19:45:34,247 train 050 4.832282e-03 0.833779
2019-12-03 19:45:46,789 train 100 4.810250e-03 0.838131
2019-12-03 19:45:59,326 train 150 4.812007e-03 0.838522
2019-12-03 19:46:11,862 train 200 4.756357e-03 0.837405
2019-12-03 19:46:17,502 training loss; R2: 5.079959e-03 0.828085
2019-12-03 19:46:17,619 valid 000 3.730354e+00 -95.735317
2019-12-03 19:46:18,950 validation loss; R2: 3.856443e+00 -132.201854
2019-12-03 19:46:18,973 epoch 18 lr 2.000000e-03
2019-12-03 19:46:19,299 train 000 5.863536e-03 0.902806
2019-12-03 19:46:31,822 train 050 5.140932e-03 0.819836
2019-12-03 19:46:44,345 train 100 5.041586e-03 0.831487
2019-12-03 19:46:56,861 train 150 4.809310e-03 0.835548
2019-12-03 19:47:09,380 train 200 4.705227e-03 0.839493
2019-12-03 19:47:15,009 training loss; R2: 4.683964e-03 0.840204
2019-12-03 19:47:15,129 valid 000 2.146479e-01 -5.481225
2019-12-03 19:47:16,460 validation loss; R2: 2.153161e-01 -6.445647
2019-12-03 19:47:16,485 epoch 19 lr 2.000000e-03
2019-12-03 19:47:16,802 train 000 4.130657e-03 0.787925
2019-12-03 19:47:29,299 train 050 4.653272e-03 0.832727
2019-12-03 19:47:41,806 train 100 4.690604e-03 0.838867
2019-12-03 19:47:54,308 train 150 4.725950e-03 0.839858
2019-12-03 19:48:06,814 train 200 4.687024e-03 0.839271
2019-12-03 19:48:12,435 training loss; R2: 4.669303e-03 0.840153
2019-12-03 19:48:12,556 valid 000 7.312202e-02 -1.018779
2019-12-03 19:48:13,885 validation loss; R2: 7.226511e-02 -1.528688
