2019-11-13 01:50:58,070 gpu device = 1
2019-11-13 01:50:58,070 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name='galaxy-zoo-cleaned', gpu=1, grad_clip=5, gz_dtree=False, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-galaxy-zoo-EXP-20191113-015057', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-11-13 01:51:09,529 param size = 0.279605MB
2019-11-13 01:51:09,533 epoch 0 lr 1.000000e-03
2019-11-13 01:51:11,741 train 000 3.900177e-01 -198.636761
2019-11-13 01:51:19,193 train 050 5.142122e-02 -10.810313
2019-11-13 01:51:26,472 train 100 3.998415e-02 -5.818216
2019-11-13 01:51:33,820 train 150 3.530268e-02 -4.022532
2019-11-13 01:51:41,062 train 200 3.243164e-02 -3.093615
2019-11-13 01:51:48,366 train 250 3.075130e-02 -2.510260
2019-11-13 01:51:55,475 train 300 2.948934e-02 -2.113823
2019-11-13 01:52:02,664 train 350 2.852896e-02 -1.825346
2019-11-13 01:52:09,706 train 400 2.767454e-02 -1.609242
2019-11-13 01:52:16,892 train 450 2.698721e-02 -1.436712
2019-11-13 01:52:23,908 train 500 2.635363e-02 -1.302517
2019-11-13 01:52:30,943 train 550 2.581370e-02 -1.190729
2019-11-13 01:52:37,963 train 600 2.535398e-02 -1.089765
2019-11-13 01:52:44,983 train 650 2.490416e-02 -1.003238
2019-11-13 01:52:52,004 train 700 2.453746e-02 -0.932304
2019-11-13 01:52:59,040 train 750 2.419962e-02 -0.870794
2019-11-13 01:53:06,060 train 800 2.386333e-02 -0.814872
2019-11-13 01:53:13,078 train 850 2.354958e-02 -0.763048
2019-11-13 01:53:15,948 training loss; R2: 2.345920e-02 -0.748077
2019-11-13 01:53:16,258 valid 000 2.088859e-02 0.187053
2019-11-13 01:53:17,960 valid 050 1.748597e-02 0.165657
2019-11-13 01:53:19,582 validation loss; R2: 1.744773e-02 0.160569
2019-11-13 01:53:19,606 epoch 1 lr 1.000000e-03
2019-11-13 01:53:20,186 train 000 1.700703e-02 -0.325960
2019-11-13 01:53:27,208 train 050 1.883002e-02 0.078985
2019-11-13 01:53:34,232 train 100 1.845978e-02 -0.165462
2019-11-13 01:53:41,249 train 150 1.830897e-02 -0.079291
2019-11-13 01:53:48,265 train 200 1.823477e-02 -0.041349
2019-11-13 01:53:55,280 train 250 1.813577e-02 -0.015653
2019-11-13 01:54:02,304 train 300 1.794586e-02 0.006710
2019-11-13 01:54:09,330 train 350 1.786291e-02 0.020911
2019-11-13 01:54:16,347 train 400 1.777333e-02 0.037381
2019-11-13 01:54:23,359 train 450 1.772559e-02 0.044918
2019-11-13 01:54:30,378 train 500 1.763794e-02 0.054602
2019-11-13 01:54:37,403 train 550 1.755602e-02 0.061306
2019-11-13 01:54:44,425 train 600 1.745980e-02 0.069476
2019-11-13 01:54:51,439 train 650 1.737117e-02 0.075538
2019-11-13 01:54:58,453 train 700 1.728661e-02 0.071054
2019-11-13 01:55:05,480 train 750 1.719253e-02 0.068907
2019-11-13 01:55:12,500 train 800 1.708192e-02 0.070884
2019-11-13 01:55:19,517 train 850 1.700394e-02 0.075340
2019-11-13 01:55:21,618 training loss; R2: 1.698543e-02 0.076927
2019-11-13 01:55:21,905 valid 000 1.696094e-02 0.167012
2019-11-13 01:55:23,598 valid 050 1.768796e-02 -0.384743
2019-11-13 01:55:25,165 validation loss; R2: 1.772015e-02 -0.229423
2019-11-13 01:55:25,190 epoch 2 lr 1.000000e-03
2019-11-13 01:55:25,608 train 000 1.412315e-02 0.282733
2019-11-13 01:55:32,672 train 050 1.559592e-02 0.176018
2019-11-13 01:55:39,716 train 100 1.586673e-02 0.150318
2019-11-13 01:55:46,734 train 150 1.570828e-02 0.162920
2019-11-13 01:55:53,757 train 200 1.561602e-02 0.159705
2019-11-13 01:56:00,785 train 250 1.551523e-02 0.168614
2019-11-13 01:56:07,809 train 300 1.542550e-02 0.171387
2019-11-13 01:56:14,840 train 350 1.542633e-02 0.169257
2019-11-13 01:56:21,865 train 400 1.539315e-02 0.160343
2019-11-13 01:56:28,883 train 450 1.533225e-02 0.164184
2019-11-13 01:56:35,907 train 500 1.531251e-02 0.163491
2019-11-13 01:56:42,932 train 550 1.524322e-02 0.169428
2019-11-13 01:56:49,951 train 600 1.520017e-02 0.168269
2019-11-13 01:56:56,967 train 650 1.514968e-02 0.171776
2019-11-13 01:57:03,988 train 700 1.512582e-02 0.170610
2019-11-13 01:57:11,003 train 750 1.509252e-02 0.172290
2019-11-13 01:57:18,023 train 800 1.504006e-02 0.171276
2019-11-13 01:57:25,040 train 850 1.499402e-02 0.173215
2019-11-13 01:57:27,140 training loss; R2: 1.497686e-02 0.173009
2019-11-13 01:57:27,440 valid 000 1.838778e-02 -0.019824
2019-11-13 01:57:29,144 valid 050 1.571954e-02 -0.101320
2019-11-13 01:57:30,713 validation loss; R2: 1.557665e-02 0.032227
2019-11-13 01:57:30,736 epoch 3 lr 1.000000e-03
2019-11-13 01:57:31,154 train 000 1.493357e-02 0.202043
2019-11-13 01:57:38,410 train 050 1.432524e-02 0.204422
2019-11-13 01:57:45,446 train 100 1.435056e-02 0.206527
2019-11-13 01:57:52,473 train 150 1.428713e-02 0.208666
2019-11-13 01:57:59,498 train 200 1.413356e-02 0.205980
2019-11-13 01:58:06,520 train 250 1.406202e-02 0.208879
2019-11-13 01:58:13,539 train 300 1.405846e-02 0.202968
2019-11-13 01:58:20,569 train 350 1.393442e-02 0.209971
2019-11-13 01:58:27,588 train 400 1.393791e-02 0.213861
2019-11-13 01:58:34,607 train 450 1.391709e-02 0.211679
2019-11-13 01:58:41,639 train 500 1.387296e-02 0.211368
2019-11-13 01:58:48,660 train 550 1.386608e-02 0.214256
2019-11-13 01:58:55,696 train 600 1.384209e-02 0.208625
2019-11-13 01:59:02,713 train 650 1.381917e-02 0.208996
2019-11-13 01:59:09,729 train 700 1.381257e-02 0.211695
2019-11-13 01:59:16,748 train 750 1.377524e-02 0.214072
2019-11-13 01:59:23,768 train 800 1.372137e-02 0.215181
2019-11-13 01:59:30,826 train 850 1.366720e-02 0.217692
2019-11-13 01:59:32,956 training loss; R2: 1.365153e-02 0.218902
2019-11-13 01:59:33,261 valid 000 1.253115e-02 0.361532
2019-11-13 01:59:34,982 valid 050 1.248824e-02 0.272466
2019-11-13 01:59:36,540 validation loss; R2: 1.233462e-02 0.291771
2019-11-13 01:59:36,557 epoch 4 lr 1.000000e-03
2019-11-13 01:59:36,962 train 000 1.302770e-02 0.295591
2019-11-13 01:59:44,004 train 050 1.302520e-02 0.253437
2019-11-13 01:59:51,027 train 100 1.307264e-02 0.253264
2019-11-13 01:59:58,043 train 150 1.306994e-02 0.256748
2019-11-13 02:00:05,107 train 200 1.305659e-02 0.256800
2019-11-13 02:00:12,132 train 250 1.306238e-02 0.237670
2019-11-13 02:00:19,155 train 300 1.304922e-02 0.236785
2019-11-13 02:00:26,174 train 350 1.303041e-02 0.208818
2019-11-13 02:00:33,191 train 400 1.298065e-02 0.210667
2019-11-13 02:00:40,209 train 450 1.294875e-02 0.215451
2019-11-13 02:00:47,241 train 500 1.294632e-02 0.217065
2019-11-13 02:00:54,293 train 550 1.290637e-02 0.220538
2019-11-13 02:01:01,308 train 600 1.289028e-02 0.223848
2019-11-13 02:01:08,324 train 650 1.287911e-02 0.225767
2019-11-13 02:01:15,338 train 700 1.283605e-02 0.223898
2019-11-13 02:01:22,352 train 750 1.282118e-02 0.227633
2019-11-13 02:01:29,370 train 800 1.281526e-02 0.229304
2019-11-13 02:01:36,388 train 850 1.279756e-02 0.229272
2019-11-13 02:01:38,487 training loss; R2: 1.279651e-02 0.230656
2019-11-13 02:01:38,775 valid 000 1.243518e-01 -9.020129
2019-11-13 02:01:40,497 valid 050 1.229532e-01 -6.843143
2019-11-13 02:01:42,049 validation loss; R2: 1.231780e-01 -6.729404
2019-11-13 02:01:42,066 epoch 5 lr 1.000000e-03
2019-11-13 02:01:42,451 train 000 1.549705e-02 -0.280017
2019-11-13 02:01:49,664 train 050 1.239778e-02 0.267527
2019-11-13 02:01:56,714 train 100 1.240389e-02 0.267797
2019-11-13 02:02:03,771 train 150 1.244390e-02 0.227269
2019-11-13 02:02:10,863 train 200 1.233755e-02 0.240722
2019-11-13 02:02:17,982 train 250 1.232179e-02 0.247106
2019-11-13 02:02:25,083 train 300 1.235747e-02 0.254657
2019-11-13 02:02:32,136 train 350 1.236176e-02 0.248422
2019-11-13 02:02:39,197 train 400 1.234955e-02 0.252323
2019-11-13 02:02:46,281 train 450 1.235031e-02 0.258258
2019-11-13 02:02:53,339 train 500 1.234101e-02 0.259457
2019-11-13 02:03:00,459 train 550 1.231995e-02 0.260455
2019-11-13 02:03:07,523 train 600 1.230888e-02 0.222278
2019-11-13 02:03:14,750 train 650 1.231100e-02 0.227085
2019-11-13 02:03:21,812 train 700 1.229951e-02 0.225538
2019-11-13 02:03:28,952 train 750 1.227888e-02 0.230616
2019-11-13 02:03:36,063 train 800 1.226515e-02 0.233907
2019-11-13 02:03:43,162 train 850 1.224262e-02 0.238398
2019-11-13 02:03:45,272 training loss; R2: 1.224265e-02 0.232090
2019-11-13 02:03:45,573 valid 000 1.318574e-01 -8.907891
2019-11-13 02:03:47,295 valid 050 1.293093e-01 -8.385676
2019-11-13 02:03:48,840 validation loss; R2: 1.284055e-01 -7.482453
2019-11-13 02:03:48,861 epoch 6 lr 1.000000e-03
2019-11-13 02:03:49,270 train 000 1.292199e-02 -0.203696
2019-11-13 02:03:56,485 train 050 1.215702e-02 0.268104
2019-11-13 02:04:03,710 train 100 1.210592e-02 0.266769
2019-11-13 02:04:10,941 train 150 1.195908e-02 0.265522
2019-11-13 02:04:18,032 train 200 1.190172e-02 0.272885
2019-11-13 02:04:25,314 train 250 1.189399e-02 0.274471
2019-11-13 02:04:32,468 train 300 1.192494e-02 -0.317406
2019-11-13 02:04:39,671 train 350 1.193415e-02 -0.239922
2019-11-13 02:04:46,980 train 400 1.192818e-02 -0.179184
2019-11-13 02:04:54,168 train 450 1.190845e-02 -0.124647
2019-11-13 02:05:01,389 train 500 1.188735e-02 -0.084413
2019-11-13 02:05:08,550 train 550 1.186311e-02 -0.049735
2019-11-13 02:05:15,839 train 600 1.183590e-02 -0.018063
2019-11-13 02:05:23,030 train 650 1.183958e-02 0.001914
2019-11-13 02:05:30,146 train 700 1.185196e-02 0.020579
2019-11-13 02:05:37,379 train 750 1.183930e-02 0.039651
2019-11-13 02:05:44,634 train 800 1.179066e-02 0.054440
2019-11-13 02:05:51,898 train 850 1.176782e-02 0.069941
2019-11-13 02:05:54,055 training loss; R2: 1.176919e-02 0.073728
2019-11-13 02:05:54,359 valid 000 4.056831e+00 -229.340959
2019-11-13 02:05:56,072 valid 050 4.061933e+00 -252.287141
2019-11-13 02:05:57,647 validation loss; R2: 4.058390e+00 -298.896796
2019-11-13 02:05:57,668 epoch 7 lr 1.000000e-03
2019-11-13 02:05:58,094 train 000 1.378102e-02 -0.071622
2019-11-13 02:06:05,237 train 050 1.180920e-02 0.020493
2019-11-13 02:06:12,489 train 100 1.178651e-02 0.129000
2019-11-13 02:06:19,746 train 150 1.176334e-02 0.190771
2019-11-13 02:06:26,946 train 200 1.172494e-02 0.189087
2019-11-13 02:06:34,055 train 250 1.167773e-02 0.215262
2019-11-13 02:06:41,201 train 300 1.161342e-02 0.209701
2019-11-13 02:06:48,338 train 350 1.158858e-02 0.224024
2019-11-13 02:06:55,501 train 400 1.155307e-02 0.236208
2019-11-13 02:07:02,648 train 450 1.153539e-02 0.242472
2019-11-13 02:07:09,872 train 500 1.151342e-02 0.242654
2019-11-13 02:07:17,038 train 550 1.151333e-02 0.248548
2019-11-13 02:07:24,281 train 600 1.150200e-02 0.250827
2019-11-13 02:07:31,422 train 650 1.147215e-02 0.256437
2019-11-13 02:07:38,708 train 700 1.146666e-02 0.239754
2019-11-13 02:07:45,918 train 750 1.144718e-02 0.244391
2019-11-13 02:07:53,124 train 800 1.145730e-02 0.248524
2019-11-13 02:08:00,271 train 850 1.143739e-02 0.251166
2019-11-13 02:08:02,405 training loss; R2: 1.143676e-02 0.247687
2019-11-13 02:08:02,730 valid 000 3.747984e+00 -209.942549
2019-11-13 02:08:04,479 valid 050 3.706587e+00 -218.181513
2019-11-13 02:08:06,022 validation loss; R2: 3.706822e+00 -224.602304
2019-11-13 02:08:06,047 epoch 8 lr 1.000000e-03
2019-11-13 02:08:06,490 train 000 1.098213e-02 0.127076
2019-11-13 02:08:13,841 train 050 1.155598e-02 0.282152
2019-11-13 02:08:21,113 train 100 1.137811e-02 0.273051
2019-11-13 02:08:28,315 train 150 1.127648e-02 0.200220
2019-11-13 02:08:35,534 train 200 1.123528e-02 0.211665
2019-11-13 02:08:42,862 train 250 1.122358e-02 0.153159
2019-11-13 02:08:50,159 train 300 1.121275e-02 0.156214
2019-11-13 02:08:57,285 train 350 1.118400e-02 0.142036
2019-11-13 02:09:04,299 train 400 1.115190e-02 0.162067
2019-11-13 02:09:11,309 train 450 1.117487e-02 0.167088
2019-11-13 02:09:18,314 train 500 1.117350e-02 0.179949
2019-11-13 02:09:25,340 train 550 1.114919e-02 0.193523
2019-11-13 02:09:32,344 train 600 1.112872e-02 0.204691
2019-11-13 02:09:39,352 train 650 1.114569e-02 0.202587
2019-11-13 02:09:46,358 train 700 1.114405e-02 0.210484
2019-11-13 02:09:53,368 train 750 1.113424e-02 0.217256
2019-11-13 02:10:00,377 train 800 1.112451e-02 0.224751
2019-11-13 02:10:07,386 train 850 1.111812e-02 0.229899
2019-11-13 02:10:09,483 training loss; R2: 1.112595e-02 0.231781
2019-11-13 02:10:09,784 valid 000 1.144130e+00 -154.244648
2019-11-13 02:10:11,459 valid 050 1.150562e+00 -126.615782
2019-11-13 02:10:12,986 validation loss; R2: 1.149868e+00 -245.200463
2019-11-13 02:10:13,001 epoch 9 lr 1.000000e-03
2019-11-13 02:10:13,406 train 000 1.140170e-02 0.384342
2019-11-13 02:10:20,589 train 050 1.064130e-02 0.342860
2019-11-13 02:10:27,753 train 100 1.082635e-02 0.291505
2019-11-13 02:10:34,936 train 150 1.083738e-02 0.307745
2019-11-13 02:10:42,176 train 200 1.088750e-02 0.255067
2019-11-13 02:10:49,323 train 250 1.090679e-02 0.262857
2019-11-13 02:10:56,497 train 300 1.095118e-02 0.251530
2019-11-13 02:11:03,623 train 350 1.091435e-02 0.261219
2019-11-13 02:11:10,801 train 400 1.093788e-02 0.264084
2019-11-13 02:11:17,914 train 450 1.092462e-02 0.270289
2019-11-13 02:11:25,170 train 500 1.091460e-02 0.276777
2019-11-13 02:11:32,336 train 550 1.092851e-02 0.278601
2019-11-13 02:11:39,561 train 600 1.093706e-02 0.276953
2019-11-13 02:11:46,793 train 650 1.093602e-02 0.279079
2019-11-13 02:11:53,932 train 700 1.093390e-02 0.283723
2019-11-13 02:12:01,163 train 750 1.092788e-02 0.281943
2019-11-13 02:12:08,283 train 800 1.092798e-02 0.282665
2019-11-13 02:12:15,414 train 850 1.091894e-02 0.286599
2019-11-13 02:12:17,537 training loss; R2: 1.091658e-02 0.283458
2019-11-13 02:12:17,843 valid 000 1.868912e+01 -1217.578008
2019-11-13 02:12:19,587 valid 050 1.865621e+01 -949.300351
2019-11-13 02:12:21,137 validation loss; R2: 1.865919e+01 -948.838622
2019-11-13 02:12:21,153 epoch 10 lr 1.000000e-03
2019-11-13 02:12:21,552 train 000 1.221326e-02 0.364113
2019-11-13 02:12:28,850 train 050 1.087347e-02 0.328996
2019-11-13 02:12:36,077 train 100 1.081152e-02 0.321426
2019-11-13 02:12:43,256 train 150 1.083888e-02 0.327053
2019-11-13 02:12:50,528 train 200 1.083966e-02 0.319857
2019-11-13 02:12:57,847 train 250 1.087959e-02 0.323182
2019-11-13 02:13:05,126 train 300 1.083736e-02 0.317353
2019-11-13 02:13:12,359 train 350 1.082802e-02 0.313890
2019-11-13 02:13:19,532 train 400 1.080590e-02 0.314751
2019-11-13 02:13:26,750 train 450 1.080506e-02 0.316885
2019-11-13 02:13:33,920 train 500 1.077000e-02 0.317807
2019-11-13 02:13:41,098 train 550 1.076646e-02 0.315466
2019-11-13 02:13:48,324 train 600 1.075014e-02 0.314208
2019-11-13 02:13:55,427 train 650 1.076335e-02 0.304081
2019-11-13 02:14:02,661 train 700 1.078743e-02 0.307070
2019-11-13 02:14:09,880 train 750 1.077533e-02 0.310581
2019-11-13 02:14:17,024 train 800 1.076584e-02 0.305268
2019-11-13 02:14:24,271 train 850 1.075377e-02 0.305410
2019-11-13 02:14:26,389 training loss; R2: 1.075753e-02 0.305501
2019-11-13 02:14:26,696 valid 000 1.246935e+01 -3125.360397
2019-11-13 02:14:28,435 valid 050 1.238436e+01 -3410.139863
2019-11-13 02:14:29,985 validation loss; R2: 1.238317e+01 -2975.206676
2019-11-13 02:14:30,013 epoch 11 lr 1.000000e-03
2019-11-13 02:14:30,452 train 000 9.898689e-03 0.106074
2019-11-13 02:14:37,765 train 050 1.069926e-02 0.359479
2019-11-13 02:14:45,099 train 100 1.055095e-02 0.357669
2019-11-13 02:14:52,424 train 150 1.051088e-02 0.348584
2019-11-13 02:14:59,747 train 200 1.057546e-02 0.337975
2019-11-13 02:15:07,058 train 250 1.055854e-02 0.316840
2019-11-13 02:15:14,358 train 300 1.052319e-02 0.320389
2019-11-13 02:15:21,659 train 350 1.056373e-02 0.320291
2019-11-13 02:15:28,970 train 400 1.057331e-02 0.318704
2019-11-13 02:15:36,274 train 450 1.057091e-02 0.321052
2019-11-13 02:15:43,573 train 500 1.057600e-02 0.325298
2019-11-13 02:15:50,885 train 550 1.056997e-02 0.323302
2019-11-13 02:15:58,180 train 600 1.056070e-02 0.321983
2019-11-13 02:16:05,479 train 650 1.054915e-02 0.321149
2019-11-13 02:16:12,775 train 700 1.054753e-02 0.321219
2019-11-13 02:16:20,066 train 750 1.054120e-02 0.321872
2019-11-13 02:16:27,083 train 800 1.055451e-02 0.320418
2019-11-13 02:16:34,098 train 850 1.056985e-02 0.320580
2019-11-13 02:16:36,199 training loss; R2: 1.057639e-02 0.320202
2019-11-13 02:16:36,502 valid 000 6.711987e+01 -4488.861698
2019-11-13 02:16:38,168 valid 050 6.731755e+01 -4023.591841
2019-11-13 02:16:39,685 validation loss; R2: 6.732682e+01 -3825.039116
2019-11-13 02:16:39,701 epoch 12 lr 1.000000e-03
2019-11-13 02:16:40,097 train 000 9.503958e-03 0.340151
2019-11-13 02:16:47,463 train 050 1.020604e-02 0.356019
2019-11-13 02:16:54,812 train 100 1.035980e-02 0.332887
2019-11-13 02:17:01,910 train 150 1.053042e-02 0.308951
2019-11-13 02:17:08,972 train 200 1.047593e-02 0.269168
2019-11-13 02:17:16,012 train 250 1.049576e-02 0.284624
2019-11-13 02:17:23,056 train 300 1.047254e-02 0.291767
2019-11-13 02:17:30,094 train 350 1.045344e-02 0.295461
2019-11-13 02:17:37,138 train 400 1.044589e-02 0.300041
2019-11-13 02:17:44,192 train 450 1.044116e-02 0.300220
2019-11-13 02:17:51,226 train 500 1.043049e-02 0.305205
2019-11-13 02:17:58,265 train 550 1.045045e-02 0.304596
2019-11-13 02:18:05,307 train 600 1.044763e-02 0.304762
2019-11-13 02:18:12,358 train 650 1.045420e-02 0.279114
2019-11-13 02:18:19,405 train 700 1.045201e-02 0.283501
2019-11-13 02:18:26,445 train 750 1.045125e-02 0.285808
2019-11-13 02:18:33,500 train 800 1.043240e-02 0.289709
2019-11-13 02:18:40,549 train 850 1.042539e-02 0.286442
2019-11-13 02:18:42,655 training loss; R2: 1.042698e-02 0.287682
2019-11-13 02:18:42,948 valid 000 1.325233e+02 -12231.577284
2019-11-13 02:18:44,641 valid 050 1.324666e+02 -16263.819662
2019-11-13 02:18:46,157 validation loss; R2: 1.324752e+02 -17557.450119
2019-11-13 02:18:46,173 epoch 13 lr 1.000000e-03
2019-11-13 02:18:46,571 train 000 1.011402e-02 0.447345
2019-11-13 02:18:53,786 train 050 1.031766e-02 0.314088
2019-11-13 02:19:00,884 train 100 1.032936e-02 0.308208
2019-11-13 02:19:08,082 train 150 1.046424e-02 0.314074
2019-11-13 02:19:15,292 train 200 1.044390e-02 0.324418
2019-11-13 02:19:22,442 train 250 1.046860e-02 0.317334
2019-11-13 02:19:29,613 train 300 1.042923e-02 0.323216
2019-11-13 02:19:36,806 train 350 1.043267e-02 0.323661
2019-11-13 02:19:43,943 train 400 1.042054e-02 0.325592
2019-11-13 02:19:51,116 train 450 1.038888e-02 0.326743
2019-11-13 02:19:58,439 train 500 1.039851e-02 0.326747
2019-11-13 02:20:05,781 train 550 1.039029e-02 0.329291
2019-11-13 02:20:13,088 train 600 1.038837e-02 0.329462
2019-11-13 02:20:20,287 train 650 1.036579e-02 0.326361
2019-11-13 02:20:27,517 train 700 1.036500e-02 0.319455
2019-11-13 02:20:34,790 train 750 1.034791e-02 0.322718
2019-11-13 02:20:42,119 train 800 1.036250e-02 0.324376
2019-11-13 02:20:49,452 train 850 1.035552e-02 0.327120
2019-11-13 02:20:51,617 training loss; R2: 1.035250e-02 0.327980
2019-11-13 02:20:51,919 valid 000 1.537783e+02 -15889.269985
2019-11-13 02:20:53,608 valid 050 1.540183e+02 -21162.036473
2019-11-13 02:20:55,156 validation loss; R2: 1.540208e+02 -20261.809303
2019-11-13 02:20:55,173 epoch 14 lr 1.000000e-03
2019-11-13 02:20:55,570 train 000 9.033823e-03 0.351842
2019-11-13 02:21:02,868 train 050 1.041708e-02 0.358236
2019-11-13 02:21:10,177 train 100 1.050205e-02 0.333931
2019-11-13 02:21:17,486 train 150 1.046930e-02 0.337892
2019-11-13 02:21:24,801 train 200 1.043414e-02 0.344753
2019-11-13 02:21:32,015 train 250 1.044521e-02 0.342563
2019-11-13 02:21:39,278 train 300 1.040195e-02 0.345330
2019-11-13 02:21:46,554 train 350 1.038397e-02 0.343976
2019-11-13 02:21:53,807 train 400 1.037413e-02 0.344071
2019-11-13 02:22:01,073 train 450 1.034224e-02 0.330961
2019-11-13 02:22:08,306 train 500 1.033360e-02 0.333118
2019-11-13 02:22:15,546 train 550 1.034707e-02 0.334428
2019-11-13 02:22:22,782 train 600 1.030627e-02 0.336187
2019-11-13 02:22:30,082 train 650 1.029961e-02 0.332059
2019-11-13 02:22:37,321 train 700 1.028191e-02 0.331549
2019-11-13 02:22:44,560 train 750 1.026597e-02 0.329449
2019-11-13 02:22:51,908 train 800 1.026786e-02 0.321770
2019-11-13 02:22:59,327 train 850 1.026057e-02 0.324758
2019-11-13 02:23:01,534 training loss; R2: 1.025519e-02 0.324960
2019-11-13 02:23:01,838 valid 000 1.507498e+02 -68522.112768
2019-11-13 02:23:03,500 valid 050 1.506561e+02 -34962.790119
2019-11-13 02:23:05,014 validation loss; R2: 1.506436e+02 -32933.224825
2019-11-13 02:23:05,035 epoch 15 lr 1.000000e-03
2019-11-13 02:23:05,536 train 000 1.009728e-02 0.364947
2019-11-13 02:23:12,933 train 050 1.005580e-02 0.338241
2019-11-13 02:23:20,304 train 100 1.000832e-02 0.300067
2019-11-13 02:23:27,622 train 150 1.002616e-02 0.324476
2019-11-13 02:23:34,864 train 200 1.013166e-02 0.330813
2019-11-13 02:23:42,133 train 250 1.015164e-02 0.322876
2019-11-13 02:23:49,362 train 300 1.020118e-02 0.323458
2019-11-13 02:23:56,623 train 350 1.019695e-02 0.329713
2019-11-13 02:24:03,942 train 400 1.022381e-02 0.336349
2019-11-13 02:24:11,218 train 450 1.018977e-02 0.338357
2019-11-13 02:24:18,508 train 500 1.016250e-02 0.341327
2019-11-13 02:24:25,788 train 550 1.016261e-02 0.339543
2019-11-13 02:24:33,016 train 600 1.018607e-02 0.340558
2019-11-13 02:24:40,302 train 650 1.021138e-02 0.339158
2019-11-13 02:24:47,534 train 700 1.022335e-02 0.339292
2019-11-13 02:24:54,838 train 750 1.021617e-02 0.336610
2019-11-13 02:25:02,219 train 800 1.020392e-02 0.337421
2019-11-13 02:25:09,559 train 850 1.020367e-02 0.298338
2019-11-13 02:25:11,752 training loss; R2: 1.019919e-02 0.299164
2019-11-13 02:25:12,040 valid 000 1.332365e+01 -1452.017350
2019-11-13 02:25:13,738 valid 050 1.326275e+01 -1466.698055
2019-11-13 02:25:15,312 validation loss; R2: 1.325781e+01 -1422.451773
2019-11-13 02:25:15,334 epoch 16 lr 1.000000e-03
2019-11-13 02:25:15,763 train 000 1.217376e-02 0.267022
2019-11-13 02:25:23,252 train 050 1.027775e-02 0.280769
2019-11-13 02:25:30,681 train 100 1.029533e-02 0.311927
2019-11-13 02:25:38,064 train 150 1.018858e-02 0.334318
2019-11-13 02:25:45,539 train 200 1.022147e-02 0.339055
2019-11-13 02:25:52,892 train 250 1.018987e-02 -0.297569
2019-11-13 02:26:00,143 train 300 1.018439e-02 -0.191903
2019-11-13 02:26:07,599 train 350 1.018355e-02 -0.113907
2019-11-13 02:26:14,969 train 400 1.021236e-02 -0.058821
2019-11-13 02:26:22,220 train 450 1.025145e-02 -0.020107
2019-11-13 02:26:29,479 train 500 1.027253e-02 0.016969
2019-11-13 02:26:36,781 train 550 1.025855e-02 0.044342
2019-11-13 02:26:44,046 train 600 1.026985e-02 0.067505
2019-11-13 02:26:51,272 train 650 1.027757e-02 0.088008
2019-11-13 02:26:58,546 train 700 1.027571e-02 0.102149
2019-11-13 02:27:05,889 train 750 1.028502e-02 0.117114
2019-11-13 02:27:13,145 train 800 1.027729e-02 0.129820
2019-11-13 02:27:20,422 train 850 1.027665e-02 0.143779
2019-11-13 02:27:22,585 training loss; R2: 1.027391e-02 0.145734
2019-11-13 02:27:22,884 valid 000 3.944076e+01 -11644.668162
2019-11-13 02:27:24,565 valid 050 3.941018e+01 -12593.643904
2019-11-13 02:27:26,102 validation loss; R2: 3.941176e+01 -12464.518129
2019-11-13 02:27:26,123 epoch 17 lr 1.000000e-03
2019-11-13 02:27:26,584 train 000 1.040867e-02 0.357977
2019-11-13 02:27:34,024 train 050 1.000742e-02 0.373561
2019-11-13 02:27:41,407 train 100 1.011831e-02 0.341337
2019-11-13 02:27:48,658 train 150 1.004184e-02 0.344040
2019-11-13 02:27:55,916 train 200 1.003935e-02 0.329403
2019-11-13 02:28:03,117 train 250 1.009048e-02 0.332732
2019-11-13 02:28:10,305 train 300 1.015520e-02 0.333793
2019-11-13 02:28:17,565 train 350 1.015668e-02 0.325355
2019-11-13 02:28:24,788 train 400 1.018230e-02 0.317795
2019-11-13 02:28:31,983 train 450 1.020842e-02 0.310579
2019-11-13 02:28:39,316 train 500 1.021743e-02 0.314459
2019-11-13 02:28:46,662 train 550 1.022741e-02 0.315610
2019-11-13 02:28:53,865 train 600 1.021161e-02 0.318699
2019-11-13 02:29:01,152 train 650 1.023328e-02 0.320608
2019-11-13 02:29:08,438 train 700 1.023119e-02 0.314884
2019-11-13 02:29:15,735 train 750 1.024630e-02 0.223113
2019-11-13 02:29:22,903 train 800 1.027675e-02 0.228531
2019-11-13 02:29:30,146 train 850 1.030022e-02 0.234411
2019-11-13 02:29:32,297 training loss; R2: 1.029698e-02 0.237146
2019-11-13 02:29:32,593 valid 000 3.092852e+01 -3246.869929
2019-11-13 02:29:34,290 valid 050 3.095433e+01 -6789.309426
2019-11-13 02:29:35,831 validation loss; R2: 3.096140e+01 -5244.666956
2019-11-13 02:29:35,852 epoch 18 lr 1.000000e-03
2019-11-13 02:29:36,308 train 000 9.628124e-03 0.310496
2019-11-13 02:29:43,643 train 050 1.023939e-02 0.187496
2019-11-13 02:29:50,885 train 100 1.020836e-02 0.272663
2019-11-13 02:29:58,155 train 150 1.022453e-02 0.287047
2019-11-13 02:30:05,622 train 200 1.027173e-02 0.298196
2019-11-13 02:30:13,086 train 250 1.024741e-02 0.304344
2019-11-13 02:30:20,590 train 300 1.025717e-02 0.306112
2019-11-13 02:30:27,791 train 350 1.024522e-02 0.308927
2019-11-13 02:30:35,199 train 400 1.025471e-02 0.311974
2019-11-13 02:30:42,620 train 450 1.023348e-02 0.313773
2019-11-13 02:30:49,997 train 500 1.021701e-02 0.312896
2019-11-13 02:30:57,272 train 550 1.019963e-02 0.316053
2019-11-13 02:31:04,474 train 600 1.020537e-02 0.319397
2019-11-13 02:31:11,725 train 650 1.020064e-02 0.313595
2019-11-13 02:31:18,902 train 700 1.020703e-02 0.315579
2019-11-13 02:31:26,186 train 750 1.022454e-02 0.313933
2019-11-13 02:31:33,445 train 800 1.025581e-02 0.313959
2019-11-13 02:31:40,744 train 850 1.029901e-02 0.315267
2019-11-13 02:31:42,884 training loss; R2: 1.030063e-02 0.316222
2019-11-13 02:31:43,190 valid 000 2.143370e+02 -46355.366787
2019-11-13 02:31:44,863 valid 050 2.144015e+02 -43962.141550
2019-11-13 02:31:46,393 validation loss; R2: 2.143977e+02 -39159.938740
2019-11-13 02:31:46,414 epoch 19 lr 1.000000e-03
2019-11-13 02:31:46,854 train 000 1.042700e-02 0.416004
2019-11-13 02:31:54,193 train 050 1.080631e-02 0.328295
2019-11-13 02:32:01,454 train 100 1.058451e-02 0.143230
2019-11-13 02:32:08,756 train 150 1.061848e-02 0.198379
2019-11-13 02:32:15,976 train 200 1.056971e-02 0.224659
2019-11-13 02:32:23,368 train 250 1.054308e-02 0.243061
2019-11-13 02:32:30,831 train 300 1.050239e-02 0.239920
2019-11-13 02:32:38,256 train 350 1.047571e-02 0.254554
2019-11-13 02:32:45,552 train 400 1.046294e-02 0.259331
2019-11-13 02:32:52,809 train 450 1.042424e-02 0.267427
2019-11-13 02:33:00,054 train 500 1.041576e-02 0.275991
2019-11-13 02:33:07,481 train 550 1.037924e-02 0.277947
2019-11-13 02:33:14,763 train 600 1.034416e-02 0.283732
2019-11-13 02:33:22,004 train 650 1.035529e-02 0.283669
2019-11-13 02:33:29,252 train 700 1.033507e-02 0.287778
2019-11-13 02:33:36,560 train 750 1.030480e-02 0.288089
2019-11-13 02:33:43,881 train 800 1.027896e-02 0.290901
2019-11-13 02:33:51,377 train 850 1.026489e-02 0.290826
2019-11-13 02:33:53,583 training loss; R2: 1.025779e-02 0.291435
2019-11-13 02:33:53,867 valid 000 1.279556e+02 -22149.528320
2019-11-13 02:33:55,556 valid 050 1.279760e+02 -29958.018224
2019-11-13 02:33:57,062 validation loss; R2: 1.279640e+02 -29725.390462
