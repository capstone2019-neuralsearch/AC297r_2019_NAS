2019-12-03 21:15:44,516 gpu device = 1
2019-12-03 21:15:44,516 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=96, cutout=False, cutout_length=16, data='../data', dataset='graphene', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name=None, gpu=1, grad_clip=5, gz_dtree=False, init_channels=36, layers=8, learning_rate=0.002, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-graphene-EXP-20191203-211544', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-12-03 21:15:47,783 param size = 1.271989MB
2019-12-03 21:15:47,786 epoch 0 lr 2.000000e-03
2019-12-03 21:15:50,521 train 000 6.177384e-01 -14.681424
2019-12-03 21:16:04,701 train 050 2.087175e-01 -5.466293
2019-12-03 21:16:18,791 train 100 1.207780e-01 -2.770990
2019-12-03 21:16:32,887 train 150 9.040054e-02 -1.825391
2019-12-03 21:16:46,996 train 200 7.399756e-02 -1.331015
2019-12-03 21:16:54,380 training loss; R2: 6.929943e-02 -1.187749
2019-12-03 21:16:54,521 valid 000 3.575624e-02 0.234404
2019-12-03 21:16:56,304 validation loss; R2: 2.240219e-02 0.292654
2019-12-03 21:16:56,328 epoch 1 lr 2.000000e-03
2019-12-03 21:16:56,748 train 000 2.891222e-02 0.204304
2019-12-03 21:17:10,820 train 050 2.675062e-02 0.139708
2019-12-03 21:17:24,901 train 100 2.556991e-02 0.183468
2019-12-03 21:17:38,986 train 150 2.424195e-02 0.222998
2019-12-03 21:17:53,068 train 200 2.309201e-02 0.255555
2019-12-03 21:17:59,405 training loss; R2: 2.261198e-02 0.262450
2019-12-03 21:17:59,545 valid 000 2.238721e-02 0.278914
2019-12-03 21:18:01,024 validation loss; R2: 2.097894e-02 0.318448
2019-12-03 21:18:01,050 epoch 2 lr 2.000000e-03
2019-12-03 21:18:01,410 train 000 1.782331e-02 0.413449
2019-12-03 21:18:15,494 train 050 1.746170e-02 0.384361
2019-12-03 21:18:29,573 train 100 1.633372e-02 0.442497
2019-12-03 21:18:43,650 train 150 1.688669e-02 0.442783
2019-12-03 21:18:57,725 train 200 1.681867e-02 0.451198
2019-12-03 21:19:04,058 training loss; R2: 1.659755e-02 0.459856
2019-12-03 21:19:04,198 valid 000 1.052747e-02 0.716227
2019-12-03 21:19:05,677 validation loss; R2: 1.067091e-02 0.625680
2019-12-03 21:19:05,703 epoch 3 lr 2.000000e-03
2019-12-03 21:19:06,064 train 000 9.976610e-03 0.395957
2019-12-03 21:19:20,154 train 050 1.285064e-02 0.545892
2019-12-03 21:19:34,282 train 100 1.236218e-02 0.583619
2019-12-03 21:19:48,387 train 150 1.230626e-02 0.577839
2019-12-03 21:20:02,490 train 200 1.250488e-02 0.591425
2019-12-03 21:20:08,847 training loss; R2: 1.241016e-02 0.595154
2019-12-03 21:20:08,984 valid 000 5.054998e-03 0.775399
2019-12-03 21:20:10,465 validation loss; R2: 7.728926e-03 0.752185
2019-12-03 21:20:10,492 epoch 4 lr 2.000000e-03
2019-12-03 21:20:10,850 train 000 6.186103e-03 0.697346
2019-12-03 21:20:24,943 train 050 1.236284e-02 0.629802
2019-12-03 21:20:39,014 train 100 1.185291e-02 0.625361
2019-12-03 21:20:53,102 train 150 1.132548e-02 0.638902
2019-12-03 21:21:07,394 train 200 1.108434e-02 0.639914
2019-12-03 21:21:13,924 training loss; R2: 1.102112e-02 0.642730
2019-12-03 21:21:14,061 valid 000 7.988025e-03 0.849269
2019-12-03 21:21:15,543 validation loss; R2: 5.353895e-03 0.809481
2019-12-03 21:21:15,571 epoch 5 lr 2.000000e-03
2019-12-03 21:21:15,941 train 000 8.415098e-03 0.742557
2019-12-03 21:21:30,404 train 050 1.135187e-02 0.627539
2019-12-03 21:21:44,860 train 100 1.137659e-02 0.637083
2019-12-03 21:21:59,323 train 150 1.123305e-02 0.635748
2019-12-03 21:22:13,781 train 200 1.122643e-02 0.627221
2019-12-03 21:22:20,292 training loss; R2: 1.112043e-02 0.627978
2019-12-03 21:22:20,428 valid 000 7.428552e-03 0.728615
2019-12-03 21:22:21,908 validation loss; R2: 6.993182e-03 0.766484
2019-12-03 21:22:21,934 epoch 6 lr 2.000000e-03
2019-12-03 21:22:22,301 train 000 9.266031e-03 0.473003
2019-12-03 21:22:36,397 train 050 1.102435e-02 0.614398
2019-12-03 21:22:50,493 train 100 9.892239e-03 0.660001
2019-12-03 21:23:04,610 train 150 9.505514e-03 0.670348
2019-12-03 21:23:18,764 train 200 9.533743e-03 0.678200
2019-12-03 21:23:25,125 training loss; R2: 9.487716e-03 0.681757
2019-12-03 21:23:25,258 valid 000 5.388413e-03 0.786424
2019-12-03 21:23:26,736 validation loss; R2: 7.846635e-03 0.750038
2019-12-03 21:23:26,762 epoch 7 lr 2.000000e-03
2019-12-03 21:23:27,122 train 000 7.526724e-03 0.800675
2019-12-03 21:23:41,227 train 050 9.461823e-03 0.679354
2019-12-03 21:23:55,314 train 100 9.103796e-03 0.691446
2019-12-03 21:24:09,389 train 150 9.354101e-03 0.688033
2019-12-03 21:24:23,466 train 200 9.177204e-03 0.692556
2019-12-03 21:24:29,797 training loss; R2: 9.023852e-03 0.696899
2019-12-03 21:24:29,932 valid 000 3.858597e-03 0.893999
2019-12-03 21:24:31,410 validation loss; R2: 5.657523e-03 0.809588
2019-12-03 21:24:31,435 epoch 8 lr 2.000000e-03
2019-12-03 21:24:31,791 train 000 5.312629e-03 0.762253
2019-12-03 21:24:45,860 train 050 7.563361e-03 0.740159
2019-12-03 21:24:59,934 train 100 7.915795e-03 0.720779
2019-12-03 21:25:14,015 train 150 7.986605e-03 0.728902
2019-12-03 21:25:28,085 train 200 7.995842e-03 0.729699
2019-12-03 21:25:34,417 training loss; R2: 7.981527e-03 0.731080
2019-12-03 21:25:34,549 valid 000 2.861924e-03 0.915776
2019-12-03 21:25:36,029 validation loss; R2: 4.197079e-03 0.859865
2019-12-03 21:25:36,055 epoch 9 lr 2.000000e-03
2019-12-03 21:25:36,412 train 000 6.249428e-03 0.754251
2019-12-03 21:25:50,489 train 050 8.199725e-03 0.743290
2019-12-03 21:26:04,572 train 100 8.562650e-03 0.730230
2019-12-03 21:26:18,677 train 150 8.176759e-03 0.726722
2019-12-03 21:26:32,812 train 200 7.758347e-03 0.741144
2019-12-03 21:26:39,156 training loss; R2: 7.682603e-03 0.742689
2019-12-03 21:26:39,293 valid 000 2.955833e-03 0.907168
2019-12-03 21:26:40,773 validation loss; R2: 3.790222e-03 0.868703
2019-12-03 21:26:40,800 epoch 10 lr 2.000000e-03
2019-12-03 21:26:41,158 train 000 6.737914e-03 0.699554
2019-12-03 21:26:55,251 train 050 7.458410e-03 0.749356
2019-12-03 21:27:09,370 train 100 7.713562e-03 0.741629
2019-12-03 21:27:23,504 train 150 8.038246e-03 0.734237
2019-12-03 21:27:37,630 train 200 7.777889e-03 0.735091
2019-12-03 21:27:43,963 training loss; R2: 7.858760e-03 0.729959
2019-12-03 21:27:44,103 valid 000 6.112026e-03 0.807992
2019-12-03 21:27:45,582 validation loss; R2: 6.695248e-03 0.771385
2019-12-03 21:27:45,615 epoch 11 lr 2.000000e-03
2019-12-03 21:27:45,988 train 000 1.121160e-02 0.758918
2019-12-03 21:28:00,075 train 050 8.432576e-03 0.708170
2019-12-03 21:28:14,153 train 100 7.763865e-03 0.736887
2019-12-03 21:28:28,238 train 150 7.791070e-03 0.732644
2019-12-03 21:28:42,399 train 200 7.544176e-03 0.741046
2019-12-03 21:28:48,783 training loss; R2: 7.640296e-03 0.740125
2019-12-03 21:28:48,924 valid 000 4.869346e-03 0.798361
2019-12-03 21:28:50,407 validation loss; R2: 4.931803e-03 0.826484
2019-12-03 21:28:50,444 epoch 12 lr 2.000000e-03
2019-12-03 21:28:50,806 train 000 6.857585e-03 0.819257
2019-12-03 21:29:04,950 train 050 7.067147e-03 0.771542
2019-12-03 21:29:19,030 train 100 7.041400e-03 0.765454
2019-12-03 21:29:33,097 train 150 6.748832e-03 0.768098
2019-12-03 21:29:47,243 train 200 6.698841e-03 0.772441
2019-12-03 21:29:53,592 training loss; R2: 6.865095e-03 0.769551
2019-12-03 21:29:53,738 valid 000 7.373748e-03 0.835985
2019-12-03 21:29:55,219 validation loss; R2: 6.964611e-03 0.754276
2019-12-03 21:29:55,246 epoch 13 lr 2.000000e-03
2019-12-03 21:29:55,603 train 000 1.291110e-02 0.782462
2019-12-03 21:30:09,708 train 050 6.663723e-03 0.771759
2019-12-03 21:30:24,137 train 100 6.451150e-03 0.773875
2019-12-03 21:30:38,599 train 150 6.746170e-03 0.768516
2019-12-03 21:30:53,057 train 200 7.000753e-03 0.767921
2019-12-03 21:30:59,568 training loss; R2: 6.948920e-03 0.767156
2019-12-03 21:30:59,701 valid 000 2.009389e-02 0.256525
2019-12-03 21:31:01,181 validation loss; R2: 1.851005e-02 0.404914
2019-12-03 21:31:01,209 epoch 14 lr 2.000000e-03
2019-12-03 21:31:01,577 train 000 5.609002e-03 0.768722
2019-12-03 21:31:15,848 train 050 6.009124e-03 0.788256
2019-12-03 21:31:29,934 train 100 6.516866e-03 0.775951
2019-12-03 21:31:43,986 train 150 6.688971e-03 0.774545
2019-12-03 21:31:57,982 train 200 6.842219e-03 0.769214
2019-12-03 21:32:04,298 training loss; R2: 6.861180e-03 0.768782
2019-12-03 21:32:04,441 valid 000 4.747137e-03 0.845719
2019-12-03 21:32:05,916 validation loss; R2: 4.345550e-03 0.852182
2019-12-03 21:32:05,943 epoch 15 lr 2.000000e-03
2019-12-03 21:32:06,308 train 000 6.265961e-03 0.801910
2019-12-03 21:32:20,483 train 050 6.528285e-03 0.789376
2019-12-03 21:32:34,473 train 100 6.332739e-03 0.787832
2019-12-03 21:32:48,453 train 150 6.675589e-03 0.781476
2019-12-03 21:33:02,510 train 200 6.733118e-03 0.775817
2019-12-03 21:33:08,982 training loss; R2: 6.614196e-03 0.778165
2019-12-03 21:33:09,119 valid 000 3.069556e-03 0.836695
2019-12-03 21:33:10,595 validation loss; R2: 4.219307e-03 0.852281
2019-12-03 21:33:10,620 epoch 16 lr 2.000000e-03
2019-12-03 21:33:10,986 train 000 5.060557e-03 0.798931
2019-12-03 21:33:25,330 train 050 6.611618e-03 0.755711
2019-12-03 21:33:39,694 train 100 6.581766e-03 0.766466
2019-12-03 21:33:54,036 train 150 6.606217e-03 0.774183
2019-12-03 21:34:08,375 train 200 6.563340e-03 0.779465
2019-12-03 21:34:14,826 training loss; R2: 6.514106e-03 0.780718
2019-12-03 21:34:14,959 valid 000 1.969768e-02 0.048710
2019-12-03 21:34:16,435 validation loss; R2: 2.114705e-02 0.234777
2019-12-03 21:34:16,462 epoch 17 lr 2.000000e-03
2019-12-03 21:34:16,827 train 000 6.351785e-03 0.780169
2019-12-03 21:34:31,165 train 050 5.317109e-03 0.819467
2019-12-03 21:34:45,499 train 100 5.728714e-03 0.799624
2019-12-03 21:34:59,846 train 150 6.339467e-03 0.787189
2019-12-03 21:35:14,152 train 200 6.485396e-03 0.786251
2019-12-03 21:35:20,490 training loss; R2: 6.404087e-03 0.787240
2019-12-03 21:35:20,623 valid 000 4.630534e-03 0.896252
2019-12-03 21:35:22,097 validation loss; R2: 5.263885e-03 0.817594
2019-12-03 21:35:22,123 epoch 18 lr 2.000000e-03
2019-12-03 21:35:22,483 train 000 5.032342e-03 0.890428
2019-12-03 21:35:36,474 train 050 5.901718e-03 0.801722
2019-12-03 21:35:50,462 train 100 5.808889e-03 0.799895
2019-12-03 21:36:04,487 train 150 5.966780e-03 0.795783
2019-12-03 21:36:18,502 train 200 6.171447e-03 0.795317
2019-12-03 21:36:24,810 training loss; R2: 6.190712e-03 0.793779
2019-12-03 21:36:24,946 valid 000 3.454431e-03 0.909661
2019-12-03 21:36:26,421 validation loss; R2: 2.968680e-03 0.901706
2019-12-03 21:36:26,457 epoch 19 lr 2.000000e-03
2019-12-03 21:36:26,815 train 000 4.821154e-03 0.872846
2019-12-03 21:36:40,899 train 050 5.783183e-03 0.793646
2019-12-03 21:36:54,940 train 100 6.016166e-03 0.792038
2019-12-03 21:37:08,949 train 150 6.047149e-03 0.791919
2019-12-03 21:37:22,916 train 200 5.999652e-03 0.796337
2019-12-03 21:37:29,203 training loss; R2: 5.934524e-03 0.799209
2019-12-03 21:37:29,340 valid 000 1.884486e-03 0.941251
2019-12-03 21:37:30,814 validation loss; R2: 2.980696e-03 0.895643
