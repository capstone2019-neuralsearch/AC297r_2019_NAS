2019-12-03 22:18:59,394 gpu device = 1
2019-12-03 22:18:59,395 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=96, cutout=False, cutout_length=16, data='../data', dataset='graphene', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name=None, gpu=1, grad_clip=5, gz_dtree=False, init_channels=36, layers=8, learning_rate=0.002, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-graphene-EXP-20191203-221859', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-12-03 22:19:02,628 param size = 0.765613MB
2019-12-03 22:19:02,631 epoch 0 lr 2.000000e-03
2019-12-03 22:19:04,949 train 000 4.269699e-01 -7.903666
2019-12-03 22:19:13,163 train 050 2.174061e-01 -6.163850
2019-12-03 22:19:21,333 train 100 1.237760e-01 -3.057902
2019-12-03 22:19:29,502 train 150 9.018933e-02 -1.964528
2019-12-03 22:19:37,667 train 200 7.324151e-02 -1.403243
2019-12-03 22:19:42,278 training loss; R2: 6.783027e-02 -1.227013
2019-12-03 22:19:42,407 valid 000 3.313091e-02 0.446765
2019-12-03 22:19:43,651 validation loss; R2: 1.553335e-02 0.504983
2019-12-03 22:19:43,664 epoch 1 lr 2.000000e-03
2019-12-03 22:19:43,953 train 000 1.587114e-02 0.381851
2019-12-03 22:19:52,115 train 050 2.071928e-02 0.370275
2019-12-03 22:20:00,282 train 100 2.100610e-02 0.354865
2019-12-03 22:20:08,444 train 150 1.922727e-02 0.382470
2019-12-03 22:20:16,609 train 200 1.776788e-02 0.420286
2019-12-03 22:20:20,279 training loss; R2: 1.750493e-02 0.432787
2019-12-03 22:20:20,402 valid 000 7.881084e-03 0.527645
2019-12-03 22:20:21,340 validation loss; R2: 1.029410e-02 0.655195
2019-12-03 22:20:21,353 epoch 2 lr 2.000000e-03
2019-12-03 22:20:21,592 train 000 1.366281e-02 0.588815
2019-12-03 22:20:29,753 train 050 1.303245e-02 0.556110
2019-12-03 22:20:37,919 train 100 1.204971e-02 0.589350
2019-12-03 22:20:46,084 train 150 1.180091e-02 0.596663
2019-12-03 22:20:54,250 train 200 1.188054e-02 0.597331
2019-12-03 22:20:57,919 training loss; R2: 1.197509e-02 0.592996
2019-12-03 22:20:58,040 valid 000 5.398219e-03 0.804948
2019-12-03 22:20:58,978 validation loss; R2: 6.266221e-03 0.794322
2019-12-03 22:20:58,997 epoch 3 lr 2.000000e-03
2019-12-03 22:20:59,242 train 000 9.787649e-03 0.666305
2019-12-03 22:21:07,409 train 050 1.066615e-02 0.637799
2019-12-03 22:21:15,577 train 100 1.067458e-02 0.630143
2019-12-03 22:21:23,745 train 150 1.071296e-02 0.637038
2019-12-03 22:21:31,914 train 200 1.038444e-02 0.648231
2019-12-03 22:21:35,585 training loss; R2: 1.054977e-02 0.648265
2019-12-03 22:21:35,706 valid 000 2.113622e-02 0.612244
2019-12-03 22:21:36,644 validation loss; R2: 1.432091e-02 0.472999
2019-12-03 22:21:36,657 epoch 4 lr 2.000000e-03
2019-12-03 22:21:36,894 train 000 1.437666e-02 0.729064
2019-12-03 22:21:45,062 train 050 1.001426e-02 0.690425
2019-12-03 22:21:53,233 train 100 1.022618e-02 0.679477
2019-12-03 22:22:01,404 train 150 9.761640e-03 0.683390
2019-12-03 22:22:09,573 train 200 9.394466e-03 0.685016
2019-12-03 22:22:13,245 training loss; R2: 9.140828e-03 0.690117
2019-12-03 22:22:13,360 valid 000 2.606506e-03 0.781594
2019-12-03 22:22:14,297 validation loss; R2: 4.748442e-03 0.843785
2019-12-03 22:22:14,317 epoch 5 lr 2.000000e-03
2019-12-03 22:22:14,554 train 000 5.153039e-03 0.786662
2019-12-03 22:22:22,719 train 050 7.336959e-03 0.736137
2019-12-03 22:22:30,898 train 100 7.492369e-03 0.745705
2019-12-03 22:22:39,066 train 150 7.594411e-03 0.736476
2019-12-03 22:22:47,236 train 200 7.802284e-03 0.735795
2019-12-03 22:22:50,908 training loss; R2: 8.001679e-03 0.729507
2019-12-03 22:22:51,026 valid 000 3.528334e-03 0.868007
2019-12-03 22:22:51,964 validation loss; R2: 4.105462e-03 0.856453
2019-12-03 22:22:51,977 epoch 6 lr 2.000000e-03
2019-12-03 22:22:52,217 train 000 1.876876e-02 0.685784
2019-12-03 22:23:00,386 train 050 7.531224e-03 0.767086
2019-12-03 22:23:08,557 train 100 7.550368e-03 0.741507
2019-12-03 22:23:16,729 train 150 7.992902e-03 0.735785
2019-12-03 22:23:24,898 train 200 7.636222e-03 0.747449
2019-12-03 22:23:28,572 training loss; R2: 7.590596e-03 0.747847
2019-12-03 22:23:28,688 valid 000 5.389199e-03 0.923077
2019-12-03 22:23:29,626 validation loss; R2: 4.323525e-03 0.853850
2019-12-03 22:23:29,639 epoch 7 lr 2.000000e-03
2019-12-03 22:23:29,879 train 000 7.148462e-03 0.590773
2019-12-03 22:23:38,051 train 050 6.927612e-03 0.758401
2019-12-03 22:23:46,221 train 100 6.830947e-03 0.767854
2019-12-03 22:23:54,393 train 150 7.049166e-03 0.758945
2019-12-03 22:24:02,560 train 200 7.286216e-03 0.747801
2019-12-03 22:24:06,234 training loss; R2: 7.227996e-03 0.748943
2019-12-03 22:24:06,365 valid 000 4.757876e-03 0.824777
2019-12-03 22:24:07,304 validation loss; R2: 4.840556e-03 0.821243
2019-12-03 22:24:07,317 epoch 8 lr 2.000000e-03
2019-12-03 22:24:07,555 train 000 3.767802e-03 0.899873
2019-12-03 22:24:15,717 train 050 6.722662e-03 0.763067
2019-12-03 22:24:23,880 train 100 6.233195e-03 0.783700
2019-12-03 22:24:32,043 train 150 6.343806e-03 0.784789
2019-12-03 22:24:40,203 train 200 6.656051e-03 0.776487
2019-12-03 22:24:43,871 training loss; R2: 6.541659e-03 0.777623
2019-12-03 22:24:43,987 valid 000 2.612986e-03 0.927739
2019-12-03 22:24:44,925 validation loss; R2: 3.551591e-03 0.875723
2019-12-03 22:24:44,937 epoch 9 lr 2.000000e-03
2019-12-03 22:24:45,176 train 000 6.248106e-03 0.871312
2019-12-03 22:24:53,337 train 050 6.120242e-03 0.810646
2019-12-03 22:25:01,496 train 100 6.329800e-03 0.804573
2019-12-03 22:25:09,656 train 150 6.323423e-03 0.796647
2019-12-03 22:25:17,813 train 200 6.082531e-03 0.797828
2019-12-03 22:25:21,482 training loss; R2: 6.051368e-03 0.797798
2019-12-03 22:25:21,611 valid 000 5.238229e-03 0.871541
2019-12-03 22:25:22,549 validation loss; R2: 4.569087e-03 0.834889
2019-12-03 22:25:22,562 epoch 10 lr 2.000000e-03
2019-12-03 22:25:22,803 train 000 7.034966e-03 0.695981
2019-12-03 22:25:30,969 train 050 6.528086e-03 0.758754
2019-12-03 22:25:39,130 train 100 6.612436e-03 0.762919
2019-12-03 22:25:47,292 train 150 6.151615e-03 0.779138
2019-12-03 22:25:55,454 train 200 6.262922e-03 0.781873
2019-12-03 22:25:59,125 training loss; R2: 6.235223e-03 0.787168
2019-12-03 22:25:59,248 valid 000 1.769835e-03 0.941383
2019-12-03 22:26:00,186 validation loss; R2: 3.096874e-03 0.892073
2019-12-03 22:26:00,199 epoch 11 lr 2.000000e-03
2019-12-03 22:26:00,438 train 000 5.004163e-03 0.883121
2019-12-03 22:26:08,603 train 050 5.849153e-03 0.796934
2019-12-03 22:26:16,765 train 100 6.119436e-03 0.786988
2019-12-03 22:26:24,923 train 150 6.180828e-03 0.790126
2019-12-03 22:26:33,084 train 200 5.898699e-03 0.795049
2019-12-03 22:26:36,750 training loss; R2: 5.928577e-03 0.796609
2019-12-03 22:26:36,874 valid 000 4.284641e-03 0.930127
2019-12-03 22:26:37,812 validation loss; R2: 3.407984e-03 0.878761
2019-12-03 22:26:37,824 epoch 12 lr 2.000000e-03
2019-12-03 22:26:38,065 train 000 8.043828e-03 0.713222
2019-12-03 22:26:46,221 train 050 6.744556e-03 0.773718
2019-12-03 22:26:54,379 train 100 5.975634e-03 0.799037
2019-12-03 22:27:02,535 train 150 5.844749e-03 0.798132
2019-12-03 22:27:10,692 train 200 6.056261e-03 0.798416
2019-12-03 22:27:14,358 training loss; R2: 6.104926e-03 0.793862
2019-12-03 22:27:14,473 valid 000 4.313765e-03 0.918904
2019-12-03 22:27:15,411 validation loss; R2: 4.523797e-03 0.845708
2019-12-03 22:27:15,423 epoch 13 lr 2.000000e-03
2019-12-03 22:27:15,665 train 000 6.159549e-03 0.901285
2019-12-03 22:27:23,825 train 050 5.426169e-03 0.820819
2019-12-03 22:27:31,980 train 100 5.604394e-03 0.815099
2019-12-03 22:27:40,136 train 150 5.458398e-03 0.815014
2019-12-03 22:27:48,292 train 200 5.419414e-03 0.814228
2019-12-03 22:27:51,956 training loss; R2: 5.458508e-03 0.816335
2019-12-03 22:27:52,073 valid 000 3.158415e-03 0.845330
2019-12-03 22:27:53,011 validation loss; R2: 3.212765e-03 0.885129
2019-12-03 22:27:53,024 epoch 14 lr 2.000000e-03
2019-12-03 22:27:53,265 train 000 4.285809e-03 0.880240
2019-12-03 22:28:01,415 train 050 5.310682e-03 0.814258
2019-12-03 22:28:09,562 train 100 5.241938e-03 0.819232
2019-12-03 22:28:17,709 train 150 5.287004e-03 0.817841
2019-12-03 22:28:25,852 train 200 5.422455e-03 0.815444
2019-12-03 22:28:29,515 training loss; R2: 5.362045e-03 0.817537
2019-12-03 22:28:29,630 valid 000 1.903395e-03 0.836966
2019-12-03 22:28:30,568 validation loss; R2: 3.765938e-03 0.871326
2019-12-03 22:28:30,581 epoch 15 lr 2.000000e-03
2019-12-03 22:28:30,824 train 000 7.856677e-03 0.770157
2019-12-03 22:28:38,971 train 050 5.595811e-03 0.813030
2019-12-03 22:28:47,115 train 100 5.397497e-03 0.823689
2019-12-03 22:28:55,258 train 150 5.685966e-03 0.813704
2019-12-03 22:29:03,402 train 200 5.547582e-03 0.814060
2019-12-03 22:29:07,064 training loss; R2: 5.549509e-03 0.812317
2019-12-03 22:29:07,179 valid 000 2.982226e-03 0.919555
2019-12-03 22:29:08,116 validation loss; R2: 3.439657e-03 0.882154
2019-12-03 22:29:08,129 epoch 16 lr 2.000000e-03
2019-12-03 22:29:08,365 train 000 3.143183e-03 0.776908
2019-12-03 22:29:16,507 train 050 4.850174e-03 0.821479
2019-12-03 22:29:24,645 train 100 4.853703e-03 0.822454
2019-12-03 22:29:32,789 train 150 4.912178e-03 0.823716
2019-12-03 22:29:40,925 train 200 5.003582e-03 0.826579
2019-12-03 22:29:44,583 training loss; R2: 4.920774e-03 0.829908
2019-12-03 22:29:44,706 valid 000 1.740357e-03 0.884930
2019-12-03 22:29:45,644 validation loss; R2: 3.934758e-03 0.865445
2019-12-03 22:29:45,657 epoch 17 lr 2.000000e-03
2019-12-03 22:29:45,903 train 000 6.752979e-03 0.750947
2019-12-03 22:29:54,040 train 050 5.999546e-03 0.810507
2019-12-03 22:30:02,177 train 100 5.641011e-03 0.816356
2019-12-03 22:30:10,310 train 150 5.616124e-03 0.810841
2019-12-03 22:30:18,444 train 200 5.457100e-03 0.815686
2019-12-03 22:30:22,099 training loss; R2: 5.479914e-03 0.815915
2019-12-03 22:30:22,221 valid 000 2.568748e-03 0.941327
2019-12-03 22:30:23,158 validation loss; R2: 2.780854e-03 0.905558
2019-12-03 22:30:23,171 epoch 18 lr 2.000000e-03
2019-12-03 22:30:23,410 train 000 4.306284e-03 0.713651
2019-12-03 22:30:31,538 train 050 5.298099e-03 0.813510
2019-12-03 22:30:39,665 train 100 5.467520e-03 0.818083
2019-12-03 22:30:47,793 train 150 5.159638e-03 0.828606
2019-12-03 22:30:55,917 train 200 5.028637e-03 0.829887
2019-12-03 22:30:59,567 training loss; R2: 5.026901e-03 0.828618
2019-12-03 22:30:59,690 valid 000 1.656055e-03 0.902400
2019-12-03 22:31:00,627 validation loss; R2: 2.806392e-03 0.902945
2019-12-03 22:31:00,640 epoch 19 lr 2.000000e-03
2019-12-03 22:31:00,877 train 000 3.083659e-03 0.859383
2019-12-03 22:31:09,007 train 050 4.751203e-03 0.838237
2019-12-03 22:31:17,133 train 100 4.968483e-03 0.832893
2019-12-03 22:31:25,265 train 150 5.231087e-03 0.824025
2019-12-03 22:31:33,388 train 200 5.112021e-03 0.831084
2019-12-03 22:31:37,041 training loss; R2: 5.056873e-03 0.831616
2019-12-03 22:31:37,162 valid 000 1.912227e-03 0.933154
2019-12-03 22:31:38,099 validation loss; R2: 2.956213e-03 0.898393
