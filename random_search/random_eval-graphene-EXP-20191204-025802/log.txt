2019-12-04 02:58:02,444 gpu device = 1
2019-12-04 02:58:02,444 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=96, cutout=False, cutout_length=16, data='../data', dataset='graphene', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name=None, gpu=1, grad_clip=5, gz_dtree=False, init_channels=36, layers=8, learning_rate=0.002, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-graphene-EXP-20191204-025802', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-12-04 02:58:05,663 param size = 1.010269MB
2019-12-04 02:58:05,666 epoch 0 lr 2.000000e-03
2019-12-04 02:58:08,120 train 000 3.808228e-01 -8.373872
2019-12-04 02:58:18,505 train 050 1.196608e-01 -4.113736
2019-12-04 02:58:28,828 train 100 7.305236e-02 -2.011374
2019-12-04 02:58:39,149 train 150 5.754356e-02 -1.266130
2019-12-04 02:58:49,469 train 200 4.870635e-02 -0.876300
2019-12-04 02:58:55,108 training loss; R2: 4.588007e-02 -0.753805
2019-12-04 02:58:55,251 valid 000 2.179727e-02 0.335857
2019-12-04 02:58:56,654 validation loss; R2: 2.074468e-02 0.296398
2019-12-04 02:58:56,674 epoch 1 lr 2.000000e-03
2019-12-04 02:58:57,032 train 000 2.162462e-02 0.440985
2019-12-04 02:59:07,331 train 050 1.666869e-02 0.405074
2019-12-04 02:59:17,633 train 100 1.661995e-02 0.438164
2019-12-04 02:59:27,936 train 150 1.574832e-02 0.473685
2019-12-04 02:59:38,243 train 200 1.574838e-02 0.486569
2019-12-04 02:59:42,879 training loss; R2: 1.565352e-02 0.489638
2019-12-04 02:59:43,009 valid 000 8.975503e-03 0.595442
2019-12-04 02:59:44,104 validation loss; R2: 1.245534e-02 0.604731
2019-12-04 02:59:44,123 epoch 2 lr 2.000000e-03
2019-12-04 02:59:44,411 train 000 1.443418e-02 0.612895
2019-12-04 02:59:54,711 train 050 1.108191e-02 0.637628
2019-12-04 03:00:05,014 train 100 1.156830e-02 0.622570
2019-12-04 03:00:15,316 train 150 1.135453e-02 0.630460
2019-12-04 03:00:25,619 train 200 1.100493e-02 0.641353
2019-12-04 03:00:30,256 training loss; R2: 1.086242e-02 0.642393
2019-12-04 03:00:30,385 valid 000 7.525706e-03 0.831145
2019-12-04 03:00:31,479 validation loss; R2: 5.696361e-03 0.811128
2019-12-04 03:00:31,498 epoch 3 lr 2.000000e-03
2019-12-04 03:00:31,787 train 000 9.038087e-03 0.791671
2019-12-04 03:00:42,094 train 050 9.242357e-03 0.700132
2019-12-04 03:00:52,401 train 100 9.087644e-03 0.706130
2019-12-04 03:01:02,707 train 150 9.276980e-03 0.697314
2019-12-04 03:01:13,012 train 200 9.264406e-03 0.692495
2019-12-04 03:01:17,650 training loss; R2: 9.154360e-03 0.696286
2019-12-04 03:01:17,775 valid 000 4.475708e-03 0.886907
2019-12-04 03:01:18,869 validation loss; R2: 5.885700e-03 0.795227
2019-12-04 03:01:18,888 epoch 4 lr 2.000000e-03
2019-12-04 03:01:19,177 train 000 4.591185e-03 0.812669
2019-12-04 03:01:29,475 train 050 9.053617e-03 0.710094
2019-12-04 03:01:39,776 train 100 8.389948e-03 0.722629
2019-12-04 03:01:50,078 train 150 8.286441e-03 0.726450
2019-12-04 03:02:00,377 train 200 8.201476e-03 0.726838
2019-12-04 03:02:05,012 training loss; R2: 8.264022e-03 0.724637
2019-12-04 03:02:05,143 valid 000 8.788566e-03 0.564596
2019-12-04 03:02:06,237 validation loss; R2: 1.133085e-02 0.605989
2019-12-04 03:02:06,256 epoch 5 lr 2.000000e-03
2019-12-04 03:02:06,547 train 000 8.699250e-03 0.709113
2019-12-04 03:02:16,856 train 050 9.098929e-03 0.695225
2019-12-04 03:02:27,162 train 100 8.696256e-03 0.720546
2019-12-04 03:02:37,471 train 150 8.154346e-03 0.734041
2019-12-04 03:02:47,780 train 200 7.880636e-03 0.736835
2019-12-04 03:02:52,420 training loss; R2: 7.713001e-03 0.742588
2019-12-04 03:02:52,549 valid 000 5.684426e-03 0.861851
2019-12-04 03:02:53,643 validation loss; R2: 5.081805e-03 0.829718
2019-12-04 03:02:53,663 epoch 6 lr 2.000000e-03
2019-12-04 03:02:53,952 train 000 4.550229e-03 0.866976
2019-12-04 03:03:04,261 train 050 7.844793e-03 0.735340
2019-12-04 03:03:14,564 train 100 7.073734e-03 0.751012
2019-12-04 03:03:24,873 train 150 7.397324e-03 0.752160
2019-12-04 03:03:35,178 train 200 7.148823e-03 0.756443
2019-12-04 03:03:39,814 training loss; R2: 7.098992e-03 0.758702
2019-12-04 03:03:39,940 valid 000 4.202492e-03 0.738499
2019-12-04 03:03:41,035 validation loss; R2: 3.916821e-03 0.859915
2019-12-04 03:03:41,054 epoch 7 lr 2.000000e-03
2019-12-04 03:03:41,341 train 000 4.314078e-03 0.856000
2019-12-04 03:03:51,647 train 050 6.726180e-03 0.777535
2019-12-04 03:04:01,954 train 100 6.816799e-03 0.777621
2019-12-04 03:04:12,262 train 150 6.646763e-03 0.769850
2019-12-04 03:04:22,565 train 200 6.625944e-03 0.770380
2019-12-04 03:04:27,201 training loss; R2: 6.667345e-03 0.770803
2019-12-04 03:04:27,327 valid 000 4.128858e-03 0.719418
2019-12-04 03:04:28,422 validation loss; R2: 5.121399e-03 0.817444
2019-12-04 03:04:28,441 epoch 8 lr 2.000000e-03
2019-12-04 03:04:28,732 train 000 4.499723e-03 0.829445
2019-12-04 03:04:39,041 train 050 6.987748e-03 0.794932
2019-12-04 03:04:49,347 train 100 6.683304e-03 0.788131
2019-12-04 03:04:59,650 train 150 6.440125e-03 0.793426
2019-12-04 03:05:09,954 train 200 6.347471e-03 0.791601
2019-12-04 03:05:14,592 training loss; R2: 6.252085e-03 0.788134
2019-12-04 03:05:14,716 valid 000 5.149527e-03 0.879368
2019-12-04 03:05:15,810 validation loss; R2: 3.617908e-03 0.874515
2019-12-04 03:05:15,830 epoch 9 lr 2.000000e-03
2019-12-04 03:05:16,121 train 000 5.958164e-03 0.801901
2019-12-04 03:05:26,425 train 050 6.170755e-03 0.793996
2019-12-04 03:05:36,723 train 100 6.473391e-03 0.784745
2019-12-04 03:05:47,023 train 150 6.298659e-03 0.784341
2019-12-04 03:05:57,323 train 200 6.183983e-03 0.787007
2019-12-04 03:06:01,959 training loss; R2: 6.179133e-03 0.788664
2019-12-04 03:06:02,090 valid 000 4.622560e-03 0.898378
2019-12-04 03:06:03,185 validation loss; R2: 4.947744e-03 0.833916
2019-12-04 03:06:03,204 epoch 10 lr 2.000000e-03
2019-12-04 03:06:03,500 train 000 4.213948e-03 0.783336
2019-12-04 03:06:13,790 train 050 6.324398e-03 0.786383
2019-12-04 03:06:23,971 train 100 6.308941e-03 0.796089
2019-12-04 03:06:34,019 train 150 5.957069e-03 0.799818
2019-12-04 03:06:44,069 train 200 5.967290e-03 0.798068
2019-12-04 03:06:48,587 training loss; R2: 5.995593e-03 0.798394
2019-12-04 03:06:48,711 valid 000 3.093407e-03 0.927434
2019-12-04 03:06:49,804 validation loss; R2: 3.762858e-03 0.869153
2019-12-04 03:06:49,822 epoch 11 lr 2.000000e-03
2019-12-04 03:06:50,103 train 000 6.332104e-03 0.808207
2019-12-04 03:07:00,146 train 050 5.966006e-03 0.783720
2019-12-04 03:07:10,185 train 100 5.758867e-03 0.792804
2019-12-04 03:07:20,225 train 150 5.755495e-03 0.798406
2019-12-04 03:07:30,265 train 200 5.685147e-03 0.804213
2019-12-04 03:07:34,778 training loss; R2: 5.720979e-03 0.802021
2019-12-04 03:07:34,903 valid 000 4.077849e-03 0.825978
2019-12-04 03:07:35,995 validation loss; R2: 4.890177e-03 0.838254
2019-12-04 03:07:36,014 epoch 12 lr 2.000000e-03
2019-12-04 03:07:36,297 train 000 4.310815e-03 0.857743
2019-12-04 03:07:46,330 train 050 5.094933e-03 0.822027
2019-12-04 03:07:56,370 train 100 5.141861e-03 0.825799
2019-12-04 03:08:06,405 train 150 5.195447e-03 0.823246
2019-12-04 03:08:16,443 train 200 5.154471e-03 0.825373
2019-12-04 03:08:20,958 training loss; R2: 5.091167e-03 0.825211
2019-12-04 03:08:21,085 valid 000 3.712734e-03 0.806024
2019-12-04 03:08:22,178 validation loss; R2: 5.011933e-03 0.831286
2019-12-04 03:08:22,196 epoch 13 lr 2.000000e-03
2019-12-04 03:08:22,480 train 000 5.766179e-03 0.733404
2019-12-04 03:08:32,515 train 050 5.631852e-03 0.815008
2019-12-04 03:08:42,549 train 100 5.272175e-03 0.817596
2019-12-04 03:08:52,583 train 150 5.374093e-03 0.812898
2019-12-04 03:09:02,621 train 200 5.453657e-03 0.812543
2019-12-04 03:09:07,133 training loss; R2: 5.520837e-03 0.811828
2019-12-04 03:09:07,259 valid 000 3.662191e-03 0.896171
2019-12-04 03:09:08,351 validation loss; R2: 3.820883e-03 0.869816
2019-12-04 03:09:08,375 epoch 14 lr 2.000000e-03
2019-12-04 03:09:08,660 train 000 4.431124e-03 0.816640
2019-12-04 03:09:18,692 train 050 5.989066e-03 0.815950
2019-12-04 03:09:28,720 train 100 5.468407e-03 0.820062
2019-12-04 03:09:38,748 train 150 5.196975e-03 0.823561
2019-12-04 03:09:48,776 train 200 5.195564e-03 0.822321
2019-12-04 03:09:53,284 training loss; R2: 5.198927e-03 0.822170
2019-12-04 03:09:53,407 valid 000 3.541677e-03 0.906618
2019-12-04 03:09:54,499 validation loss; R2: 3.645408e-03 0.870135
2019-12-04 03:09:54,517 epoch 15 lr 2.000000e-03
2019-12-04 03:09:54,797 train 000 4.765228e-03 0.826415
2019-12-04 03:10:04,833 train 050 4.961733e-03 0.824730
2019-12-04 03:10:14,862 train 100 5.077159e-03 0.828408
2019-12-04 03:10:24,887 train 150 5.077876e-03 0.828239
2019-12-04 03:10:34,919 train 200 5.131479e-03 0.824813
2019-12-04 03:10:39,430 training loss; R2: 5.043489e-03 0.828011
2019-12-04 03:10:39,557 valid 000 2.947557e-03 0.904393
2019-12-04 03:10:40,649 validation loss; R2: 3.680236e-03 0.879866
2019-12-04 03:10:40,667 epoch 16 lr 2.000000e-03
2019-12-04 03:10:40,949 train 000 4.785010e-03 0.826688
2019-12-04 03:10:50,975 train 050 4.533323e-03 0.842847
2019-12-04 03:11:00,998 train 100 5.007387e-03 0.832391
2019-12-04 03:11:11,029 train 150 5.061858e-03 0.827569
2019-12-04 03:11:21,051 train 200 4.845783e-03 0.833586
2019-12-04 03:11:25,560 training loss; R2: 4.834003e-03 0.835836
2019-12-04 03:11:25,684 valid 000 3.106240e-03 0.942937
2019-12-04 03:11:26,776 validation loss; R2: 2.827085e-03 0.904181
2019-12-04 03:11:26,794 epoch 17 lr 2.000000e-03
2019-12-04 03:11:27,076 train 000 4.276308e-03 0.861180
2019-12-04 03:11:37,093 train 050 5.320042e-03 0.832053
2019-12-04 03:11:47,106 train 100 5.157802e-03 0.832145
2019-12-04 03:11:57,121 train 150 5.193439e-03 0.832355
2019-12-04 03:12:07,135 train 200 5.208001e-03 0.824461
2019-12-04 03:12:11,638 training loss; R2: 5.126034e-03 0.827512
2019-12-04 03:12:11,762 valid 000 2.807333e-03 0.935601
2019-12-04 03:12:12,855 validation loss; R2: 3.496345e-03 0.879743
2019-12-04 03:12:12,873 epoch 18 lr 2.000000e-03
2019-12-04 03:12:13,155 train 000 3.330250e-03 0.887192
2019-12-04 03:12:23,176 train 050 4.399845e-03 0.863382
2019-12-04 03:12:33,191 train 100 4.572341e-03 0.847602
2019-12-04 03:12:43,206 train 150 5.015712e-03 0.835242
2019-12-04 03:12:53,224 train 200 4.951355e-03 0.834047
2019-12-04 03:12:57,725 training loss; R2: 4.955275e-03 0.833724
2019-12-04 03:12:57,850 valid 000 2.536645e-03 0.887053
2019-12-04 03:12:58,941 validation loss; R2: 2.841201e-03 0.903476
2019-12-04 03:12:58,959 epoch 19 lr 2.000000e-03
2019-12-04 03:12:59,240 train 000 1.082986e-02 0.808900
2019-12-04 03:13:09,234 train 050 4.379164e-03 0.848584
2019-12-04 03:13:19,228 train 100 4.600115e-03 0.840921
2019-12-04 03:13:29,217 train 150 4.768618e-03 0.834626
2019-12-04 03:13:39,207 train 200 4.826534e-03 0.833167
2019-12-04 03:13:43,699 training loss; R2: 4.832205e-03 0.835019
2019-12-04 03:13:43,825 valid 000 3.447753e-03 0.912973
2019-12-04 03:13:44,916 validation loss; R2: 3.894674e-03 0.863412
