2019-12-04 02:03:14,220 gpu device = 1
2019-12-04 02:03:14,220 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=96, cutout=False, cutout_length=16, data='../data', dataset='graphene', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name=None, gpu=1, grad_clip=5, gz_dtree=False, init_channels=36, layers=8, learning_rate=0.002, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-graphene-EXP-20191204-020314', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-12-04 02:03:17,402 param size = 1.195957MB
2019-12-04 02:03:17,405 epoch 0 lr 2.000000e-03
2019-12-04 02:03:20,173 train 000 5.535517e-01 -7.836320
2019-12-04 02:03:33,484 train 050 1.278190e-01 -2.741180
2019-12-04 02:03:46,739 train 100 8.077220e-02 -1.433679
2019-12-04 02:04:00,003 train 150 6.332527e-02 -0.943594
2019-12-04 02:04:13,270 train 200 5.355368e-02 -0.657238
2019-12-04 02:04:20,364 training loss; R2: 5.025081e-02 -0.565524
2019-12-04 02:04:20,517 valid 000 1.515298e-02 0.378288
2019-12-04 02:04:22,278 validation loss; R2: 1.772225e-02 0.430760
2019-12-04 02:04:22,300 epoch 1 lr 2.000000e-03
2019-12-04 02:04:22,723 train 000 1.443973e-02 0.372294
2019-12-04 02:04:35,982 train 050 2.181017e-02 0.266725
2019-12-04 02:04:49,245 train 100 2.226898e-02 0.273936
2019-12-04 02:05:02,510 train 150 2.049501e-02 0.333071
2019-12-04 02:05:15,777 train 200 1.925119e-02 0.372349
2019-12-04 02:05:21,743 training loss; R2: 1.875544e-02 0.382690
2019-12-04 02:05:21,882 valid 000 7.965387e-03 0.707171
2019-12-04 02:05:23,300 validation loss; R2: 1.191220e-02 0.622938
2019-12-04 02:05:23,323 epoch 2 lr 2.000000e-03
2019-12-04 02:05:23,671 train 000 1.524949e-02 0.653073
2019-12-04 02:05:36,936 train 050 1.471903e-02 0.495192
2019-12-04 02:05:50,202 train 100 1.465406e-02 0.497217
2019-12-04 02:06:03,474 train 150 1.399254e-02 0.516359
2019-12-04 02:06:16,749 train 200 1.329638e-02 0.543713
2019-12-04 02:06:22,861 training loss; R2: 1.316900e-02 0.548395
2019-12-04 02:06:23,005 valid 000 4.951771e-03 0.760164
2019-12-04 02:06:24,424 validation loss; R2: 6.288791e-03 0.797566
2019-12-04 02:06:24,448 epoch 3 lr 2.000000e-03
2019-12-04 02:06:24,805 train 000 5.521835e-03 0.717724
2019-12-04 02:06:38,387 train 050 1.148433e-02 0.641884
2019-12-04 02:06:51,967 train 100 1.036851e-02 0.666203
2019-12-04 02:07:05,551 train 150 1.019852e-02 0.675545
2019-12-04 02:07:19,131 train 200 1.011912e-02 0.670080
2019-12-04 02:07:25,243 training loss; R2: 9.936118e-03 0.672604
2019-12-04 02:07:25,387 valid 000 3.213666e-03 0.841888
2019-12-04 02:07:26,806 validation loss; R2: 5.906970e-03 0.809616
2019-12-04 02:07:26,836 epoch 4 lr 2.000000e-03
2019-12-04 02:07:27,200 train 000 6.000813e-03 0.872152
2019-12-04 02:07:40,786 train 050 9.759237e-03 0.680013
2019-12-04 02:07:54,151 train 100 9.920608e-03 0.662169
2019-12-04 02:08:07,417 train 150 9.257466e-03 0.688570
2019-12-04 02:08:20,686 train 200 9.378776e-03 0.685414
2019-12-04 02:08:26,649 training loss; R2: 9.253778e-03 0.688127
2019-12-04 02:08:26,787 valid 000 3.822244e-03 0.845697
2019-12-04 02:08:28,206 validation loss; R2: 5.151551e-03 0.828221
2019-12-04 02:08:28,235 epoch 5 lr 2.000000e-03
2019-12-04 02:08:28,584 train 000 6.356627e-03 0.710103
2019-12-04 02:08:41,848 train 050 8.318281e-03 0.752985
2019-12-04 02:08:55,115 train 100 7.784707e-03 0.741354
2019-12-04 02:09:08,383 train 150 7.902116e-03 0.743572
2019-12-04 02:09:21,650 train 200 8.204990e-03 0.725461
2019-12-04 02:09:27,614 training loss; R2: 8.073566e-03 0.728800
2019-12-04 02:09:27,752 valid 000 8.457186e-03 0.752700
2019-12-04 02:09:29,170 validation loss; R2: 8.092213e-03 0.732999
2019-12-04 02:09:29,195 epoch 6 lr 2.000000e-03
2019-12-04 02:09:29,544 train 000 8.512313e-03 0.692678
2019-12-04 02:09:42,804 train 050 7.186459e-03 0.732863
2019-12-04 02:09:56,069 train 100 7.792538e-03 0.734848
2019-12-04 02:10:09,332 train 150 7.412832e-03 0.744069
2019-12-04 02:10:22,602 train 200 7.295817e-03 0.754065
2019-12-04 02:10:28,567 training loss; R2: 7.197553e-03 0.755456
2019-12-04 02:10:28,704 valid 000 3.658772e-03 0.799148
2019-12-04 02:10:30,123 validation loss; R2: 4.093307e-03 0.857057
2019-12-04 02:10:30,148 epoch 7 lr 2.000000e-03
2019-12-04 02:10:30,506 train 000 7.796612e-03 0.824132
2019-12-04 02:10:43,771 train 050 6.501245e-03 0.794694
2019-12-04 02:10:57,040 train 100 6.354576e-03 0.782322
2019-12-04 02:11:10,304 train 150 6.343517e-03 0.778449
2019-12-04 02:11:23,569 train 200 6.659988e-03 0.769109
2019-12-04 02:11:29,538 training loss; R2: 6.666103e-03 0.768413
2019-12-04 02:11:29,680 valid 000 3.502001e-03 0.832668
2019-12-04 02:11:31,098 validation loss; R2: 3.779771e-03 0.870963
2019-12-04 02:11:31,122 epoch 8 lr 2.000000e-03
2019-12-04 02:11:31,475 train 000 5.552996e-03 0.870507
2019-12-04 02:11:44,743 train 050 6.435429e-03 0.776260
2019-12-04 02:11:58,008 train 100 6.541637e-03 0.779291
2019-12-04 02:12:11,273 train 150 6.314089e-03 0.783100
2019-12-04 02:12:24,541 train 200 6.319006e-03 0.787132
2019-12-04 02:12:30,506 training loss; R2: 6.258644e-03 0.789709
2019-12-04 02:12:30,647 valid 000 4.261717e-03 0.934161
2019-12-04 02:12:32,066 validation loss; R2: 4.145431e-03 0.851584
2019-12-04 02:12:32,091 epoch 9 lr 2.000000e-03
2019-12-04 02:12:32,443 train 000 7.847100e-03 0.764703
2019-12-04 02:12:45,711 train 050 5.760085e-03 0.791061
2019-12-04 02:12:58,980 train 100 5.882909e-03 0.794197
2019-12-04 02:13:12,260 train 150 5.943148e-03 0.789465
2019-12-04 02:13:25,540 train 200 5.873634e-03 0.793506
2019-12-04 02:13:31,513 training loss; R2: 5.906642e-03 0.794058
2019-12-04 02:13:31,655 valid 000 6.917307e-03 0.752033
2019-12-04 02:13:33,075 validation loss; R2: 3.406658e-03 0.883286
2019-12-04 02:13:33,100 epoch 10 lr 2.000000e-03
2019-12-04 02:13:33,459 train 000 6.843431e-03 0.806969
2019-12-04 02:13:47,036 train 050 6.342041e-03 0.791684
2019-12-04 02:14:00,619 train 100 6.053153e-03 0.800424
2019-12-04 02:14:14,196 train 150 5.858209e-03 0.799487
2019-12-04 02:14:27,775 train 200 5.735890e-03 0.804517
2019-12-04 02:14:33,881 training loss; R2: 5.669740e-03 0.807621
2019-12-04 02:14:34,026 valid 000 3.114145e-03 0.866071
2019-12-04 02:14:35,446 validation loss; R2: 3.102540e-03 0.893495
2019-12-04 02:14:35,471 epoch 11 lr 2.000000e-03
2019-12-04 02:14:35,839 train 000 5.485593e-03 0.683639
2019-12-04 02:14:49,411 train 050 5.430751e-03 0.813321
2019-12-04 02:15:02,983 train 100 5.356273e-03 0.817283
2019-12-04 02:15:16,558 train 150 5.232458e-03 0.820378
2019-12-04 02:15:30,127 train 200 5.343943e-03 0.818810
2019-12-04 02:15:36,234 training loss; R2: 5.315710e-03 0.820381
2019-12-04 02:15:36,380 valid 000 2.344211e-03 0.883103
2019-12-04 02:15:37,799 validation loss; R2: 3.497979e-03 0.885666
2019-12-04 02:15:37,829 epoch 12 lr 2.000000e-03
2019-12-04 02:15:38,188 train 000 9.685670e-03 0.733233
2019-12-04 02:15:51,759 train 050 5.818232e-03 0.809132
2019-12-04 02:16:05,329 train 100 5.589179e-03 0.816516
2019-12-04 02:16:18,903 train 150 5.600083e-03 0.816868
2019-12-04 02:16:32,469 train 200 5.319784e-03 0.818322
2019-12-04 02:16:38,572 training loss; R2: 5.232600e-03 0.819133
2019-12-04 02:16:38,716 valid 000 3.059369e-03 0.875556
2019-12-04 02:16:40,136 validation loss; R2: 3.474763e-03 0.876978
2019-12-04 02:16:40,160 epoch 13 lr 2.000000e-03
2019-12-04 02:16:40,524 train 000 9.809873e-03 0.619438
2019-12-04 02:16:54,087 train 050 5.622370e-03 0.799313
2019-12-04 02:17:07,648 train 100 5.720114e-03 0.807850
2019-12-04 02:17:21,209 train 150 5.599380e-03 0.811244
2019-12-04 02:17:34,767 train 200 5.446985e-03 0.813067
2019-12-04 02:17:40,869 training loss; R2: 5.386491e-03 0.812046
2019-12-04 02:17:41,012 valid 000 6.106322e-03 0.881964
2019-12-04 02:17:42,431 validation loss; R2: 3.004987e-03 0.899009
2019-12-04 02:17:42,456 epoch 14 lr 2.000000e-03
2019-12-04 02:17:42,819 train 000 5.187095e-03 0.874314
2019-12-04 02:17:56,377 train 050 4.396536e-03 0.853220
2019-12-04 02:18:09,935 train 100 4.594893e-03 0.843087
2019-12-04 02:18:23,493 train 150 4.710881e-03 0.837822
2019-12-04 02:18:37,050 train 200 5.087410e-03 0.831211
2019-12-04 02:18:43,151 training loss; R2: 5.140575e-03 0.827973
2019-12-04 02:18:43,295 valid 000 2.792416e-03 0.729284
2019-12-04 02:18:44,714 validation loss; R2: 5.498518e-03 0.821129
2019-12-04 02:18:44,739 epoch 15 lr 2.000000e-03
2019-12-04 02:18:45,100 train 000 3.658727e-03 0.797768
2019-12-04 02:18:58,653 train 050 4.898183e-03 0.830326
2019-12-04 02:19:12,208 train 100 5.055332e-03 0.821212
2019-12-04 02:19:25,761 train 150 4.958820e-03 0.823349
2019-12-04 02:19:39,310 train 200 5.080542e-03 0.824222
2019-12-04 02:19:45,406 training loss; R2: 5.213388e-03 0.821958
2019-12-04 02:19:45,550 valid 000 3.926347e-03 0.834794
2019-12-04 02:19:46,968 validation loss; R2: 4.790786e-03 0.826811
2019-12-04 02:19:46,992 epoch 16 lr 2.000000e-03
2019-12-04 02:19:47,351 train 000 3.901766e-03 0.788230
2019-12-04 02:20:00,895 train 050 5.438098e-03 0.819916
2019-12-04 02:20:14,440 train 100 5.311902e-03 0.822249
2019-12-04 02:20:27,977 train 150 5.023091e-03 0.824764
2019-12-04 02:20:41,510 train 200 5.098282e-03 0.821684
2019-12-04 02:20:47,599 training loss; R2: 5.121480e-03 0.821506
2019-12-04 02:20:47,743 valid 000 3.523892e-01 -13.115202
2019-12-04 02:20:49,159 validation loss; R2: 3.436616e-01 -11.612400
2019-12-04 02:20:49,184 epoch 17 lr 2.000000e-03
2019-12-04 02:20:49,541 train 000 4.526000e-03 0.905491
2019-12-04 02:21:03,088 train 050 5.449357e-03 0.819228
2019-12-04 02:21:16,631 train 100 5.122226e-03 0.830206
2019-12-04 02:21:30,178 train 150 5.207866e-03 0.826827
2019-12-04 02:21:43,417 train 200 5.165071e-03 0.828593
2019-12-04 02:21:49,363 training loss; R2: 5.127372e-03 0.828459
2019-12-04 02:21:49,514 valid 000 1.096657e+01 -293.431170
2019-12-04 02:21:50,930 validation loss; R2: 1.102492e+01 -412.210262
2019-12-04 02:21:50,954 epoch 18 lr 2.000000e-03
2019-12-04 02:21:51,301 train 000 3.982118e-03 0.877869
2019-12-04 02:22:04,521 train 050 4.648569e-03 0.838152
2019-12-04 02:22:17,735 train 100 4.914824e-03 0.835906
2019-12-04 02:22:30,973 train 150 4.913044e-03 0.836271
2019-12-04 02:22:44,216 train 200 4.751159e-03 0.838417
2019-12-04 02:22:50,179 training loss; R2: 4.698847e-03 0.840337
2019-12-04 02:22:50,319 valid 000 1.952398e+01 -558.262982
2019-12-04 02:22:51,734 validation loss; R2: 1.954676e+01 -724.453090
2019-12-04 02:22:51,758 epoch 19 lr 2.000000e-03
2019-12-04 02:22:52,110 train 000 6.750932e-03 0.827704
2019-12-04 02:23:05,363 train 050 4.961904e-03 0.834538
2019-12-04 02:23:18,631 train 100 4.769503e-03 0.836840
2019-12-04 02:23:31,880 train 150 4.530970e-03 0.842843
2019-12-04 02:23:45,119 train 200 4.513551e-03 0.845916
2019-12-04 02:23:51,073 training loss; R2: 4.606295e-03 0.845517
2019-12-04 02:23:51,219 valid 000 1.206678e+01 -523.053037
2019-12-04 02:23:52,636 validation loss; R2: 1.196938e+01 -431.894375
