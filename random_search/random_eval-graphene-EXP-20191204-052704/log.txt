2019-12-04 05:27:04,069 gpu device = 1
2019-12-04 05:27:04,069 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=96, cutout=False, cutout_length=16, data='../data', dataset='graphene', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name=None, gpu=1, grad_clip=5, gz_dtree=False, init_channels=36, layers=8, learning_rate=0.002, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-graphene-EXP-20191204-052704', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-12-04 05:27:07,327 param size = 1.383373MB
2019-12-04 05:27:07,330 epoch 0 lr 2.000000e-03
2019-12-04 05:27:10,095 train 000 3.246956e-01 -8.422985
2019-12-04 05:27:24,880 train 050 1.413076e-01 -3.804095
2019-12-04 05:27:39,419 train 100 8.690628e-02 -1.915784
2019-12-04 05:27:53,965 train 150 6.642519e-02 -1.232452
2019-12-04 05:28:08,506 train 200 5.673080e-02 -0.893422
2019-12-04 05:28:16,166 training loss; R2: 5.369006e-02 -0.790927
2019-12-04 05:28:16,320 valid 000 1.992743e-02 0.411586
2019-12-04 05:28:18,151 validation loss; R2: 2.002360e-02 0.368707
2019-12-04 05:28:18,176 epoch 1 lr 2.000000e-03
2019-12-04 05:28:18,774 train 000 2.442409e-02 0.327166
2019-12-04 05:28:33,331 train 050 2.643911e-02 0.209231
2019-12-04 05:28:47,870 train 100 2.295282e-02 0.253112
2019-12-04 05:29:02,418 train 150 2.096159e-02 0.317793
2019-12-04 05:29:16,966 train 200 2.021879e-02 0.348536
2019-12-04 05:29:23,513 training loss; R2: 1.962972e-02 0.370290
2019-12-04 05:29:23,662 valid 000 9.979247e-03 0.543516
2019-12-04 05:29:25,195 validation loss; R2: 1.260482e-02 0.573294
2019-12-04 05:29:25,221 epoch 2 lr 2.000000e-03
2019-12-04 05:29:25,600 train 000 1.398146e-02 0.268279
2019-12-04 05:29:40,137 train 050 1.406419e-02 0.560643
2019-12-04 05:29:54,677 train 100 1.264966e-02 0.587118
2019-12-04 05:30:09,216 train 150 1.221012e-02 0.602041
2019-12-04 05:30:23,764 train 200 1.143976e-02 0.624413
2019-12-04 05:30:30,304 training loss; R2: 1.157363e-02 0.623249
2019-12-04 05:30:30,451 valid 000 4.779110e-03 0.797315
2019-12-04 05:30:31,985 validation loss; R2: 7.545252e-03 0.764267
2019-12-04 05:30:32,012 epoch 3 lr 2.000000e-03
2019-12-04 05:30:32,388 train 000 1.645223e-02 0.686186
2019-12-04 05:30:46,935 train 050 1.056690e-02 0.631936
2019-12-04 05:31:01,484 train 100 1.000518e-02 0.657567
2019-12-04 05:31:16,036 train 150 9.532881e-03 0.679803
2019-12-04 05:31:30,584 train 200 9.197404e-03 0.687967
2019-12-04 05:31:37,127 training loss; R2: 9.264473e-03 0.684724
2019-12-04 05:31:37,274 valid 000 4.333626e-03 0.902897
2019-12-04 05:31:38,808 validation loss; R2: 4.851060e-03 0.835952
2019-12-04 05:31:38,842 epoch 4 lr 2.000000e-03
2019-12-04 05:31:39,219 train 000 7.504872e-03 0.807881
2019-12-04 05:31:53,767 train 050 8.077477e-03 0.716326
2019-12-04 05:32:08,315 train 100 8.049898e-03 0.724266
2019-12-04 05:32:22,854 train 150 7.887405e-03 0.740606
2019-12-04 05:32:37,401 train 200 7.873064e-03 0.741802
2019-12-04 05:32:43,942 training loss; R2: 7.795411e-03 0.740450
2019-12-04 05:32:44,088 valid 000 3.595260e-03 0.879640
2019-12-04 05:32:45,621 validation loss; R2: 4.161016e-03 0.857573
2019-12-04 05:32:45,650 epoch 5 lr 2.000000e-03
2019-12-04 05:32:46,027 train 000 5.374081e-03 0.793773
2019-12-04 05:33:00,570 train 050 7.831510e-03 0.735070
2019-12-04 05:33:15,117 train 100 7.643682e-03 0.754820
2019-12-04 05:33:29,664 train 150 7.375331e-03 0.752373
2019-12-04 05:33:44,212 train 200 7.251667e-03 0.758280
2019-12-04 05:33:50,753 training loss; R2: 7.122969e-03 0.759707
2019-12-04 05:33:50,898 valid 000 2.812430e-03 0.849705
2019-12-04 05:33:52,432 validation loss; R2: 4.323275e-03 0.859786
2019-12-04 05:33:52,458 epoch 6 lr 2.000000e-03
2019-12-04 05:33:52,836 train 000 6.455699e-03 0.687312
2019-12-04 05:34:07,379 train 050 6.856037e-03 0.777427
2019-12-04 05:34:21,922 train 100 6.529976e-03 0.783761
2019-12-04 05:34:36,467 train 150 6.442561e-03 0.789285
2019-12-04 05:34:51,013 train 200 6.281540e-03 0.791646
2019-12-04 05:34:57,558 training loss; R2: 6.243504e-03 0.790795
2019-12-04 05:34:57,702 valid 000 3.893193e-03 0.867471
2019-12-04 05:34:59,236 validation loss; R2: 4.918353e-03 0.834216
2019-12-04 05:34:59,264 epoch 7 lr 2.000000e-03
2019-12-04 05:34:59,641 train 000 7.007663e-03 0.840449
2019-12-04 05:35:14,182 train 050 6.091330e-03 0.802371
2019-12-04 05:35:28,724 train 100 6.280368e-03 0.788144
2019-12-04 05:35:43,268 train 150 6.431324e-03 0.779013
2019-12-04 05:35:57,811 train 200 6.368352e-03 0.786156
2019-12-04 05:36:04,351 training loss; R2: 6.629930e-03 0.779989
2019-12-04 05:36:04,501 valid 000 4.948480e-03 0.872987
2019-12-04 05:36:06,034 validation loss; R2: 5.595783e-03 0.813184
2019-12-04 05:36:06,062 epoch 8 lr 2.000000e-03
2019-12-04 05:36:06,438 train 000 5.507285e-03 0.855005
2019-12-04 05:36:20,984 train 050 6.646892e-03 0.772447
2019-12-04 05:36:35,522 train 100 6.569446e-03 0.773822
2019-12-04 05:36:50,062 train 150 6.409025e-03 0.780374
2019-12-04 05:37:04,604 train 200 6.079568e-03 0.789170
2019-12-04 05:37:11,143 training loss; R2: 6.078988e-03 0.791251
2019-12-04 05:37:11,297 valid 000 3.185021e-03 0.842758
2019-12-04 05:37:12,830 validation loss; R2: 3.802612e-03 0.858599
2019-12-04 05:37:12,858 epoch 9 lr 2.000000e-03
2019-12-04 05:37:13,233 train 000 4.966347e-03 0.654333
2019-12-04 05:37:27,776 train 050 5.793090e-03 0.797360
2019-12-04 05:37:42,318 train 100 6.263091e-03 0.783560
2019-12-04 05:37:56,859 train 150 6.116957e-03 0.791849
2019-12-04 05:38:11,403 train 200 5.784518e-03 0.802637
2019-12-04 05:38:17,947 training loss; R2: 5.735366e-03 0.804590
2019-12-04 05:38:18,097 valid 000 4.535112e-03 0.786110
2019-12-04 05:38:19,630 validation loss; R2: 6.691885e-03 0.767879
2019-12-04 05:38:19,658 epoch 10 lr 2.000000e-03
2019-12-04 05:38:20,034 train 000 8.960172e-03 0.778376
2019-12-04 05:38:34,567 train 050 5.375263e-03 0.812065
2019-12-04 05:38:49,118 train 100 5.316992e-03 0.814333
2019-12-04 05:39:03,656 train 150 5.330016e-03 0.814265
2019-12-04 05:39:18,577 train 200 5.468643e-03 0.815504
2019-12-04 05:39:25,294 training loss; R2: 5.465720e-03 0.814263
2019-12-04 05:39:25,440 valid 000 4.971558e-03 0.755265
2019-12-04 05:39:26,974 validation loss; R2: 4.634935e-03 0.843828
2019-12-04 05:39:27,003 epoch 11 lr 2.000000e-03
2019-12-04 05:39:27,389 train 000 5.851157e-03 0.835804
2019-12-04 05:39:42,330 train 050 4.385318e-03 0.842149
2019-12-04 05:39:57,269 train 100 4.844508e-03 0.838189
2019-12-04 05:40:12,206 train 150 5.180442e-03 0.828207
2019-12-04 05:40:27,136 train 200 5.245887e-03 0.828032
2019-12-04 05:40:33,858 training loss; R2: 5.282002e-03 0.822486
2019-12-04 05:40:34,006 valid 000 4.094564e-03 0.897237
2019-12-04 05:40:35,541 validation loss; R2: 4.734503e-03 0.840532
2019-12-04 05:40:35,568 epoch 12 lr 2.000000e-03
2019-12-04 05:40:35,954 train 000 8.292680e-03 0.823057
2019-12-04 05:40:50,872 train 050 4.489654e-03 0.824784
2019-12-04 05:41:05,783 train 100 4.871560e-03 0.831757
2019-12-04 05:41:20,697 train 150 4.959032e-03 0.827930
2019-12-04 05:41:35,606 train 200 4.982330e-03 0.827941
2019-12-04 05:41:42,318 training loss; R2: 5.055441e-03 0.827529
2019-12-04 05:41:42,468 valid 000 5.018435e-03 0.825682
2019-12-04 05:41:44,002 validation loss; R2: 5.193823e-03 0.817475
2019-12-04 05:41:44,030 epoch 13 lr 2.000000e-03
2019-12-04 05:41:44,422 train 000 4.912675e-03 0.879246
2019-12-04 05:41:59,352 train 050 4.707171e-03 0.829447
2019-12-04 05:42:14,280 train 100 4.666995e-03 0.839399
2019-12-04 05:42:29,217 train 150 5.031719e-03 0.832348
2019-12-04 05:42:44,152 train 200 5.034233e-03 0.830737
2019-12-04 05:42:50,877 training loss; R2: 5.045327e-03 0.831012
2019-12-04 05:42:51,028 valid 000 3.480275e-03 0.886467
2019-12-04 05:42:52,561 validation loss; R2: 4.278920e-03 0.853989
2019-12-04 05:42:52,590 epoch 14 lr 2.000000e-03
2019-12-04 05:42:52,976 train 000 5.841361e-03 0.772638
2019-12-04 05:43:07,899 train 050 4.424527e-03 0.833457
2019-12-04 05:43:22,815 train 100 4.725860e-03 0.837853
2019-12-04 05:43:37,737 train 150 4.974150e-03 0.829191
2019-12-04 05:43:52,653 train 200 5.040153e-03 0.829813
2019-12-04 05:43:59,370 training loss; R2: 4.985202e-03 0.832354
2019-12-04 05:43:59,524 valid 000 4.203941e-03 0.842164
2019-12-04 05:44:01,058 validation loss; R2: 4.194473e-03 0.847748
2019-12-04 05:44:01,086 epoch 15 lr 2.000000e-03
2019-12-04 05:44:01,473 train 000 5.547886e-03 0.772867
2019-12-04 05:44:16,402 train 050 5.207551e-03 0.816794
2019-12-04 05:44:31,329 train 100 4.584928e-03 0.829743
2019-12-04 05:44:46,253 train 150 4.563327e-03 0.835379
2019-12-04 05:45:01,182 train 200 4.716680e-03 0.836437
2019-12-04 05:45:07,897 training loss; R2: 4.738191e-03 0.834808
2019-12-04 05:45:08,047 valid 000 3.503971e-03 0.761847
2019-12-04 05:45:09,581 validation loss; R2: 3.948414e-03 0.863926
2019-12-04 05:45:09,609 epoch 16 lr 2.000000e-03
2019-12-04 05:45:09,997 train 000 5.967967e-03 0.786375
2019-12-04 05:45:24,916 train 050 4.769330e-03 0.830987
2019-12-04 05:45:39,836 train 100 4.813149e-03 0.825822
2019-12-04 05:45:54,762 train 150 5.040711e-03 0.827979
2019-12-04 05:46:09,680 train 200 5.004934e-03 0.831068
2019-12-04 05:46:16,393 training loss; R2: 4.904731e-03 0.833695
2019-12-04 05:46:16,543 valid 000 3.707578e-03 0.760314
2019-12-04 05:46:18,077 validation loss; R2: 4.291148e-03 0.851654
2019-12-04 05:46:18,104 epoch 17 lr 2.000000e-03
2019-12-04 05:46:18,492 train 000 3.018441e-03 0.901663
2019-12-04 05:46:33,395 train 050 5.118572e-03 0.822620
2019-12-04 05:46:48,299 train 100 4.675353e-03 0.837432
2019-12-04 05:47:03,200 train 150 4.632634e-03 0.842725
2019-12-04 05:47:18,104 train 200 4.628373e-03 0.842312
2019-12-04 05:47:24,811 training loss; R2: 4.576695e-03 0.843399
2019-12-04 05:47:24,957 valid 000 3.220206e-03 0.913267
2019-12-04 05:47:26,491 validation loss; R2: 2.752602e-03 0.895107
2019-12-04 05:47:26,519 epoch 18 lr 2.000000e-03
2019-12-04 05:47:26,906 train 000 2.450446e-03 0.923080
2019-12-04 05:47:41,828 train 050 4.429746e-03 0.846464
2019-12-04 05:47:56,739 train 100 4.349950e-03 0.839481
2019-12-04 05:48:11,649 train 150 4.422147e-03 0.845848
2019-12-04 05:48:26,560 train 200 4.409473e-03 0.844596
2019-12-04 05:48:33,234 training loss; R2: 4.550295e-03 0.842234
2019-12-04 05:48:33,379 valid 000 3.240736e-03 0.880517
2019-12-04 05:48:34,911 validation loss; R2: 3.929640e-03 0.867911
2019-12-04 05:48:34,945 epoch 19 lr 2.000000e-03
2019-12-04 05:48:35,324 train 000 4.470527e-03 0.855337
2019-12-04 05:48:49,830 train 050 4.208502e-03 0.857285
2019-12-04 05:49:04,337 train 100 4.611806e-03 0.843440
2019-12-04 05:49:18,845 train 150 4.685374e-03 0.844085
2019-12-04 05:49:33,352 train 200 4.692571e-03 0.840029
2019-12-04 05:49:39,877 training loss; R2: 4.686942e-03 0.840425
2019-12-04 05:49:40,024 valid 000 5.926277e-03 0.846433
2019-12-04 05:49:41,556 validation loss; R2: 4.308392e-03 0.851184
