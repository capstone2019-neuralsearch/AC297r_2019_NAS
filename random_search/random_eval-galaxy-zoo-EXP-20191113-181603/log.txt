2019-11-13 18:16:04,071 gpu device = 1
2019-11-13 18:16:04,071 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name='galaxy-zoo-cleaned', gpu=1, grad_clip=5, gz_dtree=False, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-galaxy-zoo-EXP-20191113-181603', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-11-13 18:16:15,725 param size = 0.234869MB
2019-11-13 18:16:15,730 epoch 0 lr 1.000000e-03
2019-11-13 18:16:17,958 train 000 9.347058e-01 -304.958393
2019-11-13 18:16:24,336 train 050 6.644385e-02 -16.223829
2019-11-13 18:16:30,553 train 100 4.856782e-02 -8.858266
2019-11-13 18:16:36,764 train 150 4.151526e-02 -6.239106
2019-11-13 18:16:42,965 train 200 3.769516e-02 -4.838140
2019-11-13 18:16:49,182 train 250 3.525111e-02 -3.949998
2019-11-13 18:16:55,091 train 300 3.352756e-02 -3.383263
2019-11-13 18:17:01,004 train 350 3.220037e-02 -2.933378
2019-11-13 18:17:06,915 train 400 3.119837e-02 -2.607450
2019-11-13 18:17:12,824 train 450 3.032674e-02 -2.341654
2019-11-13 18:17:18,729 train 500 2.965862e-02 -2.129010
2019-11-13 18:17:24,638 train 550 2.900149e-02 -1.949968
2019-11-13 18:17:30,542 train 600 2.844421e-02 -1.801159
2019-11-13 18:17:36,456 train 650 2.791870e-02 -1.671590
2019-11-13 18:17:42,371 train 700 2.744446e-02 -1.560021
2019-11-13 18:17:48,289 train 750 2.699754e-02 -1.467670
2019-11-13 18:17:54,197 train 800 2.665004e-02 -1.379470
2019-11-13 18:18:00,100 train 850 2.628176e-02 -1.310392
2019-11-13 18:18:02,565 training loss; R2: 2.617759e-02 -1.289269
2019-11-13 18:18:02,918 valid 000 1.369444e-02 0.269145
2019-11-13 18:18:04,694 valid 050 1.803103e-02 0.052136
2019-11-13 18:18:06,384 validation loss; R2: 1.815561e-02 0.088506
2019-11-13 18:18:06,399 epoch 1 lr 1.000000e-03
2019-11-13 18:18:06,985 train 000 2.130206e-02 0.076275
2019-11-13 18:18:12,906 train 050 2.079841e-02 -0.019167
2019-11-13 18:18:18,828 train 100 2.044653e-02 -0.065691
2019-11-13 18:18:24,744 train 150 2.027508e-02 -0.066215
2019-11-13 18:18:30,669 train 200 2.017061e-02 -0.049258
2019-11-13 18:18:36,779 train 250 2.007653e-02 -0.039572
2019-11-13 18:18:42,706 train 300 1.996025e-02 -0.032832
2019-11-13 18:18:48,628 train 350 1.982051e-02 -0.026008
2019-11-13 18:18:54,543 train 400 1.963974e-02 -0.019083
2019-11-13 18:19:00,462 train 450 1.950689e-02 -0.019225
2019-11-13 18:19:06,389 train 500 1.937452e-02 -0.010531
2019-11-13 18:19:12,317 train 550 1.927929e-02 -0.004064
2019-11-13 18:19:18,249 train 600 1.916369e-02 -0.000768
2019-11-13 18:19:24,172 train 650 1.903838e-02 -0.001583
2019-11-13 18:19:30,116 train 700 1.889522e-02 0.005880
2019-11-13 18:19:36,034 train 750 1.880607e-02 0.011521
2019-11-13 18:19:41,940 train 800 1.874719e-02 0.012838
2019-11-13 18:19:47,852 train 850 1.866669e-02 0.015119
2019-11-13 18:19:49,625 training loss; R2: 1.864818e-02 0.016676
2019-11-13 18:19:49,974 valid 000 1.225715e-02 0.278316
2019-11-13 18:19:51,737 valid 050 1.583910e-02 0.173443
2019-11-13 18:19:53,392 validation loss; R2: 1.554981e-02 0.095067
2019-11-13 18:19:53,405 epoch 2 lr 1.000000e-03
2019-11-13 18:19:53,806 train 000 1.592435e-02 0.003762
2019-11-13 18:20:00,095 train 050 1.699209e-02 0.099733
2019-11-13 18:20:06,255 train 100 1.700602e-02 0.096009
2019-11-13 18:20:12,205 train 150 1.689822e-02 0.079413
2019-11-13 18:20:18,166 train 200 1.690065e-02 0.069970
2019-11-13 18:20:24,280 train 250 1.683056e-02 0.075636
2019-11-13 18:20:30,587 train 300 1.673768e-02 0.084452
2019-11-13 18:20:36,854 train 350 1.667066e-02 0.079974
2019-11-13 18:20:42,845 train 400 1.666045e-02 0.080327
2019-11-13 18:20:48,824 train 450 1.660471e-02 0.084996
2019-11-13 18:20:54,897 train 500 1.658445e-02 -1.148444
2019-11-13 18:21:01,156 train 550 1.654513e-02 -1.033946
2019-11-13 18:21:07,367 train 600 1.653465e-02 -0.985686
2019-11-13 18:21:13,467 train 650 1.647702e-02 -0.898339
2019-11-13 18:21:19,632 train 700 1.644446e-02 -0.827634
2019-11-13 18:21:25,938 train 750 1.640790e-02 -0.763673
2019-11-13 18:21:32,233 train 800 1.636108e-02 -0.710519
2019-11-13 18:21:38,485 train 850 1.634029e-02 -0.658064
2019-11-13 18:21:40,353 training loss; R2: 1.632980e-02 -0.643609
2019-11-13 18:21:40,703 valid 000 1.292874e-02 0.220671
2019-11-13 18:21:42,520 valid 050 1.324823e-02 0.251658
2019-11-13 18:21:44,143 validation loss; R2: 1.350065e-02 0.234977
2019-11-13 18:21:44,157 epoch 3 lr 1.000000e-03
2019-11-13 18:21:44,577 train 000 1.556693e-02 0.082314
2019-11-13 18:21:50,869 train 050 1.544610e-02 0.127768
2019-11-13 18:21:57,106 train 100 1.545033e-02 0.069450
2019-11-13 18:22:03,465 train 150 1.539435e-02 0.101115
2019-11-13 18:22:09,504 train 200 1.532377e-02 0.116837
2019-11-13 18:22:15,470 train 250 1.523857e-02 0.127292
2019-11-13 18:22:21,390 train 300 1.523058e-02 0.133523
2019-11-13 18:22:27,305 train 350 1.520834e-02 0.140299
2019-11-13 18:22:33,278 train 400 1.515865e-02 0.146605
2019-11-13 18:22:39,498 train 450 1.516431e-02 0.149318
2019-11-13 18:22:45,703 train 500 1.512164e-02 0.148680
2019-11-13 18:22:51,907 train 550 1.509447e-02 0.151320
2019-11-13 18:22:58,111 train 600 1.506367e-02 0.152311
2019-11-13 18:23:04,229 train 650 1.504509e-02 0.155188
2019-11-13 18:23:10,149 train 700 1.502396e-02 0.159267
2019-11-13 18:23:16,060 train 750 1.499695e-02 0.163174
2019-11-13 18:23:21,975 train 800 1.497851e-02 0.164711
2019-11-13 18:23:27,886 train 850 1.494295e-02 0.167464
2019-11-13 18:23:29,655 training loss; R2: 1.493019e-02 0.168299
2019-11-13 18:23:29,997 valid 000 1.409192e-02 0.320351
2019-11-13 18:23:31,803 valid 050 1.289885e-02 0.285468
2019-11-13 18:23:33,444 validation loss; R2: 1.282760e-02 0.294048
2019-11-13 18:23:33,463 epoch 4 lr 1.000000e-03
2019-11-13 18:23:33,976 train 000 1.326263e-02 0.285606
2019-11-13 18:23:40,139 train 050 1.429627e-02 0.167792
2019-11-13 18:23:46,047 train 100 1.435302e-02 0.117388
2019-11-13 18:23:51,969 train 150 1.443131e-02 0.141962
2019-11-13 18:23:57,895 train 200 1.453703e-02 0.157286
2019-11-13 18:24:03,819 train 250 1.443632e-02 0.162914
2019-11-13 18:24:09,740 train 300 1.438878e-02 0.120281
2019-11-13 18:24:15,653 train 350 1.433901e-02 0.131727
2019-11-13 18:24:21,572 train 400 1.427828e-02 0.137455
2019-11-13 18:24:27,486 train 450 1.424543e-02 0.143928
2019-11-13 18:24:33,425 train 500 1.418757e-02 0.151033
2019-11-13 18:24:39,339 train 550 1.418217e-02 0.158372
2019-11-13 18:24:45,254 train 600 1.418693e-02 0.159289
2019-11-13 18:24:51,178 train 650 1.421691e-02 0.163161
2019-11-13 18:24:57,118 train 700 1.421305e-02 0.166530
2019-11-13 18:25:03,041 train 750 1.418333e-02 0.169955
2019-11-13 18:25:08,959 train 800 1.415666e-02 0.171726
2019-11-13 18:25:14,875 train 850 1.412413e-02 0.175331
2019-11-13 18:25:16,646 training loss; R2: 1.411242e-02 0.176939
2019-11-13 18:25:16,992 valid 000 1.110221e-02 0.383133
2019-11-13 18:25:18,774 valid 050 1.160071e-02 0.346185
2019-11-13 18:25:20,468 validation loss; R2: 1.166000e-02 0.338521
2019-11-13 18:25:20,498 epoch 5 lr 1.000000e-03
2019-11-13 18:25:20,932 train 000 1.360631e-02 -0.018775
2019-11-13 18:25:27,283 train 050 1.381550e-02 0.248030
2019-11-13 18:25:33,606 train 100 1.359631e-02 0.217484
2019-11-13 18:25:39,863 train 150 1.365368e-02 0.219607
2019-11-13 18:25:46,097 train 200 1.355058e-02 0.226198
2019-11-13 18:25:52,343 train 250 1.357132e-02 0.221523
2019-11-13 18:25:58,564 train 300 1.355675e-02 0.211276
2019-11-13 18:26:04,774 train 350 1.354026e-02 0.207785
2019-11-13 18:26:10,983 train 400 1.358281e-02 0.208708
2019-11-13 18:26:17,143 train 450 1.358215e-02 0.210779
2019-11-13 18:26:23,067 train 500 1.354855e-02 0.214955
2019-11-13 18:26:28,972 train 550 1.353590e-02 0.214656
2019-11-13 18:26:35,015 train 600 1.348385e-02 0.215839
2019-11-13 18:26:41,216 train 650 1.349250e-02 0.207501
2019-11-13 18:26:47,429 train 700 1.348642e-02 0.199972
2019-11-13 18:26:53,635 train 750 1.344643e-02 0.203453
2019-11-13 18:26:59,837 train 800 1.342865e-02 0.205306
2019-11-13 18:27:06,041 train 850 1.340235e-02 0.207177
2019-11-13 18:27:07,900 training loss; R2: 1.340514e-02 0.207057
2019-11-13 18:27:08,262 valid 000 1.129534e-02 0.289364
2019-11-13 18:27:10,046 valid 050 1.092461e-02 0.341792
2019-11-13 18:27:11,649 validation loss; R2: 1.109196e-02 0.340459
2019-11-13 18:27:11,669 epoch 6 lr 1.000000e-03
2019-11-13 18:27:12,124 train 000 1.324913e-02 0.253548
2019-11-13 18:27:18,315 train 050 1.307332e-02 0.195081
2019-11-13 18:27:24,354 train 100 1.300880e-02 0.229155
2019-11-13 18:27:30,316 train 150 1.306183e-02 0.241130
2019-11-13 18:27:36,253 train 200 1.309016e-02 0.227873
2019-11-13 18:27:42,194 train 250 1.301420e-02 0.229496
2019-11-13 18:27:48,136 train 300 1.295214e-02 0.207626
2019-11-13 18:27:54,071 train 350 1.293374e-02 0.210488
2019-11-13 18:28:00,015 train 400 1.293228e-02 0.214537
2019-11-13 18:28:05,952 train 450 1.295125e-02 0.218340
2019-11-13 18:28:11,898 train 500 1.296596e-02 0.220277
2019-11-13 18:28:17,849 train 550 1.296414e-02 0.222268
2019-11-13 18:28:23,963 train 600 1.298085e-02 0.224737
2019-11-13 18:28:30,066 train 650 1.297180e-02 0.224860
2019-11-13 18:28:36,205 train 700 1.297392e-02 0.223643
2019-11-13 18:28:42,482 train 750 1.295057e-02 0.213587
2019-11-13 18:28:48,669 train 800 1.292984e-02 0.217410
2019-11-13 18:28:54,657 train 850 1.290828e-02 0.216050
2019-11-13 18:28:56,437 training loss; R2: 1.290822e-02 0.215619
2019-11-13 18:28:56,794 valid 000 1.857707e-01 -11.637651
2019-11-13 18:28:58,545 valid 050 1.847948e-01 -18.612359
2019-11-13 18:29:00,160 validation loss; R2: 1.851434e-01 -19.512768
2019-11-13 18:29:00,173 epoch 7 lr 1.000000e-03
2019-11-13 18:29:00,640 train 000 1.215203e-02 0.170689
2019-11-13 18:29:06,926 train 050 1.297105e-02 0.233406
2019-11-13 18:29:13,140 train 100 1.258053e-02 0.238939
2019-11-13 18:29:19,388 train 150 1.259991e-02 0.215366
2019-11-13 18:29:25,631 train 200 1.257999e-02 0.223377
2019-11-13 18:29:31,884 train 250 1.262489e-02 0.232493
2019-11-13 18:29:37,929 train 300 1.262906e-02 0.241497
2019-11-13 18:29:43,873 train 350 1.266733e-02 0.245209
2019-11-13 18:29:49,807 train 400 1.268376e-02 0.246762
2019-11-13 18:29:55,746 train 450 1.265889e-02 0.248568
2019-11-13 18:30:01,833 train 500 1.263889e-02 0.247510
2019-11-13 18:30:08,071 train 550 1.262893e-02 0.232478
2019-11-13 18:30:13,987 train 600 1.264248e-02 0.232704
2019-11-13 18:30:19,902 train 650 1.263022e-02 0.232280
2019-11-13 18:30:25,819 train 700 1.262734e-02 0.233026
2019-11-13 18:30:31,743 train 750 1.261194e-02 0.233122
2019-11-13 18:30:37,656 train 800 1.261991e-02 0.233463
2019-11-13 18:30:43,673 train 850 1.260920e-02 0.234985
2019-11-13 18:30:45,440 training loss; R2: 1.262671e-02 0.235748
2019-11-13 18:30:45,809 valid 000 2.319304e+00 -182.619600
2019-11-13 18:30:47,622 valid 050 2.329811e+00 -233.347797
2019-11-13 18:30:49,232 validation loss; R2: 2.328479e+00 -218.420578
2019-11-13 18:30:49,246 epoch 8 lr 1.000000e-03
2019-11-13 18:30:49,646 train 000 1.520904e-02 0.225744
2019-11-13 18:30:55,938 train 050 1.259636e-02 0.277347
2019-11-13 18:31:01,912 train 100 1.246334e-02 0.256413
2019-11-13 18:31:07,862 train 150 1.243863e-02 0.262854
2019-11-13 18:31:13,798 train 200 1.243797e-02 0.254624
2019-11-13 18:31:19,743 train 250 1.241035e-02 0.252971
2019-11-13 18:31:25,688 train 300 1.247263e-02 0.253291
2019-11-13 18:31:31,632 train 350 1.241886e-02 0.244097
2019-11-13 18:31:37,593 train 400 1.240134e-02 0.245872
2019-11-13 18:31:43,640 train 450 1.237575e-02 0.245696
2019-11-13 18:31:49,884 train 500 1.239282e-02 0.240835
2019-11-13 18:31:56,118 train 550 1.236646e-02 0.241360
2019-11-13 18:32:02,069 train 600 1.233142e-02 0.242827
2019-11-13 18:32:08,011 train 650 1.232613e-02 0.242260
2019-11-13 18:32:13,966 train 700 1.232131e-02 0.236241
2019-11-13 18:32:19,913 train 750 1.232512e-02 0.238074
2019-11-13 18:32:25,848 train 800 1.231515e-02 0.236874
2019-11-13 18:32:31,765 train 850 1.231363e-02 0.240001
2019-11-13 18:32:33,537 training loss; R2: 1.231374e-02 0.238444
2019-11-13 18:32:33,906 valid 000 4.734653e+00 -702.609445
2019-11-13 18:32:35,680 valid 050 4.708447e+00 -483.691184
2019-11-13 18:32:37,293 validation loss; R2: 4.708784e+00 -510.675054
2019-11-13 18:32:37,306 epoch 9 lr 1.000000e-03
2019-11-13 18:32:37,719 train 000 9.972273e-03 0.381755
2019-11-13 18:32:43,730 train 050 1.208710e-02 0.269938
2019-11-13 18:32:49,681 train 100 1.213401e-02 0.266210
2019-11-13 18:32:55,617 train 150 1.214886e-02 0.272944
2019-11-13 18:33:01,559 train 200 1.211017e-02 0.276112
2019-11-13 18:33:07,745 train 250 1.209166e-02 0.273650
2019-11-13 18:33:13,962 train 300 1.207764e-02 0.268575
2019-11-13 18:33:20,172 train 350 1.204119e-02 0.271379
2019-11-13 18:33:26,108 train 400 1.203579e-02 0.258031
2019-11-13 18:33:32,027 train 450 1.206442e-02 0.260183
2019-11-13 18:33:37,931 train 500 1.209942e-02 0.260408
2019-11-13 18:33:43,838 train 550 1.211469e-02 0.262512
2019-11-13 18:33:49,752 train 600 1.209208e-02 0.261783
2019-11-13 18:33:55,682 train 650 1.209266e-02 0.258131
2019-11-13 18:34:01,590 train 700 1.209942e-02 0.258610
2019-11-13 18:34:07,499 train 750 1.208957e-02 0.258845
2019-11-13 18:34:13,414 train 800 1.207750e-02 0.260291
2019-11-13 18:34:19,331 train 850 1.209678e-02 0.261244
2019-11-13 18:34:21,102 training loss; R2: 1.208966e-02 0.261849
2019-11-13 18:34:21,472 valid 000 7.890271e+00 -519.673545
2019-11-13 18:34:23,296 valid 050 7.873697e+00 -804.900073
2019-11-13 18:34:24,875 validation loss; R2: 7.874160e+00 -782.913617
2019-11-13 18:34:24,895 epoch 10 lr 1.000000e-03
2019-11-13 18:34:25,323 train 000 1.261143e-02 0.304345
2019-11-13 18:34:31,456 train 050 1.197196e-02 0.271664
2019-11-13 18:34:37,504 train 100 1.197399e-02 0.289543
2019-11-13 18:34:43,623 train 150 1.192157e-02 0.243088
2019-11-13 18:34:49,752 train 200 1.185596e-02 0.258759
2019-11-13 18:34:55,757 train 250 1.187250e-02 0.265133
2019-11-13 18:35:01,767 train 300 1.186905e-02 0.273933
2019-11-13 18:35:07,872 train 350 1.187994e-02 0.265729
2019-11-13 18:35:13,954 train 400 1.190069e-02 0.256846
2019-11-13 18:35:19,947 train 450 1.188838e-02 0.264021
2019-11-13 18:35:26,068 train 500 1.185220e-02 0.259790
2019-11-13 18:35:32,144 train 550 1.188243e-02 0.263881
2019-11-13 18:35:38,138 train 600 1.188872e-02 0.267233
2019-11-13 18:35:44,136 train 650 1.187149e-02 0.270231
2019-11-13 18:35:50,117 train 700 1.189969e-02 0.268145
2019-11-13 18:35:56,105 train 750 1.189982e-02 0.268962
2019-11-13 18:36:02,105 train 800 1.189742e-02 0.267645
2019-11-13 18:36:08,206 train 850 1.189498e-02 0.264310
2019-11-13 18:36:09,995 training loss; R2: 1.187861e-02 0.265660
2019-11-13 18:36:10,385 valid 000 1.430818e+01 -869.495056
2019-11-13 18:36:12,140 valid 050 1.429084e+01 -1286.164147
2019-11-13 18:36:13,739 validation loss; R2: 1.429042e+01 -1248.893004
2019-11-13 18:36:13,758 epoch 11 lr 1.000000e-03
2019-11-13 18:36:14,214 train 000 1.027186e-02 0.375389
2019-11-13 18:36:20,526 train 050 1.193578e-02 0.293554
2019-11-13 18:36:26,759 train 100 1.180355e-02 0.275511
2019-11-13 18:36:33,117 train 150 1.181948e-02 0.225856
2019-11-13 18:36:39,420 train 200 1.177207e-02 -1.982799
2019-11-13 18:36:45,594 train 250 1.180408e-02 -1.535073
2019-11-13 18:36:51,587 train 300 1.184863e-02 -1.229610
2019-11-13 18:36:57,886 train 350 1.186043e-02 -1.013213
2019-11-13 18:37:04,169 train 400 1.183494e-02 -0.865595
2019-11-13 18:37:10,184 train 450 1.184181e-02 -0.744927
2019-11-13 18:37:16,157 train 500 1.183105e-02 -0.640022
2019-11-13 18:37:22,164 train 550 1.180282e-02 -0.554047
2019-11-13 18:37:28,144 train 600 1.180802e-02 -0.481677
2019-11-13 18:37:34,127 train 650 1.180901e-02 -0.421782
2019-11-13 18:37:40,100 train 700 1.179902e-02 -0.373748
2019-11-13 18:37:46,078 train 750 1.178625e-02 -0.329052
2019-11-13 18:37:52,053 train 800 1.177491e-02 -0.290748
2019-11-13 18:37:58,090 train 850 1.174663e-02 -0.258387
2019-11-13 18:37:59,887 training loss; R2: 1.174409e-02 -0.249427
2019-11-13 18:38:00,271 valid 000 2.300264e+01 -1011.790707
2019-11-13 18:38:02,060 valid 050 2.300209e+01 -1410.035315
2019-11-13 18:38:03,667 validation loss; R2: 2.301169e+01 -1500.706058
2019-11-13 18:38:03,686 epoch 12 lr 1.000000e-03
2019-11-13 18:38:04,151 train 000 1.084525e-02 0.261187
2019-11-13 18:38:10,511 train 050 1.148249e-02 0.326464
2019-11-13 18:38:16,740 train 100 1.161177e-02 0.319598
2019-11-13 18:38:22,774 train 150 1.168038e-02 0.302182
2019-11-13 18:38:28,803 train 200 1.170549e-02 0.291359
2019-11-13 18:38:34,960 train 250 1.172856e-02 0.295133
2019-11-13 18:38:41,193 train 300 1.172938e-02 0.278205
2019-11-13 18:38:47,430 train 350 1.170825e-02 0.282643
2019-11-13 18:38:53,830 train 400 1.169319e-02 0.275909
2019-11-13 18:39:00,042 train 450 1.167370e-02 0.278733
2019-11-13 18:39:06,120 train 500 1.165573e-02 0.276558
2019-11-13 18:39:12,288 train 550 1.166108e-02 0.277762
2019-11-13 18:39:18,359 train 600 1.161537e-02 0.281227
2019-11-13 18:39:24,485 train 650 1.162957e-02 0.280193
2019-11-13 18:39:30,613 train 700 1.163761e-02 0.263020
2019-11-13 18:39:36,726 train 750 1.164067e-02 0.261958
2019-11-13 18:39:42,965 train 800 1.164547e-02 0.263462
2019-11-13 18:39:49,194 train 850 1.164357e-02 0.263263
2019-11-13 18:39:51,052 training loss; R2: 1.163935e-02 0.264247
2019-11-13 18:39:51,463 valid 000 1.174669e+00 -139.401691
2019-11-13 18:39:53,144 valid 050 1.166645e+00 -100.611339
2019-11-13 18:39:54,678 validation loss; R2: 1.164836e+00 -99.921605
2019-11-13 18:39:54,691 epoch 13 lr 1.000000e-03
2019-11-13 18:39:55,254 train 000 1.269761e-02 0.273946
2019-11-13 18:40:01,474 train 050 1.143042e-02 0.235202
2019-11-13 18:40:07,455 train 100 1.132698e-02 0.264467
2019-11-13 18:40:13,443 train 150 1.153724e-02 0.270898
2019-11-13 18:40:19,649 train 200 1.150801e-02 0.277465
2019-11-13 18:40:25,642 train 250 1.146521e-02 0.281953
2019-11-13 18:40:31,619 train 300 1.150543e-02 0.285299
2019-11-13 18:40:37,566 train 350 1.150926e-02 0.272686
2019-11-13 18:40:43,521 train 400 1.155032e-02 0.276928
2019-11-13 18:40:49,471 train 450 1.154558e-02 0.279803
2019-11-13 18:40:55,410 train 500 1.151130e-02 0.282106
2019-11-13 18:41:01,355 train 550 1.152094e-02 0.279690
2019-11-13 18:41:07,301 train 600 1.152263e-02 0.282982
2019-11-13 18:41:13,231 train 650 1.151067e-02 0.282229
2019-11-13 18:41:19,161 train 700 1.151174e-02 0.280920
2019-11-13 18:41:25,091 train 750 1.150552e-02 0.282319
2019-11-13 18:41:31,049 train 800 1.149236e-02 0.283964
2019-11-13 18:41:37,021 train 850 1.148071e-02 0.284212
2019-11-13 18:41:38,822 training loss; R2: 1.147362e-02 0.285492
2019-11-13 18:41:39,227 valid 000 1.407819e+01 -776.454805
2019-11-13 18:41:40,922 valid 050 1.407819e+01 -3576.548705
2019-11-13 18:41:42,548 validation loss; R2: 1.407943e+01 -2712.198773
2019-11-13 18:41:42,562 epoch 14 lr 1.000000e-03
2019-11-13 18:41:43,028 train 000 1.281234e-02 0.344301
2019-11-13 18:41:49,207 train 050 1.121096e-02 0.306805
2019-11-13 18:41:55,506 train 100 1.130431e-02 0.298252
2019-11-13 18:42:01,789 train 150 1.121286e-02 0.302805
2019-11-13 18:42:07,981 train 200 1.124897e-02 0.295746
2019-11-13 18:42:13,930 train 250 1.129198e-02 0.300158
2019-11-13 18:42:19,895 train 300 1.128002e-02 0.304786
2019-11-13 18:42:25,845 train 350 1.130049e-02 0.307945
2019-11-13 18:42:31,962 train 400 1.133005e-02 0.309032
2019-11-13 18:42:37,896 train 450 1.131019e-02 0.308705
2019-11-13 18:42:43,839 train 500 1.128144e-02 0.308528
2019-11-13 18:42:49,776 train 550 1.131280e-02 0.308019
2019-11-13 18:42:55,730 train 600 1.130537e-02 0.308960
2019-11-13 18:43:01,652 train 650 1.132236e-02 0.307954
2019-11-13 18:43:07,577 train 700 1.134963e-02 0.305799
2019-11-13 18:43:13,506 train 750 1.135888e-02 0.306571
2019-11-13 18:43:19,438 train 800 1.137192e-02 0.305594
2019-11-13 18:43:25,451 train 850 1.138627e-02 -0.091605
2019-11-13 18:43:27,217 training loss; R2: 1.138666e-02 -0.084490
2019-11-13 18:43:27,640 valid 000 3.588833e+01 -3568.688155
2019-11-13 18:43:29,383 valid 050 3.581062e+01 -3388.400537
2019-11-13 18:43:30,964 validation loss; R2: 3.582686e+01 -3455.574814
2019-11-13 18:43:30,986 epoch 15 lr 1.000000e-03
2019-11-13 18:43:31,455 train 000 1.164978e-02 0.303657
2019-11-13 18:43:37,704 train 050 1.126167e-02 0.335112
2019-11-13 18:43:43,907 train 100 1.122461e-02 0.317004
2019-11-13 18:43:50,171 train 150 1.125501e-02 0.317376
2019-11-13 18:43:56,161 train 200 1.130011e-02 0.327845
2019-11-13 18:44:02,141 train 250 1.133827e-02 0.313981
2019-11-13 18:44:08,117 train 300 1.129327e-02 0.303414
2019-11-13 18:44:14,087 train 350 1.126518e-02 0.297134
2019-11-13 18:44:20,083 train 400 1.122202e-02 0.302236
2019-11-13 18:44:26,074 train 450 1.119423e-02 0.304028
2019-11-13 18:44:32,059 train 500 1.118288e-02 0.303577
2019-11-13 18:44:38,056 train 550 1.116745e-02 0.296556
2019-11-13 18:44:44,029 train 600 1.117826e-02 0.296408
2019-11-13 18:44:50,000 train 650 1.118215e-02 0.299987
2019-11-13 18:44:55,957 train 700 1.119876e-02 0.290783
2019-11-13 18:45:01,924 train 750 1.122779e-02 0.284168
2019-11-13 18:45:07,914 train 800 1.123907e-02 0.287689
2019-11-13 18:45:13,894 train 850 1.124437e-02 0.286143
2019-11-13 18:45:15,688 training loss; R2: 1.124070e-02 0.276310
2019-11-13 18:45:16,089 valid 000 1.859374e+01 -3248.922501
2019-11-13 18:45:17,834 valid 050 1.855648e+01 -2527.940936
2019-11-13 18:45:19,419 validation loss; R2: 1.856204e+01 -2221.123274
2019-11-13 18:45:19,433 epoch 16 lr 1.000000e-03
2019-11-13 18:45:19,896 train 000 1.170324e-02 0.361291
2019-11-13 18:45:26,197 train 050 1.120462e-02 0.317671
2019-11-13 18:45:32,231 train 100 1.102757e-02 0.255450
2019-11-13 18:45:38,279 train 150 1.088822e-02 0.270457
2019-11-13 18:45:44,596 train 200 1.084724e-02 0.281725
2019-11-13 18:45:50,960 train 250 1.088670e-02 0.287886
2019-11-13 18:45:57,266 train 300 1.097641e-02 0.293699
2019-11-13 18:46:03,454 train 350 1.097922e-02 0.303082
2019-11-13 18:46:09,472 train 400 1.102894e-02 0.306527
2019-11-13 18:46:15,461 train 450 1.105541e-02 0.306092
2019-11-13 18:46:21,449 train 500 1.105555e-02 0.305313
2019-11-13 18:46:27,427 train 550 1.104678e-02 0.308532
2019-11-13 18:46:33,430 train 600 1.106181e-02 0.307156
2019-11-13 18:46:39,423 train 650 1.108499e-02 0.305072
2019-11-13 18:46:45,423 train 700 1.107950e-02 0.306924
2019-11-13 18:46:51,421 train 750 1.107983e-02 0.310214
2019-11-13 18:46:57,418 train 800 1.107391e-02 0.312675
2019-11-13 18:47:03,428 train 850 1.104565e-02 0.314622
2019-11-13 18:47:05,221 training loss; R2: 1.103454e-02 0.315176
2019-11-13 18:47:05,607 valid 000 3.522310e+00 -376.165457
2019-11-13 18:47:07,365 valid 050 3.539567e+00 -391.305139
2019-11-13 18:47:08,934 validation loss; R2: 3.544907e+00 -396.404834
2019-11-13 18:47:08,954 epoch 17 lr 1.000000e-03
2019-11-13 18:47:09,424 train 000 9.683245e-03 0.250431
2019-11-13 18:47:15,754 train 050 1.068428e-02 0.346178
2019-11-13 18:47:22,073 train 100 1.069483e-02 0.331725
2019-11-13 18:47:28,433 train 150 1.083211e-02 0.328476
2019-11-13 18:47:34,452 train 200 1.094826e-02 0.279658
2019-11-13 18:47:40,453 train 250 1.092743e-02 0.290901
2019-11-13 18:47:46,423 train 300 1.095310e-02 0.298158
2019-11-13 18:47:52,489 train 350 1.096439e-02 0.303328
2019-11-13 18:47:58,471 train 400 1.098323e-02 0.292881
2019-11-13 18:48:04,457 train 450 1.096458e-02 0.293008
2019-11-13 18:48:10,428 train 500 1.095200e-02 0.298347
2019-11-13 18:48:16,407 train 550 1.094850e-02 0.300160
2019-11-13 18:48:22,380 train 600 1.095959e-02 0.299526
2019-11-13 18:48:28,354 train 650 1.092659e-02 0.299339
2019-11-13 18:48:34,344 train 700 1.092860e-02 0.302145
2019-11-13 18:48:40,319 train 750 1.092222e-02 0.302294
2019-11-13 18:48:46,287 train 800 1.094370e-02 0.303663
2019-11-13 18:48:52,259 train 850 1.093961e-02 0.304663
2019-11-13 18:48:54,043 training loss; R2: 1.093930e-02 0.305361
2019-11-13 18:48:54,418 valid 000 3.156583e+00 -223.142750
2019-11-13 18:48:56,257 valid 050 3.171878e+00 -2261.873229
2019-11-13 18:48:57,834 validation loss; R2: 3.173986e+00 -1654.742842
2019-11-13 18:48:57,853 epoch 18 lr 1.000000e-03
2019-11-13 18:48:58,387 train 000 9.603473e-03 0.464573
2019-11-13 18:49:04,402 train 050 1.056143e-02 0.346684
2019-11-13 18:49:10,640 train 100 1.088981e-02 0.346559
2019-11-13 18:49:16,930 train 150 1.089695e-02 0.348526
2019-11-13 18:49:23,295 train 200 1.081407e-02 0.331891
2019-11-13 18:49:29,355 train 250 1.080638e-02 0.321257
2019-11-13 18:49:35,585 train 300 1.086455e-02 0.325481
2019-11-13 18:49:41,896 train 350 1.081272e-02 0.326175
2019-11-13 18:49:48,070 train 400 1.081482e-02 0.332293
2019-11-13 18:49:54,063 train 450 1.082142e-02 0.335846
2019-11-13 18:50:00,059 train 500 1.077833e-02 0.336970
2019-11-13 18:50:06,055 train 550 1.080932e-02 0.337615
2019-11-13 18:50:12,035 train 600 1.084036e-02 0.323880
2019-11-13 18:50:18,020 train 650 1.085382e-02 0.317455
2019-11-13 18:50:24,111 train 700 1.084636e-02 0.315994
2019-11-13 18:50:30,132 train 750 1.086557e-02 0.317051
2019-11-13 18:50:36,115 train 800 1.090243e-02 0.317857
2019-11-13 18:50:42,171 train 850 1.092916e-02 0.317081
2019-11-13 18:50:44,068 training loss; R2: 1.093157e-02 0.315981
2019-11-13 18:50:44,468 valid 000 8.102346e+00 -583.440907
2019-11-13 18:50:46,253 valid 050 7.975344e+00 -1025.622349
2019-11-13 18:50:47,829 validation loss; R2: 7.964105e+00 -932.639392
2019-11-13 18:50:47,843 epoch 19 lr 1.000000e-03
2019-11-13 18:50:48,281 train 000 1.091139e-02 0.364846
2019-11-13 18:50:54,517 train 050 1.173396e-02 0.302956
2019-11-13 18:51:00,989 train 100 1.160619e-02 0.290702
2019-11-13 18:51:07,358 train 150 1.151987e-02 0.290309
2019-11-13 18:51:13,672 train 200 1.143159e-02 0.293619
2019-11-13 18:51:20,051 train 250 1.130941e-02 0.305795
2019-11-13 18:51:26,340 train 300 1.127103e-02 0.307311
2019-11-13 18:51:32,456 train 350 1.125180e-02 0.302426
2019-11-13 18:51:38,723 train 400 1.123792e-02 0.303344
2019-11-13 18:51:45,004 train 450 1.126006e-02 0.302133
2019-11-13 18:51:51,243 train 500 1.131172e-02 0.299702
2019-11-13 18:51:57,400 train 550 1.130054e-02 0.267619
2019-11-13 18:52:03,638 train 600 1.130299e-02 0.270011
2019-11-13 18:52:09,898 train 650 1.133709e-02 0.269847
2019-11-13 18:52:16,291 train 700 1.130700e-02 0.270413
2019-11-13 18:52:22,703 train 750 1.131613e-02 0.269223
2019-11-13 18:52:29,170 train 800 1.132563e-02 0.271388
2019-11-13 18:52:35,498 train 850 1.130350e-02 0.276200
2019-11-13 18:52:37,355 training loss; R2: 1.128930e-02 0.277385
2019-11-13 18:52:37,782 valid 000 4.613160e-02 -3.315814
2019-11-13 18:52:39,475 valid 050 4.705792e-02 -2.736619
2019-11-13 18:52:41,078 validation loss; R2: 4.752301e-02 -2.645222
