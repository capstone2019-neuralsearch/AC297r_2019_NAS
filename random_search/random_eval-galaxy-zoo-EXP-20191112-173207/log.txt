2019-11-12 17:32:07,397 gpu device = 1
2019-11-12 17:32:07,397 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name='galaxy-zoo-cleaned', gpu=1, grad_clip=5, gz_dtree=False, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-galaxy-zoo-EXP-20191112-173207', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-11-12 17:32:18,966 param size = 0.269109MB
2019-11-12 17:32:18,970 epoch 0 lr 1.000000e-03
2019-11-12 17:32:21,138 train 000 2.454999e-01 -117.301523
2019-11-12 17:32:28,109 train 050 5.353023e-02 -6.651390
2019-11-12 17:32:35,116 train 100 4.144520e-02 -3.640950
2019-11-12 17:32:42,115 train 150 3.673765e-02 -2.552935
2019-11-12 17:32:49,085 train 200 3.416579e-02 -1.984638
2019-11-12 17:32:56,142 train 250 3.236355e-02 -1.618507
2019-11-12 17:33:03,294 train 300 3.113603e-02 -1.379389
2019-11-12 17:33:10,316 train 350 3.008388e-02 -1.201383
2019-11-12 17:33:17,401 train 400 2.930826e-02 -1.069357
2019-11-12 17:33:24,501 train 450 2.861170e-02 -0.961089
2019-11-12 17:33:31,636 train 500 2.803876e-02 -0.868981
2019-11-12 17:33:38,550 train 550 2.748106e-02 -0.794104
2019-11-12 17:33:45,583 train 600 2.700431e-02 -0.733339
2019-11-12 17:33:52,594 train 650 2.659704e-02 -0.679764
2019-11-12 17:33:59,640 train 700 2.622945e-02 -0.631748
2019-11-12 17:34:06,760 train 750 2.584305e-02 -0.589112
2019-11-12 17:34:13,889 train 800 2.554466e-02 -0.549182
2019-11-12 17:34:20,866 train 850 2.523635e-02 -0.514133
2019-11-12 17:34:23,715 training loss; R2: 2.514671e-02 -0.504618
2019-11-12 17:34:23,990 valid 000 2.168859e-02 0.134201
2019-11-12 17:34:25,662 valid 050 1.876187e-02 0.060564
2019-11-12 17:34:27,263 validation loss; R2: 1.876885e-02 0.033567
2019-11-12 17:34:27,280 epoch 1 lr 1.000000e-03
2019-11-12 17:34:27,783 train 000 2.246220e-02 -0.000303
2019-11-12 17:34:34,649 train 050 1.994827e-02 0.055620
2019-11-12 17:34:41,431 train 100 1.973741e-02 -0.035848
2019-11-12 17:34:48,205 train 150 1.978294e-02 -0.008600
2019-11-12 17:34:54,966 train 200 1.981309e-02 0.004311
2019-11-12 17:35:01,734 train 250 1.966327e-02 -0.258517
2019-11-12 17:35:08,489 train 300 1.952062e-02 -0.201737
2019-11-12 17:35:15,246 train 350 1.939784e-02 -0.159722
2019-11-12 17:35:22,003 train 400 1.924269e-02 -0.127111
2019-11-12 17:35:28,772 train 450 1.917770e-02 -0.099804
2019-11-12 17:35:35,540 train 500 1.910259e-02 -0.078195
2019-11-12 17:35:42,309 train 550 1.902061e-02 -0.059265
2019-11-12 17:35:49,079 train 600 1.895126e-02 -0.046018
2019-11-12 17:35:55,847 train 650 1.887529e-02 -0.032424
2019-11-12 17:36:02,622 train 700 1.877933e-02 -0.022794
2019-11-12 17:36:09,400 train 750 1.872538e-02 -0.014722
2019-11-12 17:36:16,176 train 800 1.867911e-02 -0.006321
2019-11-12 17:36:22,958 train 850 1.859802e-02 0.001446
2019-11-12 17:36:24,988 training loss; R2: 1.857652e-02 0.004126
2019-11-12 17:36:25,280 valid 000 1.985950e-02 0.101565
2019-11-12 17:36:26,926 valid 050 1.823032e-02 0.121708
2019-11-12 17:36:28,447 validation loss; R2: 1.821704e-02 0.124725
2019-11-12 17:36:28,462 epoch 2 lr 1.000000e-03
2019-11-12 17:36:28,843 train 000 1.425110e-02 0.215760
2019-11-12 17:36:35,626 train 050 1.714843e-02 0.104678
2019-11-12 17:36:42,416 train 100 1.714952e-02 0.125666
2019-11-12 17:36:49,378 train 150 1.708233e-02 0.110559
2019-11-12 17:36:56,169 train 200 1.698443e-02 0.119990
2019-11-12 17:37:02,962 train 250 1.703019e-02 0.125871
2019-11-12 17:37:09,750 train 300 1.702896e-02 0.132713
2019-11-12 17:37:16,542 train 350 1.700976e-02 0.136585
2019-11-12 17:37:23,329 train 400 1.692389e-02 0.141699
2019-11-12 17:37:30,114 train 450 1.686884e-02 0.139939
2019-11-12 17:37:36,902 train 500 1.679877e-02 0.136360
2019-11-12 17:37:43,690 train 550 1.676938e-02 0.138320
2019-11-12 17:37:50,471 train 600 1.672874e-02 0.142041
2019-11-12 17:37:57,300 train 650 1.671102e-02 0.142017
2019-11-12 17:38:04,096 train 700 1.668500e-02 0.144305
2019-11-12 17:38:10,879 train 750 1.662234e-02 0.141930
2019-11-12 17:38:17,662 train 800 1.660885e-02 0.141765
2019-11-12 17:38:24,443 train 850 1.656019e-02 0.144453
2019-11-12 17:38:26,471 training loss; R2: 1.654264e-02 0.144687
2019-11-12 17:38:26,775 valid 000 1.706363e-02 0.125279
2019-11-12 17:38:28,511 valid 050 1.620729e-02 0.142464
2019-11-12 17:38:30,052 validation loss; R2: 1.625991e-02 0.140588
2019-11-12 17:38:30,072 epoch 3 lr 1.000000e-03
2019-11-12 17:38:30,465 train 000 1.502924e-02 0.242424
2019-11-12 17:38:37,307 train 050 1.620624e-02 0.190922
2019-11-12 17:38:44,211 train 100 1.608642e-02 0.134389
2019-11-12 17:38:51,095 train 150 1.582396e-02 0.151063
2019-11-12 17:38:58,073 train 200 1.572719e-02 0.151501
2019-11-12 17:39:04,918 train 250 1.574991e-02 0.161865
2019-11-12 17:39:11,822 train 300 1.569358e-02 0.166342
2019-11-12 17:39:18,621 train 350 1.569048e-02 0.170846
2019-11-12 17:39:25,409 train 400 1.566415e-02 0.166871
2019-11-12 17:39:32,204 train 450 1.569444e-02 0.170398
2019-11-12 17:39:38,991 train 500 1.565778e-02 0.167941
2019-11-12 17:39:45,769 train 550 1.559564e-02 0.168591
2019-11-12 17:39:52,555 train 600 1.558060e-02 0.172267
2019-11-12 17:39:59,331 train 650 1.557662e-02 0.173948
2019-11-12 17:40:06,109 train 700 1.555494e-02 0.166827
2019-11-12 17:40:12,897 train 750 1.552129e-02 0.083824
2019-11-12 17:40:19,680 train 800 1.550090e-02 0.087977
2019-11-12 17:40:26,451 train 850 1.548647e-02 0.092466
2019-11-12 17:40:28,476 training loss; R2: 1.548647e-02 0.094439
2019-11-12 17:40:28,764 valid 000 1.315347e-02 0.235318
2019-11-12 17:40:30,509 valid 050 1.395084e-02 0.231169
2019-11-12 17:40:32,040 validation loss; R2: 1.387256e-02 0.239830
2019-11-12 17:40:32,060 epoch 4 lr 1.000000e-03
2019-11-12 17:40:32,469 train 000 1.454801e-02 0.248054
2019-11-12 17:40:39,350 train 050 1.528290e-02 0.214241
2019-11-12 17:40:46,155 train 100 1.507836e-02 0.198353
2019-11-12 17:40:53,102 train 150 1.504989e-02 0.197116
2019-11-12 17:41:00,045 train 200 1.511362e-02 0.200666
2019-11-12 17:41:06,850 train 250 1.514466e-02 0.202462
2019-11-12 17:41:13,651 train 300 1.510366e-02 0.203143
2019-11-12 17:41:20,452 train 350 1.512453e-02 0.202154
2019-11-12 17:41:27,258 train 400 1.509282e-02 0.204656
2019-11-12 17:41:34,085 train 450 1.506300e-02 0.202402
2019-11-12 17:41:40,894 train 500 1.502624e-02 0.204260
2019-11-12 17:41:47,701 train 550 1.500790e-02 0.203701
2019-11-12 17:41:54,508 train 600 1.498268e-02 0.205646
2019-11-12 17:42:01,320 train 650 1.496134e-02 0.202859
2019-11-12 17:42:08,136 train 700 1.490642e-02 0.203920
2019-11-12 17:42:14,943 train 750 1.489692e-02 0.202999
2019-11-12 17:42:21,756 train 800 1.487511e-02 0.203138
2019-11-12 17:42:28,566 train 850 1.485053e-02 0.204320
2019-11-12 17:42:30,602 training loss; R2: 1.484005e-02 0.204146
2019-11-12 17:42:30,880 valid 000 1.625072e-02 0.072982
2019-11-12 17:42:32,536 valid 050 1.590272e-02 0.132311
2019-11-12 17:42:34,051 validation loss; R2: 1.563620e-02 0.141832
2019-11-12 17:42:34,072 epoch 5 lr 1.000000e-03
2019-11-12 17:42:34,424 train 000 1.401733e-02 0.260361
2019-11-12 17:42:41,267 train 050 1.418712e-02 0.199517
2019-11-12 17:42:48,053 train 100 1.445765e-02 0.207163
2019-11-12 17:42:54,835 train 150 1.451266e-02 0.219646
2019-11-12 17:43:01,611 train 200 1.442939e-02 0.205525
2019-11-12 17:43:08,387 train 250 1.439853e-02 0.209345
2019-11-12 17:43:15,155 train 300 1.444607e-02 0.211718
2019-11-12 17:43:21,928 train 350 1.449573e-02 0.210205
2019-11-12 17:43:28,698 train 400 1.448455e-02 0.212644
2019-11-12 17:43:35,466 train 450 1.448755e-02 0.214015
2019-11-12 17:43:42,233 train 500 1.444835e-02 0.216570
2019-11-12 17:43:48,995 train 550 1.444410e-02 0.208256
2019-11-12 17:43:55,757 train 600 1.442164e-02 0.211208
2019-11-12 17:44:02,523 train 650 1.441388e-02 0.212982
2019-11-12 17:44:09,286 train 700 1.438807e-02 0.212481
2019-11-12 17:44:16,046 train 750 1.437133e-02 0.213980
2019-11-12 17:44:22,807 train 800 1.437519e-02 0.210938
2019-11-12 17:44:29,567 train 850 1.435454e-02 0.211762
2019-11-12 17:44:31,592 training loss; R2: 1.435562e-02 0.211966
2019-11-12 17:44:31,894 valid 000 1.338280e-02 0.266550
2019-11-12 17:44:33,556 valid 050 1.388121e-02 0.240567
2019-11-12 17:44:35,064 validation loss; R2: 1.396118e-02 0.253764
2019-11-12 17:44:35,080 epoch 6 lr 1.000000e-03
2019-11-12 17:44:35,465 train 000 1.248094e-02 0.248170
2019-11-12 17:44:42,229 train 050 1.411209e-02 0.212167
2019-11-12 17:44:48,985 train 100 1.411011e-02 0.220377
2019-11-12 17:44:55,743 train 150 1.417072e-02 0.207959
2019-11-12 17:45:02,503 train 200 1.407532e-02 0.217769
2019-11-12 17:45:09,261 train 250 1.408579e-02 0.217056
2019-11-12 17:45:16,025 train 300 1.401175e-02 0.222382
2019-11-12 17:45:22,789 train 350 1.401562e-02 0.225979
2019-11-12 17:45:29,546 train 400 1.401507e-02 0.220090
2019-11-12 17:45:36,301 train 450 1.399877e-02 0.221875
2019-11-12 17:45:43,067 train 500 1.399388e-02 0.218883
2019-11-12 17:45:49,827 train 550 1.401637e-02 0.218186
2019-11-12 17:45:56,583 train 600 1.400881e-02 0.216993
2019-11-12 17:46:03,345 train 650 1.399385e-02 0.219910
2019-11-12 17:46:10,109 train 700 1.395728e-02 0.214027
2019-11-12 17:46:16,869 train 750 1.395165e-02 0.216569
2019-11-12 17:46:23,624 train 800 1.393541e-02 0.218346
2019-11-12 17:46:30,382 train 850 1.391986e-02 0.219127
2019-11-12 17:46:32,403 training loss; R2: 1.391914e-02 0.219611
2019-11-12 17:46:32,693 valid 000 4.860002e-02 -1.092152
2019-11-12 17:46:34,389 valid 050 5.209412e-02 -1.226416
2019-11-12 17:46:35,918 validation loss; R2: 5.248927e-02 -1.320754
2019-11-12 17:46:35,939 epoch 7 lr 1.000000e-03
2019-11-12 17:46:36,331 train 000 1.324935e-02 -0.057711
2019-11-12 17:46:43,450 train 050 1.363703e-02 0.090643
2019-11-12 17:46:50,407 train 100 1.369717e-02 0.124551
2019-11-12 17:46:57,437 train 150 1.366716e-02 0.160081
2019-11-12 17:47:04,321 train 200 1.374287e-02 0.178069
2019-11-12 17:47:11,153 train 250 1.372352e-02 0.183981
2019-11-12 17:47:18,119 train 300 1.365902e-02 0.189425
2019-11-12 17:47:24,938 train 350 1.364798e-02 0.200520
2019-11-12 17:47:31,742 train 400 1.363081e-02 0.204408
2019-11-12 17:47:38,556 train 450 1.359902e-02 0.207283
2019-11-12 17:47:45,355 train 500 1.359631e-02 0.213927
2019-11-12 17:47:52,160 train 550 1.359988e-02 0.216887
2019-11-12 17:47:58,961 train 600 1.359480e-02 0.220500
2019-11-12 17:48:05,757 train 650 1.360205e-02 0.221363
2019-11-12 17:48:12,561 train 700 1.358046e-02 0.222913
2019-11-12 17:48:19,366 train 750 1.355426e-02 0.225101
2019-11-12 17:48:26,163 train 800 1.354699e-02 0.223570
2019-11-12 17:48:32,961 train 850 1.354070e-02 0.222114
2019-11-12 17:48:34,993 training loss; R2: 1.353244e-02 0.219667
2019-11-12 17:48:35,284 valid 000 5.699953e-02 -2.287104
2019-11-12 17:48:37,005 valid 050 5.504268e-02 -1.899193
2019-11-12 17:48:38,554 validation loss; R2: 5.496726e-02 -1.878885
2019-11-12 17:48:38,575 epoch 8 lr 1.000000e-03
2019-11-12 17:48:38,975 train 000 1.238404e-02 0.320438
2019-11-12 17:48:46,015 train 050 1.368933e-02 0.262496
2019-11-12 17:48:53,033 train 100 1.328059e-02 0.256684
2019-11-12 17:49:00,060 train 150 1.325550e-02 0.253930
2019-11-12 17:49:07,081 train 200 1.322248e-02 0.243960
2019-11-12 17:49:14,088 train 250 1.325932e-02 0.240546
2019-11-12 17:49:21,088 train 300 1.323076e-02 0.233142
2019-11-12 17:49:28,091 train 350 1.324748e-02 0.230102
2019-11-12 17:49:35,096 train 400 1.326300e-02 0.229815
2019-11-12 17:49:42,099 train 450 1.329534e-02 0.234277
2019-11-12 17:49:49,103 train 500 1.326354e-02 0.235797
2019-11-12 17:49:56,106 train 550 1.321638e-02 0.238300
2019-11-12 17:50:02,988 train 600 1.320892e-02 0.236564
2019-11-12 17:50:09,719 train 650 1.319772e-02 0.239399
2019-11-12 17:50:16,452 train 700 1.318983e-02 0.234787
2019-11-12 17:50:23,181 train 750 1.318509e-02 0.232944
2019-11-12 17:50:29,914 train 800 1.317454e-02 0.235541
2019-11-12 17:50:36,648 train 850 1.316489e-02 0.232127
2019-11-12 17:50:38,661 training loss; R2: 1.316430e-02 0.232930
2019-11-12 17:50:38,942 valid 000 2.915818e-02 -0.275308
2019-11-12 17:50:40,600 valid 050 2.834216e-02 -0.295408
2019-11-12 17:50:42,098 validation loss; R2: 2.811743e-02 -0.275348
2019-11-12 17:50:42,113 epoch 9 lr 1.000000e-03
2019-11-12 17:50:42,463 train 000 1.068425e-02 0.260733
2019-11-12 17:50:49,477 train 050 1.334327e-02 0.262910
2019-11-12 17:50:56,491 train 100 1.327264e-02 0.263701
2019-11-12 17:51:03,532 train 150 1.312666e-02 0.266925
2019-11-12 17:51:10,453 train 200 1.312828e-02 0.266466
2019-11-12 17:51:17,426 train 250 1.318048e-02 0.260386
2019-11-12 17:51:24,442 train 300 1.306361e-02 0.231690
2019-11-12 17:51:31,548 train 350 1.306922e-02 0.230852
2019-11-12 17:51:38,506 train 400 1.308465e-02 0.234199
2019-11-12 17:51:45,538 train 450 1.306636e-02 0.238339
2019-11-12 17:51:52,610 train 500 1.307102e-02 0.220482
2019-11-12 17:51:59,555 train 550 1.308082e-02 0.220375
2019-11-12 17:52:06,577 train 600 1.303372e-02 0.225741
2019-11-12 17:52:13,616 train 650 1.304991e-02 0.228624
2019-11-12 17:52:20,698 train 700 1.303801e-02 0.232375
2019-11-12 17:52:27,695 train 750 1.303062e-02 0.233542
2019-11-12 17:52:34,678 train 800 1.301076e-02 -0.291983
2019-11-12 17:52:41,712 train 850 1.300594e-02 -0.260104
2019-11-12 17:52:43,857 training loss; R2: 1.300694e-02 -0.251341
2019-11-12 17:52:44,126 valid 000 3.876923e-02 -0.761857
2019-11-12 17:52:45,799 valid 050 3.390121e-02 -0.604467
2019-11-12 17:52:47,327 validation loss; R2: 3.441672e-02 -0.595787
2019-11-12 17:52:47,347 epoch 10 lr 1.000000e-03
2019-11-12 17:52:47,776 train 000 1.101762e-02 0.329774
2019-11-12 17:52:54,760 train 050 1.289906e-02 0.295261
2019-11-12 17:53:01,814 train 100 1.288804e-02 0.278535
2019-11-12 17:53:08,581 train 150 1.289010e-02 0.282031
2019-11-12 17:53:15,343 train 200 1.281799e-02 0.282180
2019-11-12 17:53:22,112 train 250 1.279838e-02 0.279422
2019-11-12 17:53:28,865 train 300 1.281167e-02 0.279992
2019-11-12 17:53:35,616 train 350 1.280622e-02 0.278805
2019-11-12 17:53:42,379 train 400 1.280918e-02 0.274010
2019-11-12 17:53:49,136 train 450 1.278145e-02 0.273494
2019-11-12 17:53:55,892 train 500 1.278101e-02 0.274090
2019-11-12 17:54:02,641 train 550 1.274267e-02 0.272720
2019-11-12 17:54:09,392 train 600 1.276060e-02 0.270046
2019-11-12 17:54:16,148 train 650 1.275817e-02 0.270039
2019-11-12 17:54:22,896 train 700 1.276818e-02 0.269334
2019-11-12 17:54:29,642 train 750 1.275752e-02 0.269317
2019-11-12 17:54:36,389 train 800 1.275834e-02 0.270123
2019-11-12 17:54:43,137 train 850 1.274224e-02 0.270911
2019-11-12 17:54:45,157 training loss; R2: 1.274291e-02 0.271556
2019-11-12 17:54:45,468 valid 000 9.635732e-02 -6.501255
2019-11-12 17:54:47,153 valid 050 9.246400e-02 -3.986346
2019-11-12 17:54:48,676 validation loss; R2: 9.241932e-02 -3.972349
2019-11-12 17:54:48,692 epoch 11 lr 1.000000e-03
2019-11-12 17:54:49,096 train 000 1.362152e-02 0.336094
2019-11-12 17:54:56,070 train 050 1.262920e-02 0.268847
2019-11-12 17:55:02,941 train 100 1.239919e-02 0.250435
2019-11-12 17:55:09,793 train 150 1.251837e-02 0.263860
2019-11-12 17:55:16,785 train 200 1.253366e-02 0.253386
2019-11-12 17:55:23,831 train 250 1.262031e-02 0.262159
2019-11-12 17:55:30,864 train 300 1.257604e-02 0.268628
2019-11-12 17:55:37,890 train 350 1.263781e-02 0.264908
2019-11-12 17:55:44,919 train 400 1.265287e-02 0.266122
2019-11-12 17:55:51,948 train 450 1.265108e-02 0.264725
2019-11-12 17:55:58,984 train 500 1.263744e-02 0.266591
2019-11-12 17:56:06,004 train 550 1.262048e-02 0.270068
2019-11-12 17:56:13,024 train 600 1.258565e-02 0.271677
2019-11-12 17:56:20,047 train 650 1.259204e-02 0.269464
2019-11-12 17:56:27,070 train 700 1.256971e-02 -5.386475
2019-11-12 17:56:34,087 train 750 1.256972e-02 -5.009582
2019-11-12 17:56:41,101 train 800 1.253709e-02 -4.678663
2019-11-12 17:56:48,117 train 850 1.251935e-02 -4.391224
2019-11-12 17:56:50,215 training loss; R2: 1.252863e-02 -4.310139
2019-11-12 17:56:50,489 valid 000 9.927156e-02 -2.376420
2019-11-12 17:56:52,134 valid 050 1.004410e-01 -2.308526
2019-11-12 17:56:53,635 validation loss; R2: 1.009733e-01 -2.437122
2019-11-12 17:56:53,657 epoch 12 lr 1.000000e-03
2019-11-12 17:56:54,072 train 000 1.059200e-02 0.285907
2019-11-12 17:57:01,080 train 050 1.228675e-02 0.308559
2019-11-12 17:57:08,094 train 100 1.235651e-02 0.256960
2019-11-12 17:57:15,146 train 150 1.239039e-02 0.264640
2019-11-12 17:57:22,153 train 200 1.235374e-02 0.211129
2019-11-12 17:57:29,105 train 250 1.239401e-02 0.231066
2019-11-12 17:57:36,168 train 300 1.237766e-02 0.239213
2019-11-12 17:57:43,169 train 350 1.240358e-02 0.245445
2019-11-12 17:57:50,235 train 400 1.242851e-02 0.251333
2019-11-12 17:57:57,239 train 450 1.246469e-02 0.253795
2019-11-12 17:58:04,245 train 500 1.245990e-02 0.254992
2019-11-12 17:58:11,275 train 550 1.247046e-02 0.255797
2019-11-12 17:58:18,316 train 600 1.245815e-02 0.255078
2019-11-12 17:58:25,243 train 650 1.244335e-02 0.257442
2019-11-12 17:58:32,119 train 700 1.244356e-02 0.259140
2019-11-12 17:58:39,061 train 750 1.243160e-02 0.253927
2019-11-12 17:58:46,109 train 800 1.241089e-02 0.255548
2019-11-12 17:58:53,123 train 850 1.242452e-02 0.254076
2019-11-12 17:58:55,175 training loss; R2: 1.243279e-02 0.255284
2019-11-12 17:58:55,446 valid 000 6.013536e-01 -71.688756
2019-11-12 17:58:57,125 valid 050 5.946000e-01 -68.570583
2019-11-12 17:58:58,602 validation loss; R2: 5.944596e-01 -66.231379
2019-11-12 17:58:58,617 epoch 13 lr 1.000000e-03
2019-11-12 17:58:59,000 train 000 1.483331e-02 0.140532
2019-11-12 17:59:05,976 train 050 1.236368e-02 0.172219
2019-11-12 17:59:12,759 train 100 1.242249e-02 0.223558
2019-11-12 17:59:19,514 train 150 1.234308e-02 0.237096
2019-11-12 17:59:26,266 train 200 1.237308e-02 0.248730
2019-11-12 17:59:33,033 train 250 1.238181e-02 0.254873
2019-11-12 17:59:39,780 train 300 1.242635e-02 0.243594
2019-11-12 17:59:46,535 train 350 1.240405e-02 0.225668
2019-11-12 17:59:53,297 train 400 1.236951e-02 0.233361
2019-11-12 18:00:00,060 train 450 1.237932e-02 0.241245
2019-11-12 18:00:06,820 train 500 1.236110e-02 0.245188
2019-11-12 18:00:13,580 train 550 1.233212e-02 0.241551
2019-11-12 18:00:20,343 train 600 1.233181e-02 0.236465
2019-11-12 18:00:27,105 train 650 1.235779e-02 0.238982
2019-11-12 18:00:33,868 train 700 1.238515e-02 0.241656
2019-11-12 18:00:40,632 train 750 1.237863e-02 0.243696
2019-11-12 18:00:47,398 train 800 1.237398e-02 0.245856
2019-11-12 18:00:54,162 train 850 1.235559e-02 0.248445
2019-11-12 18:00:56,183 training loss; R2: 1.234970e-02 0.248649
2019-11-12 18:00:56,489 valid 000 6.528973e-01 -52.144987
2019-11-12 18:00:58,136 valid 050 6.412366e-01 -62.315404
2019-11-12 18:00:59,650 validation loss; R2: 6.427665e-01 -65.649063
2019-11-12 18:00:59,665 epoch 14 lr 1.000000e-03
2019-11-12 18:01:00,043 train 000 1.110830e-02 0.232459
2019-11-12 18:01:06,803 train 050 1.225090e-02 0.269186
2019-11-12 18:01:13,560 train 100 1.227566e-02 0.271577
2019-11-12 18:01:20,312 train 150 1.233392e-02 0.276508
2019-11-12 18:01:27,062 train 200 1.233845e-02 0.180895
2019-11-12 18:01:33,813 train 250 1.228320e-02 0.197545
2019-11-12 18:01:40,564 train 300 1.227936e-02 0.202480
2019-11-12 18:01:47,328 train 350 1.230427e-02 0.209750
2019-11-12 18:01:54,081 train 400 1.228147e-02 0.213295
2019-11-12 18:02:00,835 train 450 1.227945e-02 0.223539
2019-11-12 18:02:07,588 train 500 1.226683e-02 0.229212
2019-11-12 18:02:14,343 train 550 1.226999e-02 0.233385
2019-11-12 18:02:21,101 train 600 1.226982e-02 0.237447
2019-11-12 18:02:27,854 train 650 1.226854e-02 0.241588
2019-11-12 18:02:34,607 train 700 1.227710e-02 0.243635
2019-11-12 18:02:41,361 train 750 1.228807e-02 0.241847
2019-11-12 18:02:48,121 train 800 1.229891e-02 0.242567
2019-11-12 18:02:54,877 train 850 1.231728e-02 0.245410
2019-11-12 18:02:56,898 training loss; R2: 1.231359e-02 0.245044
2019-11-12 18:02:57,196 valid 000 2.282908e+01 -2763.627331
2019-11-12 18:02:58,905 valid 050 2.270668e+01 -10440.682698
2019-11-12 18:03:00,443 validation loss; R2: 2.270834e+01 -7868.604895
2019-11-12 18:03:00,459 epoch 15 lr 1.000000e-03
2019-11-12 18:03:00,828 train 000 1.463464e-02 0.262277
2019-11-12 18:03:07,787 train 050 1.195589e-02 0.269238
2019-11-12 18:03:14,671 train 100 1.204305e-02 0.276975
2019-11-12 18:03:21,579 train 150 1.223382e-02 0.255541
2019-11-12 18:03:28,668 train 200 1.218733e-02 0.263856
2019-11-12 18:03:35,718 train 250 1.222378e-02 0.263533
2019-11-12 18:03:42,749 train 300 1.223235e-02 0.225011
2019-11-12 18:03:49,785 train 350 1.223910e-02 0.235659
2019-11-12 18:03:56,815 train 400 1.221122e-02 0.244890
2019-11-12 18:04:03,846 train 450 1.224838e-02 0.245698
2019-11-12 18:04:10,877 train 500 1.225377e-02 0.250285
2019-11-12 18:04:17,901 train 550 1.226054e-02 0.252846
2019-11-12 18:04:24,927 train 600 1.225736e-02 0.250650
2019-11-12 18:04:31,950 train 650 1.225473e-02 0.251263
2019-11-12 18:04:38,992 train 700 1.223756e-02 0.247531
2019-11-12 18:04:46,018 train 750 1.224050e-02 0.249279
2019-11-12 18:04:53,043 train 800 1.224314e-02 0.247879
2019-11-12 18:05:00,062 train 850 1.224059e-02 0.247364
2019-11-12 18:05:02,163 training loss; R2: 1.224380e-02 0.247630
2019-11-12 18:05:02,456 valid 000 4.499608e+00 -6691.093770
2019-11-12 18:05:04,116 valid 050 4.478575e+00 -1971.350343
2019-11-12 18:05:05,627 validation loss; R2: 4.479013e+00 -1674.984478
2019-11-12 18:05:05,642 epoch 16 lr 1.000000e-03
2019-11-12 18:05:06,000 train 000 1.542230e-02 0.324035
2019-11-12 18:05:13,058 train 050 1.230497e-02 -19.587421
2019-11-12 18:05:20,109 train 100 1.239706e-02 -9.764323
2019-11-12 18:05:27,164 train 150 1.235508e-02 -6.441848
2019-11-12 18:05:34,220 train 200 1.238266e-02 -4.770680
2019-11-12 18:05:41,269 train 250 1.240020e-02 -3.768767
2019-11-12 18:05:48,321 train 300 1.236879e-02 -3.101442
2019-11-12 18:05:55,371 train 350 1.233073e-02 -2.619176
2019-11-12 18:06:02,421 train 400 1.227216e-02 -2.276725
2019-11-12 18:06:09,470 train 450 1.229021e-02 -1.996588
2019-11-12 18:06:16,520 train 500 1.229858e-02 -1.776106
2019-11-12 18:06:23,576 train 550 1.229319e-02 -1.590064
2019-11-12 18:06:30,628 train 600 1.225812e-02 -1.438492
2019-11-12 18:06:37,695 train 650 1.225339e-02 -1.307606
2019-11-12 18:06:44,773 train 700 1.223019e-02 -1.195523
2019-11-12 18:06:51,863 train 750 1.223257e-02 -1.095982
2019-11-12 18:06:58,977 train 800 1.222895e-02 -1.012118
2019-11-12 18:07:06,058 train 850 1.221650e-02 -0.937600
2019-11-12 18:07:08,183 training loss; R2: 1.221690e-02 -0.916155
2019-11-12 18:07:08,450 valid 000 5.368308e+00 -1135.043458
2019-11-12 18:07:10,142 valid 050 5.357179e+00 -2964.712962
2019-11-12 18:07:11,652 validation loss; R2: 5.356723e+00 -2976.309044
2019-11-12 18:07:11,674 epoch 17 lr 1.000000e-03
2019-11-12 18:07:12,070 train 000 1.157651e-02 0.298595
2019-11-12 18:07:19,024 train 050 1.212576e-02 0.259862
2019-11-12 18:07:26,126 train 100 1.213369e-02 0.233762
2019-11-12 18:07:33,107 train 150 1.222211e-02 0.239805
2019-11-12 18:07:39,938 train 200 1.216792e-02 0.253011
2019-11-12 18:07:46,765 train 250 1.211568e-02 0.252500
2019-11-12 18:07:53,590 train 300 1.203314e-02 0.246874
2019-11-12 18:08:00,410 train 350 1.200969e-02 0.247406
2019-11-12 18:08:07,247 train 400 1.199846e-02 0.219544
2019-11-12 18:08:14,077 train 450 1.198979e-02 0.224719
2019-11-12 18:08:20,909 train 500 1.197365e-02 0.227930
2019-11-12 18:08:27,739 train 550 1.195707e-02 0.230314
2019-11-12 18:08:34,570 train 600 1.195868e-02 0.237657
2019-11-12 18:08:41,399 train 650 1.194995e-02 0.243235
2019-11-12 18:08:48,231 train 700 1.196460e-02 0.243529
2019-11-12 18:08:55,060 train 750 1.197087e-02 0.233578
2019-11-12 18:09:01,887 train 800 1.199673e-02 0.233983
2019-11-12 18:09:08,710 train 850 1.199507e-02 0.236569
2019-11-12 18:09:10,748 training loss; R2: 1.199943e-02 0.236217
2019-11-12 18:09:11,047 valid 000 2.109324e-01 -6.323112
2019-11-12 18:09:12,688 valid 050 2.047591e-01 -10.664831
2019-11-12 18:09:14,188 validation loss; R2: 2.041627e-01 -11.907546
2019-11-12 18:09:14,204 epoch 18 lr 1.000000e-03
2019-11-12 18:09:14,595 train 000 1.311675e-02 0.301706
2019-11-12 18:09:21,688 train 050 1.254484e-02 -1.714412
2019-11-12 18:09:28,666 train 100 1.219706e-02 -0.725642
2019-11-12 18:09:35,623 train 150 1.221250e-02 -0.439351
2019-11-12 18:09:42,559 train 200 1.221134e-02 -0.261882
2019-11-12 18:09:49,519 train 250 1.215091e-02 -0.156437
2019-11-12 18:09:56,456 train 300 1.218722e-02 -0.093119
2019-11-12 18:10:03,471 train 350 1.220789e-02 -0.042181
2019-11-12 18:10:10,410 train 400 1.220260e-02 -0.065260
2019-11-12 18:10:17,371 train 450 1.219674e-02 -0.024052
2019-11-12 18:10:24,311 train 500 1.217552e-02 0.005520
2019-11-12 18:10:31,267 train 550 1.215065e-02 0.030488
2019-11-12 18:10:38,208 train 600 1.213034e-02 0.048410
2019-11-12 18:10:45,158 train 650 1.212397e-02 0.065354
2019-11-12 18:10:52,080 train 700 1.213904e-02 0.074660
2019-11-12 18:10:59,123 train 750 1.211357e-02 0.074883
2019-11-12 18:11:06,183 train 800 1.214098e-02 0.086900
2019-11-12 18:11:13,198 train 850 1.215521e-02 0.089093
2019-11-12 18:11:15,265 training loss; R2: 1.215811e-02 0.086213
2019-11-12 18:11:15,559 valid 000 9.176279e+00 -1731.175761
2019-11-12 18:11:17,254 valid 050 9.189940e+00 -2109.499419
2019-11-12 18:11:18,804 validation loss; R2: 9.191096e+00 -2288.226286
2019-11-12 18:11:18,826 epoch 19 lr 1.000000e-03
2019-11-12 18:11:19,238 train 000 1.201296e-02 0.265531
2019-11-12 18:11:26,446 train 050 1.254346e-02 0.259165
2019-11-12 18:11:33,374 train 100 1.248204e-02 0.240799
2019-11-12 18:11:40,149 train 150 1.248135e-02 0.246073
2019-11-12 18:11:46,929 train 200 1.242537e-02 0.240891
2019-11-12 18:11:53,674 train 250 1.246928e-02 0.202444
2019-11-12 18:12:00,507 train 300 1.246724e-02 0.193298
2019-11-12 18:12:07,255 train 350 1.249991e-02 0.199588
2019-11-12 18:12:14,003 train 400 1.247598e-02 0.196413
2019-11-12 18:12:20,761 train 450 1.244951e-02 0.178552
2019-11-12 18:12:27,511 train 500 1.242719e-02 0.182760
2019-11-12 18:12:34,259 train 550 1.241964e-02 0.190203
2019-11-12 18:12:41,004 train 600 1.241183e-02 0.195346
2019-11-12 18:12:47,764 train 650 1.240517e-02 0.195353
2019-11-12 18:12:54,516 train 700 1.240489e-02 0.197969
2019-11-12 18:13:01,262 train 750 1.238477e-02 0.197224
2019-11-12 18:13:08,057 train 800 1.237448e-02 0.199991
2019-11-12 18:13:14,799 train 850 1.235899e-02 0.202286
2019-11-12 18:13:16,816 training loss; R2: 1.235185e-02 0.203604
2019-11-12 18:13:17,111 valid 000 1.128932e+00 -67.575058
2019-11-12 18:13:18,769 valid 050 1.140452e+00 -73.546264
2019-11-12 18:13:20,287 validation loss; R2: 1.137722e+00 -73.093647
