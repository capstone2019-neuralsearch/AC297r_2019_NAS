2019-11-12 19:30:41,462 gpu device = 1
2019-11-12 19:30:41,462 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name='galaxy-zoo-cleaned', gpu=1, grad_clip=5, gz_dtree=False, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-galaxy-zoo-EXP-20191112-193041', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-11-12 19:30:52,989 param size = 0.321973MB
2019-11-12 19:30:52,993 epoch 0 lr 1.000000e-03
2019-11-12 19:30:55,413 train 000 2.309492e-01 -43.931594
2019-11-12 19:31:03,954 train 050 5.192677e-02 -4.003799
2019-11-12 19:31:12,311 train 100 3.925394e-02 -2.172134
2019-11-12 19:31:20,666 train 150 3.468218e-02 -1.519550
2019-11-12 19:31:28,869 train 200 3.228491e-02 -1.187827
2019-11-12 19:31:36,905 train 250 3.067281e-02 -0.991054
2019-11-12 19:31:44,950 train 300 2.951771e-02 -0.840642
2019-11-12 19:31:52,993 train 350 2.872188e-02 -0.742049
2019-11-12 19:32:01,138 train 400 2.809383e-02 -0.657298
2019-11-12 19:32:09,514 train 450 2.749804e-02 -0.596584
2019-11-12 19:32:17,739 train 500 2.709190e-02 -0.546662
2019-11-12 19:32:25,792 train 550 2.668225e-02 -0.502156
2019-11-12 19:32:33,789 train 600 2.635216e-02 -0.464228
2019-11-12 19:32:41,784 train 650 2.605842e-02 -0.432933
2019-11-12 19:32:49,787 train 700 2.577015e-02 -0.407410
2019-11-12 19:32:57,801 train 750 2.549874e-02 -0.381304
2019-11-12 19:33:05,811 train 800 2.524850e-02 -0.355140
2019-11-12 19:33:13,824 train 850 2.499211e-02 -0.330759
2019-11-12 19:33:17,082 training loss; R2: 2.492675e-02 -0.379984
2019-11-12 19:33:17,356 valid 000 2.353420e-02 -0.024145
2019-11-12 19:33:19,064 valid 050 2.223663e-02 0.018128
2019-11-12 19:33:20,670 validation loss; R2: 2.228867e-02 -0.019599
2019-11-12 19:33:20,689 epoch 1 lr 1.000000e-03
2019-11-12 19:33:21,277 train 000 1.818640e-02 0.090995
2019-11-12 19:33:29,571 train 050 2.042877e-02 0.030112
2019-11-12 19:33:37,965 train 100 2.040353e-02 0.040193
2019-11-12 19:33:46,355 train 150 2.038256e-02 0.042216
2019-11-12 19:33:54,744 train 200 2.026067e-02 0.053206
2019-11-12 19:34:03,134 train 250 2.021949e-02 0.053085
2019-11-12 19:34:11,391 train 300 2.026553e-02 0.049396
2019-11-12 19:34:19,705 train 350 2.021464e-02 0.052165
2019-11-12 19:34:27,973 train 400 2.010516e-02 0.052133
2019-11-12 19:34:36,332 train 450 2.003780e-02 0.044695
2019-11-12 19:34:44,471 train 500 2.000988e-02 0.045270
2019-11-12 19:34:52,483 train 550 1.992287e-02 0.049566
2019-11-12 19:35:00,493 train 600 1.985543e-02 0.052314
2019-11-12 19:35:08,503 train 650 1.983980e-02 0.052806
2019-11-12 19:35:16,520 train 700 1.977681e-02 0.055584
2019-11-12 19:35:24,557 train 750 1.970211e-02 0.059098
2019-11-12 19:35:32,535 train 800 1.963345e-02 0.060912
2019-11-12 19:35:40,667 train 850 1.955503e-02 0.065562
2019-11-12 19:35:43,044 training loss; R2: 1.952563e-02 0.065647
2019-11-12 19:35:43,343 valid 000 1.758541e-02 0.115044
2019-11-12 19:35:45,072 valid 050 1.956746e-02 -0.147156
2019-11-12 19:35:46,651 validation loss; R2: 1.981736e-02 -0.024344
2019-11-12 19:35:46,672 epoch 2 lr 1.000000e-03
2019-11-12 19:35:47,103 train 000 1.832006e-02 0.160700
2019-11-12 19:35:55,144 train 050 1.815643e-02 0.115901
2019-11-12 19:36:03,236 train 100 1.791898e-02 0.128156
2019-11-12 19:36:11,184 train 150 1.814088e-02 0.129587
2019-11-12 19:36:19,137 train 200 1.816813e-02 0.113235
2019-11-12 19:36:27,095 train 250 1.809726e-02 0.115491
2019-11-12 19:36:35,047 train 300 1.793881e-02 0.114539
2019-11-12 19:36:42,998 train 350 1.788029e-02 0.120011
2019-11-12 19:36:50,952 train 400 1.781836e-02 0.123623
2019-11-12 19:36:58,895 train 450 1.783305e-02 0.124702
2019-11-12 19:37:06,838 train 500 1.779779e-02 0.124172
2019-11-12 19:37:14,781 train 550 1.775059e-02 0.128189
2019-11-12 19:37:22,726 train 600 1.770678e-02 0.130426
2019-11-12 19:37:30,671 train 650 1.766402e-02 0.130173
2019-11-12 19:37:38,612 train 700 1.761155e-02 0.132024
2019-11-12 19:37:46,560 train 750 1.753379e-02 0.134910
2019-11-12 19:37:54,501 train 800 1.744969e-02 0.137501
2019-11-12 19:38:02,443 train 850 1.738490e-02 0.139923
2019-11-12 19:38:04,819 training loss; R2: 1.736722e-02 0.140678
2019-11-12 19:38:05,118 valid 000 2.478847e-02 -0.135576
2019-11-12 19:38:06,856 valid 050 2.398348e-02 -0.126529
2019-11-12 19:38:08,434 validation loss; R2: 2.392677e-02 -0.164241
2019-11-12 19:38:08,454 epoch 3 lr 1.000000e-03
2019-11-12 19:38:08,880 train 000 1.820910e-02 0.083394
2019-11-12 19:38:16,905 train 050 1.641215e-02 0.167834
2019-11-12 19:38:24,979 train 100 1.620980e-02 0.173147
2019-11-12 19:38:33,025 train 150 1.610152e-02 0.169191
2019-11-12 19:38:41,094 train 200 1.597879e-02 0.164131
2019-11-12 19:38:49,166 train 250 1.599895e-02 0.168364
2019-11-12 19:38:57,191 train 300 1.592257e-02 0.175260
2019-11-12 19:39:05,305 train 350 1.593930e-02 0.177434
2019-11-12 19:39:13,309 train 400 1.590001e-02 0.170284
2019-11-12 19:39:21,309 train 450 1.584197e-02 0.169660
2019-11-12 19:39:29,307 train 500 1.577490e-02 -9.732162
2019-11-12 19:39:37,296 train 550 1.569725e-02 -8.830934
2019-11-12 19:39:45,271 train 600 1.559686e-02 -8.080730
2019-11-12 19:39:53,248 train 650 1.552354e-02 -7.444429
2019-11-12 19:40:01,227 train 700 1.545680e-02 -6.897963
2019-11-12 19:40:09,202 train 750 1.541142e-02 -6.425849
2019-11-12 19:40:17,185 train 800 1.537088e-02 -6.011123
2019-11-12 19:40:25,162 train 850 1.532553e-02 -5.644219
2019-11-12 19:40:27,548 training loss; R2: 1.531481e-02 -5.543201
2019-11-12 19:40:27,848 valid 000 1.492553e-02 0.264735
2019-11-12 19:40:29,595 valid 050 1.327645e-02 0.247418
2019-11-12 19:40:31,170 validation loss; R2: 1.329978e-02 0.252835
2019-11-12 19:40:31,191 epoch 4 lr 1.000000e-03
2019-11-12 19:40:31,569 train 000 1.301214e-02 -11.619334
2019-11-12 19:40:39,752 train 050 1.438198e-02 -0.006067
2019-11-12 19:40:47,826 train 100 1.427989e-02 0.107346
2019-11-12 19:40:55,889 train 150 1.430199e-02 0.147346
2019-11-12 19:41:03,928 train 200 1.432734e-02 0.165145
2019-11-12 19:41:11,964 train 250 1.429633e-02 -0.232539
2019-11-12 19:41:20,003 train 300 1.426389e-02 -0.162997
2019-11-12 19:41:28,028 train 350 1.417722e-02 -0.105520
2019-11-12 19:41:36,052 train 400 1.412588e-02 -0.069524
2019-11-12 19:41:44,083 train 450 1.410720e-02 -0.035908
2019-11-12 19:41:52,108 train 500 1.402148e-02 -0.008592
2019-11-12 19:42:00,133 train 550 1.396984e-02 0.015237
2019-11-12 19:42:08,157 train 600 1.391604e-02 0.033373
2019-11-12 19:42:16,223 train 650 1.389559e-02 0.045264
2019-11-12 19:42:24,243 train 700 1.385003e-02 0.058543
2019-11-12 19:42:32,261 train 750 1.381286e-02 0.067942
2019-11-12 19:42:40,279 train 800 1.378038e-02 0.077852
2019-11-12 19:42:48,298 train 850 1.375056e-02 0.089492
2019-11-12 19:42:50,693 training loss; R2: 1.374062e-02 0.092221
2019-11-12 19:42:50,992 valid 000 1.047819e-02 0.247958
2019-11-12 19:42:52,695 valid 050 1.172075e-02 0.256913
2019-11-12 19:42:54,307 validation loss; R2: 1.197302e-02 0.275672
2019-11-12 19:42:54,327 epoch 5 lr 1.000000e-03
2019-11-12 19:42:54,745 train 000 1.198619e-02 0.275206
2019-11-12 19:43:02,756 train 050 1.315647e-02 0.274763
2019-11-12 19:43:10,771 train 100 1.333880e-02 0.260426
2019-11-12 19:43:18,843 train 150 1.327068e-02 0.223299
2019-11-12 19:43:26,867 train 200 1.319250e-02 0.233957
2019-11-12 19:43:34,905 train 250 1.319945e-02 0.226842
2019-11-12 19:43:42,979 train 300 1.319032e-02 0.230173
2019-11-12 19:43:51,024 train 350 1.313471e-02 0.233541
2019-11-12 19:43:59,095 train 400 1.309683e-02 0.240353
2019-11-12 19:44:07,134 train 450 1.304996e-02 0.240482
2019-11-12 19:44:15,147 train 500 1.301940e-02 0.240689
2019-11-12 19:44:23,224 train 550 1.301582e-02 0.244534
2019-11-12 19:44:31,262 train 600 1.297384e-02 0.177369
2019-11-12 19:44:39,291 train 650 1.296390e-02 0.181038
2019-11-12 19:44:47,303 train 700 1.293953e-02 0.187006
2019-11-12 19:44:55,374 train 750 1.293804e-02 0.192733
2019-11-12 19:45:03,411 train 800 1.290321e-02 0.197188
2019-11-12 19:45:11,514 train 850 1.288625e-02 0.201587
2019-11-12 19:45:13,911 training loss; R2: 1.288181e-02 0.203157
2019-11-12 19:45:14,273 valid 000 1.175565e-02 -0.096948
2019-11-12 19:45:16,019 valid 050 1.165586e-02 0.319803
2019-11-12 19:45:17,566 validation loss; R2: 1.160661e-02 0.314346
2019-11-12 19:45:17,587 epoch 6 lr 1.000000e-03
2019-11-12 19:45:18,032 train 000 1.171027e-02 0.326643
2019-11-12 19:45:26,089 train 050 1.268635e-02 0.235148
2019-11-12 19:45:34,163 train 100 1.265489e-02 0.258544
2019-11-12 19:45:42,225 train 150 1.265872e-02 0.236989
2019-11-12 19:45:50,259 train 200 1.257285e-02 0.236509
2019-11-12 19:45:58,362 train 250 1.257390e-02 0.245899
2019-11-12 19:46:06,399 train 300 1.257292e-02 0.254786
2019-11-12 19:46:14,426 train 350 1.253373e-02 0.250825
2019-11-12 19:46:22,450 train 400 1.252487e-02 0.252321
2019-11-12 19:46:30,465 train 450 1.243918e-02 0.260162
2019-11-12 19:46:38,488 train 500 1.244338e-02 0.262134
2019-11-12 19:46:46,518 train 550 1.243448e-02 0.267000
2019-11-12 19:46:54,528 train 600 1.239878e-02 0.268005
2019-11-12 19:47:02,544 train 650 1.238266e-02 0.270050
2019-11-12 19:47:10,553 train 700 1.238446e-02 0.270573
2019-11-12 19:47:18,564 train 750 1.236614e-02 0.257054
2019-11-12 19:47:26,576 train 800 1.237524e-02 0.258748
2019-11-12 19:47:34,586 train 850 1.236234e-02 0.260321
2019-11-12 19:47:36,977 training loss; R2: 1.235711e-02 0.260836
2019-11-12 19:47:37,268 valid 000 1.068732e-02 0.393878
2019-11-12 19:47:39,025 valid 050 1.115113e-02 0.261467
2019-11-12 19:47:40,612 validation loss; R2: 1.145437e-02 0.302860
2019-11-12 19:47:40,633 epoch 7 lr 1.000000e-03
2019-11-12 19:47:41,046 train 000 1.412731e-02 0.270587
2019-11-12 19:47:49,113 train 050 1.223234e-02 0.199097
2019-11-12 19:47:57,204 train 100 1.213507e-02 0.250479
2019-11-12 19:48:05,278 train 150 1.220056e-02 0.258341
2019-11-12 19:48:13,292 train 200 1.216572e-02 0.258091
2019-11-12 19:48:21,309 train 250 1.217938e-02 0.249727
2019-11-12 19:48:29,344 train 300 1.218333e-02 0.255276
2019-11-12 19:48:37,350 train 350 1.215998e-02 0.257921
2019-11-12 19:48:45,383 train 400 1.214046e-02 0.261805
2019-11-12 19:48:53,424 train 450 1.211325e-02 0.266134
2019-11-12 19:49:01,435 train 500 1.210112e-02 0.259696
2019-11-12 19:49:09,463 train 550 1.206213e-02 0.264900
2019-11-12 19:49:17,601 train 600 1.204588e-02 0.266994
2019-11-12 19:49:25,627 train 650 1.203718e-02 0.267676
2019-11-12 19:49:33,657 train 700 1.199127e-02 0.270222
2019-11-12 19:49:41,659 train 750 1.199534e-02 0.270421
2019-11-12 19:49:49,697 train 800 1.198363e-02 0.271700
2019-11-12 19:49:57,701 train 850 1.198130e-02 0.273109
2019-11-12 19:50:00,092 training loss; R2: 1.198393e-02 0.273452
2019-11-12 19:50:00,387 valid 000 2.803658e-02 -0.083181
2019-11-12 19:50:02,140 valid 050 2.898857e-02 -0.275274
2019-11-12 19:50:03,713 validation loss; R2: 2.943837e-02 -0.283544
2019-11-12 19:50:03,747 epoch 8 lr 1.000000e-03
2019-11-12 19:50:04,183 train 000 1.360669e-02 0.190175
2019-11-12 19:50:12,218 train 050 1.194537e-02 0.287231
2019-11-12 19:50:20,317 train 100 1.193920e-02 0.285880
2019-11-12 19:50:28,354 train 150 1.186906e-02 0.287469
2019-11-12 19:50:36,388 train 200 1.175480e-02 0.292300
2019-11-12 19:50:44,407 train 250 1.171893e-02 0.293434
2019-11-12 19:50:52,425 train 300 1.170360e-02 0.286825
2019-11-12 19:51:00,480 train 350 1.170071e-02 0.288387
2019-11-12 19:51:08,516 train 400 1.169845e-02 0.290814
2019-11-12 19:51:16,576 train 450 1.167329e-02 0.259834
2019-11-12 19:51:24,621 train 500 1.168130e-02 0.263456
2019-11-12 19:51:32,679 train 550 1.165409e-02 0.264737
2019-11-12 19:51:40,777 train 600 1.163624e-02 0.266433
2019-11-12 19:51:48,725 train 650 1.163427e-02 0.267131
2019-11-12 19:51:56,679 train 700 1.163175e-02 0.271317
2019-11-12 19:52:04,606 train 750 1.160681e-02 0.269353
2019-11-12 19:52:12,536 train 800 1.160477e-02 0.271904
2019-11-12 19:52:20,478 train 850 1.160704e-02 0.255015
2019-11-12 19:52:22,846 training loss; R2: 1.160426e-02 0.256411
2019-11-12 19:52:23,149 valid 000 4.104050e-01 -23.912542
2019-11-12 19:52:24,856 valid 050 4.137737e-01 -40.974220
2019-11-12 19:52:26,394 validation loss; R2: 4.138321e-01 -32.958702
2019-11-12 19:52:26,413 epoch 9 lr 1.000000e-03
2019-11-12 19:52:26,832 train 000 1.228271e-02 0.420506
2019-11-12 19:52:34,940 train 050 1.139059e-02 0.208113
2019-11-12 19:52:43,063 train 100 1.130299e-02 0.244549
2019-11-12 19:52:51,147 train 150 1.141583e-02 0.157833
2019-11-12 19:52:59,157 train 200 1.141640e-02 0.202997
2019-11-12 19:53:07,277 train 250 1.137108e-02 0.222557
2019-11-12 19:53:15,378 train 300 1.136475e-02 0.241082
2019-11-12 19:53:23,388 train 350 1.136054e-02 0.247208
2019-11-12 19:53:31,523 train 400 1.136262e-02 0.252029
2019-11-12 19:53:39,649 train 450 1.134649e-02 0.259336
2019-11-12 19:53:47,669 train 500 1.137517e-02 0.263536
2019-11-12 19:53:55,712 train 550 1.137302e-02 0.266555
2019-11-12 19:54:03,973 train 600 1.137729e-02 0.270086
2019-11-12 19:54:12,051 train 650 1.137480e-02 0.274345
2019-11-12 19:54:20,192 train 700 1.134859e-02 0.276390
2019-11-12 19:54:28,517 train 750 1.133901e-02 0.276347
2019-11-12 19:54:36,834 train 800 1.132775e-02 0.280402
2019-11-12 19:54:45,007 train 850 1.131910e-02 0.282408
2019-11-12 19:54:47,406 training loss; R2: 1.131411e-02 0.282208
2019-11-12 19:54:47,710 valid 000 4.681416e-01 -49.450203
2019-11-12 19:54:49,417 valid 050 4.643776e-01 -52.176838
2019-11-12 19:54:50,958 validation loss; R2: 4.631412e-01 -50.267246
2019-11-12 19:54:50,986 epoch 10 lr 1.000000e-03
2019-11-12 19:54:51,393 train 000 1.079450e-02 0.222081
2019-11-12 19:54:59,443 train 050 1.117084e-02 0.304815
2019-11-12 19:55:07,756 train 100 1.119048e-02 0.308918
2019-11-12 19:55:16,151 train 150 1.121807e-02 0.301414
2019-11-12 19:55:24,543 train 200 1.125700e-02 0.300250
2019-11-12 19:55:32,935 train 250 1.129225e-02 0.303298
2019-11-12 19:55:41,107 train 300 1.130492e-02 0.306600
2019-11-12 19:55:49,151 train 350 1.127972e-02 0.288703
2019-11-12 19:55:57,194 train 400 1.124388e-02 0.292139
2019-11-12 19:56:05,401 train 450 1.123990e-02 0.296229
2019-11-12 19:56:13,710 train 500 1.123440e-02 0.296730
2019-11-12 19:56:21,981 train 550 1.121530e-02 0.300855
2019-11-12 19:56:30,142 train 600 1.118869e-02 0.300848
2019-11-12 19:56:38,177 train 650 1.116943e-02 0.259836
2019-11-12 19:56:46,446 train 700 1.116015e-02 0.261613
2019-11-12 19:56:54,504 train 750 1.115876e-02 0.265050
2019-11-12 19:57:02,540 train 800 1.116053e-02 0.269239
2019-11-12 19:57:10,754 train 850 1.115725e-02 0.270492
2019-11-12 19:57:13,258 training loss; R2: 1.115203e-02 0.271611
2019-11-12 19:57:13,534 valid 000 1.673607e+00 -61.218778
2019-11-12 19:57:15,252 valid 050 1.657110e+00 -78.270783
2019-11-12 19:57:16,803 validation loss; R2: 1.657204e+00 -79.226048
2019-11-12 19:57:16,822 epoch 11 lr 1.000000e-03
2019-11-12 19:57:17,208 train 000 1.028174e-02 0.301432
2019-11-12 19:57:25,398 train 050 1.102993e-02 0.288704
2019-11-12 19:57:33,777 train 100 1.119045e-02 0.291643
2019-11-12 19:57:42,151 train 150 1.118002e-02 0.299624
2019-11-12 19:57:50,533 train 200 1.114298e-02 0.299577
2019-11-12 19:57:58,648 train 250 1.105994e-02 0.305418
2019-11-12 19:58:06,885 train 300 1.100252e-02 0.309740
2019-11-12 19:58:15,269 train 350 1.101953e-02 0.298235
2019-11-12 19:58:23,613 train 400 1.099225e-02 0.303729
2019-11-12 19:58:31,718 train 450 1.102068e-02 0.303195
2019-11-12 19:58:40,102 train 500 1.100729e-02 0.307019
2019-11-12 19:58:48,482 train 550 1.097984e-02 0.308276
2019-11-12 19:58:56,857 train 600 1.096462e-02 0.309593
2019-11-12 19:59:05,234 train 650 1.095406e-02 0.308441
2019-11-12 19:59:13,604 train 700 1.094521e-02 0.309965
2019-11-12 19:59:21,969 train 750 1.095265e-02 0.291087
2019-11-12 19:59:30,342 train 800 1.093835e-02 0.294583
2019-11-12 19:59:38,716 train 850 1.092663e-02 0.294682
2019-11-12 19:59:41,214 training loss; R2: 1.091714e-02 0.294979
2019-11-12 19:59:41,499 valid 000 4.575597e+00 -433.166751
2019-11-12 19:59:43,225 valid 050 4.594496e+00 -637.213931
2019-11-12 19:59:44,769 validation loss; R2: 4.593754e+00 -650.111934
2019-11-12 19:59:44,790 epoch 12 lr 1.000000e-03
2019-11-12 19:59:45,188 train 000 9.818737e-03 0.423301
2019-11-12 19:59:53,490 train 050 1.076600e-02 0.312741
2019-11-12 20:00:01,868 train 100 1.064512e-02 0.333761
2019-11-12 20:00:10,241 train 150 1.069308e-02 0.309940
2019-11-12 20:00:18,613 train 200 1.072720e-02 0.317357
2019-11-12 20:00:26,975 train 250 1.076284e-02 0.308440
2019-11-12 20:00:35,343 train 300 1.079686e-02 0.310438
2019-11-12 20:00:43,497 train 350 1.080698e-02 0.313974
2019-11-12 20:00:51,495 train 400 1.078908e-02 0.311038
2019-11-12 20:00:59,806 train 450 1.078720e-02 0.313157
2019-11-12 20:01:08,198 train 500 1.078435e-02 0.315242
2019-11-12 20:01:16,575 train 550 1.080174e-02 0.309936
2019-11-12 20:01:24,784 train 600 1.082339e-02 0.301069
2019-11-12 20:01:32,871 train 650 1.081204e-02 0.301282
2019-11-12 20:01:41,003 train 700 1.079974e-02 0.303414
2019-11-12 20:01:49,016 train 750 1.080325e-02 0.283761
2019-11-12 20:01:57,168 train 800 1.082045e-02 0.287179
2019-11-12 20:02:05,194 train 850 1.081467e-02 0.260823
2019-11-12 20:02:07,598 training loss; R2: 1.081934e-02 0.262047
2019-11-12 20:02:07,893 valid 000 8.959567e-01 -47.103735
2019-11-12 20:02:09,602 valid 050 9.070205e-01 -53.788883
2019-11-12 20:02:11,167 validation loss; R2: 9.102419e-01 -53.562584
2019-11-12 20:02:11,188 epoch 13 lr 1.000000e-03
2019-11-12 20:02:11,594 train 000 1.307982e-02 0.032699
2019-11-12 20:02:19,577 train 050 1.098451e-02 0.307852
2019-11-12 20:02:27,894 train 100 1.080975e-02 0.287550
2019-11-12 20:02:36,086 train 150 1.072206e-02 0.300643
2019-11-12 20:02:44,094 train 200 1.071924e-02 0.298241
2019-11-12 20:02:52,115 train 250 1.077627e-02 0.303661
2019-11-12 20:03:00,151 train 300 1.076599e-02 0.308668
2019-11-12 20:03:08,147 train 350 1.076366e-02 0.308960
2019-11-12 20:03:16,315 train 400 1.074578e-02 0.301538
2019-11-12 20:03:24,434 train 450 1.073299e-02 0.309088
2019-11-12 20:03:32,581 train 500 1.073266e-02 0.313693
2019-11-12 20:03:40,975 train 550 1.072289e-02 0.308503
2019-11-12 20:03:49,126 train 600 1.074471e-02 0.308002
2019-11-12 20:03:57,150 train 650 1.074458e-02 0.308872
2019-11-12 20:04:05,271 train 700 1.073642e-02 0.311387
2019-11-12 20:04:13,654 train 750 1.070736e-02 0.314289
2019-11-12 20:04:22,033 train 800 1.070557e-02 0.313859
2019-11-12 20:04:30,398 train 850 1.069376e-02 0.314899
2019-11-12 20:04:32,893 training loss; R2: 1.069799e-02 0.313440
2019-11-12 20:04:33,190 valid 000 6.054818e+00 -467.671858
2019-11-12 20:04:34,900 valid 050 6.121002e+00 -360.692809
2019-11-12 20:04:36,454 validation loss; R2: 6.115809e+00 -364.395276
2019-11-12 20:04:36,474 epoch 14 lr 1.000000e-03
2019-11-12 20:04:36,864 train 000 1.004891e-02 0.392355
2019-11-12 20:04:44,917 train 050 1.060817e-02 0.362880
2019-11-12 20:04:52,931 train 100 1.068674e-02 0.348316
2019-11-12 20:05:00,925 train 150 1.070837e-02 0.337994
2019-11-12 20:05:08,921 train 200 1.070768e-02 0.321527
2019-11-12 20:05:16,991 train 250 1.066693e-02 0.323203
2019-11-12 20:05:25,360 train 300 1.068340e-02 0.325029
2019-11-12 20:05:33,712 train 350 1.072288e-02 0.325413
2019-11-12 20:05:42,062 train 400 1.072729e-02 0.323582
2019-11-12 20:05:50,327 train 450 1.073460e-02 0.324408
2019-11-12 20:05:58,462 train 500 1.069902e-02 0.325336
2019-11-12 20:06:06,429 train 550 1.069576e-02 0.325607
2019-11-12 20:06:14,378 train 600 1.067976e-02 0.324231
2019-11-12 20:06:22,331 train 650 1.070631e-02 0.118653
2019-11-12 20:06:30,274 train 700 1.070674e-02 0.129867
2019-11-12 20:06:38,225 train 750 1.069729e-02 0.142589
2019-11-12 20:06:46,172 train 800 1.067884e-02 0.154215
2019-11-12 20:06:54,122 train 850 1.067958e-02 0.165122
2019-11-12 20:06:56,495 training loss; R2: 1.067554e-02 0.167608
2019-11-12 20:06:56,799 valid 000 3.637689e+00 -180.767465
2019-11-12 20:06:58,515 valid 050 3.621076e+00 -343.413231
2019-11-12 20:07:00,089 validation loss; R2: 3.626219e+00 -309.067373
2019-11-12 20:07:00,110 epoch 15 lr 1.000000e-03
2019-11-12 20:07:00,551 train 000 1.006160e-02 0.227976
2019-11-12 20:07:08,689 train 050 1.059313e-02 0.307732
2019-11-12 20:07:16,802 train 100 1.041116e-02 0.323164
2019-11-12 20:07:24,881 train 150 1.048310e-02 0.312383
2019-11-12 20:07:32,899 train 200 1.050945e-02 0.320178
2019-11-12 20:07:40,966 train 250 1.049633e-02 0.324256
2019-11-12 20:07:49,044 train 300 1.049431e-02 0.325805
2019-11-12 20:07:57,062 train 350 1.050520e-02 0.323923
2019-11-12 20:08:05,076 train 400 1.047342e-02 0.325970
2019-11-12 20:08:13,086 train 450 1.049833e-02 0.330276
2019-11-12 20:08:21,092 train 500 1.050390e-02 0.328219
2019-11-12 20:08:29,137 train 550 1.052316e-02 0.325947
2019-11-12 20:08:37,191 train 600 1.053818e-02 0.323436
2019-11-12 20:08:45,219 train 650 1.054097e-02 0.320988
2019-11-12 20:08:53,225 train 700 1.055620e-02 0.320242
2019-11-12 20:09:01,231 train 750 1.054621e-02 0.321554
2019-11-12 20:09:09,244 train 800 1.057534e-02 0.321760
2019-11-12 20:09:17,245 train 850 1.055913e-02 0.322952
2019-11-12 20:09:19,634 training loss; R2: 1.055511e-02 0.323624
2019-11-12 20:09:19,928 valid 000 5.893770e-01 -112.072998
2019-11-12 20:09:21,673 valid 050 5.929335e-01 -104.891013
2019-11-12 20:09:23,248 validation loss; R2: 5.930903e-01 -105.805999
2019-11-12 20:09:23,268 epoch 16 lr 1.000000e-03
2019-11-12 20:09:23,678 train 000 9.666337e-03 0.390633
2019-11-12 20:09:31,826 train 050 1.036655e-02 0.354620
2019-11-12 20:09:39,886 train 100 1.043934e-02 0.347527
2019-11-12 20:09:47,937 train 150 1.039944e-02 0.357920
2019-11-12 20:09:55,978 train 200 1.043840e-02 0.346729
2019-11-12 20:10:03,995 train 250 1.042174e-02 0.344731
2019-11-12 20:10:11,995 train 300 1.044931e-02 0.345257
2019-11-12 20:10:20,068 train 350 1.049368e-02 0.343813
2019-11-12 20:10:28,123 train 400 1.056513e-02 0.263333
2019-11-12 20:10:36,141 train 450 1.064359e-02 0.251082
2019-11-12 20:10:44,156 train 500 1.066370e-02 0.258857
2019-11-12 20:10:52,157 train 550 1.064520e-02 0.269423
2019-11-12 20:11:00,160 train 600 1.063318e-02 0.275216
2019-11-12 20:11:08,238 train 650 1.064036e-02 0.279001
2019-11-12 20:11:16,291 train 700 1.064067e-02 0.281653
2019-11-12 20:11:24,301 train 750 1.063047e-02 0.284526
2019-11-12 20:11:32,301 train 800 1.060575e-02 0.289790
2019-11-12 20:11:40,307 train 850 1.059683e-02 0.293387
2019-11-12 20:11:42,701 training loss; R2: 1.059615e-02 -0.029273
2019-11-12 20:11:42,995 valid 000 5.614097e+00 -419.191955
2019-11-12 20:11:44,734 valid 050 5.644561e+00 -475.103564
2019-11-12 20:11:46,332 validation loss; R2: 5.644539e+00 -446.646195
2019-11-12 20:11:46,352 epoch 17 lr 1.000000e-03
2019-11-12 20:11:46,778 train 000 1.021093e-02 0.410321
2019-11-12 20:11:54,783 train 050 1.032655e-02 0.347973
2019-11-12 20:12:02,908 train 100 1.032505e-02 0.345819
2019-11-12 20:12:11,004 train 150 1.029548e-02 0.325780
2019-11-12 20:12:19,043 train 200 1.034591e-02 0.335305
2019-11-12 20:12:27,046 train 250 1.037135e-02 0.333378
2019-11-12 20:12:35,079 train 300 1.032322e-02 0.337805
2019-11-12 20:12:43,149 train 350 1.032966e-02 0.334800
2019-11-12 20:12:51,172 train 400 1.034418e-02 0.330506
2019-11-12 20:12:59,206 train 450 1.033383e-02 0.334809
2019-11-12 20:13:07,560 train 500 1.032844e-02 0.338018
2019-11-12 20:13:15,716 train 550 1.033628e-02 0.339133
2019-11-12 20:13:23,752 train 600 1.038409e-02 0.339470
2019-11-12 20:13:31,777 train 650 1.040890e-02 0.339154
2019-11-12 20:13:39,777 train 700 1.041803e-02 0.340140
2019-11-12 20:13:47,779 train 750 1.040442e-02 0.338863
2019-11-12 20:13:55,778 train 800 1.040387e-02 0.337009
2019-11-12 20:14:03,775 train 850 1.038475e-02 0.337215
2019-11-12 20:14:06,165 training loss; R2: 1.038316e-02 0.335925
2019-11-12 20:14:06,485 valid 000 1.435671e+01 -2195.264390
2019-11-12 20:14:08,157 valid 050 1.433311e+01 -1993.928856
2019-11-12 20:14:09,709 validation loss; R2: 1.433326e+01 -1741.686237
2019-11-12 20:14:09,731 epoch 18 lr 1.000000e-03
2019-11-12 20:14:10,169 train 000 9.775668e-03 0.408793
2019-11-12 20:14:18,173 train 050 1.057879e-02 0.349800
2019-11-12 20:14:26,193 train 100 1.045761e-02 0.338780
2019-11-12 20:14:34,242 train 150 1.060406e-02 0.324338
2019-11-12 20:14:42,312 train 200 1.055204e-02 0.324675
2019-11-12 20:14:50,325 train 250 1.069911e-02 0.228328
2019-11-12 20:14:58,332 train 300 1.098844e-02 0.209739
2019-11-12 20:15:06,342 train 350 1.100604e-02 0.216137
2019-11-12 20:15:14,391 train 400 1.102678e-02 0.231173
2019-11-12 20:15:22,431 train 450 1.099536e-02 0.238967
2019-11-12 20:15:30,564 train 500 1.098654e-02 0.243605
2019-11-12 20:15:38,609 train 550 1.094371e-02 0.253830
2019-11-12 20:15:46,689 train 600 1.090648e-02 0.262543
2019-11-12 20:15:54,720 train 650 1.087247e-02 0.269055
2019-11-12 20:16:02,725 train 700 1.085299e-02 0.273944
2019-11-12 20:16:10,726 train 750 1.081489e-02 0.280313
2019-11-12 20:16:18,801 train 800 1.079528e-02 0.280037
2019-11-12 20:16:26,813 train 850 1.075724e-02 0.274799
2019-11-12 20:16:29,206 training loss; R2: 1.075007e-02 0.276199
2019-11-12 20:16:29,500 valid 000 1.766108e+00 -111.241035
2019-11-12 20:16:31,235 valid 050 1.772015e+00 -147.480467
2019-11-12 20:16:32,832 validation loss; R2: 1.772883e+00 -150.403604
2019-11-12 20:16:32,853 epoch 19 lr 1.000000e-03
2019-11-12 20:16:33,283 train 000 8.936748e-03 0.352097
2019-11-12 20:16:41,411 train 050 1.012003e-02 0.351421
2019-11-12 20:16:49,424 train 100 1.011520e-02 0.337373
2019-11-12 20:16:57,463 train 150 1.023644e-02 0.334373
2019-11-12 20:17:05,572 train 200 1.025894e-02 0.327643
2019-11-12 20:17:13,601 train 250 1.023051e-02 0.322999
2019-11-12 20:17:21,653 train 300 1.028810e-02 0.325138
2019-11-12 20:17:29,686 train 350 1.024258e-02 0.333112
2019-11-12 20:17:37,754 train 400 1.026584e-02 0.331049
2019-11-12 20:17:45,778 train 450 1.028749e-02 0.271668
2019-11-12 20:17:53,773 train 500 1.031165e-02 0.273579
2019-11-12 20:18:01,781 train 550 1.030686e-02 0.280890
2019-11-12 20:18:09,803 train 600 1.031132e-02 0.283899
2019-11-12 20:18:18,067 train 650 1.030415e-02 0.287318
2019-11-12 20:18:26,113 train 700 1.030525e-02 0.289501
2019-11-12 20:18:34,129 train 750 1.029658e-02 0.291469
2019-11-12 20:18:42,175 train 800 1.030973e-02 0.293628
2019-11-12 20:18:50,253 train 850 1.030730e-02 0.297317
2019-11-12 20:18:52,672 training loss; R2: 1.030726e-02 0.294477
2019-11-12 20:18:52,971 valid 000 3.041307e-01 -12.239221
2019-11-12 20:18:54,786 valid 050 2.920132e-01 -15.858202
2019-11-12 20:18:56,343 validation loss; R2: 2.915247e-01 -15.533597
