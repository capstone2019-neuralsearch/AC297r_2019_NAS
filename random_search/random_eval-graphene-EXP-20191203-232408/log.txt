2019-12-03 23:24:08,862 gpu device = 1
2019-12-03 23:24:08,862 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=96, cutout=False, cutout_length=16, data='../data', dataset='graphene', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name=None, gpu=1, grad_clip=5, gz_dtree=False, init_channels=36, layers=8, learning_rate=0.002, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-graphene-EXP-20191203-232408', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-12-03 23:24:12,061 param size = 0.992845MB
2019-12-03 23:24:12,064 epoch 0 lr 2.000000e-03
2019-12-03 23:24:14,613 train 000 6.794847e-01 -16.566748
2019-12-03 23:24:25,745 train 050 1.003967e-01 -2.141822
2019-12-03 23:24:36,804 train 100 6.430590e-02 -0.998241
2019-12-03 23:24:47,868 train 150 5.049402e-02 -0.584565
2019-12-03 23:24:58,922 train 200 4.320503e-02 -0.363121
2019-12-03 23:25:04,930 training loss; R2: 4.075446e-02 -0.284178
2019-12-03 23:25:05,063 valid 000 1.520065e-02 0.661143
2019-12-03 23:25:06,527 validation loss; R2: 1.345318e-02 0.568007
2019-12-03 23:25:06,546 epoch 1 lr 2.000000e-03
2019-12-03 23:25:06,916 train 000 1.305164e-02 0.309840
2019-12-03 23:25:17,977 train 050 1.437730e-02 0.464696
2019-12-03 23:25:29,037 train 100 1.442769e-02 0.502513
2019-12-03 23:25:40,100 train 150 1.474654e-02 0.507833
2019-12-03 23:25:51,159 train 200 1.462071e-02 0.527544
2019-12-03 23:25:56,135 training loss; R2: 1.434301e-02 0.535357
2019-12-03 23:25:56,264 valid 000 8.724923e-03 0.629003
2019-12-03 23:25:57,417 validation loss; R2: 1.018209e-02 0.652388
2019-12-03 23:25:57,437 epoch 2 lr 2.000000e-03
2019-12-03 23:25:57,742 train 000 9.315424e-03 0.673758
2019-12-03 23:26:08,805 train 050 1.099007e-02 0.650338
2019-12-03 23:26:19,873 train 100 1.051421e-02 0.658241
2019-12-03 23:26:30,940 train 150 1.055010e-02 0.658484
2019-12-03 23:26:42,006 train 200 1.032320e-02 0.666305
2019-12-03 23:26:46,983 training loss; R2: 1.036670e-02 0.659168
2019-12-03 23:26:47,115 valid 000 1.631741e-02 0.717978
2019-12-03 23:26:48,269 validation loss; R2: 6.075274e-03 0.802408
2019-12-03 23:26:48,288 epoch 3 lr 2.000000e-03
2019-12-03 23:26:48,591 train 000 9.424824e-03 0.775571
2019-12-03 23:26:59,636 train 050 1.018602e-02 0.660498
2019-12-03 23:27:10,427 train 100 9.948388e-03 0.666945
2019-12-03 23:27:21,220 train 150 9.394636e-03 0.675462
2019-12-03 23:27:32,010 train 200 9.184814e-03 0.688487
2019-12-03 23:27:36,861 training loss; R2: 9.125455e-03 0.690586
2019-12-03 23:27:36,992 valid 000 5.287106e-03 0.894353
2019-12-03 23:27:38,145 validation loss; R2: 4.980753e-03 0.833878
2019-12-03 23:27:38,163 epoch 4 lr 2.000000e-03
2019-12-03 23:27:38,462 train 000 6.821577e-03 0.749598
2019-12-03 23:27:49,258 train 050 8.903980e-03 0.706577
2019-12-03 23:28:00,055 train 100 8.653650e-03 0.715112
2019-12-03 23:28:10,854 train 150 8.397904e-03 0.714717
2019-12-03 23:28:21,649 train 200 8.140931e-03 0.715767
2019-12-03 23:28:26,504 training loss; R2: 8.196158e-03 0.717687
2019-12-03 23:28:26,632 valid 000 5.864860e-03 0.826703
2019-12-03 23:28:27,784 validation loss; R2: 4.261060e-03 0.849066
2019-12-03 23:28:27,803 epoch 5 lr 2.000000e-03
2019-12-03 23:28:28,100 train 000 6.762424e-03 0.709680
2019-12-03 23:28:38,885 train 050 7.200987e-03 0.766791
2019-12-03 23:28:49,685 train 100 7.635829e-03 0.747657
2019-12-03 23:29:00,495 train 150 7.534389e-03 0.751501
2019-12-03 23:29:11,294 train 200 7.417752e-03 0.753783
2019-12-03 23:29:16,146 training loss; R2: 7.317221e-03 0.756395
2019-12-03 23:29:16,284 valid 000 8.011274e-03 0.859079
2019-12-03 23:29:17,437 validation loss; R2: 4.057460e-03 0.863546
2019-12-03 23:29:17,456 epoch 6 lr 2.000000e-03
2019-12-03 23:29:17,753 train 000 5.444856e-03 0.738931
2019-12-03 23:29:28,550 train 050 7.196198e-03 0.779532
2019-12-03 23:29:39,341 train 100 7.164779e-03 0.774761
2019-12-03 23:29:50,131 train 150 7.152970e-03 0.771763
2019-12-03 23:30:00,919 train 200 7.137183e-03 0.761089
2019-12-03 23:30:05,771 training loss; R2: 7.102958e-03 0.761855
2019-12-03 23:30:05,903 valid 000 3.665420e-03 0.594377
2019-12-03 23:30:07,056 validation loss; R2: 6.812305e-03 0.758374
2019-12-03 23:30:07,075 epoch 7 lr 2.000000e-03
2019-12-03 23:30:07,372 train 000 4.900450e-03 0.828637
2019-12-03 23:30:18,159 train 050 6.260793e-03 0.781517
2019-12-03 23:30:28,942 train 100 6.586718e-03 0.775124
2019-12-03 23:30:39,727 train 150 6.770424e-03 0.766521
2019-12-03 23:30:50,510 train 200 6.793786e-03 0.767841
2019-12-03 23:30:55,360 training loss; R2: 6.726092e-03 0.771946
2019-12-03 23:30:55,486 valid 000 3.231246e-03 0.863748
2019-12-03 23:30:56,638 validation loss; R2: 5.422850e-03 0.801923
2019-12-03 23:30:56,657 epoch 8 lr 2.000000e-03
2019-12-03 23:30:56,953 train 000 4.294655e-03 0.889551
2019-12-03 23:31:07,735 train 050 6.753856e-03 0.773120
2019-12-03 23:31:18,515 train 100 6.612478e-03 0.777169
2019-12-03 23:31:29,293 train 150 6.646911e-03 0.779876
2019-12-03 23:31:40,072 train 200 6.595008e-03 0.782811
2019-12-03 23:31:44,918 training loss; R2: 6.511720e-03 0.782966
2019-12-03 23:31:45,045 valid 000 3.194309e-03 0.930100
2019-12-03 23:31:46,197 validation loss; R2: 3.464807e-03 0.877893
2019-12-03 23:31:46,216 epoch 9 lr 2.000000e-03
2019-12-03 23:31:46,512 train 000 4.412382e-03 0.839417
2019-12-03 23:31:57,297 train 050 6.466932e-03 0.796584
2019-12-03 23:32:08,083 train 100 6.556436e-03 0.785044
2019-12-03 23:32:18,867 train 150 6.372454e-03 0.784016
2019-12-03 23:32:29,652 train 200 6.305282e-03 0.784495
2019-12-03 23:32:34,502 training loss; R2: 6.195424e-03 0.788407
2019-12-03 23:32:34,628 valid 000 2.460014e-03 0.941557
2019-12-03 23:32:35,780 validation loss; R2: 3.839179e-03 0.866470
2019-12-03 23:32:35,799 epoch 10 lr 2.000000e-03
2019-12-03 23:32:36,095 train 000 1.004691e-02 0.816742
2019-12-03 23:32:46,872 train 050 5.884549e-03 0.800106
2019-12-03 23:32:57,647 train 100 6.020138e-03 0.795659
2019-12-03 23:33:08,421 train 150 6.138012e-03 0.793135
2019-12-03 23:33:19,194 train 200 5.987556e-03 0.796868
2019-12-03 23:33:24,039 training loss; R2: 6.069555e-03 0.795605
2019-12-03 23:33:24,167 valid 000 1.006148e-01 -1.142358
2019-12-03 23:33:25,318 validation loss; R2: 9.111900e-02 -2.370924
2019-12-03 23:33:25,336 epoch 11 lr 2.000000e-03
2019-12-03 23:33:25,633 train 000 8.179261e-03 0.619483
2019-12-03 23:33:36,410 train 050 6.377292e-03 0.774703
2019-12-03 23:33:47,177 train 100 5.861773e-03 0.795346
2019-12-03 23:33:57,950 train 150 5.738995e-03 0.801940
2019-12-03 23:34:08,728 train 200 5.736425e-03 0.805022
2019-12-03 23:34:13,570 training loss; R2: 5.752987e-03 0.804919
2019-12-03 23:34:13,699 valid 000 8.485673e-01 -28.271793
2019-12-03 23:34:14,850 validation loss; R2: 8.958173e-01 -32.059646
2019-12-03 23:34:14,869 epoch 12 lr 2.000000e-03
2019-12-03 23:34:15,162 train 000 6.187193e-03 0.781559
2019-12-03 23:34:25,937 train 050 5.586636e-03 0.791133
2019-12-03 23:34:36,715 train 100 5.463782e-03 0.798318
2019-12-03 23:34:47,492 train 150 5.401833e-03 0.807189
2019-12-03 23:34:58,266 train 200 5.520712e-03 0.809946
2019-12-03 23:35:03,108 training loss; R2: 5.554705e-03 0.809405
2019-12-03 23:35:03,235 valid 000 5.106840e-02 -1.012350
2019-12-03 23:35:04,388 validation loss; R2: 5.822506e-02 -1.034477
2019-12-03 23:35:04,406 epoch 13 lr 2.000000e-03
2019-12-03 23:35:04,704 train 000 3.703328e-03 0.711648
2019-12-03 23:35:15,471 train 050 5.780072e-03 0.812111
2019-12-03 23:35:26,241 train 100 5.617709e-03 0.810547
2019-12-03 23:35:37,013 train 150 5.354487e-03 0.818718
2019-12-03 23:35:47,781 train 200 5.328267e-03 0.820956
2019-12-03 23:35:52,623 training loss; R2: 5.261900e-03 0.821675
2019-12-03 23:35:52,754 valid 000 4.338952e-02 -0.191520
2019-12-03 23:35:53,905 validation loss; R2: 3.711670e-02 -0.238374
2019-12-03 23:35:53,924 epoch 14 lr 2.000000e-03
2019-12-03 23:35:54,221 train 000 2.616484e-03 0.893180
2019-12-03 23:36:04,985 train 050 5.132801e-03 0.817097
2019-12-03 23:36:15,745 train 100 5.446411e-03 0.819530
2019-12-03 23:36:26,510 train 150 5.334219e-03 0.823516
2019-12-03 23:36:37,269 train 200 5.337735e-03 0.821408
2019-12-03 23:36:42,108 training loss; R2: 5.364845e-03 0.819092
2019-12-03 23:36:42,236 valid 000 3.881982e-01 -16.001065
2019-12-03 23:36:43,389 validation loss; R2: 3.840583e-01 -12.957085
2019-12-03 23:36:43,408 epoch 15 lr 2.000000e-03
2019-12-03 23:36:43,712 train 000 3.531400e-03 0.919687
2019-12-03 23:36:54,475 train 050 5.170818e-03 0.813233
2019-12-03 23:37:05,239 train 100 4.791569e-03 0.827386
2019-12-03 23:37:15,994 train 150 4.861910e-03 0.828124
2019-12-03 23:37:26,750 train 200 5.000101e-03 0.824703
2019-12-03 23:37:31,584 training loss; R2: 5.166858e-03 0.818998
2019-12-03 23:37:31,719 valid 000 3.007057e-02 -0.542868
2019-12-03 23:37:32,870 validation loss; R2: 4.800124e-02 -0.553349
2019-12-03 23:37:32,889 epoch 16 lr 2.000000e-03
2019-12-03 23:37:33,189 train 000 6.615415e-03 0.856169
2019-12-03 23:37:43,947 train 050 5.144250e-03 0.836764
2019-12-03 23:37:54,698 train 100 5.083485e-03 0.824832
2019-12-03 23:38:05,450 train 150 4.971570e-03 0.831317
2019-12-03 23:38:16,201 train 200 5.060192e-03 0.824307
2019-12-03 23:38:21,035 training loss; R2: 5.085610e-03 0.823529
2019-12-03 23:38:21,168 valid 000 1.351297e+00 -66.681925
2019-12-03 23:38:22,320 validation loss; R2: 1.344012e+00 -47.852252
2019-12-03 23:38:22,338 epoch 17 lr 2.000000e-03
2019-12-03 23:38:22,636 train 000 5.547426e-03 0.893891
2019-12-03 23:38:33,384 train 050 5.499472e-03 0.815991
2019-12-03 23:38:44,127 train 100 5.254419e-03 0.817712
2019-12-03 23:38:54,865 train 150 5.306375e-03 0.817126
2019-12-03 23:39:05,610 train 200 5.505081e-03 0.811945
2019-12-03 23:39:10,440 training loss; R2: 5.452700e-03 0.814176
2019-12-03 23:39:10,568 valid 000 1.559760e-01 -5.402780
2019-12-03 23:39:11,720 validation loss; R2: 1.608201e-01 -5.316675
2019-12-03 23:39:11,738 epoch 18 lr 2.000000e-03
2019-12-03 23:39:12,037 train 000 2.570227e-03 0.899604
2019-12-03 23:39:22,781 train 050 5.559838e-03 0.808650
2019-12-03 23:39:33,527 train 100 5.308962e-03 0.817723
2019-12-03 23:39:44,267 train 150 5.168027e-03 0.822378
2019-12-03 23:39:55,005 train 200 4.953230e-03 0.828388
2019-12-03 23:39:59,835 training loss; R2: 5.049965e-03 0.828042
2019-12-03 23:39:59,964 valid 000 5.510072e-01 -11.225448
2019-12-03 23:40:01,115 validation loss; R2: 5.184620e-01 -17.496576
2019-12-03 23:40:01,134 epoch 19 lr 2.000000e-03
2019-12-03 23:40:01,430 train 000 3.831232e-03 0.827640
2019-12-03 23:40:12,164 train 050 5.120693e-03 0.834569
2019-12-03 23:40:22,895 train 100 5.250508e-03 0.821881
2019-12-03 23:40:33,626 train 150 5.284018e-03 0.822053
2019-12-03 23:40:44,346 train 200 5.147084e-03 0.824759
2019-12-03 23:40:49,167 training loss; R2: 5.096938e-03 0.827156
2019-12-03 23:40:49,307 valid 000 2.071388e+00 -86.056734
2019-12-03 23:40:50,459 validation loss; R2: 2.125395e+00 -75.274241
