2019-11-12 14:15:51,535 gpu device = 1
2019-11-12 14:15:51,536 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name='galaxy-zoo-cleaned', gpu=1, grad_clip=5, gz_dtree=False, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-galaxy-zoo-EXP-20191112-141551', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-11-12 14:16:03,012 param size = 0.275317MB
2019-11-12 14:16:03,016 epoch 0 lr 1.000000e-03
2019-11-12 14:16:05,204 train 000 2.005978e-01 -40.501758
2019-11-12 14:16:12,168 train 050 4.268738e-02 -3.665910
2019-11-12 14:16:18,889 train 100 3.430510e-02 -2.119268
2019-11-12 14:16:25,506 train 150 3.096876e-02 -1.496482
2019-11-12 14:16:32,044 train 200 2.896986e-02 -1.168454
2019-11-12 14:16:38,732 train 250 2.759766e-02 -0.945893
2019-11-12 14:16:45,344 train 300 2.653118e-02 -0.826182
2019-11-12 14:16:52,013 train 350 2.563511e-02 -0.710565
2019-11-12 14:16:58,657 train 400 2.495451e-02 -0.621953
2019-11-12 14:17:05,345 train 450 2.431190e-02 -0.551170
2019-11-12 14:17:11,982 train 500 2.376914e-02 -0.489348
2019-11-12 14:17:18,579 train 550 2.331666e-02 -0.438699
2019-11-12 14:17:25,125 train 600 2.297117e-02 -0.398079
2019-11-12 14:17:31,844 train 650 2.264854e-02 -0.362411
2019-11-12 14:17:38,730 train 700 2.230970e-02 -0.329369
2019-11-12 14:17:45,523 train 750 2.201928e-02 -0.301029
2019-11-12 14:17:52,098 train 800 2.177303e-02 -0.274589
2019-11-12 14:17:58,622 train 850 2.151345e-02 -0.250965
2019-11-12 14:18:01,350 training loss; R2: 2.143859e-02 -0.244303
2019-11-12 14:18:01,613 valid 000 1.973159e-02 0.074544
2019-11-12 14:18:03,293 valid 050 1.788297e-02 0.025205
2019-11-12 14:18:04,893 validation loss; R2: 1.771787e-02 0.027142
2019-11-12 14:18:04,914 epoch 1 lr 1.000000e-03
2019-11-12 14:18:05,463 train 000 1.563451e-02 0.090710
2019-11-12 14:18:12,068 train 050 1.671485e-02 0.148960
2019-11-12 14:18:18,798 train 100 1.672572e-02 0.155134
2019-11-12 14:18:25,439 train 150 1.658627e-02 0.153517
2019-11-12 14:18:32,113 train 200 1.639559e-02 0.127294
2019-11-12 14:18:38,569 train 250 1.637149e-02 0.120441
2019-11-12 14:18:44,996 train 300 1.625315e-02 0.129990
2019-11-12 14:18:51,434 train 350 1.615762e-02 0.135532
2019-11-12 14:18:57,862 train 400 1.605745e-02 0.138570
2019-11-12 14:19:04,299 train 450 1.596771e-02 0.142166
2019-11-12 14:19:10,728 train 500 1.588601e-02 0.147292
2019-11-12 14:19:17,161 train 550 1.581452e-02 0.147936
2019-11-12 14:19:23,585 train 600 1.572210e-02 0.152355
2019-11-12 14:19:30,011 train 650 1.564328e-02 0.157051
2019-11-12 14:19:36,444 train 700 1.559081e-02 0.157068
2019-11-12 14:19:42,879 train 750 1.552071e-02 0.154677
2019-11-12 14:19:49,313 train 800 1.541513e-02 0.133756
2019-11-12 14:19:55,742 train 850 1.532993e-02 0.127745
2019-11-12 14:19:57,667 training loss; R2: 1.531446e-02 0.129546
2019-11-12 14:19:57,967 valid 000 1.113531e-02 0.316380
2019-11-12 14:19:59,633 valid 050 1.351590e-02 0.184065
2019-11-12 14:20:01,161 validation loss; R2: 1.338080e-02 0.197012
2019-11-12 14:20:01,177 epoch 2 lr 1.000000e-03
2019-11-12 14:20:01,560 train 000 1.175648e-02 -0.397105
2019-11-12 14:20:08,003 train 050 1.381470e-02 0.107272
2019-11-12 14:20:14,441 train 100 1.389627e-02 0.134394
2019-11-12 14:20:20,880 train 150 1.387766e-02 0.171040
2019-11-12 14:20:27,316 train 200 1.377609e-02 0.167468
2019-11-12 14:20:33,748 train 250 1.371614e-02 0.150904
2019-11-12 14:20:40,176 train 300 1.369473e-02 0.165612
2019-11-12 14:20:46,608 train 350 1.363580e-02 0.177776
2019-11-12 14:20:53,044 train 400 1.355621e-02 0.184923
2019-11-12 14:20:59,480 train 450 1.348716e-02 0.194320
2019-11-12 14:21:05,920 train 500 1.344278e-02 0.196096
2019-11-12 14:21:12,425 train 550 1.340374e-02 0.200171
2019-11-12 14:21:18,864 train 600 1.338641e-02 0.200364
2019-11-12 14:21:25,302 train 650 1.334690e-02 0.206082
2019-11-12 14:21:31,922 train 700 1.331095e-02 0.205759
2019-11-12 14:21:38,356 train 750 1.327483e-02 0.209828
2019-11-12 14:21:44,803 train 800 1.322129e-02 0.213744
2019-11-12 14:21:51,246 train 850 1.319534e-02 0.215779
2019-11-12 14:21:53,173 training loss; R2: 1.318367e-02 0.216782
2019-11-12 14:21:53,452 valid 000 1.170699e-02 0.283556
2019-11-12 14:21:55,179 valid 050 1.233974e-02 0.294162
2019-11-12 14:21:56,733 validation loss; R2: 1.231153e-02 0.247975
2019-11-12 14:21:56,755 epoch 3 lr 1.000000e-03
2019-11-12 14:21:57,121 train 000 1.070680e-02 0.416531
2019-11-12 14:22:03,819 train 050 1.253805e-02 0.287187
2019-11-12 14:22:10,372 train 100 1.248890e-02 0.283395
2019-11-12 14:22:16,819 train 150 1.244563e-02 0.277900
2019-11-12 14:22:23,253 train 200 1.245513e-02 0.266727
2019-11-12 14:22:29,697 train 250 1.244103e-02 0.265359
2019-11-12 14:22:36,132 train 300 1.242699e-02 0.267849
2019-11-12 14:22:42,565 train 350 1.237739e-02 0.270303
2019-11-12 14:22:49,004 train 400 1.236014e-02 0.272894
2019-11-12 14:22:55,438 train 450 1.236758e-02 0.272595
2019-11-12 14:23:01,875 train 500 1.233921e-02 0.267672
2019-11-12 14:23:08,310 train 550 1.231866e-02 0.259205
2019-11-12 14:23:14,749 train 600 1.229831e-02 0.259152
2019-11-12 14:23:21,184 train 650 1.230747e-02 0.261395
2019-11-12 14:23:27,616 train 700 1.228690e-02 0.261532
2019-11-12 14:23:34,052 train 750 1.228884e-02 0.255996
2019-11-12 14:23:40,487 train 800 1.226562e-02 0.257216
2019-11-12 14:23:46,928 train 850 1.225822e-02 0.260532
2019-11-12 14:23:48,856 training loss; R2: 1.224425e-02 0.260461
2019-11-12 14:23:49,124 valid 000 1.102941e-02 0.406572
2019-11-12 14:23:50,801 valid 050 1.143556e-02 0.339062
2019-11-12 14:23:52,351 validation loss; R2: 1.157002e-02 0.330242
2019-11-12 14:23:52,371 epoch 4 lr 1.000000e-03
2019-11-12 14:23:52,755 train 000 1.118728e-02 0.383963
2019-11-12 14:23:59,610 train 050 1.195933e-02 0.315322
2019-11-12 14:24:06,102 train 100 1.204138e-02 0.311982
2019-11-12 14:24:12,625 train 150 1.186297e-02 0.303724
2019-11-12 14:24:19,095 train 200 1.184629e-02 0.304728
2019-11-12 14:24:25,639 train 250 1.181311e-02 0.298893
2019-11-12 14:24:32,115 train 300 1.184041e-02 0.299749
2019-11-12 14:24:38,592 train 350 1.184558e-02 0.256869
2019-11-12 14:24:45,073 train 400 1.182566e-02 0.262910
2019-11-12 14:24:51,744 train 450 1.183331e-02 0.263045
2019-11-12 14:24:58,219 train 500 1.182300e-02 0.264152
2019-11-12 14:25:04,827 train 550 1.176003e-02 0.269922
2019-11-12 14:25:11,671 train 600 1.173489e-02 0.273802
2019-11-12 14:25:18,178 train 650 1.170732e-02 0.274119
2019-11-12 14:25:24,682 train 700 1.165447e-02 0.275686
2019-11-12 14:25:31,170 train 750 1.165168e-02 0.276131
2019-11-12 14:25:37,644 train 800 1.163677e-02 0.270443
2019-11-12 14:25:44,187 train 850 1.161691e-02 0.273826
2019-11-12 14:25:46,122 training loss; R2: 1.161873e-02 0.275025
2019-11-12 14:25:46,398 valid 000 3.218637e-02 -0.429317
2019-11-12 14:25:48,115 valid 050 3.161385e-02 -0.652865
2019-11-12 14:25:49,663 validation loss; R2: 3.209010e-02 -0.658222
2019-11-12 14:25:49,678 epoch 5 lr 1.000000e-03
2019-11-12 14:25:50,049 train 000 9.342318e-03 0.377424
2019-11-12 14:25:56,868 train 050 1.117733e-02 0.317896
2019-11-12 14:26:03,576 train 100 1.124646e-02 0.313307
2019-11-12 14:26:10,156 train 150 1.120236e-02 0.302594
2019-11-12 14:26:17,021 train 200 1.118530e-02 0.303368
2019-11-12 14:26:23,637 train 250 1.118922e-02 0.308525
2019-11-12 14:26:30,313 train 300 1.126000e-02 0.309185
2019-11-12 14:26:36,884 train 350 1.127103e-02 0.312570
2019-11-12 14:26:43,455 train 400 1.128423e-02 0.306236
2019-11-12 14:26:50,014 train 450 1.127734e-02 0.297387
2019-11-12 14:26:56,619 train 500 1.127247e-02 0.300223
2019-11-12 14:27:03,120 train 550 1.126055e-02 0.300645
2019-11-12 14:27:09,836 train 600 1.123724e-02 0.300211
2019-11-12 14:27:16,478 train 650 1.124952e-02 0.297322
2019-11-12 14:27:23,023 train 700 1.123290e-02 0.298744
2019-11-12 14:27:29,553 train 750 1.123933e-02 0.298418
2019-11-12 14:27:36,170 train 800 1.121484e-02 0.301188
2019-11-12 14:27:42,702 train 850 1.123259e-02 0.302286
2019-11-12 14:27:44,641 training loss; R2: 1.122598e-02 0.302562
2019-11-12 14:27:44,925 valid 000 1.891757e-01 -10.704744
2019-11-12 14:27:46,649 valid 050 2.005938e-01 -8.124429
2019-11-12 14:27:48,215 validation loss; R2: 2.009196e-01 -8.077012
2019-11-12 14:27:48,235 epoch 6 lr 1.000000e-03
2019-11-12 14:27:48,650 train 000 9.585206e-03 0.358693
2019-11-12 14:27:55,431 train 050 1.081723e-02 0.347158
2019-11-12 14:28:02,144 train 100 1.098361e-02 0.344471
2019-11-12 14:28:08,851 train 150 1.092882e-02 0.334656
2019-11-12 14:28:15,557 train 200 1.094871e-02 0.330748
2019-11-12 14:28:22,252 train 250 1.093351e-02 0.321793
2019-11-12 14:28:28,951 train 300 1.096313e-02 0.322530
2019-11-12 14:28:35,665 train 350 1.090532e-02 0.326126
2019-11-12 14:28:42,362 train 400 1.087593e-02 0.325790
2019-11-12 14:28:49,059 train 450 1.088419e-02 0.323491
2019-11-12 14:28:55,774 train 500 1.090384e-02 0.321649
2019-11-12 14:29:02,475 train 550 1.089296e-02 0.320139
2019-11-12 14:29:09,183 train 600 1.093197e-02 0.320438
2019-11-12 14:29:15,879 train 650 1.091038e-02 0.313076
2019-11-12 14:29:22,569 train 700 1.090590e-02 0.315960
2019-11-12 14:29:29,275 train 750 1.090294e-02 0.317954
2019-11-12 14:29:35,973 train 800 1.090723e-02 0.317666
2019-11-12 14:29:42,664 train 850 1.090922e-02 0.317299
2019-11-12 14:29:44,667 training loss; R2: 1.090296e-02 0.318001
2019-11-12 14:29:44,949 valid 000 5.564432e-01 -17.407304
2019-11-12 14:29:46,626 valid 050 5.388193e-01 -22.103262
2019-11-12 14:29:48,150 validation loss; R2: 5.405676e-01 -22.929981
2019-11-12 14:29:48,177 epoch 7 lr 1.000000e-03
2019-11-12 14:29:48,561 train 000 1.034908e-02 0.428652
2019-11-12 14:29:55,384 train 050 1.098371e-02 0.346682
2019-11-12 14:30:02,152 train 100 1.086369e-02 0.346526
2019-11-12 14:30:08,875 train 150 1.085157e-02 0.335223
2019-11-12 14:30:15,608 train 200 1.084949e-02 0.337569
2019-11-12 14:30:22,290 train 250 1.077033e-02 0.343234
2019-11-12 14:30:28,979 train 300 1.078455e-02 0.339455
2019-11-12 14:30:35,540 train 350 1.077716e-02 0.339072
2019-11-12 14:30:42,054 train 400 1.080601e-02 0.338077
2019-11-12 14:30:48,738 train 450 1.079271e-02 0.290886
2019-11-12 14:30:55,366 train 500 1.077488e-02 0.291734
2019-11-12 14:31:01,914 train 550 1.078112e-02 0.194286
2019-11-12 14:31:08,660 train 600 1.074643e-02 0.203584
2019-11-12 14:31:15,458 train 650 1.073324e-02 0.212833
2019-11-12 14:31:22,206 train 700 1.072815e-02 0.222043
2019-11-12 14:31:28,924 train 750 1.071348e-02 0.229980
2019-11-12 14:31:35,639 train 800 1.070939e-02 0.236477
2019-11-12 14:31:42,385 train 850 1.070664e-02 0.242968
2019-11-12 14:31:44,404 training loss; R2: 1.071099e-02 0.245110
2019-11-12 14:31:44,678 valid 000 8.637593e-02 -4.176341
2019-11-12 14:31:46,394 valid 050 8.407765e-02 -3.590402
2019-11-12 14:31:47,926 validation loss; R2: 8.372260e-02 -3.712863
2019-11-12 14:31:47,947 epoch 8 lr 1.000000e-03
2019-11-12 14:31:48,335 train 000 9.843112e-03 0.065742
2019-11-12 14:31:55,142 train 050 1.038727e-02 0.180856
2019-11-12 14:32:01,757 train 100 1.053673e-02 0.251537
2019-11-12 14:32:08,475 train 150 1.052256e-02 0.280064
2019-11-12 14:32:15,164 train 200 1.057021e-02 0.294509
2019-11-12 14:32:21,599 train 250 1.064502e-02 0.304797
2019-11-12 14:32:28,028 train 300 1.060405e-02 0.301160
2019-11-12 14:32:34,458 train 350 1.057579e-02 0.299434
2019-11-12 14:32:40,885 train 400 1.056707e-02 0.302626
2019-11-12 14:32:47,327 train 450 1.060584e-02 0.305815
2019-11-12 14:32:53,756 train 500 1.058912e-02 0.308566
2019-11-12 14:33:00,184 train 550 1.058832e-02 0.311589
2019-11-12 14:33:06,605 train 600 1.058491e-02 0.312792
2019-11-12 14:33:13,034 train 650 1.057040e-02 0.312855
2019-11-12 14:33:19,478 train 700 1.056485e-02 0.314187
2019-11-12 14:33:25,906 train 750 1.053731e-02 0.317452
2019-11-12 14:33:32,333 train 800 1.052488e-02 0.318026
2019-11-12 14:33:38,752 train 850 1.052469e-02 0.317845
2019-11-12 14:33:40,671 training loss; R2: 1.052821e-02 0.317934
2019-11-12 14:33:40,953 valid 000 2.568953e-02 -0.281078
2019-11-12 14:33:42,640 valid 050 2.644962e-02 -0.676429
2019-11-12 14:33:44,154 validation loss; R2: 2.634313e-02 -0.726235
2019-11-12 14:33:44,171 epoch 9 lr 1.000000e-03
2019-11-12 14:33:44,584 train 000 9.722181e-03 0.406238
2019-11-12 14:33:51,036 train 050 1.041207e-02 0.360540
2019-11-12 14:33:57,480 train 100 1.036436e-02 0.345653
2019-11-12 14:34:03,919 train 150 1.034080e-02 0.343205
2019-11-12 14:34:10,361 train 200 1.028365e-02 0.347658
2019-11-12 14:34:16,798 train 250 1.030247e-02 0.340829
2019-11-12 14:34:23,236 train 300 1.029028e-02 0.342141
2019-11-12 14:34:29,671 train 350 1.026220e-02 0.341653
2019-11-12 14:34:36,109 train 400 1.027839e-02 0.345490
2019-11-12 14:34:42,544 train 450 1.028236e-02 0.346199
2019-11-12 14:34:48,981 train 500 1.028110e-02 0.341479
2019-11-12 14:34:55,443 train 550 1.029581e-02 0.342591
2019-11-12 14:35:01,890 train 600 1.032729e-02 0.342431
2019-11-12 14:35:08,348 train 650 1.035498e-02 0.335626
2019-11-12 14:35:14,798 train 700 1.033741e-02 0.336315
2019-11-12 14:35:21,252 train 750 1.036491e-02 0.337228
2019-11-12 14:35:27,697 train 800 1.035403e-02 0.338284
2019-11-12 14:35:34,480 train 850 1.033362e-02 0.337766
2019-11-12 14:35:36,490 training loss; R2: 1.033987e-02 0.338081
2019-11-12 14:35:36,755 valid 000 6.583069e-02 -26.813979
2019-11-12 14:35:38,447 valid 050 6.496576e-02 -9.037753
2019-11-12 14:35:39,975 validation loss; R2: 6.484197e-02 -10.280377
2019-11-12 14:35:39,991 epoch 10 lr 1.000000e-03
2019-11-12 14:35:40,350 train 000 1.042854e-02 0.317984
2019-11-12 14:35:47,004 train 050 1.031228e-02 0.345798
2019-11-12 14:35:53,764 train 100 1.017820e-02 0.347727
2019-11-12 14:36:00,607 train 150 1.015597e-02 0.350840
2019-11-12 14:36:07,072 train 200 1.010650e-02 0.346677
2019-11-12 14:36:13,518 train 250 1.017235e-02 0.337393
2019-11-12 14:36:19,979 train 300 1.016316e-02 0.316919
2019-11-12 14:36:26,424 train 350 1.018710e-02 0.319969
2019-11-12 14:36:32,884 train 400 1.018242e-02 0.323964
2019-11-12 14:36:39,696 train 450 1.023777e-02 0.326484
2019-11-12 14:36:46,333 train 500 1.026181e-02 0.323728
2019-11-12 14:36:53,111 train 550 1.028466e-02 0.326053
2019-11-12 14:36:59,841 train 600 1.027362e-02 0.328724
2019-11-12 14:37:06,290 train 650 1.027648e-02 0.326009
2019-11-12 14:37:12,740 train 700 1.026577e-02 0.325761
2019-11-12 14:37:19,184 train 750 1.024751e-02 0.326846
2019-11-12 14:37:25,720 train 800 1.023855e-02 0.329865
2019-11-12 14:37:32,167 train 850 1.023861e-02 0.330554
2019-11-12 14:37:34,104 training loss; R2: 1.023730e-02 0.330063
2019-11-12 14:37:34,395 valid 000 2.143180e-02 -0.246485
2019-11-12 14:37:36,079 valid 050 1.947196e-02 -0.582691
2019-11-12 14:37:37,627 validation loss; R2: 1.937107e-02 -0.503020
2019-11-12 14:37:37,642 epoch 11 lr 1.000000e-03
2019-11-12 14:37:38,038 train 000 9.783258e-03 0.378416
2019-11-12 14:37:44,801 train 050 1.040601e-02 0.336344
2019-11-12 14:37:51,437 train 100 1.036946e-02 0.345121
2019-11-12 14:37:58,129 train 150 1.037055e-02 0.343078
2019-11-12 14:38:04,878 train 200 1.040829e-02 0.335616
2019-11-12 14:38:11,410 train 250 1.031707e-02 0.334078
2019-11-12 14:38:18,103 train 300 1.024767e-02 0.339366
2019-11-12 14:38:24,909 train 350 1.022592e-02 0.334701
2019-11-12 14:38:31,667 train 400 1.024090e-02 0.337601
2019-11-12 14:38:38,403 train 450 1.021773e-02 0.329509
2019-11-12 14:38:45,199 train 500 1.017499e-02 0.322559
2019-11-12 14:38:52,007 train 550 1.018769e-02 0.317993
2019-11-12 14:38:58,818 train 600 1.016546e-02 0.316693
2019-11-12 14:39:05,396 train 650 1.015436e-02 0.318639
2019-11-12 14:39:12,007 train 700 1.014057e-02 0.320428
2019-11-12 14:39:18,640 train 750 1.014407e-02 0.242685
2019-11-12 14:39:25,083 train 800 1.014634e-02 0.249637
2019-11-12 14:39:31,691 train 850 1.014585e-02 0.256761
2019-11-12 14:39:33,614 training loss; R2: 1.013761e-02 0.258643
2019-11-12 14:39:33,920 valid 000 3.563565e-02 -2.080540
2019-11-12 14:39:35,618 valid 050 3.207335e-02 -2.103832
2019-11-12 14:39:37,149 validation loss; R2: 3.203796e-02 -2.142607
2019-11-12 14:39:37,165 epoch 12 lr 1.000000e-03
2019-11-12 14:39:37,543 train 000 9.297105e-03 0.360152
2019-11-12 14:39:44,444 train 050 9.931619e-03 0.342607
2019-11-12 14:39:51,313 train 100 1.000289e-02 0.349227
2019-11-12 14:39:57,911 train 150 1.010337e-02 0.344198
2019-11-12 14:40:04,523 train 200 1.006114e-02 0.351707
2019-11-12 14:40:11,117 train 250 1.002258e-02 0.351992
2019-11-12 14:40:17,989 train 300 1.001057e-02 0.349144
2019-11-12 14:40:24,577 train 350 9.960586e-03 0.350943
2019-11-12 14:40:31,075 train 400 9.963219e-03 0.353490
2019-11-12 14:40:37,833 train 450 9.950132e-03 0.357370
2019-11-12 14:40:44,419 train 500 9.961221e-03 0.356241
2019-11-12 14:40:51,113 train 550 9.972023e-03 0.357818
2019-11-12 14:40:57,736 train 600 9.988020e-03 0.356000
2019-11-12 14:41:04,239 train 650 9.975724e-03 0.289413
2019-11-12 14:41:10,746 train 700 9.964388e-03 0.290940
2019-11-12 14:41:17,437 train 750 9.950278e-03 0.296540
2019-11-12 14:41:24,097 train 800 9.950303e-03 0.297811
2019-11-12 14:41:30,841 train 850 9.948212e-03 0.294266
2019-11-12 14:41:32,841 training loss; R2: 9.953190e-03 0.295994
2019-11-12 14:41:33,133 valid 000 7.302770e-02 -2.169878
2019-11-12 14:41:34,811 valid 050 7.506962e-02 -3.286014
2019-11-12 14:41:36,328 validation loss; R2: 7.483770e-02 -4.003309
2019-11-12 14:41:36,349 epoch 13 lr 1.000000e-03
2019-11-12 14:41:36,792 train 000 1.079245e-02 0.450034
2019-11-12 14:41:43,395 train 050 1.007359e-02 0.378353
2019-11-12 14:41:50,073 train 100 9.904208e-03 0.375538
2019-11-12 14:41:56,556 train 150 9.870202e-03 0.376838
2019-11-12 14:42:03,019 train 200 9.963793e-03 0.362986
2019-11-12 14:42:09,483 train 250 9.967420e-03 0.367018
2019-11-12 14:42:16,065 train 300 9.979863e-03 0.366112
2019-11-12 14:42:22,530 train 350 9.929260e-03 0.365905
2019-11-12 14:42:29,013 train 400 9.913907e-03 0.368967
2019-11-12 14:42:35,699 train 450 9.894549e-03 0.365506
2019-11-12 14:42:42,157 train 500 9.919858e-03 0.364300
2019-11-12 14:42:48,611 train 550 9.918799e-03 0.364682
2019-11-12 14:42:55,064 train 600 9.909591e-03 0.363670
2019-11-12 14:43:01,516 train 650 9.911688e-03 0.362748
2019-11-12 14:43:07,960 train 700 9.929255e-03 0.360340
2019-11-12 14:43:14,572 train 750 9.914785e-03 0.355047
2019-11-12 14:43:21,026 train 800 9.911119e-03 0.351331
2019-11-12 14:43:27,468 train 850 9.904066e-03 0.346634
2019-11-12 14:43:29,396 training loss; R2: 9.903677e-03 0.346396
2019-11-12 14:43:29,692 valid 000 1.872476e-01 -6.708794
2019-11-12 14:43:31,408 valid 050 1.903659e-01 -8.918533
2019-11-12 14:43:32,967 validation loss; R2: 1.894082e-01 -8.462174
2019-11-12 14:43:32,989 epoch 14 lr 1.000000e-03
2019-11-12 14:43:33,386 train 000 8.538942e-03 0.237561
2019-11-12 14:43:40,128 train 050 9.989276e-03 0.253595
2019-11-12 14:43:46,843 train 100 1.000591e-02 0.263790
2019-11-12 14:43:53,539 train 150 9.943031e-03 0.301452
2019-11-12 14:44:00,232 train 200 9.937274e-03 0.314030
2019-11-12 14:44:06,912 train 250 9.900836e-03 0.325324
2019-11-12 14:44:13,598 train 300 9.879743e-03 0.331315
2019-11-12 14:44:20,281 train 350 9.885154e-03 0.338833
2019-11-12 14:44:26,964 train 400 9.888871e-03 0.335780
2019-11-12 14:44:33,652 train 450 9.856222e-03 0.245674
2019-11-12 14:44:40,327 train 500 9.835902e-03 0.258557
2019-11-12 14:44:47,040 train 550 9.860514e-03 0.261892
2019-11-12 14:44:53,747 train 600 9.865952e-03 0.270549
2019-11-12 14:45:00,429 train 650 9.856951e-03 0.279615
2019-11-12 14:45:07,110 train 700 9.851114e-03 0.286326
2019-11-12 14:45:13,792 train 750 9.860945e-03 0.288838
2019-11-12 14:45:20,479 train 800 9.859414e-03 0.293016
2019-11-12 14:45:27,167 train 850 9.837430e-03 0.295069
2019-11-12 14:45:29,175 training loss; R2: 9.834373e-03 0.296485
2019-11-12 14:45:29,462 valid 000 1.482710e+00 -87.821269
2019-11-12 14:45:31,138 valid 050 1.496027e+00 -117.829217
2019-11-12 14:45:32,668 validation loss; R2: 1.493126e+00 -127.707235
2019-11-12 14:45:32,690 epoch 15 lr 1.000000e-03
2019-11-12 14:45:33,086 train 000 8.287665e-03 0.382883
2019-11-12 14:45:39,772 train 050 9.460587e-03 0.350389
2019-11-12 14:45:46,525 train 100 9.534759e-03 0.359667
2019-11-12 14:45:53,032 train 150 9.546787e-03 0.346098
2019-11-12 14:45:59,517 train 200 9.650886e-03 0.354807
2019-11-12 14:46:06,124 train 250 9.685868e-03 -0.511046
2019-11-12 14:46:12,970 train 300 9.715114e-03 -0.375487
2019-11-12 14:46:19,742 train 350 9.725204e-03 -0.272741
2019-11-12 14:46:26,670 train 400 9.737885e-03 -0.192602
2019-11-12 14:46:33,487 train 450 9.752149e-03 -0.136198
2019-11-12 14:46:40,315 train 500 9.750282e-03 -0.087654
2019-11-12 14:46:46,885 train 550 9.740861e-03 -0.047780
2019-11-12 14:46:53,395 train 600 9.717680e-03 -0.013167
2019-11-12 14:46:59,878 train 650 9.704798e-03 0.014151
2019-11-12 14:47:06,385 train 700 9.698726e-03 0.040527
2019-11-12 14:47:12,951 train 750 9.695999e-03 0.059465
2019-11-12 14:47:19,661 train 800 9.699503e-03 0.074380
2019-11-12 14:47:26,507 train 850 9.703289e-03 0.091565
2019-11-12 14:47:28,440 training loss; R2: 9.699376e-03 0.096898
2019-11-12 14:47:28,712 valid 000 3.929466e-01 -30.456381
2019-11-12 14:47:30,399 valid 050 3.882974e-01 -24.176212
2019-11-12 14:47:31,928 validation loss; R2: 3.883800e-01 -23.846756
2019-11-12 14:47:31,954 epoch 16 lr 1.000000e-03
2019-11-12 14:47:32,366 train 000 9.239437e-03 0.363343
2019-11-12 14:47:39,222 train 050 9.599071e-03 0.372873
2019-11-12 14:47:46,002 train 100 9.682719e-03 -0.219562
2019-11-12 14:47:52,688 train 150 9.707912e-03 -0.173994
2019-11-12 14:47:59,483 train 200 9.716386e-03 -0.037438
2019-11-12 14:48:06,080 train 250 9.714884e-03 0.043605
2019-11-12 14:48:12,779 train 300 9.739140e-03 0.090393
2019-11-12 14:48:19,529 train 350 9.733895e-03 0.127494
2019-11-12 14:48:26,094 train 400 9.714268e-03 0.152971
2019-11-12 14:48:32,756 train 450 9.736194e-03 0.175994
2019-11-12 14:48:39,286 train 500 9.730814e-03 0.197366
2019-11-12 14:48:45,911 train 550 9.705767e-03 0.209731
2019-11-12 14:48:52,430 train 600 9.711669e-03 0.222769
2019-11-12 14:48:59,226 train 650 9.713595e-03 0.232142
2019-11-12 14:49:05,846 train 700 9.702578e-03 0.240937
2019-11-12 14:49:12,495 train 750 9.703754e-03 0.244804
2019-11-12 14:49:19,213 train 800 9.680385e-03 0.254329
2019-11-12 14:49:26,010 train 850 9.669708e-03 0.260548
2019-11-12 14:49:28,046 training loss; R2: 9.665817e-03 0.261821
2019-11-12 14:49:28,330 valid 000 6.380637e-02 -5.938263
2019-11-12 14:49:30,024 valid 050 6.972775e-02 -4.628031
2019-11-12 14:49:31,588 validation loss; R2: 6.978372e-02 -4.885818
2019-11-12 14:49:31,604 epoch 17 lr 1.000000e-03
2019-11-12 14:49:31,984 train 000 8.124718e-03 0.296796
2019-11-12 14:49:38,691 train 050 9.587831e-03 0.351030
2019-11-12 14:49:45,255 train 100 9.657691e-03 0.345949
2019-11-12 14:49:51,927 train 150 9.673008e-03 0.358270
2019-11-12 14:49:58,453 train 200 9.578729e-03 0.359506
2019-11-12 14:50:05,050 train 250 9.584746e-03 0.364842
2019-11-12 14:50:11,694 train 300 9.625265e-03 0.357980
2019-11-12 14:50:18,214 train 350 9.642225e-03 0.361912
2019-11-12 14:50:24,662 train 400 9.640220e-03 0.363640
2019-11-12 14:50:31,095 train 450 9.658346e-03 0.364122
2019-11-12 14:50:37,544 train 500 9.646214e-03 0.360464
2019-11-12 14:50:43,994 train 550 9.632188e-03 0.356417
2019-11-12 14:50:50,427 train 600 9.620280e-03 0.355146
2019-11-12 14:50:56,849 train 650 9.647444e-03 0.354072
2019-11-12 14:51:03,293 train 700 9.641959e-03 0.356669
2019-11-12 14:51:09,730 train 750 9.634279e-03 0.356670
2019-11-12 14:51:16,156 train 800 9.635609e-03 0.356435
2019-11-12 14:51:22,586 train 850 9.657100e-03 0.352489
2019-11-12 14:51:24,507 training loss; R2: 9.657723e-03 0.352107
2019-11-12 14:51:24,805 valid 000 8.455018e+01 -3463.433651
2019-11-12 14:51:26,551 valid 050 8.439526e+01 -4469.219834
2019-11-12 14:51:28,119 validation loss; R2: 8.437160e+01 -4714.081863
2019-11-12 14:51:28,146 epoch 18 lr 1.000000e-03
2019-11-12 14:51:28,573 train 000 1.024568e-02 0.327842
2019-11-12 14:51:35,140 train 050 9.602397e-03 0.360209
2019-11-12 14:51:42,028 train 100 9.623089e-03 0.333782
2019-11-12 14:51:48,912 train 150 9.645635e-03 0.341779
2019-11-12 14:51:55,553 train 200 9.559630e-03 0.349795
2019-11-12 14:52:02,079 train 250 9.527913e-03 0.353001
2019-11-12 14:52:08,678 train 300 9.552069e-03 0.351584
2019-11-12 14:52:15,287 train 350 9.582363e-03 0.350666
2019-11-12 14:52:21,902 train 400 9.593423e-03 0.355554
2019-11-12 14:52:28,774 train 450 9.605731e-03 0.327233
2019-11-12 14:52:35,578 train 500 9.573180e-03 0.333519
2019-11-12 14:52:42,216 train 550 9.559948e-03 0.338790
2019-11-12 14:52:48,828 train 600 9.570794e-03 0.337218
2019-11-12 14:52:55,640 train 650 9.596862e-03 0.340890
2019-11-12 14:53:02,344 train 700 9.605311e-03 0.342410
2019-11-12 14:53:08,985 train 750 9.597433e-03 0.342393
2019-11-12 14:53:15,829 train 800 9.588439e-03 0.344636
2019-11-12 14:53:22,461 train 850 9.599403e-03 0.346536
2019-11-12 14:53:24,406 training loss; R2: 9.609866e-03 0.346513
2019-11-12 14:53:24,683 valid 000 1.933035e+03 -75571.065866
2019-11-12 14:53:26,412 valid 050 1.932145e+03 -71185.407760
2019-11-12 14:53:27,967 validation loss; R2: 1.932484e+03 -74354.223146
2019-11-12 14:53:27,990 epoch 19 lr 1.000000e-03
2019-11-12 14:53:28,420 train 000 9.771757e-03 0.443421
2019-11-12 14:53:35,125 train 050 9.522002e-03 0.366611
2019-11-12 14:53:41,713 train 100 9.495091e-03 0.359524
2019-11-12 14:53:48,357 train 150 9.571734e-03 0.360579
2019-11-12 14:53:55,062 train 200 9.590973e-03 0.293279
2019-11-12 14:54:01,779 train 250 9.596325e-03 0.313677
2019-11-12 14:54:08,496 train 300 9.634918e-03 0.319554
2019-11-12 14:54:15,275 train 350 9.613703e-03 0.326465
2019-11-12 14:54:21,767 train 400 9.601586e-03 0.330692
2019-11-12 14:54:28,237 train 450 9.611623e-03 0.331982
2019-11-12 14:54:34,739 train 500 9.607057e-03 0.264943
2019-11-12 14:54:41,153 train 550 9.608948e-03 0.271883
2019-11-12 14:54:47,572 train 600 9.618591e-03 0.277872
2019-11-12 14:54:53,992 train 650 9.599754e-03 0.284882
2019-11-12 14:55:00,420 train 700 9.588506e-03 0.290588
2019-11-12 14:55:06,842 train 750 9.602076e-03 0.293222
2019-11-12 14:55:13,259 train 800 9.609348e-03 0.293422
2019-11-12 14:55:19,680 train 850 9.623381e-03 0.296152
2019-11-12 14:55:21,596 training loss; R2: 9.626141e-03 0.296204
2019-11-12 14:55:21,876 valid 000 1.143719e+03 -56920.708753
2019-11-12 14:55:23,592 valid 050 1.143911e+03 -62644.999221
2019-11-12 14:55:25,132 validation loss; R2: 1.143956e+03 -67069.124197
