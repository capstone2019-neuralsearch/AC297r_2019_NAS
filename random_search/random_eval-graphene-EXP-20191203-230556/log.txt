2019-12-03 23:05:56,608 gpu device = 1
2019-12-03 23:05:56,608 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=96, cutout=False, cutout_length=16, data='../data', dataset='graphene', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name=None, gpu=1, grad_clip=5, gz_dtree=False, init_channels=36, layers=8, learning_rate=0.002, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-graphene-EXP-20191203-230556', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-12-03 23:05:59,805 param size = 1.029997MB
2019-12-03 23:05:59,808 epoch 0 lr 2.000000e-03
2019-12-03 23:06:02,389 train 000 5.792013e-01 -27.096526
2019-12-03 23:06:14,510 train 050 1.829762e-01 -4.273303
2019-12-03 23:06:26,575 train 100 1.052032e-01 -2.109170
2019-12-03 23:06:38,636 train 150 7.906993e-02 -1.366254
2019-12-03 23:06:50,702 train 200 6.568030e-02 -0.961307
2019-12-03 23:06:57,176 training loss; R2: 6.141802e-02 -0.835083
2019-12-03 23:06:57,317 valid 000 1.571748e-02 0.464741
2019-12-03 23:06:58,900 validation loss; R2: 1.530527e-02 0.514215
2019-12-03 23:06:58,919 epoch 1 lr 2.000000e-03
2019-12-03 23:06:59,289 train 000 1.718125e-02 0.409197
2019-12-03 23:07:11,069 train 050 1.996712e-02 0.353797
2019-12-03 23:07:22,850 train 100 1.855150e-02 0.394329
2019-12-03 23:07:34,636 train 150 1.790109e-02 0.438862
2019-12-03 23:07:46,419 train 200 1.692772e-02 0.458029
2019-12-03 23:07:51,716 training loss; R2: 1.639095e-02 0.472214
2019-12-03 23:07:51,845 valid 000 7.979800e-03 0.814719
2019-12-03 23:07:53,111 validation loss; R2: 8.145708e-03 0.732561
2019-12-03 23:07:53,130 epoch 2 lr 2.000000e-03
2019-12-03 23:07:53,445 train 000 7.098796e-03 0.726642
2019-12-03 23:08:05,237 train 050 1.162380e-02 0.619345
2019-12-03 23:08:17,024 train 100 1.135771e-02 0.638074
2019-12-03 23:08:28,811 train 150 1.117820e-02 0.639932
2019-12-03 23:08:40,597 train 200 1.053926e-02 0.651672
2019-12-03 23:08:45,901 training loss; R2: 1.046090e-02 0.655096
2019-12-03 23:08:46,035 valid 000 6.149574e-03 0.822439
2019-12-03 23:08:47,300 validation loss; R2: 6.537370e-03 0.775472
2019-12-03 23:08:47,319 epoch 3 lr 2.000000e-03
2019-12-03 23:08:47,634 train 000 5.397622e-03 0.796051
2019-12-03 23:08:59,416 train 050 8.842295e-03 0.704250
2019-12-03 23:09:11,198 train 100 8.899396e-03 0.695987
2019-12-03 23:09:22,982 train 150 8.957275e-03 0.705304
2019-12-03 23:09:34,773 train 200 8.666873e-03 0.704470
2019-12-03 23:09:40,074 training loss; R2: 8.494035e-03 0.709163
2019-12-03 23:09:40,211 valid 000 4.958120e-03 0.741875
2019-12-03 23:09:41,476 validation loss; R2: 5.977910e-03 0.794621
2019-12-03 23:09:41,497 epoch 4 lr 2.000000e-03
2019-12-03 23:09:41,813 train 000 7.003110e-03 0.845122
2019-12-03 23:09:53,610 train 050 7.847300e-03 0.718120
2019-12-03 23:10:05,399 train 100 7.473051e-03 0.721877
2019-12-03 23:10:17,185 train 150 7.424342e-03 0.728129
2019-12-03 23:10:28,973 train 200 7.842141e-03 0.729586
2019-12-03 23:10:34,271 training loss; R2: 7.736891e-03 0.733428
2019-12-03 23:10:34,402 valid 000 3.646843e-03 0.872378
2019-12-03 23:10:35,667 validation loss; R2: 4.429818e-03 0.846515
2019-12-03 23:10:35,687 epoch 5 lr 2.000000e-03
2019-12-03 23:10:36,012 train 000 7.602139e-03 0.802701
2019-12-03 23:10:47,808 train 050 7.274652e-03 0.740132
2019-12-03 23:10:59,607 train 100 7.019763e-03 0.763865
2019-12-03 23:11:11,403 train 150 6.836433e-03 0.767966
2019-12-03 23:11:23,204 train 200 6.809896e-03 0.771523
2019-12-03 23:11:28,509 training loss; R2: 6.774955e-03 0.771980
2019-12-03 23:11:28,640 valid 000 9.419318e-03 0.688621
2019-12-03 23:11:29,906 validation loss; R2: 7.501379e-03 0.733480
2019-12-03 23:11:29,926 epoch 6 lr 2.000000e-03
2019-12-03 23:11:30,251 train 000 1.244592e-02 0.813801
2019-12-03 23:11:42,041 train 050 6.046137e-03 0.794396
2019-12-03 23:11:53,828 train 100 6.277592e-03 0.789231
2019-12-03 23:12:05,616 train 150 6.427886e-03 0.781343
2019-12-03 23:12:17,402 train 200 6.381846e-03 0.782013
2019-12-03 23:12:22,700 training loss; R2: 6.432488e-03 0.780627
2019-12-03 23:12:22,833 valid 000 5.269917e-03 0.833901
2019-12-03 23:12:24,098 validation loss; R2: 3.660725e-03 0.875790
2019-12-03 23:12:24,117 epoch 7 lr 2.000000e-03
2019-12-03 23:12:24,432 train 000 5.069901e-03 0.845785
2019-12-03 23:12:36,213 train 050 5.634144e-03 0.821727
2019-12-03 23:12:47,996 train 100 6.101178e-03 0.805325
2019-12-03 23:12:59,774 train 150 6.059090e-03 0.806719
2019-12-03 23:13:11,557 train 200 5.779896e-03 0.805140
2019-12-03 23:13:16,852 training loss; R2: 5.713437e-03 0.806786
2019-12-03 23:13:16,982 valid 000 5.338550e-03 0.916389
2019-12-03 23:13:18,247 validation loss; R2: 4.510751e-03 0.840166
2019-12-03 23:13:18,266 epoch 8 lr 2.000000e-03
2019-12-03 23:13:18,583 train 000 5.357098e-03 0.796794
2019-12-03 23:13:30,366 train 050 6.329655e-03 0.807255
2019-12-03 23:13:42,149 train 100 5.860960e-03 0.803343
2019-12-03 23:13:53,929 train 150 5.702336e-03 0.808922
2019-12-03 23:14:05,708 train 200 5.646501e-03 0.809489
2019-12-03 23:14:11,006 training loss; R2: 5.585048e-03 0.810796
2019-12-03 23:14:11,138 valid 000 3.403629e-03 0.898866
2019-12-03 23:14:12,403 validation loss; R2: 3.684805e-03 0.876314
2019-12-03 23:14:12,424 epoch 9 lr 2.000000e-03
2019-12-03 23:14:12,745 train 000 3.009189e-03 0.851371
2019-12-03 23:14:24,522 train 050 5.822993e-03 0.807431
2019-12-03 23:14:36,297 train 100 5.424966e-03 0.811991
2019-12-03 23:14:48,076 train 150 5.402920e-03 0.816738
2019-12-03 23:14:59,851 train 200 5.287183e-03 0.818865
2019-12-03 23:15:05,146 training loss; R2: 5.243315e-03 0.820355
2019-12-03 23:15:05,278 valid 000 4.649852e-03 0.923106
2019-12-03 23:15:06,543 validation loss; R2: 3.861790e-03 0.874435
2019-12-03 23:15:06,562 epoch 10 lr 2.000000e-03
2019-12-03 23:15:06,879 train 000 3.917609e-03 0.879666
2019-12-03 23:15:18,665 train 050 4.800661e-03 0.823478
2019-12-03 23:15:30,449 train 100 4.936390e-03 0.827359
2019-12-03 23:15:42,235 train 150 5.159080e-03 0.820562
2019-12-03 23:15:54,020 train 200 5.109939e-03 0.820533
2019-12-03 23:15:59,319 training loss; R2: 5.123233e-03 0.822162
2019-12-03 23:15:59,452 valid 000 6.097874e-03 0.635743
2019-12-03 23:16:00,717 validation loss; R2: 2.761329e-03 0.899097
2019-12-03 23:16:00,742 epoch 11 lr 2.000000e-03
2019-12-03 23:16:01,065 train 000 4.022074e-03 0.878094
2019-12-03 23:16:12,853 train 050 5.442369e-03 0.814232
2019-12-03 23:16:24,634 train 100 5.095088e-03 0.830700
2019-12-03 23:16:36,415 train 150 5.108961e-03 0.827315
2019-12-03 23:16:48,200 train 200 5.004873e-03 0.828049
2019-12-03 23:16:53,496 training loss; R2: 5.061949e-03 0.827057
2019-12-03 23:16:53,628 valid 000 2.346554e-03 0.877512
2019-12-03 23:16:54,893 validation loss; R2: 3.693991e-03 0.871051
2019-12-03 23:16:54,912 epoch 12 lr 2.000000e-03
2019-12-03 23:16:55,229 train 000 6.004538e-03 0.833198
2019-12-03 23:17:07,009 train 050 5.326479e-03 0.833890
2019-12-03 23:17:18,789 train 100 5.176190e-03 0.837120
2019-12-03 23:17:30,569 train 150 4.840935e-03 0.839325
2019-12-03 23:17:42,348 train 200 4.766598e-03 0.840262
2019-12-03 23:17:47,642 training loss; R2: 4.705766e-03 0.843557
2019-12-03 23:17:47,778 valid 000 3.306096e-02 -0.323393
2019-12-03 23:17:49,043 validation loss; R2: 3.181259e-02 -0.146245
2019-12-03 23:17:49,063 epoch 13 lr 2.000000e-03
2019-12-03 23:17:49,381 train 000 3.756352e-03 0.854926
2019-12-03 23:18:01,157 train 050 4.683187e-03 0.831555
2019-12-03 23:18:12,932 train 100 4.896951e-03 0.825180
2019-12-03 23:18:24,708 train 150 4.793059e-03 0.828755
2019-12-03 23:18:36,493 train 200 4.754058e-03 0.828942
2019-12-03 23:18:41,785 training loss; R2: 4.764688e-03 0.829750
2019-12-03 23:18:41,918 valid 000 1.996889e-02 0.317397
2019-12-03 23:18:43,183 validation loss; R2: 1.899109e-02 0.341952
2019-12-03 23:18:43,204 epoch 14 lr 2.000000e-03
2019-12-03 23:18:43,523 train 000 3.902490e-03 0.891670
2019-12-03 23:18:55,296 train 050 4.351591e-03 0.849291
2019-12-03 23:19:07,073 train 100 4.122405e-03 0.860914
2019-12-03 23:19:18,843 train 150 4.217974e-03 0.855770
2019-12-03 23:19:30,609 train 200 4.228936e-03 0.849704
2019-12-03 23:19:35,901 training loss; R2: 4.401716e-03 0.846430
2019-12-03 23:19:36,047 valid 000 8.397222e-02 -1.391849
2019-12-03 23:19:37,311 validation loss; R2: 7.522021e-02 -1.690699
2019-12-03 23:19:37,332 epoch 15 lr 2.000000e-03
2019-12-03 23:19:37,653 train 000 6.963959e-03 0.858373
2019-12-03 23:19:49,414 train 050 4.872324e-03 0.834787
2019-12-03 23:20:01,172 train 100 4.642118e-03 0.842367
2019-12-03 23:20:12,926 train 150 4.495086e-03 0.843161
2019-12-03 23:20:24,673 train 200 4.570562e-03 0.843193
2019-12-03 23:20:29,954 training loss; R2: 4.596737e-03 0.841880
2019-12-03 23:20:30,093 valid 000 5.703131e-02 -1.939862
2019-12-03 23:20:31,357 validation loss; R2: 6.756568e-02 -1.349703
2019-12-03 23:20:31,384 epoch 16 lr 2.000000e-03
2019-12-03 23:20:31,703 train 000 5.116242e-03 0.851947
2019-12-03 23:20:43,458 train 050 4.762867e-03 0.823141
2019-12-03 23:20:55,218 train 100 4.551724e-03 0.839312
2019-12-03 23:21:06,970 train 150 4.619297e-03 0.836727
2019-12-03 23:21:18,716 train 200 4.697013e-03 0.834884
2019-12-03 23:21:23,997 training loss; R2: 4.673384e-03 0.836078
2019-12-03 23:21:24,135 valid 000 2.538468e-01 -7.908778
2019-12-03 23:21:25,397 validation loss; R2: 2.781709e-01 -9.374853
2019-12-03 23:21:25,417 epoch 17 lr 2.000000e-03
2019-12-03 23:21:25,734 train 000 3.795659e-03 0.827622
2019-12-03 23:21:37,473 train 050 5.244790e-03 0.820081
2019-12-03 23:21:49,214 train 100 4.748661e-03 0.836686
2019-12-03 23:22:00,952 train 150 4.547681e-03 0.844763
2019-12-03 23:22:12,683 train 200 4.546549e-03 0.844124
2019-12-03 23:22:17,958 training loss; R2: 4.573672e-03 0.845490
2019-12-03 23:22:18,088 valid 000 5.613582e-01 -15.646569
2019-12-03 23:22:19,352 validation loss; R2: 5.788199e-01 -20.959183
2019-12-03 23:22:19,378 epoch 18 lr 2.000000e-03
2019-12-03 23:22:19,702 train 000 4.034787e-03 0.886848
2019-12-03 23:22:31,432 train 050 5.052400e-03 0.825093
2019-12-03 23:22:43,159 train 100 4.969198e-03 0.831751
2019-12-03 23:22:54,881 train 150 4.741962e-03 0.839891
2019-12-03 23:23:06,603 train 200 4.557785e-03 0.841601
2019-12-03 23:23:11,875 training loss; R2: 4.494597e-03 0.843332
2019-12-03 23:23:12,012 valid 000 2.641371e-03 0.888179
2019-12-03 23:23:13,275 validation loss; R2: 2.761074e-03 0.901004
2019-12-03 23:23:13,296 epoch 19 lr 2.000000e-03
2019-12-03 23:23:13,615 train 000 2.512240e-03 0.910516
2019-12-03 23:23:25,353 train 050 4.070685e-03 0.854576
2019-12-03 23:23:37,080 train 100 3.979515e-03 0.859537
2019-12-03 23:23:48,809 train 150 4.067464e-03 0.858231
2019-12-03 23:24:00,535 train 200 4.113759e-03 0.858953
2019-12-03 23:24:05,807 training loss; R2: 4.148486e-03 0.860048
2019-12-03 23:24:05,938 valid 000 3.358656e-03 0.825952
2019-12-03 23:24:07,201 validation loss; R2: 4.342263e-03 0.849551
