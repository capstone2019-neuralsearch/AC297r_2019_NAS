2019-11-13 13:02:04,213 gpu device = 1
2019-11-13 13:02:04,213 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name='galaxy-zoo-cleaned', gpu=1, grad_clip=5, gz_dtree=False, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-galaxy-zoo-EXP-20191113-130203', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-11-13 13:02:15,856 param size = 0.217397MB
2019-11-13 13:02:15,860 epoch 0 lr 1.000000e-03
2019-11-13 13:02:17,973 train 000 8.555697e-01 -565.471712
2019-11-13 13:02:23,867 train 050 6.722043e-02 -21.219639
2019-11-13 13:02:29,757 train 100 4.889118e-02 -11.329325
2019-11-13 13:02:35,606 train 150 4.161430e-02 -7.825680
2019-11-13 13:02:41,430 train 200 3.779304e-02 -6.029479
2019-11-13 13:02:47,275 train 250 3.534916e-02 -4.908403
2019-11-13 13:02:53,102 train 300 3.354155e-02 -4.137790
2019-11-13 13:02:58,931 train 350 3.217679e-02 -3.898553
2019-11-13 13:03:04,765 train 400 3.120107e-02 -3.801416
2019-11-13 13:03:10,598 train 450 3.036147e-02 -3.393944
2019-11-13 13:03:16,423 train 500 2.960702e-02 -3.070458
2019-11-13 13:03:22,250 train 550 2.893876e-02 -2.807950
2019-11-13 13:03:28,079 train 600 2.839519e-02 -2.582396
2019-11-13 13:03:33,905 train 650 2.786008e-02 -2.389086
2019-11-13 13:03:39,730 train 700 2.740194e-02 -2.226708
2019-11-13 13:03:45,557 train 750 2.700069e-02 -2.081661
2019-11-13 13:03:51,378 train 800 2.664334e-02 -1.960798
2019-11-13 13:03:57,203 train 850 2.632215e-02 -1.848657
2019-11-13 13:03:59,640 training loss; R2: 2.621689e-02 -1.816925
2019-11-13 13:03:59,915 valid 000 1.651255e-02 0.147952
2019-11-13 13:04:01,619 valid 050 1.817249e-02 0.162924
2019-11-13 13:04:03,201 validation loss; R2: 1.837155e-02 0.158060
2019-11-13 13:04:03,219 epoch 1 lr 1.000000e-03
2019-11-13 13:04:03,678 train 000 1.580345e-02 0.030003
2019-11-13 13:04:09,499 train 050 2.017857e-02 -0.005690
2019-11-13 13:04:15,395 train 100 2.001610e-02 -0.148386
2019-11-13 13:04:21,318 train 150 2.009100e-02 -0.119223
2019-11-13 13:04:27,213 train 200 2.009703e-02 -0.111587
2019-11-13 13:04:33,098 train 250 2.000919e-02 -0.090202
2019-11-13 13:04:38,990 train 300 2.002998e-02 -0.067520
2019-11-13 13:04:44,891 train 350 1.990954e-02 -0.060064
2019-11-13 13:04:50,786 train 400 1.985853e-02 -0.048726
2019-11-13 13:04:56,679 train 450 1.976541e-02 -0.040985
2019-11-13 13:05:02,568 train 500 1.971874e-02 -0.040245
2019-11-13 13:05:08,460 train 550 1.967783e-02 -0.032272
2019-11-13 13:05:14,361 train 600 1.962757e-02 -0.023714
2019-11-13 13:05:20,265 train 650 1.954904e-02 -0.016363
2019-11-13 13:05:26,156 train 700 1.949192e-02 -0.011111
2019-11-13 13:05:32,041 train 750 1.944108e-02 -0.006002
2019-11-13 13:05:37,934 train 800 1.939246e-02 0.000670
2019-11-13 13:05:43,826 train 850 1.930908e-02 0.003112
2019-11-13 13:05:45,581 training loss; R2: 1.928618e-02 0.004850
2019-11-13 13:05:45,867 valid 000 1.914868e-02 -0.086368
2019-11-13 13:05:47,563 valid 050 2.019786e-02 -0.020269
2019-11-13 13:05:49,089 validation loss; R2: 1.989243e-02 0.020415
2019-11-13 13:05:49,105 epoch 2 lr 1.000000e-03
2019-11-13 13:05:49,471 train 000 1.846172e-02 0.186974
2019-11-13 13:05:55,290 train 050 1.832383e-02 0.085861
2019-11-13 13:06:01,193 train 100 1.808842e-02 0.087724
2019-11-13 13:06:07,093 train 150 1.804481e-02 0.089377
2019-11-13 13:06:12,999 train 200 1.804097e-02 0.083713
2019-11-13 13:06:18,889 train 250 1.802034e-02 0.084053
2019-11-13 13:06:24,779 train 300 1.798904e-02 0.079464
2019-11-13 13:06:30,666 train 350 1.797407e-02 0.079363
2019-11-13 13:06:36,552 train 400 1.790467e-02 0.082408
2019-11-13 13:06:42,442 train 450 1.781428e-02 0.083912
2019-11-13 13:06:48,339 train 500 1.771561e-02 0.085285
2019-11-13 13:06:54,240 train 550 1.768674e-02 0.087397
2019-11-13 13:07:00,148 train 600 1.764783e-02 0.088093
2019-11-13 13:07:06,060 train 650 1.759123e-02 0.088665
2019-11-13 13:07:11,962 train 700 1.755869e-02 0.089786
2019-11-13 13:07:17,862 train 750 1.751775e-02 0.092765
2019-11-13 13:07:23,768 train 800 1.749370e-02 0.090662
2019-11-13 13:07:29,677 train 850 1.743750e-02 0.094110
2019-11-13 13:07:31,436 training loss; R2: 1.743045e-02 0.095149
2019-11-13 13:07:31,722 valid 000 1.569780e-02 0.226197
2019-11-13 13:07:33,418 valid 050 1.594428e-02 0.192305
2019-11-13 13:07:34,945 validation loss; R2: 1.615815e-02 0.114956
2019-11-13 13:07:34,957 epoch 3 lr 1.000000e-03
2019-11-13 13:07:35,287 train 000 1.591600e-02 -2.732359
2019-11-13 13:07:41,175 train 050 1.655221e-02 0.085197
2019-11-13 13:07:47,383 train 100 1.666657e-02 0.120321
2019-11-13 13:07:53,592 train 150 1.648890e-02 0.077319
2019-11-13 13:07:59,562 train 200 1.648844e-02 0.095989
2019-11-13 13:08:05,398 train 250 1.646784e-02 0.096947
2019-11-13 13:08:11,238 train 300 1.641780e-02 0.107644
2019-11-13 13:08:17,076 train 350 1.641989e-02 0.115834
2019-11-13 13:08:22,918 train 400 1.636846e-02 0.116978
2019-11-13 13:08:28,754 train 450 1.632764e-02 0.118589
2019-11-13 13:08:34,593 train 500 1.626365e-02 0.120908
2019-11-13 13:08:40,428 train 550 1.623494e-02 0.124820
2019-11-13 13:08:46,270 train 600 1.618146e-02 0.127176
2019-11-13 13:08:52,099 train 650 1.614538e-02 0.128007
2019-11-13 13:08:57,979 train 700 1.611254e-02 0.133326
2019-11-13 13:09:03,870 train 750 1.606173e-02 0.134031
2019-11-13 13:09:09,767 train 800 1.601616e-02 0.137485
2019-11-13 13:09:15,670 train 850 1.599256e-02 0.140848
2019-11-13 13:09:17,426 training loss; R2: 1.598735e-02 0.140220
2019-11-13 13:09:17,721 valid 000 1.370927e-02 0.279916
2019-11-13 13:09:19,408 valid 050 1.332909e-02 0.264018
2019-11-13 13:09:20,925 validation loss; R2: 1.331118e-02 0.277672
2019-11-13 13:09:20,941 epoch 4 lr 1.000000e-03
2019-11-13 13:09:21,291 train 000 1.411441e-02 -0.150520
2019-11-13 13:09:27,396 train 050 1.499328e-02 0.198763
2019-11-13 13:09:33,626 train 100 1.484208e-02 0.184069
2019-11-13 13:09:39,827 train 150 1.497993e-02 0.184136
2019-11-13 13:09:45,964 train 200 1.498858e-02 0.178425
2019-11-13 13:09:51,804 train 250 1.503994e-02 0.173252
2019-11-13 13:09:57,641 train 300 1.510467e-02 0.173587
2019-11-13 13:10:03,487 train 350 1.507061e-02 0.163249
2019-11-13 13:10:09,328 train 400 1.503852e-02 0.165170
2019-11-13 13:10:15,165 train 450 1.503808e-02 0.169020
2019-11-13 13:10:21,006 train 500 1.502615e-02 0.169408
2019-11-13 13:10:26,849 train 550 1.497089e-02 0.169239
2019-11-13 13:10:32,694 train 600 1.493976e-02 0.170936
2019-11-13 13:10:38,534 train 650 1.488740e-02 0.173652
2019-11-13 13:10:44,370 train 700 1.485556e-02 0.175779
2019-11-13 13:10:50,200 train 750 1.483363e-02 0.174159
2019-11-13 13:10:56,042 train 800 1.480337e-02 0.173105
2019-11-13 13:11:01,886 train 850 1.477566e-02 0.162238
2019-11-13 13:11:03,627 training loss; R2: 1.476752e-02 0.163063
2019-11-13 13:11:03,910 valid 000 1.279575e-02 0.284876
2019-11-13 13:11:05,576 valid 050 1.247762e-02 0.244487
2019-11-13 13:11:07,092 validation loss; R2: 1.249910e-02 0.269087
2019-11-13 13:11:07,107 epoch 5 lr 1.000000e-03
2019-11-13 13:11:07,443 train 000 1.456019e-02 0.217279
2019-11-13 13:11:13,351 train 050 1.430478e-02 0.198082
2019-11-13 13:11:19,338 train 100 1.410408e-02 0.177701
2019-11-13 13:11:25,292 train 150 1.410994e-02 0.199618
2019-11-13 13:11:31,234 train 200 1.413681e-02 0.201802
2019-11-13 13:11:37,176 train 250 1.410466e-02 0.206152
2019-11-13 13:11:43,120 train 300 1.404420e-02 0.205683
2019-11-13 13:11:49,056 train 350 1.405853e-02 0.206932
2019-11-13 13:11:54,975 train 400 1.403716e-02 0.212153
2019-11-13 13:12:00,889 train 450 1.402022e-02 0.212397
2019-11-13 13:12:06,794 train 500 1.401081e-02 0.213774
2019-11-13 13:12:12,703 train 550 1.400762e-02 0.211464
2019-11-13 13:12:18,612 train 600 1.399945e-02 0.213939
2019-11-13 13:12:24,513 train 650 1.398582e-02 0.197391
2019-11-13 13:12:30,422 train 700 1.395479e-02 0.199770
2019-11-13 13:12:36,348 train 750 1.393945e-02 0.201432
2019-11-13 13:12:42,254 train 800 1.391592e-02 0.204231
2019-11-13 13:12:48,158 train 850 1.390255e-02 0.206613
2019-11-13 13:12:49,918 training loss; R2: 1.389247e-02 0.204236
2019-11-13 13:12:50,217 valid 000 1.084562e-02 0.407052
2019-11-13 13:12:51,888 valid 050 1.203389e-02 0.276988
2019-11-13 13:12:53,418 validation loss; R2: 1.203550e-02 0.277260
2019-11-13 13:12:53,435 epoch 6 lr 1.000000e-03
2019-11-13 13:12:53,745 train 000 1.188344e-02 0.268993
2019-11-13 13:12:59,674 train 050 1.381699e-02 0.245593
2019-11-13 13:13:05,685 train 100 1.368113e-02 0.235368
2019-11-13 13:13:11,778 train 150 1.371724e-02 0.209196
2019-11-13 13:13:17,966 train 200 1.374665e-02 0.214256
2019-11-13 13:13:23,837 train 250 1.369184e-02 0.219711
2019-11-13 13:13:29,715 train 300 1.365540e-02 0.224615
2019-11-13 13:13:35,569 train 350 1.363968e-02 0.222937
2019-11-13 13:13:41,401 train 400 1.364844e-02 0.222702
2019-11-13 13:13:47,232 train 450 1.363828e-02 0.223788
2019-11-13 13:13:53,073 train 500 1.361004e-02 0.221735
2019-11-13 13:13:58,901 train 550 1.360005e-02 0.225556
2019-11-13 13:14:04,741 train 600 1.356481e-02 0.225149
2019-11-13 13:14:10,573 train 650 1.350833e-02 0.228217
2019-11-13 13:14:16,412 train 700 1.347655e-02 0.229806
2019-11-13 13:14:22,254 train 750 1.345738e-02 0.226428
2019-11-13 13:14:28,090 train 800 1.344748e-02 0.212100
2019-11-13 13:14:33,925 train 850 1.341076e-02 0.214909
2019-11-13 13:14:35,666 training loss; R2: 1.340796e-02 0.215698
2019-11-13 13:14:35,946 valid 000 1.069797e-02 -0.318353
2019-11-13 13:14:37,648 valid 050 1.173525e-02 0.265942
2019-11-13 13:14:39,172 validation loss; R2: 1.175973e-02 0.250356
2019-11-13 13:14:39,184 epoch 7 lr 1.000000e-03
2019-11-13 13:14:39,520 train 000 1.368644e-02 0.361306
2019-11-13 13:14:45,511 train 050 1.305481e-02 0.257546
2019-11-13 13:14:51,605 train 100 1.309641e-02 0.273622
2019-11-13 13:14:57,684 train 150 1.313954e-02 0.242178
2019-11-13 13:15:03,742 train 200 1.308983e-02 0.236518
2019-11-13 13:15:09,741 train 250 1.303003e-02 0.245818
2019-11-13 13:15:15,638 train 300 1.302915e-02 0.249966
2019-11-13 13:15:21,693 train 350 1.301189e-02 0.245242
2019-11-13 13:15:27,612 train 400 1.305111e-02 0.238993
2019-11-13 13:15:33,359 train 450 1.303931e-02 0.243774
2019-11-13 13:15:39,083 train 500 1.300907e-02 0.244503
2019-11-13 13:15:44,801 train 550 1.298326e-02 0.245770
2019-11-13 13:15:50,472 train 600 1.297444e-02 0.247842
2019-11-13 13:15:56,250 train 650 1.295143e-02 0.248080
2019-11-13 13:16:02,212 train 700 1.295647e-02 0.235076
2019-11-13 13:16:08,085 train 750 1.295016e-02 0.238236
2019-11-13 13:16:13,970 train 800 1.293910e-02 0.227555
2019-11-13 13:16:19,813 train 850 1.292362e-02 0.229410
2019-11-13 13:16:21,479 training loss; R2: 1.292536e-02 0.229013
2019-11-13 13:16:21,740 valid 000 1.257764e-02 0.293773
2019-11-13 13:16:23,465 valid 050 1.148465e-02 0.092654
2019-11-13 13:16:25,000 validation loss; R2: 1.151164e-02 0.209568
2019-11-13 13:16:25,015 epoch 8 lr 1.000000e-03
2019-11-13 13:16:25,374 train 000 1.206186e-02 0.348944
2019-11-13 13:16:31,398 train 050 1.258751e-02 0.246157
2019-11-13 13:16:37,365 train 100 1.265151e-02 0.257348
2019-11-13 13:16:43,378 train 150 1.261779e-02 0.268010
2019-11-13 13:16:49,336 train 200 1.273950e-02 0.265197
2019-11-13 13:16:55,356 train 250 1.270931e-02 0.265086
2019-11-13 13:17:01,361 train 300 1.270053e-02 0.267063
2019-11-13 13:17:07,343 train 350 1.263912e-02 0.264881
2019-11-13 13:17:13,359 train 400 1.260593e-02 0.261608
2019-11-13 13:17:19,300 train 450 1.259598e-02 0.254203
2019-11-13 13:17:25,291 train 500 1.258565e-02 0.251369
2019-11-13 13:17:31,282 train 550 1.256882e-02 -0.152510
2019-11-13 13:17:37,201 train 600 1.257289e-02 -0.117798
2019-11-13 13:17:43,276 train 650 1.257995e-02 -0.094025
2019-11-13 13:17:49,237 train 700 1.258638e-02 -0.066605
2019-11-13 13:17:55,154 train 750 1.257309e-02 -0.044928
2019-11-13 13:18:01,127 train 800 1.258573e-02 -0.025918
2019-11-13 13:18:06,974 train 850 1.258008e-02 -0.009171
2019-11-13 13:18:08,739 training loss; R2: 1.258205e-02 -0.004745
2019-11-13 13:18:08,999 valid 000 9.159234e-03 0.447106
2019-11-13 13:18:10,722 valid 050 1.059308e-02 0.340554
2019-11-13 13:18:12,292 validation loss; R2: 1.045565e-02 0.345195
2019-11-13 13:18:12,305 epoch 9 lr 1.000000e-03
2019-11-13 13:18:12,620 train 000 9.800202e-03 0.351765
2019-11-13 13:18:18,243 train 050 1.257071e-02 0.285737
2019-11-13 13:18:24,291 train 100 1.244447e-02 0.277326
2019-11-13 13:18:30,463 train 150 1.236540e-02 0.278352
2019-11-13 13:18:36,624 train 200 1.237723e-02 0.266822
2019-11-13 13:18:42,782 train 250 1.234256e-02 0.272446
2019-11-13 13:18:48,939 train 300 1.237891e-02 0.271203
2019-11-13 13:18:55,085 train 350 1.237622e-02 0.260339
2019-11-13 13:19:01,233 train 400 1.240245e-02 0.260898
2019-11-13 13:19:07,392 train 450 1.239598e-02 0.262930
2019-11-13 13:19:13,545 train 500 1.240359e-02 0.258639
2019-11-13 13:19:19,590 train 550 1.239032e-02 0.256027
2019-11-13 13:19:25,430 train 600 1.237634e-02 0.256168
2019-11-13 13:19:31,264 train 650 1.237239e-02 0.217109
2019-11-13 13:19:37,103 train 700 1.235206e-02 0.221319
2019-11-13 13:19:42,958 train 750 1.235692e-02 0.220827
2019-11-13 13:19:48,806 train 800 1.232749e-02 0.223344
2019-11-13 13:19:54,644 train 850 1.232376e-02 0.215817
2019-11-13 13:19:56,390 training loss; R2: 1.232361e-02 0.217324
2019-11-13 13:19:56,685 valid 000 1.324774e-02 0.307741
2019-11-13 13:19:58,349 valid 050 1.097832e-02 0.364118
2019-11-13 13:19:59,876 validation loss; R2: 1.101357e-02 0.330018
2019-11-13 13:19:59,888 epoch 10 lr 1.000000e-03
2019-11-13 13:20:00,229 train 000 1.248858e-02 0.426782
2019-11-13 13:20:05,803 train 050 1.212768e-02 0.310432
2019-11-13 13:20:11,375 train 100 1.206908e-02 0.282367
2019-11-13 13:20:16,947 train 150 1.202966e-02 0.267762
2019-11-13 13:20:22,520 train 200 1.210159e-02 0.275479
2019-11-13 13:20:28,095 train 250 1.209589e-02 0.273916
2019-11-13 13:20:33,663 train 300 1.207369e-02 0.250622
2019-11-13 13:20:39,232 train 350 1.206000e-02 0.251978
2019-11-13 13:20:44,812 train 400 1.206157e-02 0.243686
2019-11-13 13:20:50,387 train 450 1.204915e-02 0.246844
2019-11-13 13:20:55,956 train 500 1.206378e-02 0.250136
2019-11-13 13:21:01,527 train 550 1.206592e-02 0.252945
2019-11-13 13:21:07,100 train 600 1.208569e-02 0.247251
2019-11-13 13:21:12,678 train 650 1.207445e-02 0.251280
2019-11-13 13:21:18,250 train 700 1.208042e-02 0.255578
2019-11-13 13:21:23,828 train 750 1.208457e-02 0.258253
2019-11-13 13:21:29,399 train 800 1.207228e-02 0.259506
2019-11-13 13:21:34,969 train 850 1.209920e-02 0.258222
2019-11-13 13:21:36,636 training loss; R2: 1.210067e-02 0.259161
2019-11-13 13:21:36,910 valid 000 1.028412e-02 0.183286
2019-11-13 13:21:38,608 valid 050 1.053832e-02 0.398737
2019-11-13 13:21:40,122 validation loss; R2: 1.028025e-02 0.404978
2019-11-13 13:21:40,134 epoch 11 lr 1.000000e-03
2019-11-13 13:21:40,473 train 000 1.214624e-02 0.282502
2019-11-13 13:21:46,120 train 050 1.177296e-02 0.297395
2019-11-13 13:21:51,759 train 100 1.176233e-02 0.292560
2019-11-13 13:21:57,937 train 150 1.188565e-02 0.286664
2019-11-13 13:22:03,850 train 200 1.185400e-02 0.270363
2019-11-13 13:22:09,754 train 250 1.186777e-02 0.267377
2019-11-13 13:22:15,682 train 300 1.186083e-02 0.270178
2019-11-13 13:22:21,578 train 350 1.188998e-02 0.267665
2019-11-13 13:22:27,495 train 400 1.190281e-02 0.266681
2019-11-13 13:22:33,404 train 450 1.189666e-02 0.265463
2019-11-13 13:22:39,311 train 500 1.190950e-02 0.267281
2019-11-13 13:22:45,214 train 550 1.189091e-02 0.268199
2019-11-13 13:22:51,113 train 600 1.191412e-02 0.270564
2019-11-13 13:22:57,014 train 650 1.190202e-02 0.268449
2019-11-13 13:23:02,925 train 700 1.189472e-02 0.269995
2019-11-13 13:23:08,826 train 750 1.192000e-02 0.271589
2019-11-13 13:23:14,730 train 800 1.192357e-02 0.272865
2019-11-13 13:23:20,635 train 850 1.193557e-02 0.273544
2019-11-13 13:23:22,393 training loss; R2: 1.193506e-02 0.226004
2019-11-13 13:23:22,686 valid 000 9.726043e-03 0.344270
2019-11-13 13:23:24,610 valid 050 1.037932e-02 0.393874
2019-11-13 13:23:26,326 validation loss; R2: 1.036735e-02 0.377666
2019-11-13 13:23:26,339 epoch 12 lr 1.000000e-03
2019-11-13 13:23:26,692 train 000 1.026377e-02 0.397924
2019-11-13 13:23:32,392 train 050 1.205691e-02 0.276639
2019-11-13 13:23:38,509 train 100 1.196254e-02 0.276826
2019-11-13 13:23:44,512 train 150 1.194995e-02 0.276688
2019-11-13 13:23:50,467 train 200 1.195428e-02 0.279304
2019-11-13 13:23:56,414 train 250 1.195965e-02 0.277833
2019-11-13 13:24:02,358 train 300 1.189158e-02 0.275074
2019-11-13 13:24:08,291 train 350 1.190100e-02 0.269647
2019-11-13 13:24:14,106 train 400 1.188796e-02 0.266547
2019-11-13 13:24:19,834 train 450 1.183978e-02 0.271934
2019-11-13 13:24:25,811 train 500 1.183672e-02 0.272711
2019-11-13 13:24:31,945 train 550 1.183656e-02 0.272798
2019-11-13 13:24:37,975 train 600 1.180279e-02 0.272449
2019-11-13 13:24:43,966 train 650 1.177197e-02 0.276738
2019-11-13 13:24:50,016 train 700 1.177601e-02 0.276340
2019-11-13 13:24:56,079 train 750 1.177294e-02 0.276936
2019-11-13 13:25:02,233 train 800 1.175652e-02 0.273258
2019-11-13 13:25:08,185 train 850 1.176588e-02 0.271699
2019-11-13 13:25:09,955 training loss; R2: 1.175206e-02 0.271724
2019-11-13 13:25:10,250 valid 000 8.403060e-03 0.451517
2019-11-13 13:25:11,965 valid 050 1.029843e-02 0.379754
2019-11-13 13:25:13,526 validation loss; R2: 1.025518e-02 0.388754
2019-11-13 13:25:13,544 epoch 13 lr 1.000000e-03
2019-11-13 13:25:13,896 train 000 1.112395e-02 0.342063
2019-11-13 13:25:19,688 train 050 1.146574e-02 0.252634
2019-11-13 13:25:25,665 train 100 1.156569e-02 0.287667
2019-11-13 13:25:31,704 train 150 1.161942e-02 0.280839
2019-11-13 13:25:37,758 train 200 1.158384e-02 0.287156
2019-11-13 13:25:43,731 train 250 1.150176e-02 0.289431
2019-11-13 13:25:49,732 train 300 1.156766e-02 0.279431
2019-11-13 13:25:55,714 train 350 1.157163e-02 0.282603
2019-11-13 13:26:01,782 train 400 1.157550e-02 0.287849
2019-11-13 13:26:07,850 train 450 1.158650e-02 0.249861
2019-11-13 13:26:13,897 train 500 1.158970e-02 0.248658
2019-11-13 13:26:19,975 train 550 1.157720e-02 0.255903
2019-11-13 13:26:25,995 train 600 1.158159e-02 0.260546
2019-11-13 13:26:31,967 train 650 1.158315e-02 0.263515
2019-11-13 13:26:38,040 train 700 1.158371e-02 0.266161
2019-11-13 13:26:43,971 train 750 1.157519e-02 0.268991
2019-11-13 13:26:49,919 train 800 1.157669e-02 0.272481
2019-11-13 13:26:55,893 train 850 1.157562e-02 0.273713
2019-11-13 13:26:57,665 training loss; R2: 1.156843e-02 0.272834
2019-11-13 13:26:57,967 valid 000 9.604794e-03 0.247738
2019-11-13 13:26:59,685 valid 050 9.660205e-03 0.324049
2019-11-13 13:27:01,210 validation loss; R2: 9.752641e-03 0.350710
2019-11-13 13:27:01,226 epoch 14 lr 1.000000e-03
2019-11-13 13:27:01,617 train 000 1.169936e-02 0.433951
2019-11-13 13:27:07,654 train 050 1.135343e-02 0.301720
2019-11-13 13:27:13,645 train 100 1.138882e-02 0.254281
2019-11-13 13:27:19,343 train 150 1.140756e-02 0.221052
2019-11-13 13:27:25,186 train 200 1.144335e-02 0.236248
2019-11-13 13:27:31,110 train 250 1.142952e-02 0.252694
2019-11-13 13:27:37,076 train 300 1.148550e-02 0.263613
2019-11-13 13:27:43,164 train 350 1.146373e-02 0.266526
2019-11-13 13:27:49,067 train 400 1.151559e-02 0.271596
2019-11-13 13:27:54,974 train 450 1.153743e-02 0.273472
2019-11-13 13:28:00,848 train 500 1.151757e-02 0.275901
2019-11-13 13:28:06,715 train 550 1.151393e-02 0.277855
2019-11-13 13:28:12,504 train 600 1.151824e-02 0.280094
2019-11-13 13:28:18,302 train 650 1.148434e-02 0.283319
2019-11-13 13:28:24,111 train 700 1.146810e-02 0.285499
2019-11-13 13:28:29,893 train 750 1.144726e-02 0.287457
2019-11-13 13:28:35,672 train 800 1.143560e-02 0.289204
2019-11-13 13:28:41,464 train 850 1.141252e-02 0.290889
2019-11-13 13:28:43,190 training loss; R2: 1.141238e-02 0.290916
2019-11-13 13:28:43,483 valid 000 8.060160e-03 0.501145
2019-11-13 13:28:45,166 valid 050 9.845147e-03 0.383419
2019-11-13 13:28:46,697 validation loss; R2: 9.940020e-03 0.377019
2019-11-13 13:28:46,709 epoch 15 lr 1.000000e-03
2019-11-13 13:28:47,083 train 000 1.390720e-02 0.144594
2019-11-13 13:28:53,076 train 050 1.151687e-02 0.284619
2019-11-13 13:28:59,089 train 100 1.124483e-02 -0.088659
2019-11-13 13:29:05,105 train 150 1.134792e-02 0.049629
2019-11-13 13:29:11,089 train 200 1.132831e-02 0.107483
2019-11-13 13:29:17,161 train 250 1.141104e-02 0.133791
2019-11-13 13:29:23,176 train 300 1.134809e-02 0.162393
2019-11-13 13:29:29,045 train 350 1.134444e-02 0.184430
2019-11-13 13:29:34,931 train 400 1.135909e-02 0.188836
2019-11-13 13:29:40,802 train 450 1.135233e-02 0.199838
2019-11-13 13:29:46,624 train 500 1.133242e-02 0.190630
2019-11-13 13:29:52,593 train 550 1.134005e-02 0.201991
2019-11-13 13:29:58,522 train 600 1.137728e-02 0.209436
2019-11-13 13:30:04,498 train 650 1.138593e-02 0.217085
2019-11-13 13:30:10,429 train 700 1.139159e-02 0.220607
2019-11-13 13:30:16,355 train 750 1.139412e-02 0.222707
2019-11-13 13:30:22,274 train 800 1.138493e-02 0.226336
2019-11-13 13:30:28,217 train 850 1.137620e-02 0.232655
2019-11-13 13:30:29,964 training loss; R2: 1.136867e-02 0.234217
2019-11-13 13:30:30,262 valid 000 1.056549e-02 0.399123
2019-11-13 13:30:31,988 valid 050 9.834765e-03 0.343102
2019-11-13 13:30:33,547 validation loss; R2: 9.805947e-03 0.348591
2019-11-13 13:30:33,563 epoch 16 lr 1.000000e-03
2019-11-13 13:30:33,947 train 000 9.209572e-03 0.425225
2019-11-13 13:30:39,929 train 050 1.120614e-02 0.317952
2019-11-13 13:30:46,150 train 100 1.131383e-02 0.250514
2019-11-13 13:30:52,354 train 150 1.122211e-02 0.269499
2019-11-13 13:30:58,267 train 200 1.121031e-02 0.280597
2019-11-13 13:31:04,188 train 250 1.123732e-02 0.289510
2019-11-13 13:31:10,099 train 300 1.122638e-02 0.277759
2019-11-13 13:31:15,991 train 350 1.124148e-02 0.271389
2019-11-13 13:31:21,895 train 400 1.124879e-02 0.262423
2019-11-13 13:31:27,804 train 450 1.123781e-02 0.251516
2019-11-13 13:31:33,702 train 500 1.126230e-02 0.238279
2019-11-13 13:31:39,605 train 550 1.125111e-02 0.243799
2019-11-13 13:31:45,503 train 600 1.126095e-02 0.250797
2019-11-13 13:31:51,413 train 650 1.127382e-02 0.254807
2019-11-13 13:31:57,318 train 700 1.125849e-02 0.259586
2019-11-13 13:32:03,217 train 750 1.124917e-02 0.263945
2019-11-13 13:32:09,115 train 800 1.126684e-02 0.260969
2019-11-13 13:32:15,014 train 850 1.126234e-02 0.259796
2019-11-13 13:32:16,771 training loss; R2: 1.126263e-02 0.260822
2019-11-13 13:32:17,073 valid 000 8.423494e-03 0.170646
2019-11-13 13:32:18,745 valid 050 9.910982e-03 0.401404
2019-11-13 13:32:20,259 validation loss; R2: 9.926926e-03 0.402666
2019-11-13 13:32:20,272 epoch 17 lr 1.000000e-03
2019-11-13 13:32:20,622 train 000 1.057867e-02 0.383550
2019-11-13 13:32:26,263 train 050 1.109049e-02 0.336933
2019-11-13 13:32:32,189 train 100 1.115526e-02 0.323528
2019-11-13 13:32:38,113 train 150 1.118926e-02 0.323288
2019-11-13 13:32:44,037 train 200 1.107952e-02 0.318529
2019-11-13 13:32:49,963 train 250 1.106383e-02 0.315678
2019-11-13 13:32:55,885 train 300 1.103245e-02 0.305006
2019-11-13 13:33:01,811 train 350 1.105123e-02 0.306484
2019-11-13 13:33:07,726 train 400 1.107452e-02 0.298212
2019-11-13 13:33:13,666 train 450 1.109174e-02 0.298967
2019-11-13 13:33:19,585 train 500 1.109441e-02 0.301842
2019-11-13 13:33:25,509 train 550 1.112747e-02 0.305660
2019-11-13 13:33:31,432 train 600 1.111281e-02 0.306798
2019-11-13 13:33:37,418 train 650 1.111911e-02 0.307977
2019-11-13 13:33:43,668 train 700 1.111617e-02 0.308370
2019-11-13 13:33:49,808 train 750 1.114288e-02 0.307399
2019-11-13 13:33:55,686 train 800 1.113422e-02 0.305298
2019-11-13 13:34:01,594 train 850 1.113181e-02 0.306771
2019-11-13 13:34:03,360 training loss; R2: 1.112808e-02 0.305772
2019-11-13 13:34:03,660 valid 000 1.007156e-02 0.415475
2019-11-13 13:34:05,337 valid 050 9.517758e-03 0.425101
2019-11-13 13:34:06,902 validation loss; R2: 9.425103e-03 0.418595
2019-11-13 13:34:06,914 epoch 18 lr 1.000000e-03
2019-11-13 13:34:07,229 train 000 1.048537e-02 0.332801
2019-11-13 13:34:13,016 train 050 1.100935e-02 0.355209
2019-11-13 13:34:18,832 train 100 1.096606e-02 0.336837
2019-11-13 13:34:24,517 train 150 1.102615e-02 0.326569
2019-11-13 13:34:30,200 train 200 1.103224e-02 0.329230
2019-11-13 13:34:35,860 train 250 1.105260e-02 0.327242
2019-11-13 13:34:41,766 train 300 1.112390e-02 0.327537
2019-11-13 13:34:47,754 train 350 1.113206e-02 0.330161
2019-11-13 13:34:53,888 train 400 1.109956e-02 0.331708
2019-11-13 13:34:59,752 train 450 1.110926e-02 0.331989
2019-11-13 13:35:05,615 train 500 1.107798e-02 0.331881
2019-11-13 13:35:11,475 train 550 1.108541e-02 0.325409
2019-11-13 13:35:17,317 train 600 1.109862e-02 0.323244
2019-11-13 13:35:23,162 train 650 1.110816e-02 0.320361
2019-11-13 13:35:29,001 train 700 1.113958e-02 0.313655
2019-11-13 13:35:34,836 train 750 1.114081e-02 0.309917
2019-11-13 13:35:40,681 train 800 1.116203e-02 0.310594
2019-11-13 13:35:46,519 train 850 1.115138e-02 0.310384
2019-11-13 13:35:48,256 training loss; R2: 1.115211e-02 0.311028
2019-11-13 13:35:48,545 valid 000 3.337951e-02 -0.697891
2019-11-13 13:35:50,227 valid 050 3.125260e-02 -0.922625
2019-11-13 13:35:51,749 validation loss; R2: 3.125423e-02 -0.947125
2019-11-13 13:35:51,768 epoch 19 lr 1.000000e-03
2019-11-13 13:35:52,115 train 000 1.153363e-02 0.391116
2019-11-13 13:35:57,991 train 050 1.133593e-02 0.332966
2019-11-13 13:36:04,013 train 100 1.126482e-02 0.335382
2019-11-13 13:36:09,954 train 150 1.119079e-02 0.301196
2019-11-13 13:36:15,884 train 200 1.115486e-02 0.306792
2019-11-13 13:36:21,946 train 250 1.113875e-02 0.307796
2019-11-13 13:36:27,913 train 300 1.108469e-02 0.304529
2019-11-13 13:36:33,928 train 350 1.106745e-02 0.298434
2019-11-13 13:36:39,902 train 400 1.106058e-02 0.299635
2019-11-13 13:36:45,904 train 450 1.106835e-02 0.302730
2019-11-13 13:36:51,881 train 500 1.104817e-02 0.301486
2019-11-13 13:36:57,938 train 550 1.103688e-02 0.301993
2019-11-13 13:37:04,050 train 600 1.100532e-02 0.306346
2019-11-13 13:37:10,152 train 650 1.101924e-02 0.306631
2019-11-13 13:37:16,137 train 700 1.101647e-02 0.307640
2019-11-13 13:37:22,126 train 750 1.100787e-02 0.308594
2019-11-13 13:37:28,112 train 800 1.099365e-02 0.306707
2019-11-13 13:37:34,132 train 850 1.098856e-02 0.308068
2019-11-13 13:37:35,894 training loss; R2: 1.098607e-02 0.308058
2019-11-13 13:37:36,189 valid 000 1.192951e-02 0.436269
2019-11-13 13:37:37,905 valid 050 9.880667e-03 0.416769
2019-11-13 13:37:39,474 validation loss; R2: 9.884647e-03 0.419547
