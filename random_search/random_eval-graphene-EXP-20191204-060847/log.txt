2019-12-04 06:08:47,676 gpu device = 1
2019-12-04 06:08:47,676 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=96, cutout=False, cutout_length=16, data='../data', dataset='graphene', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name=None, gpu=1, grad_clip=5, gz_dtree=False, init_channels=36, layers=8, learning_rate=0.002, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-graphene-EXP-20191204-060847', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-12-04 06:08:50,898 param size = 1.239517MB
2019-12-04 06:08:50,901 epoch 0 lr 2.000000e-03
2019-12-04 06:08:53,649 train 000 3.720114e-01 -17.999424
2019-12-04 06:09:07,275 train 050 2.568498e-01 -11.801960
2019-12-04 06:09:20,835 train 100 1.427032e-01 -5.904605
2019-12-04 06:09:34,392 train 150 1.031727e-01 -3.867000
2019-12-04 06:09:47,953 train 200 8.269339e-02 -2.823373
2019-12-04 06:09:55,170 training loss; R2: 7.668624e-02 -2.503718
2019-12-04 06:09:55,312 valid 000 1.903824e-02 0.551154
2019-12-04 06:09:57,041 validation loss; R2: 1.583531e-02 0.491764
2019-12-04 06:09:57,064 epoch 1 lr 2.000000e-03
2019-12-04 06:09:57,482 train 000 2.128085e-02 0.225529
2019-12-04 06:10:10,714 train 050 1.734518e-02 0.393154
2019-12-04 06:10:23,945 train 100 1.632694e-02 0.454685
2019-12-04 06:10:37,179 train 150 1.547320e-02 0.490032
2019-12-04 06:10:50,412 train 200 1.464691e-02 0.510768
2019-12-04 06:10:56,362 training loss; R2: 1.463925e-02 0.524529
2019-12-04 06:10:56,500 valid 000 6.935752e-03 0.832733
2019-12-04 06:10:57,909 validation loss; R2: 6.634826e-03 0.775658
2019-12-04 06:10:57,933 epoch 2 lr 2.000000e-03
2019-12-04 06:10:58,282 train 000 1.250441e-02 0.632660
2019-12-04 06:11:11,505 train 050 1.078372e-02 0.641237
2019-12-04 06:11:24,721 train 100 1.130380e-02 0.634591
2019-12-04 06:11:37,945 train 150 1.090646e-02 0.638527
2019-12-04 06:11:51,173 train 200 1.024034e-02 0.655653
2019-12-04 06:11:57,121 training loss; R2: 1.007212e-02 0.661964
2019-12-04 06:11:57,265 valid 000 4.719642e-03 0.793953
2019-12-04 06:11:58,675 validation loss; R2: 5.443201e-03 0.811745
2019-12-04 06:11:58,699 epoch 3 lr 2.000000e-03
2019-12-04 06:11:59,049 train 000 1.171597e-02 0.807689
2019-12-04 06:12:12,280 train 050 8.846966e-03 0.727702
2019-12-04 06:12:25,512 train 100 8.241618e-03 0.718765
2019-12-04 06:12:38,743 train 150 7.891812e-03 0.733731
2019-12-04 06:12:51,978 train 200 8.033561e-03 0.730083
2019-12-04 06:12:57,927 training loss; R2: 8.033376e-03 0.731109
2019-12-04 06:12:58,065 valid 000 4.122157e-03 0.740494
2019-12-04 06:12:59,475 validation loss; R2: 5.450104e-03 0.810446
2019-12-04 06:12:59,500 epoch 4 lr 2.000000e-03
2019-12-04 06:12:59,849 train 000 6.434777e-03 0.766976
2019-12-04 06:13:13,082 train 050 7.798856e-03 0.737324
2019-12-04 06:13:26,316 train 100 7.440320e-03 0.750124
2019-12-04 06:13:39,552 train 150 7.255663e-03 0.757091
2019-12-04 06:13:52,792 train 200 7.272860e-03 0.759750
2019-12-04 06:13:58,746 training loss; R2: 7.191801e-03 0.761218
2019-12-04 06:13:58,884 valid 000 2.600992e-03 0.776569
2019-12-04 06:14:00,294 validation loss; R2: 5.131101e-03 0.830192
2019-12-04 06:14:00,318 epoch 5 lr 2.000000e-03
2019-12-04 06:14:00,669 train 000 4.234894e-03 0.774505
2019-12-04 06:14:13,907 train 050 6.182230e-03 0.776683
2019-12-04 06:14:27,145 train 100 6.518907e-03 0.775136
2019-12-04 06:14:40,381 train 150 6.585091e-03 0.777736
2019-12-04 06:14:53,616 train 200 6.588624e-03 0.780650
2019-12-04 06:14:59,566 training loss; R2: 6.513073e-03 0.779523
2019-12-04 06:14:59,706 valid 000 4.352647e-03 0.622947
2019-12-04 06:15:01,117 validation loss; R2: 6.340866e-03 0.792557
2019-12-04 06:15:01,146 epoch 6 lr 2.000000e-03
2019-12-04 06:15:01,495 train 000 1.905978e-02 0.683138
2019-12-04 06:15:14,726 train 050 6.259341e-03 0.783107
2019-12-04 06:15:27,952 train 100 6.621576e-03 0.771504
2019-12-04 06:15:41,186 train 150 6.363611e-03 0.776353
2019-12-04 06:15:54,418 train 200 6.163922e-03 0.781576
2019-12-04 06:16:00,365 training loss; R2: 6.240565e-03 0.785070
2019-12-04 06:16:00,508 valid 000 2.586897e-03 0.886088
2019-12-04 06:16:01,918 validation loss; R2: 3.928481e-03 0.868659
2019-12-04 06:16:01,942 epoch 7 lr 2.000000e-03
2019-12-04 06:16:02,290 train 000 4.913577e-03 0.757409
2019-12-04 06:16:15,527 train 050 6.320428e-03 0.806367
2019-12-04 06:16:28,760 train 100 6.455896e-03 0.789595
2019-12-04 06:16:41,995 train 150 6.464846e-03 0.789089
2019-12-04 06:16:55,227 train 200 6.227943e-03 0.790388
2019-12-04 06:17:01,179 training loss; R2: 6.175217e-03 0.791540
2019-12-04 06:17:01,317 valid 000 2.501065e-03 0.917359
2019-12-04 06:17:02,727 validation loss; R2: 3.969804e-03 0.867902
2019-12-04 06:17:02,751 epoch 8 lr 2.000000e-03
2019-12-04 06:17:03,101 train 000 3.554386e-03 0.799272
2019-12-04 06:17:16,328 train 050 5.551063e-03 0.790804
2019-12-04 06:17:29,550 train 100 5.713900e-03 0.800664
2019-12-04 06:17:42,771 train 150 5.663614e-03 0.805947
2019-12-04 06:17:55,994 train 200 5.578163e-03 0.808825
2019-12-04 06:18:01,942 training loss; R2: 5.565914e-03 0.807698
2019-12-04 06:18:02,080 valid 000 3.165400e-03 0.921687
2019-12-04 06:18:03,490 validation loss; R2: 4.024924e-03 0.861008
2019-12-04 06:18:03,514 epoch 9 lr 2.000000e-03
2019-12-04 06:18:03,863 train 000 5.281927e-03 0.838956
2019-12-04 06:18:17,097 train 050 4.913212e-03 0.825964
2019-12-04 06:18:30,336 train 100 5.064980e-03 0.825889
2019-12-04 06:18:43,566 train 150 5.201352e-03 0.826909
2019-12-04 06:18:56,797 train 200 5.321742e-03 0.815774
2019-12-04 06:19:02,749 training loss; R2: 5.329855e-03 0.816270
2019-12-04 06:19:02,888 valid 000 7.275543e-03 0.702863
2019-12-04 06:19:04,298 validation loss; R2: 8.615408e-03 0.724975
2019-12-04 06:19:04,323 epoch 10 lr 2.000000e-03
2019-12-04 06:19:04,672 train 000 6.638011e-03 0.801112
2019-12-04 06:19:17,907 train 050 5.930074e-03 0.799254
2019-12-04 06:19:31,133 train 100 5.730912e-03 0.799681
2019-12-04 06:19:44,357 train 150 5.623397e-03 0.808950
2019-12-04 06:19:57,590 train 200 5.575461e-03 0.811425
2019-12-04 06:20:03,544 training loss; R2: 5.553592e-03 0.811660
2019-12-04 06:20:03,687 valid 000 3.500178e-03 0.821441
2019-12-04 06:20:05,096 validation loss; R2: 4.141464e-03 0.860111
2019-12-04 06:20:05,121 epoch 11 lr 2.000000e-03
2019-12-04 06:20:05,471 train 000 5.538737e-03 0.779860
2019-12-04 06:20:18,697 train 050 5.431779e-03 0.828684
2019-12-04 06:20:31,925 train 100 5.381352e-03 0.821002
2019-12-04 06:20:45,146 train 150 5.389246e-03 0.819980
2019-12-04 06:20:58,371 train 200 5.310911e-03 0.818294
2019-12-04 06:21:04,323 training loss; R2: 5.303278e-03 0.819593
2019-12-04 06:21:04,468 valid 000 2.116123e-03 0.850678
2019-12-04 06:21:05,877 validation loss; R2: 3.904272e-03 0.865647
2019-12-04 06:21:05,903 epoch 12 lr 2.000000e-03
2019-12-04 06:21:06,252 train 000 2.783637e-03 0.866191
2019-12-04 06:21:19,478 train 050 4.647391e-03 0.839745
2019-12-04 06:21:32,702 train 100 4.717987e-03 0.842395
2019-12-04 06:21:45,930 train 150 4.687067e-03 0.840322
2019-12-04 06:21:59,153 train 200 4.970525e-03 0.833790
2019-12-04 06:22:05,098 training loss; R2: 4.993569e-03 0.832834
2019-12-04 06:22:05,238 valid 000 5.987049e-01 -12.205523
2019-12-04 06:22:06,648 validation loss; R2: 6.152299e-01 -21.564689
2019-12-04 06:22:06,673 epoch 13 lr 2.000000e-03
2019-12-04 06:22:07,026 train 000 2.952029e-03 0.876795
2019-12-04 06:22:20,239 train 050 4.601547e-03 0.834122
2019-12-04 06:22:33,453 train 100 4.658305e-03 0.837358
2019-12-04 06:22:46,673 train 150 5.000345e-03 0.830712
2019-12-04 06:22:59,884 train 200 5.092978e-03 0.827233
2019-12-04 06:23:05,827 training loss; R2: 5.076685e-03 0.830183
2019-12-04 06:23:05,972 valid 000 2.170948e-01 -7.551714
2019-12-04 06:23:07,381 validation loss; R2: 2.262396e-01 -7.421127
2019-12-04 06:23:07,406 epoch 14 lr 2.000000e-03
2019-12-04 06:23:07,756 train 000 3.998837e-03 0.848141
2019-12-04 06:23:20,964 train 050 4.462334e-03 0.835405
2019-12-04 06:23:34,174 train 100 4.601564e-03 0.830266
2019-12-04 06:23:47,382 train 150 5.024144e-03 0.827550
2019-12-04 06:24:00,592 train 200 5.040307e-03 0.824582
2019-12-04 06:24:06,531 training loss; R2: 4.990493e-03 0.828196
2019-12-04 06:24:06,670 valid 000 5.050676e-01 -19.119390
2019-12-04 06:24:08,079 validation loss; R2: 4.770661e-01 -17.611348
2019-12-04 06:24:08,104 epoch 15 lr 2.000000e-03
2019-12-04 06:24:08,454 train 000 3.710207e-03 0.917081
2019-12-04 06:24:21,656 train 050 4.070231e-03 0.854790
2019-12-04 06:24:34,863 train 100 4.260729e-03 0.850755
2019-12-04 06:24:48,066 train 150 4.728897e-03 0.837731
2019-12-04 06:25:01,264 train 200 4.635582e-03 0.839230
2019-12-04 06:25:07,200 training loss; R2: 4.635288e-03 0.840061
2019-12-04 06:25:07,350 valid 000 1.614508e+02 -4373.448467
2019-12-04 06:25:08,758 validation loss; R2: 1.618311e+02 -6006.968308
2019-12-04 06:25:08,783 epoch 16 lr 2.000000e-03
2019-12-04 06:25:09,129 train 000 4.400067e-03 0.764483
2019-12-04 06:25:22,332 train 050 4.299294e-03 0.832191
2019-12-04 06:25:35,534 train 100 4.505860e-03 0.837264
2019-12-04 06:25:48,736 train 150 4.631254e-03 0.833911
2019-12-04 06:26:01,938 train 200 4.693485e-03 0.834418
2019-12-04 06:26:07,873 training loss; R2: 4.725806e-03 0.833094
2019-12-04 06:26:08,012 valid 000 4.424892e+00 -176.063435
2019-12-04 06:26:09,420 validation loss; R2: 4.442345e+00 -168.229826
2019-12-04 06:26:09,446 epoch 17 lr 2.000000e-03
2019-12-04 06:26:09,795 train 000 1.168113e-02 0.786039
2019-12-04 06:26:22,984 train 050 5.216486e-03 0.831058
2019-12-04 06:26:36,173 train 100 5.042487e-03 0.834382
2019-12-04 06:26:49,358 train 150 4.815789e-03 0.838983
2019-12-04 06:27:02,540 train 200 4.614296e-03 0.842717
2019-12-04 06:27:08,467 training loss; R2: 4.598161e-03 0.842098
2019-12-04 06:27:08,607 valid 000 6.329317e+00 -97.896480
2019-12-04 06:27:10,015 validation loss; R2: 6.463022e+00 -241.176444
2019-12-04 06:27:10,041 epoch 18 lr 2.000000e-03
2019-12-04 06:27:10,390 train 000 4.094384e-03 0.891044
2019-12-04 06:27:23,565 train 050 4.201118e-03 0.844138
2019-12-04 06:27:36,745 train 100 4.382496e-03 0.835562
2019-12-04 06:27:49,930 train 150 4.456036e-03 0.840186
2019-12-04 06:28:03,107 train 200 4.540605e-03 0.839876
2019-12-04 06:28:09,035 training loss; R2: 4.541873e-03 0.840610
2019-12-04 06:28:09,172 valid 000 5.360223e-02 -0.523833
2019-12-04 06:28:10,580 validation loss; R2: 5.208730e-02 -0.878232
2019-12-04 06:28:10,605 epoch 19 lr 2.000000e-03
2019-12-04 06:28:10,953 train 000 7.093221e-03 0.825735
2019-12-04 06:28:24,126 train 050 4.759349e-03 0.846899
2019-12-04 06:28:37,299 train 100 4.694119e-03 0.852156
2019-12-04 06:28:50,466 train 150 4.528302e-03 0.852246
2019-12-04 06:29:03,636 train 200 4.441428e-03 0.853030
2019-12-04 06:29:09,558 training loss; R2: 4.397897e-03 0.850150
2019-12-04 06:29:09,695 valid 000 3.326331e+00 -61.700243
2019-12-04 06:29:11,103 validation loss; R2: 3.270941e+00 -122.797734
