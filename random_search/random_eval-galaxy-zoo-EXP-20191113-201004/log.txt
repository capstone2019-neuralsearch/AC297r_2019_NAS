2019-11-13 20:10:04,356 gpu device = 1
2019-11-13 20:10:04,356 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name='galaxy-zoo-cleaned', gpu=1, grad_clip=5, gz_dtree=False, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-galaxy-zoo-EXP-20191113-201004', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-11-13 20:10:16,043 param size = 0.232853MB
2019-11-13 20:10:16,046 epoch 0 lr 1.000000e-03
2019-11-13 20:10:18,209 train 000 5.008334e-01 -381.823086
2019-11-13 20:10:24,680 train 050 5.220198e-02 -13.021712
2019-11-13 20:10:31,049 train 100 4.093750e-02 -7.016344
2019-11-13 20:10:37,348 train 150 3.634382e-02 -4.868251
2019-11-13 20:10:43,745 train 200 3.399350e-02 -3.766862
2019-11-13 20:10:50,161 train 250 3.249379e-02 -3.065000
2019-11-13 20:10:56,619 train 300 3.130156e-02 -2.595251
2019-11-13 20:11:03,033 train 350 3.043207e-02 -2.252272
2019-11-13 20:11:09,446 train 400 2.966510e-02 -2.014580
2019-11-13 20:11:15,832 train 450 2.898396e-02 -1.806650
2019-11-13 20:11:22,176 train 500 2.840789e-02 -1.643266
2019-11-13 20:11:28,241 train 550 2.795355e-02 -1.502074
2019-11-13 20:11:34,417 train 600 2.748293e-02 -1.382010
2019-11-13 20:11:40,849 train 650 2.705830e-02 -1.279401
2019-11-13 20:11:47,238 train 700 2.665367e-02 -1.190818
2019-11-13 20:11:53,307 train 750 2.630700e-02 -1.115111
2019-11-13 20:11:59,364 train 800 2.602555e-02 -1.047699
2019-11-13 20:12:05,386 train 850 2.571298e-02 -0.986934
2019-11-13 20:12:07,912 training loss; R2: 2.561999e-02 -0.969298
2019-11-13 20:12:08,279 valid 000 1.864954e-02 0.155353
2019-11-13 20:12:10,025 valid 050 1.985357e-02 0.086041
2019-11-13 20:12:11,705 validation loss; R2: 1.949651e-02 0.045289
2019-11-13 20:12:11,721 epoch 1 lr 1.000000e-03
2019-11-13 20:12:12,253 train 000 1.987698e-02 -0.073165
2019-11-13 20:12:18,279 train 050 2.075189e-02 0.031602
2019-11-13 20:12:24,304 train 100 2.072618e-02 0.026335
2019-11-13 20:12:30,332 train 150 2.045706e-02 0.031061
2019-11-13 20:12:36,364 train 200 2.030801e-02 0.038830
2019-11-13 20:12:42,399 train 250 2.023076e-02 0.041778
2019-11-13 20:12:48,461 train 300 2.016164e-02 0.046626
2019-11-13 20:12:54,507 train 350 2.012118e-02 0.049166
2019-11-13 20:13:00,554 train 400 2.005783e-02 0.021394
2019-11-13 20:13:06,581 train 450 2.001941e-02 0.023038
2019-11-13 20:13:12,630 train 500 1.990806e-02 0.008677
2019-11-13 20:13:18,669 train 550 1.983652e-02 0.014609
2019-11-13 20:13:24,700 train 600 1.975358e-02 0.018990
2019-11-13 20:13:30,730 train 650 1.968569e-02 0.018270
2019-11-13 20:13:36,870 train 700 1.962078e-02 0.023600
2019-11-13 20:13:42,931 train 750 1.957892e-02 0.029203
2019-11-13 20:13:48,954 train 800 1.952289e-02 0.032078
2019-11-13 20:13:54,972 train 850 1.946367e-02 0.037710
2019-11-13 20:13:56,781 training loss; R2: 1.945477e-02 0.037994
2019-11-13 20:13:57,139 valid 000 1.958196e-02 0.182805
2019-11-13 20:13:58,844 valid 050 1.840299e-02 0.127576
2019-11-13 20:14:00,449 validation loss; R2: 1.829466e-02 0.098755
2019-11-13 20:14:00,463 epoch 2 lr 1.000000e-03
2019-11-13 20:14:00,893 train 000 2.032449e-02 0.138772
2019-11-13 20:14:07,244 train 050 1.818680e-02 0.092317
2019-11-13 20:14:13,342 train 100 1.791143e-02 0.092037
2019-11-13 20:14:19,428 train 150 1.781497e-02 0.102876
2019-11-13 20:14:25,840 train 200 1.787771e-02 0.112686
2019-11-13 20:14:32,249 train 250 1.779318e-02 0.116185
2019-11-13 20:14:38,382 train 300 1.775497e-02 0.113633
2019-11-13 20:14:44,523 train 350 1.767671e-02 0.116875
2019-11-13 20:14:50,568 train 400 1.770418e-02 0.118500
2019-11-13 20:14:56,611 train 450 1.770118e-02 0.117230
2019-11-13 20:15:02,656 train 500 1.763594e-02 0.119195
2019-11-13 20:15:08,707 train 550 1.762255e-02 0.123134
2019-11-13 20:15:14,827 train 600 1.758913e-02 0.125489
2019-11-13 20:15:21,117 train 650 1.754854e-02 0.128782
2019-11-13 20:15:27,483 train 700 1.748678e-02 0.130421
2019-11-13 20:15:33,533 train 750 1.744186e-02 0.131645
2019-11-13 20:15:39,592 train 800 1.742436e-02 0.133132
2019-11-13 20:15:45,633 train 850 1.740677e-02 0.133113
2019-11-13 20:15:47,442 training loss; R2: 1.738925e-02 0.134000
2019-11-13 20:15:47,794 valid 000 1.994168e-02 0.111273
2019-11-13 20:15:49,531 valid 050 1.905490e-02 -0.367755
2019-11-13 20:15:51,147 validation loss; R2: 1.899045e-02 -0.178086
2019-11-13 20:15:51,167 epoch 3 lr 1.000000e-03
2019-11-13 20:15:51,644 train 000 1.478397e-02 0.282837
2019-11-13 20:15:57,950 train 050 1.650678e-02 0.042255
2019-11-13 20:16:04,046 train 100 1.659016e-02 0.093701
2019-11-13 20:16:10,353 train 150 1.659296e-02 0.117601
2019-11-13 20:16:16,590 train 200 1.644581e-02 0.133432
2019-11-13 20:16:22,657 train 250 1.651465e-02 0.140071
2019-11-13 20:16:28,724 train 300 1.645299e-02 0.146070
2019-11-13 20:16:34,822 train 350 1.646273e-02 0.145567
2019-11-13 20:16:40,882 train 400 1.643430e-02 0.149538
2019-11-13 20:16:46,957 train 450 1.642345e-02 0.153294
2019-11-13 20:16:53,022 train 500 1.636166e-02 0.155331
2019-11-13 20:16:59,091 train 550 1.632615e-02 0.157555
2019-11-13 20:17:05,156 train 600 1.627336e-02 0.163295
2019-11-13 20:17:11,223 train 650 1.623493e-02 0.166188
2019-11-13 20:17:17,292 train 700 1.620548e-02 0.167411
2019-11-13 20:17:23,360 train 750 1.619252e-02 0.169450
2019-11-13 20:17:29,436 train 800 1.617066e-02 0.168819
2019-11-13 20:17:35,508 train 850 1.614118e-02 0.167464
2019-11-13 20:17:37,317 training loss; R2: 1.614603e-02 0.168058
2019-11-13 20:17:37,677 valid 000 1.345956e-02 0.036436
2019-11-13 20:17:39,412 valid 050 1.436519e-02 0.211631
2019-11-13 20:17:41,023 validation loss; R2: 1.435010e-02 0.206576
2019-11-13 20:17:41,037 epoch 4 lr 1.000000e-03
2019-11-13 20:17:41,508 train 000 1.559081e-02 0.133833
2019-11-13 20:17:47,882 train 050 1.571098e-02 0.196139
2019-11-13 20:17:54,120 train 100 1.566375e-02 0.197739
2019-11-13 20:18:00,335 train 150 1.574553e-02 0.200449
2019-11-13 20:18:06,509 train 200 1.567222e-02 0.195406
2019-11-13 20:18:12,565 train 250 1.555727e-02 0.199260
2019-11-13 20:18:18,626 train 300 1.558354e-02 0.201441
2019-11-13 20:18:24,683 train 350 1.559036e-02 0.190481
2019-11-13 20:18:30,740 train 400 1.552344e-02 0.184101
2019-11-13 20:18:36,794 train 450 1.546581e-02 0.186334
2019-11-13 20:18:42,855 train 500 1.545696e-02 0.185343
2019-11-13 20:18:48,913 train 550 1.543120e-02 0.187593
2019-11-13 20:18:54,969 train 600 1.537294e-02 0.188279
2019-11-13 20:19:01,037 train 650 1.534777e-02 0.192692
2019-11-13 20:19:07,120 train 700 1.532652e-02 0.193474
2019-11-13 20:19:13,181 train 750 1.529923e-02 0.194204
2019-11-13 20:19:19,248 train 800 1.527265e-02 0.190429
2019-11-13 20:19:25,297 train 850 1.525576e-02 0.192125
2019-11-13 20:19:27,105 training loss; R2: 1.524489e-02 0.193458
2019-11-13 20:19:27,463 valid 000 1.215157e-02 0.355775
2019-11-13 20:19:29,240 valid 050 1.284936e-02 0.242615
2019-11-13 20:19:30,858 validation loss; R2: 1.293804e-02 0.246026
2019-11-13 20:19:30,872 epoch 5 lr 1.000000e-03
2019-11-13 20:19:31,266 train 000 1.352436e-02 0.215991
2019-11-13 20:19:37,680 train 050 1.460312e-02 0.169637
2019-11-13 20:19:43,942 train 100 1.470468e-02 0.192477
2019-11-13 20:19:50,350 train 150 1.462545e-02 0.209322
2019-11-13 20:19:56,524 train 200 1.474219e-02 0.207132
2019-11-13 20:20:02,750 train 250 1.469930e-02 0.196513
2019-11-13 20:20:08,816 train 300 1.467086e-02 0.202142
2019-11-13 20:20:14,864 train 350 1.460981e-02 0.194201
2019-11-13 20:20:20,896 train 400 1.459099e-02 0.197555
2019-11-13 20:20:26,929 train 450 1.461142e-02 0.199970
2019-11-13 20:20:32,977 train 500 1.458916e-02 0.180165
2019-11-13 20:20:39,015 train 550 1.455025e-02 0.180436
2019-11-13 20:20:45,059 train 600 1.452457e-02 0.184063
2019-11-13 20:20:51,093 train 650 1.448074e-02 0.186595
2019-11-13 20:20:57,127 train 700 1.447583e-02 0.190576
2019-11-13 20:21:03,163 train 750 1.447356e-02 0.191536
2019-11-13 20:21:09,201 train 800 1.447367e-02 0.194914
2019-11-13 20:21:15,232 train 850 1.445375e-02 0.198549
2019-11-13 20:21:17,037 training loss; R2: 1.444754e-02 0.199167
2019-11-13 20:21:17,405 valid 000 1.008380e-02 0.346987
2019-11-13 20:21:19,188 valid 050 1.260714e-02 0.205390
2019-11-13 20:21:20,810 validation loss; R2: 1.253562e-02 0.209526
2019-11-13 20:21:20,830 epoch 6 lr 1.000000e-03
2019-11-13 20:21:21,262 train 000 1.520615e-02 0.261371
2019-11-13 20:21:27,620 train 050 1.369308e-02 0.195896
2019-11-13 20:21:33,849 train 100 1.397314e-02 0.215631
2019-11-13 20:21:40,219 train 150 1.410793e-02 0.222581
2019-11-13 20:21:46,514 train 200 1.409612e-02 0.207962
2019-11-13 20:21:52,556 train 250 1.409252e-02 0.215722
2019-11-13 20:21:58,587 train 300 1.407720e-02 0.221358
2019-11-13 20:22:04,633 train 350 1.402466e-02 0.223132
2019-11-13 20:22:10,665 train 400 1.397030e-02 0.226933
2019-11-13 20:22:16,700 train 450 1.397728e-02 0.229574
2019-11-13 20:22:22,729 train 500 1.393587e-02 0.221388
2019-11-13 20:22:28,765 train 550 1.392899e-02 0.224030
2019-11-13 20:22:34,813 train 600 1.385285e-02 0.224575
2019-11-13 20:22:40,850 train 650 1.384842e-02 0.223352
2019-11-13 20:22:46,892 train 700 1.385902e-02 0.224989
2019-11-13 20:22:52,926 train 750 1.383257e-02 0.227827
2019-11-13 20:22:58,967 train 800 1.381835e-02 0.212123
2019-11-13 20:23:05,001 train 850 1.380971e-02 0.214447
2019-11-13 20:23:06,805 training loss; R2: 1.380886e-02 0.207463
2019-11-13 20:23:07,171 valid 000 7.564034e-02 -5.961845
2019-11-13 20:23:08,973 valid 050 7.379741e-02 -4.302491
2019-11-13 20:23:10,608 validation loss; R2: 7.442630e-02 -4.728904
2019-11-13 20:23:10,622 epoch 7 lr 1.000000e-03
2019-11-13 20:23:11,020 train 000 1.267079e-02 0.272949
2019-11-13 20:23:17,366 train 050 1.374248e-02 -13.354667
2019-11-13 20:23:23,815 train 100 1.373461e-02 -6.781354
2019-11-13 20:23:30,240 train 150 1.357467e-02 -4.442059
2019-11-13 20:23:36,452 train 200 1.345335e-02 -3.277812
2019-11-13 20:23:42,521 train 250 1.341673e-02 -2.571037
2019-11-13 20:23:48,609 train 300 1.342521e-02 -2.263722
2019-11-13 20:23:54,677 train 350 1.339094e-02 -1.904353
2019-11-13 20:24:00,753 train 400 1.337501e-02 -1.650294
2019-11-13 20:24:07,006 train 450 1.336075e-02 -1.437430
2019-11-13 20:24:13,065 train 500 1.333303e-02 -1.266914
2019-11-13 20:24:19,125 train 550 1.332820e-02 -1.135417
2019-11-13 20:24:25,181 train 600 1.333631e-02 -1.022169
2019-11-13 20:24:31,217 train 650 1.331569e-02 -0.928322
2019-11-13 20:24:37,259 train 700 1.330829e-02 -0.865712
2019-11-13 20:24:43,298 train 750 1.328131e-02 -0.807869
2019-11-13 20:24:49,348 train 800 1.329302e-02 -0.741709
2019-11-13 20:24:55,389 train 850 1.327044e-02 -0.683791
2019-11-13 20:24:57,195 training loss; R2: 1.327932e-02 -0.667281
2019-11-13 20:24:57,552 valid 000 3.347754e-01 -16.365580
2019-11-13 20:24:59,342 valid 050 3.406039e-01 -22.523170
2019-11-13 20:25:00,951 validation loss; R2: 3.395424e-01 -21.590788
2019-11-13 20:25:00,972 epoch 8 lr 1.000000e-03
2019-11-13 20:25:01,405 train 000 1.360260e-02 0.271663
2019-11-13 20:25:07,768 train 050 1.337895e-02 0.186462
2019-11-13 20:25:13,961 train 100 1.314684e-02 0.133325
2019-11-13 20:25:20,032 train 150 1.309709e-02 0.177397
2019-11-13 20:25:26,098 train 200 1.310944e-02 0.205996
2019-11-13 20:25:32,160 train 250 1.314988e-02 0.220364
2019-11-13 20:25:38,239 train 300 1.308591e-02 0.231068
2019-11-13 20:25:44,316 train 350 1.308096e-02 0.232870
2019-11-13 20:25:50,383 train 400 1.306249e-02 0.221889
2019-11-13 20:25:56,432 train 450 1.308505e-02 0.228922
2019-11-13 20:26:02,465 train 500 1.308028e-02 0.233498
2019-11-13 20:26:08,507 train 550 1.302880e-02 0.236293
2019-11-13 20:26:14,546 train 600 1.302611e-02 0.238949
2019-11-13 20:26:20,583 train 650 1.299891e-02 0.242062
2019-11-13 20:26:26,615 train 700 1.298049e-02 0.243659
2019-11-13 20:26:32,653 train 750 1.297556e-02 0.246261
2019-11-13 20:26:38,706 train 800 1.295909e-02 0.247510
2019-11-13 20:26:44,750 train 850 1.293630e-02 0.250147
2019-11-13 20:26:46,555 training loss; R2: 1.294499e-02 0.250036
2019-11-13 20:26:46,913 valid 000 9.082844e-02 -5.605215
2019-11-13 20:26:48,710 valid 050 9.106275e-02 -6.012564
2019-11-13 20:26:50,295 validation loss; R2: 9.096872e-02 -6.071074
2019-11-13 20:26:50,308 epoch 9 lr 1.000000e-03
2019-11-13 20:26:50,702 train 000 1.366404e-02 0.013935
2019-11-13 20:26:56,802 train 050 1.277366e-02 0.270714
2019-11-13 20:27:03,174 train 100 1.254012e-02 0.235328
2019-11-13 20:27:09,255 train 150 1.270435e-02 0.234334
2019-11-13 20:27:15,292 train 200 1.261993e-02 0.238306
2019-11-13 20:27:21,328 train 250 1.259269e-02 0.239329
2019-11-13 20:27:27,365 train 300 1.265763e-02 0.247729
2019-11-13 20:27:33,398 train 350 1.272451e-02 0.251956
2019-11-13 20:27:39,767 train 400 1.272040e-02 0.254424
2019-11-13 20:27:46,150 train 450 1.269623e-02 0.257886
2019-11-13 20:27:52,524 train 500 1.269325e-02 0.257102
2019-11-13 20:27:58,744 train 550 1.269519e-02 0.257440
2019-11-13 20:28:04,778 train 600 1.267654e-02 0.257478
2019-11-13 20:28:10,825 train 650 1.264866e-02 0.258724
2019-11-13 20:28:16,866 train 700 1.267031e-02 0.260213
2019-11-13 20:28:22,896 train 750 1.264037e-02 0.258228
2019-11-13 20:28:28,932 train 800 1.262030e-02 0.258912
2019-11-13 20:28:34,970 train 850 1.258762e-02 0.261574
2019-11-13 20:28:36,773 training loss; R2: 1.258743e-02 0.262307
2019-11-13 20:28:37,116 valid 000 4.089455e-02 -2.715696
2019-11-13 20:28:38,892 valid 050 4.076360e-02 -2.252761
2019-11-13 20:28:40,519 validation loss; R2: 4.087470e-02 -2.068715
2019-11-13 20:28:40,537 epoch 10 lr 1.000000e-03
2019-11-13 20:28:40,945 train 000 1.245471e-02 0.302211
2019-11-13 20:28:47,394 train 050 1.241002e-02 0.302207
2019-11-13 20:28:53,796 train 100 1.238084e-02 0.296520
2019-11-13 20:29:00,024 train 150 1.246080e-02 0.293310
2019-11-13 20:29:06,092 train 200 1.240766e-02 0.270585
2019-11-13 20:29:12,355 train 250 1.242356e-02 0.271556
2019-11-13 20:29:18,428 train 300 1.237499e-02 0.257128
2019-11-13 20:29:24,503 train 350 1.233596e-02 0.265412
2019-11-13 20:29:30,580 train 400 1.231950e-02 0.266796
2019-11-13 20:29:36,648 train 450 1.231708e-02 0.267147
2019-11-13 20:29:42,996 train 500 1.230467e-02 0.270352
2019-11-13 20:29:49,365 train 550 1.231130e-02 0.272678
2019-11-13 20:29:55,675 train 600 1.228908e-02 0.271843
2019-11-13 20:30:01,723 train 650 1.228281e-02 0.274554
2019-11-13 20:30:07,762 train 700 1.229654e-02 0.275945
2019-11-13 20:30:13,793 train 750 1.229988e-02 0.275500
2019-11-13 20:30:19,848 train 800 1.231059e-02 0.273800
2019-11-13 20:30:25,898 train 850 1.229824e-02 0.274658
2019-11-13 20:30:27,713 training loss; R2: 1.230946e-02 0.275007
2019-11-13 20:30:28,082 valid 000 1.250797e-01 -5.057983
2019-11-13 20:30:29,829 valid 050 1.242800e-01 -9.758823
2019-11-13 20:30:31,462 validation loss; R2: 1.246815e-01 -10.526557
2019-11-13 20:30:31,476 epoch 11 lr 1.000000e-03
2019-11-13 20:30:31,918 train 000 1.114473e-02 0.314188
2019-11-13 20:30:38,382 train 050 1.212969e-02 0.299207
2019-11-13 20:30:44,791 train 100 1.224630e-02 0.291809
2019-11-13 20:30:50,919 train 150 1.224784e-02 0.297335
2019-11-13 20:30:56,984 train 200 1.216690e-02 0.300513
2019-11-13 20:31:03,054 train 250 1.220401e-02 0.289471
2019-11-13 20:31:09,097 train 300 1.217000e-02 0.275710
2019-11-13 20:31:15,130 train 350 1.219409e-02 0.280040
2019-11-13 20:31:21,159 train 400 1.217226e-02 0.270801
2019-11-13 20:31:27,193 train 450 1.216392e-02 0.275360
2019-11-13 20:31:33,229 train 500 1.214607e-02 0.278470
2019-11-13 20:31:39,261 train 550 1.215416e-02 0.278593
2019-11-13 20:31:45,295 train 600 1.211847e-02 0.282579
2019-11-13 20:31:51,321 train 650 1.209818e-02 0.283766
2019-11-13 20:31:57,369 train 700 1.210392e-02 0.283712
2019-11-13 20:32:03,398 train 750 1.208996e-02 0.281217
2019-11-13 20:32:09,460 train 800 1.207052e-02 0.281498
2019-11-13 20:32:15,511 train 850 1.206856e-02 0.282601
2019-11-13 20:32:17,316 training loss; R2: 1.207427e-02 0.279527
2019-11-13 20:32:17,671 valid 000 6.552136e-01 -25.243289
2019-11-13 20:32:19,442 valid 050 6.517769e-01 -32.260973
2019-11-13 20:32:21,062 validation loss; R2: 6.536418e-01 -32.982547
2019-11-13 20:32:21,076 epoch 12 lr 1.000000e-03
2019-11-13 20:32:21,480 train 000 1.183303e-02 0.432625
2019-11-13 20:32:27,865 train 050 1.175351e-02 0.293619
2019-11-13 20:32:34,100 train 100 1.177769e-02 0.287176
2019-11-13 20:32:40,163 train 150 1.187378e-02 0.296003
2019-11-13 20:32:46,232 train 200 1.194549e-02 0.302854
2019-11-13 20:32:52,297 train 250 1.193010e-02 0.306492
2019-11-13 20:32:58,361 train 300 1.196074e-02 0.308996
2019-11-13 20:33:04,422 train 350 1.200152e-02 0.301019
2019-11-13 20:33:10,480 train 400 1.198222e-02 0.301736
2019-11-13 20:33:16,533 train 450 1.201913e-02 0.300461
2019-11-13 20:33:22,570 train 500 1.198688e-02 0.301309
2019-11-13 20:33:28,608 train 550 1.196491e-02 0.300598
2019-11-13 20:33:34,654 train 600 1.196137e-02 0.301429
2019-11-13 20:33:40,691 train 650 1.193895e-02 0.301414
2019-11-13 20:33:46,725 train 700 1.192105e-02 0.302152
2019-11-13 20:33:52,757 train 750 1.190026e-02 0.302503
2019-11-13 20:33:58,838 train 800 1.189638e-02 0.301439
2019-11-13 20:34:05,086 train 850 1.189482e-02 0.301410
2019-11-13 20:34:06,892 training loss; R2: 1.189273e-02 0.301746
2019-11-13 20:34:07,256 valid 000 1.925954e-02 0.030255
2019-11-13 20:34:09,048 valid 050 1.718699e-02 -0.120263
2019-11-13 20:34:10,688 validation loss; R2: 1.707487e-02 -0.206330
2019-11-13 20:34:10,705 epoch 13 lr 1.000000e-03
2019-11-13 20:34:11,169 train 000 1.293674e-02 0.262364
2019-11-13 20:34:17,591 train 050 1.197268e-02 0.301557
2019-11-13 20:34:23,861 train 100 1.176993e-02 0.320856
2019-11-13 20:34:29,892 train 150 1.186079e-02 0.319121
2019-11-13 20:34:36,030 train 200 1.179126e-02 0.316001
2019-11-13 20:34:42,060 train 250 1.177090e-02 0.318331
2019-11-13 20:34:48,094 train 300 1.178035e-02 0.313127
2019-11-13 20:34:54,127 train 350 1.174732e-02 0.303415
2019-11-13 20:35:00,161 train 400 1.173851e-02 0.304764
2019-11-13 20:35:06,186 train 450 1.176305e-02 0.261718
2019-11-13 20:35:12,209 train 500 1.176958e-02 0.261813
2019-11-13 20:35:18,238 train 550 1.179155e-02 0.266779
2019-11-13 20:35:24,281 train 600 1.178166e-02 0.270766
2019-11-13 20:35:30,317 train 650 1.175029e-02 0.272915
2019-11-13 20:35:36,357 train 700 1.176648e-02 0.277291
2019-11-13 20:35:42,381 train 750 1.174916e-02 0.279339
2019-11-13 20:35:48,408 train 800 1.175472e-02 0.282125
2019-11-13 20:35:54,437 train 850 1.175819e-02 0.282095
2019-11-13 20:35:56,248 training loss; R2: 1.175796e-02 0.283184
2019-11-13 20:35:56,608 valid 000 1.417673e-02 0.145498
2019-11-13 20:35:58,385 valid 050 1.419172e-02 0.185447
2019-11-13 20:35:59,994 validation loss; R2: 1.424854e-02 0.191295
2019-11-13 20:36:00,016 epoch 14 lr 1.000000e-03
2019-11-13 20:36:00,454 train 000 1.094805e-02 0.430389
2019-11-13 20:36:06,568 train 050 1.204792e-02 0.271376
2019-11-13 20:36:12,645 train 100 1.176606e-02 0.300008
2019-11-13 20:36:18,682 train 150 1.173693e-02 0.300738
2019-11-13 20:36:24,712 train 200 1.169740e-02 0.296287
2019-11-13 20:36:30,738 train 250 1.166485e-02 0.288523
2019-11-13 20:36:36,763 train 300 1.161622e-02 0.288070
2019-11-13 20:36:42,801 train 350 1.155271e-02 0.294077
2019-11-13 20:36:48,833 train 400 1.158714e-02 0.295484
2019-11-13 20:36:54,853 train 450 1.158125e-02 0.297595
2019-11-13 20:37:00,878 train 500 1.160829e-02 0.299472
2019-11-13 20:37:06,895 train 550 1.160726e-02 0.301669
2019-11-13 20:37:12,917 train 600 1.160345e-02 0.302983
2019-11-13 20:37:18,943 train 650 1.163875e-02 0.304194
2019-11-13 20:37:24,960 train 700 1.163208e-02 0.302537
2019-11-13 20:37:30,980 train 750 1.163530e-02 0.304333
2019-11-13 20:37:37,213 train 800 1.162530e-02 0.303255
2019-11-13 20:37:43,321 train 850 1.162830e-02 0.302662
2019-11-13 20:37:45,225 training loss; R2: 1.162863e-02 0.302487
2019-11-13 20:37:45,614 valid 000 7.422487e-02 -2.680293
2019-11-13 20:37:47,384 valid 050 7.561657e-02 -4.185132
2019-11-13 20:37:49,005 validation loss; R2: 7.567088e-02 -4.340680
2019-11-13 20:37:49,023 epoch 15 lr 1.000000e-03
2019-11-13 20:37:49,495 train 000 1.233156e-02 0.330102
2019-11-13 20:37:55,853 train 050 1.134727e-02 0.317230
2019-11-13 20:38:01,918 train 100 1.166979e-02 0.318277
2019-11-13 20:38:07,976 train 150 1.153196e-02 0.317276
2019-11-13 20:38:14,029 train 200 1.162291e-02 0.303401
2019-11-13 20:38:20,200 train 250 1.159934e-02 0.307467
2019-11-13 20:38:26,312 train 300 1.160334e-02 0.293736
2019-11-13 20:38:32,362 train 350 1.160620e-02 0.274821
2019-11-13 20:38:38,391 train 400 1.158189e-02 0.278814
2019-11-13 20:38:44,420 train 450 1.159144e-02 0.284170
2019-11-13 20:38:50,449 train 500 1.157293e-02 0.286072
2019-11-13 20:38:56,478 train 550 1.158564e-02 0.286929
2019-11-13 20:39:02,507 train 600 1.158159e-02 0.290317
2019-11-13 20:39:08,536 train 650 1.158740e-02 0.290894
2019-11-13 20:39:14,569 train 700 1.155496e-02 0.292267
2019-11-13 20:39:20,594 train 750 1.153568e-02 0.295510
2019-11-13 20:39:26,627 train 800 1.151558e-02 0.297398
2019-11-13 20:39:32,651 train 850 1.152308e-02 0.298367
2019-11-13 20:39:34,459 training loss; R2: 1.151555e-02 0.299141
2019-11-13 20:39:34,824 valid 000 3.398208e+00 -241.561872
2019-11-13 20:39:36,594 valid 050 3.417961e+00 -277.366202
2019-11-13 20:39:38,226 validation loss; R2: 3.420441e+00 -277.230421
2019-11-13 20:39:38,241 epoch 16 lr 1.000000e-03
2019-11-13 20:39:38,645 train 000 1.330046e-02 0.306798
2019-11-13 20:39:44,957 train 050 1.157996e-02 0.311985
2019-11-13 20:39:51,220 train 100 1.151713e-02 0.309879
2019-11-13 20:39:57,650 train 150 1.141632e-02 0.307132
2019-11-13 20:40:04,070 train 200 1.134514e-02 0.305384
2019-11-13 20:40:10,476 train 250 1.138146e-02 0.312214
2019-11-13 20:40:16,550 train 300 1.140241e-02 0.314965
2019-11-13 20:40:22,607 train 350 1.141042e-02 0.311500
2019-11-13 20:40:28,669 train 400 1.144418e-02 0.240225
2019-11-13 20:40:34,723 train 450 1.147379e-02 0.249694
2019-11-13 20:40:40,777 train 500 1.148107e-02 0.260112
2019-11-13 20:40:46,837 train 550 1.145877e-02 0.267687
2019-11-13 20:40:52,863 train 600 1.145484e-02 0.266258
2019-11-13 20:40:58,905 train 650 1.143843e-02 0.270373
2019-11-13 20:41:04,940 train 700 1.142228e-02 -1.557701
2019-11-13 20:41:10,971 train 750 1.143175e-02 -1.433222
2019-11-13 20:41:17,012 train 800 1.142812e-02 -1.322114
2019-11-13 20:41:23,043 train 850 1.141811e-02 -1.226887
2019-11-13 20:41:24,846 training loss; R2: 1.142577e-02 -1.200396
2019-11-13 20:41:25,210 valid 000 2.573492e+01 -2965.766786
2019-11-13 20:41:27,052 valid 050 2.579083e+01 -3021.629144
2019-11-13 20:41:28,662 validation loss; R2: 2.579513e+01 -3102.060999
2019-11-13 20:41:28,680 epoch 17 lr 1.000000e-03
2019-11-13 20:41:29,134 train 000 1.042921e-02 0.304975
2019-11-13 20:41:35,318 train 050 1.108789e-02 0.331203
2019-11-13 20:41:41,727 train 100 1.129675e-02 0.328481
2019-11-13 20:41:47,784 train 150 1.126113e-02 0.300424
2019-11-13 20:41:53,821 train 200 1.127605e-02 0.306857
2019-11-13 20:41:59,856 train 250 1.131179e-02 0.307979
2019-11-13 20:42:05,889 train 300 1.135699e-02 0.313091
2019-11-13 20:42:11,931 train 350 1.139507e-02 0.306112
2019-11-13 20:42:17,956 train 400 1.137022e-02 0.288558
2019-11-13 20:42:23,993 train 450 1.135022e-02 0.289031
2019-11-13 20:42:30,019 train 500 1.134990e-02 0.291561
2019-11-13 20:42:36,049 train 550 1.138163e-02 0.278847
2019-11-13 20:42:42,074 train 600 1.141528e-02 0.274236
2019-11-13 20:42:48,097 train 650 1.141513e-02 0.270005
2019-11-13 20:42:54,119 train 700 1.141854e-02 0.273036
2019-11-13 20:43:00,150 train 750 1.141818e-02 0.276719
2019-11-13 20:43:06,174 train 800 1.141122e-02 0.280711
2019-11-13 20:43:12,313 train 850 1.140303e-02 0.278933
2019-11-13 20:43:14,113 training loss; R2: 1.140335e-02 0.280202
2019-11-13 20:43:14,481 valid 000 3.433856e+00 -364.274637
2019-11-13 20:43:16,275 valid 050 3.467721e+00 -4490.665028
2019-11-13 20:43:17,891 validation loss; R2: 3.468165e+00 -2674.782476
2019-11-13 20:43:17,911 epoch 18 lr 1.000000e-03
2019-11-13 20:43:18,342 train 000 1.132533e-02 0.335243
2019-11-13 20:43:24,684 train 050 1.126098e-02 0.332262
2019-11-13 20:43:30,771 train 100 1.123370e-02 0.312202
2019-11-13 20:43:36,840 train 150 1.127372e-02 -1.942552
2019-11-13 20:43:42,910 train 200 1.130550e-02 -1.388437
2019-11-13 20:43:48,971 train 250 1.126673e-02 -1.053264
2019-11-13 20:43:55,359 train 300 1.130245e-02 -0.830224
2019-11-13 20:44:01,457 train 350 1.129172e-02 -0.668896
2019-11-13 20:44:07,488 train 400 1.129591e-02 -0.542907
2019-11-13 20:44:13,831 train 450 1.129105e-02 -0.445677
2019-11-13 20:44:20,185 train 500 1.129593e-02 -0.371147
2019-11-13 20:44:26,346 train 550 1.127190e-02 -0.313251
2019-11-13 20:44:32,371 train 600 1.127415e-02 -0.266428
2019-11-13 20:44:38,400 train 650 1.129898e-02 -0.224828
2019-11-13 20:44:44,429 train 700 1.129625e-02 -0.186155
2019-11-13 20:44:50,455 train 750 1.131267e-02 -0.153606
2019-11-13 20:44:56,511 train 800 1.133410e-02 -0.123188
2019-11-13 20:45:02,542 train 850 1.132630e-02 -0.095710
2019-11-13 20:45:04,344 training loss; R2: 1.132057e-02 -0.089426
2019-11-13 20:45:04,704 valid 000 1.279767e-01 -6.850492
2019-11-13 20:45:06,507 valid 050 1.314273e-01 -13.133912
2019-11-13 20:45:08,121 validation loss; R2: 1.317642e-01 -10.748542
2019-11-13 20:45:08,136 epoch 19 lr 1.000000e-03
2019-11-13 20:45:08,559 train 000 9.746637e-03 0.232916
2019-11-13 20:45:14,992 train 050 1.111496e-02 0.320399
2019-11-13 20:45:21,206 train 100 1.121506e-02 0.321170
2019-11-13 20:45:27,256 train 150 1.121779e-02 0.324816
2019-11-13 20:45:33,294 train 200 1.120444e-02 0.327859
2019-11-13 20:45:39,325 train 250 1.128784e-02 0.328173
2019-11-13 20:45:45,356 train 300 1.126670e-02 0.324696
2019-11-13 20:45:51,382 train 350 1.128671e-02 0.321023
2019-11-13 20:45:57,411 train 400 1.130254e-02 0.321582
2019-11-13 20:46:03,443 train 450 1.125941e-02 0.326709
2019-11-13 20:46:09,472 train 500 1.126019e-02 0.327545
2019-11-13 20:46:15,611 train 550 1.126784e-02 0.329618
2019-11-13 20:46:21,868 train 600 1.125339e-02 0.325408
2019-11-13 20:46:27,921 train 650 1.125797e-02 0.326524
2019-11-13 20:46:33,956 train 700 1.125695e-02 0.327967
2019-11-13 20:46:39,983 train 750 1.128098e-02 0.327668
2019-11-13 20:46:46,015 train 800 1.127193e-02 0.327590
2019-11-13 20:46:52,056 train 850 1.125624e-02 0.326127
2019-11-13 20:46:53,858 training loss; R2: 1.125432e-02 0.326498
2019-11-13 20:46:54,172 valid 000 1.593883e+00 -159.556375
2019-11-13 20:46:55,904 valid 050 1.638610e+00 -136.727885
2019-11-13 20:46:57,519 validation loss; R2: 1.637003e+00 -137.512913
