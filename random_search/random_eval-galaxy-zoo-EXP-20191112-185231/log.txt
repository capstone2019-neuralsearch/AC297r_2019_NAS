2019-11-12 18:52:32,183 gpu device = 1
2019-11-12 18:52:32,183 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name='galaxy-zoo-cleaned', gpu=1, grad_clip=5, gz_dtree=False, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-galaxy-zoo-EXP-20191112-185231', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-11-12 18:52:43,694 param size = 0.266197MB
2019-11-12 18:52:43,698 epoch 0 lr 1.000000e-03
2019-11-12 18:52:45,827 train 000 1.606229e+00 -1692.547337
2019-11-12 18:52:52,407 train 050 9.121199e-02 -58.374052
2019-11-12 18:52:58,954 train 100 6.228406e-02 -30.300355
2019-11-12 18:53:05,466 train 150 5.127513e-02 -20.882282
2019-11-12 18:53:11,910 train 200 4.544268e-02 -15.881473
2019-11-12 18:53:18,449 train 250 4.163481e-02 -12.822812
2019-11-12 18:53:24,975 train 300 3.899031e-02 -10.775739
2019-11-12 18:53:31,497 train 350 3.697352e-02 -9.298213
2019-11-12 18:53:38,031 train 400 3.547311e-02 -8.191564
2019-11-12 18:53:44,572 train 450 3.424828e-02 -7.337160
2019-11-12 18:53:50,851 train 500 3.323752e-02 -6.631145
2019-11-12 18:53:57,337 train 550 3.234407e-02 -6.046823
2019-11-12 18:54:03,889 train 600 3.156092e-02 -5.561449
2019-11-12 18:54:10,239 train 650 3.087496e-02 -5.144162
2019-11-12 18:54:16,780 train 700 3.024970e-02 -4.785180
2019-11-12 18:54:23,229 train 750 2.971312e-02 -4.489258
2019-11-12 18:54:29,758 train 800 2.919516e-02 -4.212253
2019-11-12 18:54:36,142 train 850 2.874858e-02 -3.967968
2019-11-12 18:54:38,752 training loss; R2: 2.861645e-02 -3.901051
2019-11-12 18:54:39,012 valid 000 2.330874e-02 -0.048856
2019-11-12 18:54:40,685 valid 050 2.536711e-02 -0.173130
2019-11-12 18:54:42,263 validation loss; R2: 2.541361e-02 -0.164643
2019-11-12 18:54:42,280 epoch 1 lr 1.000000e-03
2019-11-12 18:54:42,780 train 000 2.106961e-02 -0.117021
2019-11-12 18:54:49,267 train 050 2.106999e-02 -0.029784
2019-11-12 18:54:55,781 train 100 2.063224e-02 -0.129041
2019-11-12 18:55:02,077 train 150 2.053781e-02 -0.096366
2019-11-12 18:55:08,378 train 200 2.039691e-02 -0.076091
2019-11-12 18:55:14,813 train 250 2.032923e-02 -0.083022
2019-11-12 18:55:21,207 train 300 2.020421e-02 -0.075078
2019-11-12 18:55:27,680 train 350 2.018452e-02 -0.068251
2019-11-12 18:55:34,340 train 400 2.013528e-02 -0.061024
2019-11-12 18:55:40,607 train 450 2.000477e-02 -0.056311
2019-11-12 18:55:46,873 train 500 1.995362e-02 -0.048503
2019-11-12 18:55:53,171 train 550 1.984146e-02 -0.039935
2019-11-12 18:55:59,534 train 600 1.977972e-02 -0.034122
2019-11-12 18:56:05,722 train 650 1.969245e-02 -0.026074
2019-11-12 18:56:11,909 train 700 1.959423e-02 -0.027578
2019-11-12 18:56:18,297 train 750 1.951532e-02 -0.023820
2019-11-12 18:56:24,689 train 800 1.943860e-02 -0.018232
2019-11-12 18:56:30,867 train 850 1.935334e-02 -0.011637
2019-11-12 18:56:32,717 training loss; R2: 1.932280e-02 -0.010509
2019-11-12 18:56:33,012 valid 000 1.831991e-02 0.141949
2019-11-12 18:56:34,684 valid 050 1.801280e-02 0.110786
2019-11-12 18:56:36,248 validation loss; R2: 1.779367e-02 0.108995
2019-11-12 18:56:36,262 epoch 2 lr 1.000000e-03
2019-11-12 18:56:36,649 train 000 1.786324e-02 0.146034
2019-11-12 18:56:42,837 train 050 1.822691e-02 0.064886
2019-11-12 18:56:49,023 train 100 1.806146e-02 0.067241
2019-11-12 18:56:55,206 train 150 1.787479e-02 0.079016
2019-11-12 18:57:01,391 train 200 1.780024e-02 0.082652
2019-11-12 18:57:07,576 train 250 1.770749e-02 0.080159
2019-11-12 18:57:13,763 train 300 1.766168e-02 0.082290
2019-11-12 18:57:19,951 train 350 1.763567e-02 0.089344
2019-11-12 18:57:26,136 train 400 1.759712e-02 0.092708
2019-11-12 18:57:32,321 train 450 1.759539e-02 0.092373
2019-11-12 18:57:38,509 train 500 1.757795e-02 0.094157
2019-11-12 18:57:44,697 train 550 1.754703e-02 0.096630
2019-11-12 18:57:50,888 train 600 1.750814e-02 0.097875
2019-11-12 18:57:57,086 train 650 1.744822e-02 0.098307
2019-11-12 18:58:03,281 train 700 1.740941e-02 0.097585
2019-11-12 18:58:09,466 train 750 1.732709e-02 0.097862
2019-11-12 18:58:15,655 train 800 1.728438e-02 0.097780
2019-11-12 18:58:21,843 train 850 1.723701e-02 0.101275
2019-11-12 18:58:23,693 training loss; R2: 1.720708e-02 0.102390
2019-11-12 18:58:23,980 valid 000 1.388033e-02 -0.646956
2019-11-12 18:58:25,714 valid 050 1.487612e-02 0.166528
2019-11-12 18:58:27,276 validation loss; R2: 1.495494e-02 0.179187
2019-11-12 18:58:27,289 epoch 3 lr 1.000000e-03
2019-11-12 18:58:27,635 train 000 1.515164e-02 0.239180
2019-11-12 18:58:33,869 train 050 1.613949e-02 0.072029
2019-11-12 18:58:40,068 train 100 1.633758e-02 0.091504
2019-11-12 18:58:46,257 train 150 1.617995e-02 0.114175
2019-11-12 18:58:52,599 train 200 1.606460e-02 0.123037
2019-11-12 18:58:58,797 train 250 1.605943e-02 0.126442
2019-11-12 18:59:05,207 train 300 1.593916e-02 -0.024444
2019-11-12 18:59:11,502 train 350 1.595080e-02 -0.014374
2019-11-12 18:59:17,695 train 400 1.589603e-02 0.006682
2019-11-12 18:59:23,888 train 450 1.587078e-02 0.022244
2019-11-12 18:59:30,078 train 500 1.585752e-02 0.025914
2019-11-12 18:59:36,270 train 550 1.581850e-02 0.035398
2019-11-12 18:59:42,459 train 600 1.579567e-02 0.048114
2019-11-12 18:59:48,646 train 650 1.579021e-02 0.057898
2019-11-12 18:59:55,162 train 700 1.574890e-02 0.065671
2019-11-12 19:00:01,615 train 750 1.570545e-02 0.074371
2019-11-12 19:00:07,803 train 800 1.565996e-02 0.081140
2019-11-12 19:00:13,995 train 850 1.562537e-02 0.085421
2019-11-12 19:00:15,847 training loss; R2: 1.560608e-02 0.087349
2019-11-12 19:00:16,125 valid 000 1.916001e-02 0.114784
2019-11-12 19:00:17,845 valid 050 1.732607e-02 0.044654
2019-11-12 19:00:19,433 validation loss; R2: 1.745023e-02 0.028338
2019-11-12 19:00:19,454 epoch 4 lr 1.000000e-03
2019-11-12 19:00:19,797 train 000 1.335667e-02 0.282382
2019-11-12 19:00:26,379 train 050 1.485934e-02 0.165688
2019-11-12 19:00:32,922 train 100 1.481055e-02 0.165905
2019-11-12 19:00:39,328 train 150 1.473918e-02 0.148971
2019-11-12 19:00:45,864 train 200 1.476159e-02 -0.312221
2019-11-12 19:00:52,396 train 250 1.477415e-02 -0.211449
2019-11-12 19:00:58,929 train 300 1.472591e-02 -0.140139
2019-11-12 19:01:05,461 train 350 1.472927e-02 -0.090324
2019-11-12 19:01:11,798 train 400 1.473180e-02 -0.058761
2019-11-12 19:01:17,997 train 450 1.465295e-02 -0.038171
2019-11-12 19:01:24,196 train 500 1.460872e-02 -0.013258
2019-11-12 19:01:30,396 train 550 1.457767e-02 0.003503
2019-11-12 19:01:36,600 train 600 1.457036e-02 0.018858
2019-11-12 19:01:42,807 train 650 1.455754e-02 0.035178
2019-11-12 19:01:49,008 train 700 1.451863e-02 0.047628
2019-11-12 19:01:55,207 train 750 1.448746e-02 0.059056
2019-11-12 19:02:01,405 train 800 1.444761e-02 0.071272
2019-11-12 19:02:07,620 train 850 1.439642e-02 0.082262
2019-11-12 19:02:09,474 training loss; R2: 1.438635e-02 0.081945
2019-11-12 19:02:09,764 valid 000 1.268789e-02 -0.014838
2019-11-12 19:02:11,477 valid 050 1.273653e-02 0.299340
2019-11-12 19:02:13,037 validation loss; R2: 1.270269e-02 0.306041
2019-11-12 19:02:13,052 epoch 5 lr 1.000000e-03
2019-11-12 19:02:13,400 train 000 1.689582e-02 0.220654
2019-11-12 19:02:19,917 train 050 1.403707e-02 0.216311
2019-11-12 19:02:26,520 train 100 1.380437e-02 0.224411
2019-11-12 19:02:33,027 train 150 1.381277e-02 0.226871
2019-11-12 19:02:39,299 train 200 1.380062e-02 0.215423
2019-11-12 19:02:45,554 train 250 1.380638e-02 0.217213
2019-11-12 19:02:51,804 train 300 1.382042e-02 0.219551
2019-11-12 19:02:58,110 train 350 1.377466e-02 0.218854
2019-11-12 19:03:04,502 train 400 1.373906e-02 0.216653
2019-11-12 19:03:10,722 train 450 1.374648e-02 0.219511
2019-11-12 19:03:16,952 train 500 1.372012e-02 0.223969
2019-11-12 19:03:23,190 train 550 1.371186e-02 0.225685
2019-11-12 19:03:29,416 train 600 1.370345e-02 0.214210
2019-11-12 19:03:35,646 train 650 1.366431e-02 0.216573
2019-11-12 19:03:41,881 train 700 1.368679e-02 0.220481
2019-11-12 19:03:48,115 train 750 1.368882e-02 0.222059
2019-11-12 19:03:54,682 train 800 1.365787e-02 0.223170
2019-11-12 19:04:00,930 train 850 1.364602e-02 0.222787
2019-11-12 19:04:02,791 training loss; R2: 1.363608e-02 0.223811
2019-11-12 19:04:03,076 valid 000 1.259359e-02 0.316917
2019-11-12 19:04:04,791 valid 050 1.259606e-02 0.280523
2019-11-12 19:04:06,375 validation loss; R2: 1.267370e-02 0.284386
2019-11-12 19:04:06,388 epoch 6 lr 1.000000e-03
2019-11-12 19:04:06,723 train 000 1.296288e-02 0.269164
2019-11-12 19:04:13,236 train 050 1.280532e-02 0.239620
2019-11-12 19:04:19,444 train 100 1.306029e-02 0.251995
2019-11-12 19:04:25,635 train 150 1.309036e-02 0.233323
2019-11-12 19:04:31,821 train 200 1.313664e-02 0.222104
2019-11-12 19:04:38,007 train 250 1.312714e-02 0.225059
2019-11-12 19:04:44,191 train 300 1.310110e-02 0.227282
2019-11-12 19:04:50,385 train 350 1.315551e-02 0.230941
2019-11-12 19:04:56,568 train 400 1.317714e-02 0.225493
2019-11-12 19:05:02,757 train 450 1.313977e-02 0.223076
2019-11-12 19:05:08,941 train 500 1.312431e-02 0.220701
2019-11-12 19:05:15,123 train 550 1.310756e-02 0.225983
2019-11-12 19:05:21,316 train 600 1.311426e-02 0.227234
2019-11-12 19:05:27,506 train 650 1.310849e-02 0.230501
2019-11-12 19:05:33,687 train 700 1.311675e-02 0.233421
2019-11-12 19:05:39,876 train 750 1.309875e-02 -0.149494
2019-11-12 19:05:46,059 train 800 1.305625e-02 -0.123590
2019-11-12 19:05:52,243 train 850 1.305408e-02 -0.100699
2019-11-12 19:05:54,092 training loss; R2: 1.305101e-02 -0.094442
2019-11-12 19:05:54,372 valid 000 1.356626e-02 0.305508
2019-11-12 19:05:56,058 valid 050 1.248375e-02 0.297295
2019-11-12 19:05:57,620 validation loss; R2: 1.250678e-02 0.303780
2019-11-12 19:05:57,640 epoch 7 lr 1.000000e-03
2019-11-12 19:05:58,013 train 000 1.264257e-02 0.230545
2019-11-12 19:06:04,499 train 050 1.248803e-02 0.292524
2019-11-12 19:06:10,949 train 100 1.257925e-02 0.286969
2019-11-12 19:06:17,390 train 150 1.262034e-02 0.268585
2019-11-12 19:06:23,792 train 200 1.264667e-02 0.261124
2019-11-12 19:06:30,254 train 250 1.266127e-02 0.259259
2019-11-12 19:06:36,751 train 300 1.265302e-02 0.259135
2019-11-12 19:06:43,201 train 350 1.263002e-02 0.251499
2019-11-12 19:06:49,733 train 400 1.261109e-02 0.256095
2019-11-12 19:06:56,241 train 450 1.261478e-02 0.259437
2019-11-12 19:07:02,755 train 500 1.264327e-02 0.260417
2019-11-12 19:07:09,344 train 550 1.258541e-02 0.263778
2019-11-12 19:07:15,711 train 600 1.257874e-02 0.262633
2019-11-12 19:07:22,074 train 650 1.256277e-02 0.261047
2019-11-12 19:07:28,558 train 700 1.253950e-02 0.258210
2019-11-12 19:07:35,157 train 750 1.256113e-02 0.258072
2019-11-12 19:07:41,698 train 800 1.255382e-02 0.255469
2019-11-12 19:07:48,169 train 850 1.254711e-02 0.254440
2019-11-12 19:07:50,175 training loss; R2: 1.254578e-02 0.254782
2019-11-12 19:07:50,450 valid 000 9.995989e-03 0.321458
2019-11-12 19:07:52,148 valid 050 1.114854e-02 0.351801
2019-11-12 19:07:53,639 validation loss; R2: 1.117604e-02 0.272747
2019-11-12 19:07:53,663 epoch 8 lr 1.000000e-03
2019-11-12 19:07:54,036 train 000 1.215622e-02 0.211581
2019-11-12 19:08:00,424 train 050 1.207283e-02 0.296050
2019-11-12 19:08:06,856 train 100 1.216982e-02 0.299063
2019-11-12 19:08:13,444 train 150 1.228487e-02 0.281194
2019-11-12 19:08:19,973 train 200 1.229328e-02 0.266216
2019-11-12 19:08:26,520 train 250 1.229101e-02 0.271761
2019-11-12 19:08:33,051 train 300 1.235060e-02 0.266360
2019-11-12 19:08:39,413 train 350 1.230796e-02 0.268766
2019-11-12 19:08:45,611 train 400 1.230682e-02 0.269690
2019-11-12 19:08:51,802 train 450 1.227139e-02 0.269194
2019-11-12 19:08:57,998 train 500 1.227569e-02 0.271696
2019-11-12 19:09:04,188 train 550 1.227069e-02 0.262758
2019-11-12 19:09:10,379 train 600 1.229146e-02 0.258499
2019-11-12 19:09:16,571 train 650 1.229273e-02 0.260482
2019-11-12 19:09:22,763 train 700 1.229052e-02 0.262136
2019-11-12 19:09:29,010 train 750 1.226180e-02 0.263227
2019-11-12 19:09:35,528 train 800 1.224722e-02 0.264146
2019-11-12 19:09:41,944 train 850 1.223276e-02 0.264393
2019-11-12 19:09:43,797 training loss; R2: 1.223681e-02 0.264708
2019-11-12 19:09:44,090 valid 000 1.910247e-02 0.075391
2019-11-12 19:09:45,785 valid 050 1.843284e-02 -0.065226
2019-11-12 19:09:47,370 validation loss; R2: 1.837223e-02 -0.052271
2019-11-12 19:09:47,384 epoch 9 lr 1.000000e-03
2019-11-12 19:09:47,770 train 000 1.227959e-02 0.385875
2019-11-12 19:09:54,299 train 050 1.210346e-02 0.304978
2019-11-12 19:10:00,804 train 100 1.200182e-02 0.284254
2019-11-12 19:10:07,321 train 150 1.200574e-02 0.277480
2019-11-12 19:10:13,560 train 200 1.197173e-02 0.283540
2019-11-12 19:10:19,803 train 250 1.199460e-02 0.281472
2019-11-12 19:10:26,039 train 300 1.203963e-02 0.284501
2019-11-12 19:10:32,309 train 350 1.201378e-02 0.278099
2019-11-12 19:10:38,542 train 400 1.204872e-02 0.199104
2019-11-12 19:10:44,776 train 450 1.201790e-02 0.174121
2019-11-12 19:10:51,013 train 500 1.200068e-02 0.185367
2019-11-12 19:10:57,261 train 550 1.197656e-02 0.193604
2019-11-12 19:11:03,563 train 600 1.197457e-02 0.197377
2019-11-12 19:11:09,798 train 650 1.198633e-02 0.204873
2019-11-12 19:11:16,025 train 700 1.198148e-02 0.211536
2019-11-12 19:11:22,253 train 750 1.196483e-02 0.216169
2019-11-12 19:11:28,480 train 800 1.193827e-02 0.218259
2019-11-12 19:11:34,692 train 850 1.193690e-02 0.222792
2019-11-12 19:11:36,546 training loss; R2: 1.192589e-02 0.224471
2019-11-12 19:11:36,815 valid 000 3.679534e-02 -0.528402
2019-11-12 19:11:38,513 valid 050 3.561639e-02 -0.674556
2019-11-12 19:11:40,025 validation loss; R2: 3.569732e-02 -0.720271
2019-11-12 19:11:40,040 epoch 10 lr 1.000000e-03
2019-11-12 19:11:40,425 train 000 1.269381e-02 0.291577
2019-11-12 19:11:46,862 train 050 1.171736e-02 0.315419
2019-11-12 19:11:53,432 train 100 1.175596e-02 0.309153
2019-11-12 19:11:59,976 train 150 1.171240e-02 -0.165146
2019-11-12 19:12:06,528 train 200 1.165906e-02 -0.046067
2019-11-12 19:12:13,069 train 250 1.170342e-02 -0.012119
2019-11-12 19:12:19,454 train 300 1.170776e-02 0.041645
2019-11-12 19:12:25,656 train 350 1.170610e-02 0.078571
2019-11-12 19:12:31,863 train 400 1.170883e-02 0.106254
2019-11-12 19:12:38,062 train 450 1.169220e-02 0.127832
2019-11-12 19:12:44,263 train 500 1.166932e-02 0.144378
2019-11-12 19:12:50,456 train 550 1.165180e-02 0.158065
2019-11-12 19:12:56,657 train 600 1.166243e-02 0.170042
2019-11-12 19:13:02,861 train 650 1.166478e-02 0.174916
2019-11-12 19:13:09,057 train 700 1.164614e-02 0.182478
2019-11-12 19:13:15,252 train 750 1.165838e-02 0.191239
2019-11-12 19:13:21,446 train 800 1.164959e-02 0.198937
2019-11-12 19:13:27,649 train 850 1.165768e-02 0.205037
2019-11-12 19:13:29,514 training loss; R2: 1.166105e-02 0.207054
2019-11-12 19:13:29,810 valid 000 1.366132e-01 -3.673521
2019-11-12 19:13:31,512 valid 050 1.346308e-01 -6.300574
2019-11-12 19:13:33,036 validation loss; R2: 1.342179e-01 -5.952764
2019-11-12 19:13:33,050 epoch 11 lr 1.000000e-03
2019-11-12 19:13:33,448 train 000 1.014045e-02 0.336987
2019-11-12 19:13:39,987 train 050 1.124496e-02 0.315017
2019-11-12 19:13:46,402 train 100 1.149215e-02 0.314425
2019-11-12 19:13:52,712 train 150 1.153276e-02 0.315551
2019-11-12 19:13:59,159 train 200 1.155767e-02 0.316917
2019-11-12 19:14:05,600 train 250 1.153839e-02 0.317757
2019-11-12 19:14:12,152 train 300 1.153109e-02 0.315644
2019-11-12 19:14:18,356 train 350 1.150175e-02 0.298615
2019-11-12 19:14:24,542 train 400 1.148096e-02 0.298560
2019-11-12 19:14:30,722 train 450 1.149369e-02 0.288968
2019-11-12 19:14:36,923 train 500 1.153107e-02 0.287281
2019-11-12 19:14:43,098 train 550 1.153405e-02 0.285386
2019-11-12 19:14:49,287 train 600 1.150679e-02 0.288995
2019-11-12 19:14:55,460 train 650 1.150288e-02 0.290819
2019-11-12 19:15:01,631 train 700 1.149946e-02 0.292542
2019-11-12 19:15:07,805 train 750 1.149863e-02 0.292021
2019-11-12 19:15:13,977 train 800 1.149560e-02 0.292573
2019-11-12 19:15:20,151 train 850 1.149826e-02 0.294347
2019-11-12 19:15:21,999 training loss; R2: 1.149974e-02 0.281253
2019-11-12 19:15:22,276 valid 000 1.700480e+00 -285.871576
2019-11-12 19:15:23,941 valid 050 1.668642e+00 -428.457269
2019-11-12 19:15:25,473 validation loss; R2: 1.670943e+00 -374.951988
2019-11-12 19:15:25,493 epoch 12 lr 1.000000e-03
2019-11-12 19:15:25,846 train 000 1.129996e-02 0.348478
2019-11-12 19:15:32,514 train 050 1.127981e-02 0.256116
2019-11-12 19:15:39,087 train 100 1.134661e-02 0.268650
2019-11-12 19:15:45,634 train 150 1.132946e-02 0.282362
2019-11-12 19:15:52,026 train 200 1.129539e-02 0.280203
2019-11-12 19:15:58,581 train 250 1.129491e-02 0.287727
2019-11-12 19:16:05,106 train 300 1.133109e-02 0.293281
2019-11-12 19:16:11,636 train 350 1.131645e-02 0.297534
2019-11-12 19:16:17,978 train 400 1.129123e-02 0.299681
2019-11-12 19:16:24,557 train 450 1.128909e-02 0.302110
2019-11-12 19:16:31,142 train 500 1.132073e-02 0.302681
2019-11-12 19:16:37,635 train 550 1.131480e-02 0.303026
2019-11-12 19:16:44,223 train 600 1.130310e-02 0.305466
2019-11-12 19:16:50,788 train 650 1.130091e-02 0.305938
2019-11-12 19:16:57,313 train 700 1.129824e-02 0.302144
2019-11-12 19:17:03,860 train 750 1.128363e-02 0.299860
2019-11-12 19:17:10,478 train 800 1.128648e-02 0.302351
2019-11-12 19:17:17,097 train 850 1.128724e-02 0.300968
2019-11-12 19:17:19,107 training loss; R2: 1.128421e-02 0.301066
2019-11-12 19:17:19,403 valid 000 2.755247e+00 -117.493181
2019-11-12 19:17:21,048 valid 050 2.708118e+00 -132.993500
2019-11-12 19:17:22,552 validation loss; R2: 2.704683e+00 -154.133000
2019-11-12 19:17:22,576 epoch 13 lr 1.000000e-03
2019-11-12 19:17:23,001 train 000 1.194421e-02 0.318533
2019-11-12 19:17:29,355 train 050 1.119189e-02 0.305802
2019-11-12 19:17:35,900 train 100 1.132551e-02 0.294936
2019-11-12 19:17:42,271 train 150 1.127400e-02 0.306612
2019-11-12 19:17:48,759 train 200 1.126934e-02 0.312566
2019-11-12 19:17:55,145 train 250 1.129153e-02 0.314270
2019-11-12 19:18:01,547 train 300 1.125936e-02 0.314794
2019-11-12 19:18:07,732 train 350 1.127942e-02 0.318180
2019-11-12 19:18:13,926 train 400 1.123187e-02 0.308101
2019-11-12 19:18:20,109 train 450 1.122115e-02 0.305944
2019-11-12 19:18:26,290 train 500 1.121022e-02 0.308125
2019-11-12 19:18:32,474 train 550 1.121875e-02 0.181847
2019-11-12 19:18:38,654 train 600 1.121318e-02 0.191700
2019-11-12 19:18:44,831 train 650 1.121684e-02 0.201789
2019-11-12 19:18:51,011 train 700 1.124263e-02 0.198156
2019-11-12 19:18:57,194 train 750 1.126364e-02 0.204647
2019-11-12 19:19:03,383 train 800 1.123319e-02 0.212438
2019-11-12 19:19:09,558 train 850 1.121755e-02 0.218362
2019-11-12 19:19:11,405 training loss; R2: 1.121288e-02 0.220260
2019-11-12 19:19:11,697 valid 000 2.942855e-01 -15.013981
2019-11-12 19:19:13,440 valid 050 2.865046e-01 -12.893892
2019-11-12 19:19:14,996 validation loss; R2: 2.853436e-01 -12.683449
2019-11-12 19:19:15,021 epoch 14 lr 1.000000e-03
2019-11-12 19:19:15,428 train 000 1.066556e-02 0.259047
2019-11-12 19:19:21,767 train 050 1.098020e-02 0.304782
2019-11-12 19:19:27,953 train 100 1.104887e-02 0.245568
2019-11-12 19:19:34,143 train 150 1.100454e-02 0.232322
2019-11-12 19:19:40,329 train 200 1.100151e-02 0.250323
2019-11-12 19:19:46,516 train 250 1.100156e-02 0.266850
2019-11-12 19:19:52,704 train 300 1.103512e-02 0.275812
2019-11-12 19:19:58,891 train 350 1.104752e-02 0.273113
2019-11-12 19:20:05,080 train 400 1.103660e-02 0.279278
2019-11-12 19:20:11,265 train 450 1.100401e-02 0.281182
2019-11-12 19:20:17,453 train 500 1.100144e-02 0.284452
2019-11-12 19:20:23,642 train 550 1.102814e-02 0.287434
2019-11-12 19:20:29,829 train 600 1.101588e-02 0.291055
2019-11-12 19:20:36,034 train 650 1.104063e-02 0.292274
2019-11-12 19:20:42,222 train 700 1.104048e-02 0.294712
2019-11-12 19:20:48,406 train 750 1.103499e-02 0.292881
2019-11-12 19:20:54,590 train 800 1.102707e-02 0.289138
2019-11-12 19:21:00,778 train 850 1.100334e-02 0.290854
2019-11-12 19:21:02,628 training loss; R2: 1.101511e-02 0.291287
2019-11-12 19:21:02,912 valid 000 4.163696e+00 -243.249258
2019-11-12 19:21:04,663 valid 050 4.137731e+00 -384.643429
2019-11-12 19:21:06,233 validation loss; R2: 4.142556e+00 -433.390209
2019-11-12 19:21:06,249 epoch 15 lr 1.000000e-03
2019-11-12 19:21:06,616 train 000 9.414891e-03 0.360661
2019-11-12 19:21:13,186 train 050 1.078424e-02 0.297497
2019-11-12 19:21:19,744 train 100 1.106636e-02 0.266363
2019-11-12 19:21:26,219 train 150 1.099296e-02 0.286413
2019-11-12 19:21:32,689 train 200 1.092001e-02 0.286172
2019-11-12 19:21:39,142 train 250 1.096244e-02 0.279787
2019-11-12 19:21:45,590 train 300 1.097969e-02 0.285955
2019-11-12 19:21:52,042 train 350 1.101783e-02 0.294582
2019-11-12 19:21:58,494 train 400 1.098503e-02 0.297150
2019-11-12 19:22:04,943 train 450 1.100870e-02 0.295765
2019-11-12 19:22:11,393 train 500 1.102413e-02 0.293621
2019-11-12 19:22:17,840 train 550 1.104014e-02 0.291095
2019-11-12 19:22:24,290 train 600 1.106059e-02 0.290741
2019-11-12 19:22:30,749 train 650 1.104716e-02 0.293985
2019-11-12 19:22:37,200 train 700 1.103564e-02 0.295660
2019-11-12 19:22:43,648 train 750 1.100449e-02 0.294831
2019-11-12 19:22:50,098 train 800 1.100211e-02 0.294355
2019-11-12 19:22:56,563 train 850 1.099240e-02 0.295540
2019-11-12 19:22:58,492 training loss; R2: 1.098595e-02 0.296245
2019-11-12 19:22:58,770 valid 000 1.400239e+00 -80.230388
2019-11-12 19:23:00,452 valid 050 1.380506e+00 -94.098126
2019-11-12 19:23:01,968 validation loss; R2: 1.380248e+00 -109.377298
2019-11-12 19:23:01,989 epoch 16 lr 1.000000e-03
2019-11-12 19:23:02,370 train 000 1.191779e-02 0.407449
2019-11-12 19:23:09,028 train 050 1.098703e-02 0.321040
2019-11-12 19:23:15,608 train 100 1.092946e-02 0.324194
2019-11-12 19:23:22,093 train 150 1.098250e-02 0.292211
2019-11-12 19:23:28,638 train 200 1.089957e-02 0.303080
2019-11-12 19:23:35,157 train 250 1.084331e-02 0.296889
2019-11-12 19:23:41,760 train 300 1.085575e-02 0.300995
2019-11-12 19:23:48,290 train 350 1.084621e-02 0.303750
2019-11-12 19:23:54,762 train 400 1.083051e-02 0.306883
2019-11-12 19:24:01,321 train 450 1.083964e-02 0.294645
2019-11-12 19:24:07,872 train 500 1.088568e-02 0.293838
2019-11-12 19:24:14,421 train 550 1.089488e-02 0.297688
2019-11-12 19:24:20,769 train 600 1.085941e-02 0.296933
2019-11-12 19:24:27,255 train 650 1.085622e-02 0.296962
2019-11-12 19:24:33,797 train 700 1.084278e-02 0.300874
2019-11-12 19:24:40,342 train 750 1.085006e-02 0.297752
2019-11-12 19:24:46,932 train 800 1.085606e-02 0.300295
2019-11-12 19:24:53,559 train 850 1.084505e-02 0.302588
2019-11-12 19:24:55,571 training loss; R2: 1.084009e-02 0.302928
2019-11-12 19:24:55,860 valid 000 2.660995e+00 -768.431789
2019-11-12 19:24:57,534 valid 050 2.647896e+00 -645.473601
2019-11-12 19:24:59,072 validation loss; R2: 2.647190e+00 -944.128184
2019-11-12 19:24:59,090 epoch 17 lr 1.000000e-03
2019-11-12 19:24:59,512 train 000 1.183531e-02 0.396785
2019-11-12 19:25:06,086 train 050 1.080236e-02 0.336145
2019-11-12 19:25:12,327 train 100 1.076219e-02 0.334585
2019-11-12 19:25:18,549 train 150 1.086398e-02 0.335722
2019-11-12 19:25:24,791 train 200 1.089935e-02 0.336338
2019-11-12 19:25:31,045 train 250 1.089131e-02 0.335333
2019-11-12 19:25:37,254 train 300 1.085009e-02 0.328922
2019-11-12 19:25:43,442 train 350 1.088666e-02 0.328534
2019-11-12 19:25:49,640 train 400 1.089079e-02 0.329446
2019-11-12 19:25:55,838 train 450 1.088320e-02 0.330161
2019-11-12 19:26:02,045 train 500 1.083549e-02 0.328036
2019-11-12 19:26:08,276 train 550 1.081832e-02 0.327363
2019-11-12 19:26:14,530 train 600 1.079939e-02 0.330380
2019-11-12 19:26:20,767 train 650 1.076202e-02 0.330765
2019-11-12 19:26:26,974 train 700 1.075992e-02 0.327855
2019-11-12 19:26:33,177 train 750 1.076101e-02 0.328474
2019-11-12 19:26:39,731 train 800 1.074388e-02 0.329999
2019-11-12 19:26:46,059 train 850 1.073036e-02 0.326581
2019-11-12 19:26:47,922 training loss; R2: 1.073689e-02 0.326153
2019-11-12 19:26:48,220 valid 000 5.501321e+00 -347.998947
2019-11-12 19:26:49,900 valid 050 5.468926e+00 -377.434197
2019-11-12 19:26:51,480 validation loss; R2: 5.469654e+00 -388.800201
2019-11-12 19:26:51,501 epoch 18 lr 1.000000e-03
2019-11-12 19:26:51,890 train 000 9.184862e-03 0.460421
2019-11-12 19:26:58,320 train 050 1.066430e-02 0.327447
2019-11-12 19:27:04,695 train 100 1.058962e-02 0.319490
2019-11-12 19:27:11,338 train 150 1.059915e-02 0.262456
2019-11-12 19:27:17,825 train 200 1.063154e-02 0.284122
2019-11-12 19:27:24,229 train 250 1.061326e-02 0.264975
2019-11-12 19:27:30,610 train 300 1.059188e-02 0.271002
2019-11-12 19:27:36,893 train 350 1.058684e-02 0.282570
2019-11-12 19:27:43,217 train 400 1.061480e-02 0.286478
2019-11-12 19:27:49,683 train 450 1.058392e-02 0.292592
2019-11-12 19:27:56,197 train 500 1.059684e-02 0.294237
2019-11-12 19:28:02,586 train 550 1.062289e-02 0.297856
2019-11-12 19:28:08,998 train 600 1.059742e-02 0.296286
2019-11-12 19:28:15,464 train 650 1.057577e-02 0.298688
2019-11-12 19:28:22,060 train 700 1.056380e-02 0.302694
2019-11-12 19:28:28,422 train 750 1.058557e-02 0.302115
2019-11-12 19:28:34,925 train 800 1.059768e-02 0.296025
2019-11-12 19:28:41,231 train 850 1.060667e-02 0.296902
2019-11-12 19:28:43,119 training loss; R2: 1.062379e-02 0.296836
2019-11-12 19:28:43,437 valid 000 4.699627e-01 -33.948236
2019-11-12 19:28:45,154 valid 050 4.625444e-01 -29.272379
2019-11-12 19:28:46,705 validation loss; R2: 4.605677e-01 -29.615573
2019-11-12 19:28:46,730 epoch 19 lr 1.000000e-03
2019-11-12 19:28:47,154 train 000 1.103782e-02 0.370640
2019-11-12 19:28:53,705 train 050 1.081560e-02 0.333973
2019-11-12 19:29:00,166 train 100 1.078723e-02 0.323249
2019-11-12 19:29:06,666 train 150 1.071298e-02 0.306601
2019-11-12 19:29:13,139 train 200 1.073490e-02 0.310765
2019-11-12 19:29:19,590 train 250 1.079900e-02 0.311361
2019-11-12 19:29:25,766 train 300 1.077017e-02 0.315939
2019-11-12 19:29:31,940 train 350 1.072645e-02 0.320556
2019-11-12 19:29:38,146 train 400 1.072053e-02 0.322396
2019-11-12 19:29:44,338 train 450 1.070350e-02 0.227691
2019-11-12 19:29:50,530 train 500 1.069713e-02 0.239721
2019-11-12 19:29:56,742 train 550 1.069071e-02 0.091054
2019-11-12 19:30:02,945 train 600 1.068537e-02 0.111158
2019-11-12 19:30:09,135 train 650 1.068748e-02 0.126643
2019-11-12 19:30:15,315 train 700 1.069031e-02 0.139164
2019-11-12 19:30:21,502 train 750 1.069382e-02 0.142869
2019-11-12 19:30:27,684 train 800 1.068488e-02 0.153225
2019-11-12 19:30:33,883 train 850 1.069123e-02 0.163879
2019-11-12 19:30:35,740 training loss; R2: 1.069366e-02 0.166943
2019-11-12 19:30:36,034 valid 000 2.962689e+01 -3653.196374
2019-11-12 19:30:37,720 valid 050 2.960912e+01 -1966.986470
2019-11-12 19:30:39,217 validation loss; R2: 2.961409e+01 -1761.812556
