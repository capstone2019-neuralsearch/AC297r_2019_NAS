2019-11-12 21:05:10,402 gpu device = 1
2019-11-12 21:05:10,403 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name='galaxy-zoo-cleaned', gpu=1, grad_clip=5, gz_dtree=False, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-galaxy-zoo-EXP-20191112-210510', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-11-12 21:05:22,037 param size = 0.267093MB
2019-11-12 21:05:22,041 epoch 0 lr 1.000000e-03
2019-11-12 21:05:24,204 train 000 6.605359e-01 -430.044466
2019-11-12 21:05:30,750 train 050 6.181011e-02 -15.279624
2019-11-12 21:05:37,472 train 100 4.559141e-02 -8.136059
2019-11-12 21:05:43,990 train 150 3.904655e-02 -5.610056
2019-11-12 21:05:50,424 train 200 3.579301e-02 -4.317683
2019-11-12 21:05:56,961 train 250 3.373673e-02 -3.502829
2019-11-12 21:06:03,429 train 300 3.213908e-02 -2.966741
2019-11-12 21:06:09,881 train 350 3.096762e-02 -2.570746
2019-11-12 21:06:16,365 train 400 3.002111e-02 -2.275656
2019-11-12 21:06:22,845 train 450 2.921864e-02 -2.035997
2019-11-12 21:06:29,289 train 500 2.855999e-02 -1.852128
2019-11-12 21:06:35,790 train 550 2.796232e-02 -1.696180
2019-11-12 21:06:42,371 train 600 2.740718e-02 -1.559724
2019-11-12 21:06:48,993 train 650 2.691710e-02 -1.443971
2019-11-12 21:06:55,474 train 700 2.651792e-02 -1.346385
2019-11-12 21:07:01,894 train 750 2.608355e-02 -1.266020
2019-11-12 21:07:08,382 train 800 2.574243e-02 -1.193067
2019-11-12 21:07:14,767 train 850 2.542600e-02 -1.122574
2019-11-12 21:07:17,427 training loss; R2: 2.533947e-02 -1.102951
2019-11-12 21:07:17,705 valid 000 2.292299e-02 0.017687
2019-11-12 21:07:19,346 valid 050 1.939721e-02 -0.008171
2019-11-12 21:07:20,927 validation loss; R2: 1.942003e-02 0.000920
2019-11-12 21:07:20,948 epoch 1 lr 1.000000e-03
2019-11-12 21:07:21,516 train 000 1.900286e-02 0.063415
2019-11-12 21:07:28,147 train 050 1.979673e-02 -0.064226
2019-11-12 21:07:34,955 train 100 1.962335e-02 -0.011078
2019-11-12 21:07:41,649 train 150 1.952023e-02 -0.004990
2019-11-12 21:07:48,210 train 200 1.945274e-02 0.004838
2019-11-12 21:07:54,653 train 250 1.937006e-02 0.010922
2019-11-12 21:08:01,032 train 300 1.922371e-02 0.018155
2019-11-12 21:08:07,554 train 350 1.923047e-02 0.024416
2019-11-12 21:08:14,014 train 400 1.921520e-02 0.029650
2019-11-12 21:08:20,527 train 450 1.908640e-02 0.026267
2019-11-12 21:08:27,017 train 500 1.905574e-02 0.023754
2019-11-12 21:08:33,375 train 550 1.903226e-02 0.024999
2019-11-12 21:08:39,773 train 600 1.896714e-02 0.030044
2019-11-12 21:08:46,332 train 650 1.886183e-02 0.035911
2019-11-12 21:08:52,766 train 700 1.881302e-02 0.038303
2019-11-12 21:08:59,318 train 750 1.875447e-02 0.026408
2019-11-12 21:09:05,836 train 800 1.870003e-02 0.026118
2019-11-12 21:09:12,389 train 850 1.861684e-02 0.031697
2019-11-12 21:09:14,319 training loss; R2: 1.860555e-02 0.032306
2019-11-12 21:09:14,577 valid 000 1.216430e-02 0.299187
2019-11-12 21:09:16,222 valid 050 1.626162e-02 0.163322
2019-11-12 21:09:17,733 validation loss; R2: 1.630444e-02 0.080460
2019-11-12 21:09:17,754 epoch 2 lr 1.000000e-03
2019-11-12 21:09:18,085 train 000 1.461576e-02 0.157047
2019-11-12 21:09:24,566 train 050 1.772215e-02 0.108240
2019-11-12 21:09:30,886 train 100 1.772533e-02 0.112630
2019-11-12 21:09:37,301 train 150 1.766341e-02 0.115544
2019-11-12 21:09:43,808 train 200 1.757697e-02 0.121479
2019-11-12 21:09:50,374 train 250 1.743866e-02 0.109417
2019-11-12 21:09:56,876 train 300 1.738829e-02 0.112929
2019-11-12 21:10:03,147 train 350 1.730123e-02 0.117184
2019-11-12 21:10:09,401 train 400 1.723363e-02 0.122044
2019-11-12 21:10:15,653 train 450 1.721219e-02 0.121921
2019-11-12 21:10:21,911 train 500 1.718200e-02 0.123019
2019-11-12 21:10:28,156 train 550 1.715659e-02 0.125447
2019-11-12 21:10:34,399 train 600 1.711175e-02 0.127486
2019-11-12 21:10:40,648 train 650 1.709688e-02 0.125483
2019-11-12 21:10:46,900 train 700 1.701598e-02 0.127797
2019-11-12 21:10:53,150 train 750 1.693499e-02 0.131424
2019-11-12 21:10:59,592 train 800 1.689360e-02 0.131790
2019-11-12 21:11:05,990 train 850 1.685373e-02 0.128797
2019-11-12 21:11:07,857 training loss; R2: 1.685627e-02 0.128295
2019-11-12 21:11:08,156 valid 000 2.101904e-02 0.135578
2019-11-12 21:11:09,819 valid 050 1.937292e-02 0.053162
2019-11-12 21:11:11,305 validation loss; R2: 1.932324e-02 0.071048
2019-11-12 21:11:11,328 epoch 3 lr 1.000000e-03
2019-11-12 21:11:11,693 train 000 1.872845e-02 -0.225750
2019-11-12 21:11:17,935 train 050 1.626259e-02 0.144272
2019-11-12 21:11:24,176 train 100 1.625239e-02 0.157887
2019-11-12 21:11:30,413 train 150 1.607883e-02 0.155991
2019-11-12 21:11:36,648 train 200 1.602281e-02 0.150901
2019-11-12 21:11:42,890 train 250 1.602144e-02 0.150468
2019-11-12 21:11:49,136 train 300 1.600717e-02 0.154948
2019-11-12 21:11:55,378 train 350 1.594943e-02 0.158640
2019-11-12 21:12:01,613 train 400 1.592747e-02 -1.216324
2019-11-12 21:12:07,846 train 450 1.588071e-02 -1.062467
2019-11-12 21:12:14,083 train 500 1.589160e-02 -0.938742
2019-11-12 21:12:20,321 train 550 1.584979e-02 -0.832350
2019-11-12 21:12:26,556 train 600 1.585518e-02 -0.750100
2019-11-12 21:12:32,792 train 650 1.579904e-02 -0.677226
2019-11-12 21:12:39,028 train 700 1.574930e-02 -0.617309
2019-11-12 21:12:45,264 train 750 1.571135e-02 -0.563419
2019-11-12 21:12:51,507 train 800 1.568026e-02 -0.534696
2019-11-12 21:12:57,741 train 850 1.564218e-02 -0.491195
2019-11-12 21:12:59,606 training loss; R2: 1.562965e-02 -0.478500
2019-11-12 21:12:59,892 valid 000 1.781523e-02 0.247486
2019-11-12 21:13:01,590 valid 050 1.386276e-02 0.286812
2019-11-12 21:13:03,126 validation loss; R2: 1.389012e-02 0.285791
2019-11-12 21:13:03,142 epoch 4 lr 1.000000e-03
2019-11-12 21:13:03,512 train 000 1.866431e-02 0.159531
2019-11-12 21:13:09,996 train 050 1.540977e-02 0.099626
2019-11-12 21:13:16,318 train 100 1.510493e-02 0.159749
2019-11-12 21:13:22,580 train 150 1.506231e-02 0.165898
2019-11-12 21:13:28,875 train 200 1.507046e-02 0.178006
2019-11-12 21:13:35,452 train 250 1.502992e-02 0.174616
2019-11-12 21:13:41,735 train 300 1.505328e-02 0.176791
2019-11-12 21:13:47,999 train 350 1.504859e-02 0.178595
2019-11-12 21:13:54,279 train 400 1.505028e-02 0.182412
2019-11-12 21:14:00,569 train 450 1.504421e-02 0.183056
2019-11-12 21:14:06,871 train 500 1.503019e-02 0.185368
2019-11-12 21:14:13,172 train 550 1.495854e-02 0.185870
2019-11-12 21:14:19,653 train 600 1.490417e-02 0.189152
2019-11-12 21:14:25,914 train 650 1.482757e-02 0.188275
2019-11-12 21:14:32,386 train 700 1.480984e-02 0.189051
2019-11-12 21:14:38,646 train 750 1.479258e-02 0.189895
2019-11-12 21:14:44,916 train 800 1.477461e-02 0.190055
2019-11-12 21:14:51,173 train 850 1.474602e-02 0.192278
2019-11-12 21:14:53,040 training loss; R2: 1.474406e-02 0.193426
2019-11-12 21:14:53,324 valid 000 3.431540e-01 -15.060945
2019-11-12 21:14:55,054 valid 050 3.571557e-01 -16.080949
2019-11-12 21:14:56,602 validation loss; R2: 3.554682e-01 -15.877618
2019-11-12 21:14:56,623 epoch 5 lr 1.000000e-03
2019-11-12 21:14:57,021 train 000 2.101317e-02 0.021780
2019-11-12 21:15:03,511 train 050 1.481783e-02 0.132353
2019-11-12 21:15:09,806 train 100 1.471653e-02 0.166079
2019-11-12 21:15:16,083 train 150 1.453870e-02 0.181491
2019-11-12 21:15:22,430 train 200 1.446027e-02 0.019826
2019-11-12 21:15:29,099 train 250 1.436331e-02 0.058014
2019-11-12 21:15:35,598 train 300 1.427649e-02 0.090320
2019-11-12 21:15:42,009 train 350 1.423702e-02 0.110588
2019-11-12 21:15:48,335 train 400 1.421825e-02 0.125807
2019-11-12 21:15:54,592 train 450 1.418237e-02 0.135644
2019-11-12 21:16:01,046 train 500 1.416631e-02 0.143496
2019-11-12 21:16:07,628 train 550 1.417671e-02 0.150690
2019-11-12 21:16:13,954 train 600 1.415673e-02 0.157799
2019-11-12 21:16:20,188 train 650 1.415809e-02 0.164203
2019-11-12 21:16:26,431 train 700 1.415557e-02 0.168439
2019-11-12 21:16:32,675 train 750 1.412291e-02 0.173644
2019-11-12 21:16:38,916 train 800 1.413038e-02 0.174835
2019-11-12 21:16:45,152 train 850 1.411484e-02 0.177751
2019-11-12 21:16:47,020 training loss; R2: 1.410981e-02 0.179021
2019-11-12 21:16:47,316 valid 000 5.836935e-02 -5.822727
2019-11-12 21:16:48,992 valid 050 6.147558e-02 -3.347776
2019-11-12 21:16:50,536 validation loss; R2: 6.167230e-02 -4.361805
2019-11-12 21:16:50,557 epoch 6 lr 1.000000e-03
2019-11-12 21:16:50,923 train 000 1.528189e-02 0.296425
2019-11-12 21:16:57,442 train 050 1.401438e-02 0.217455
2019-11-12 21:17:03,917 train 100 1.382144e-02 0.231470
2019-11-12 21:17:10,386 train 150 1.383172e-02 0.168712
2019-11-12 21:17:16,910 train 200 1.384528e-02 0.189648
2019-11-12 21:17:23,442 train 250 1.376881e-02 0.198456
2019-11-12 21:17:29,861 train 300 1.375576e-02 0.208047
2019-11-12 21:17:36,347 train 350 1.378251e-02 0.165541
2019-11-12 21:17:42,871 train 400 1.373847e-02 0.173037
2019-11-12 21:17:49,271 train 450 1.373170e-02 0.181032
2019-11-12 21:17:55,621 train 500 1.370468e-02 0.188049
2019-11-12 21:18:02,000 train 550 1.371944e-02 0.192627
2019-11-12 21:18:08,382 train 600 1.368912e-02 0.197177
2019-11-12 21:18:14,703 train 650 1.370145e-02 0.201789
2019-11-12 21:18:21,194 train 700 1.370226e-02 0.204800
2019-11-12 21:18:27,663 train 750 1.367792e-02 0.195952
2019-11-12 21:18:33,952 train 800 1.365467e-02 0.193386
2019-11-12 21:18:40,367 train 850 1.363762e-02 0.195104
2019-11-12 21:18:42,248 training loss; R2: 1.363497e-02 0.196275
2019-11-12 21:18:42,543 valid 000 3.891520e-01 -17.550241
2019-11-12 21:18:44,257 valid 050 3.834775e-01 -19.896268
2019-11-12 21:18:45,812 validation loss; R2: 3.818920e-01 -25.090216
2019-11-12 21:18:45,828 epoch 7 lr 1.000000e-03
2019-11-12 21:18:46,211 train 000 1.626722e-02 0.273413
2019-11-12 21:18:52,723 train 050 1.377005e-02 0.239964
2019-11-12 21:18:59,178 train 100 1.351514e-02 0.254570
2019-11-12 21:19:05,660 train 150 1.336805e-02 0.261674
2019-11-12 21:19:12,092 train 200 1.333050e-02 0.260848
2019-11-12 21:19:18,476 train 250 1.330148e-02 0.261767
2019-11-12 21:19:24,940 train 300 1.326664e-02 0.253027
2019-11-12 21:19:31,369 train 350 1.324085e-02 0.233527
2019-11-12 21:19:37,786 train 400 1.321275e-02 0.225388
2019-11-12 21:19:44,521 train 450 1.321082e-02 0.228039
2019-11-12 21:19:51,068 train 500 1.321573e-02 0.229786
2019-11-12 21:19:57,429 train 550 1.323689e-02 0.231812
2019-11-12 21:20:03,951 train 600 1.319607e-02 0.235967
2019-11-12 21:20:10,458 train 650 1.322456e-02 0.235285
2019-11-12 21:20:16,915 train 700 1.322488e-02 0.236854
2019-11-12 21:20:23,488 train 750 1.322915e-02 0.239370
2019-11-12 21:20:29,893 train 800 1.321981e-02 0.240696
2019-11-12 21:20:36,367 train 850 1.322788e-02 0.240417
2019-11-12 21:20:38,282 training loss; R2: 1.322329e-02 0.241072
2019-11-12 21:20:38,580 valid 000 2.832827e-01 -39.628980
2019-11-12 21:20:40,280 valid 050 2.895307e-01 -43.645101
2019-11-12 21:20:41,818 validation loss; R2: 2.892770e-01 -43.174875
2019-11-12 21:20:41,833 epoch 8 lr 1.000000e-03
2019-11-12 21:20:42,231 train 000 1.200673e-02 0.384675
2019-11-12 21:20:48,635 train 050 1.281064e-02 0.204158
2019-11-12 21:20:55,071 train 100 1.281401e-02 0.212486
2019-11-12 21:21:01,500 train 150 1.287801e-02 0.220905
2019-11-12 21:21:07,984 train 200 1.292175e-02 0.234705
2019-11-12 21:21:14,470 train 250 1.294654e-02 0.194965
2019-11-12 21:21:21,116 train 300 1.296651e-02 0.207963
2019-11-12 21:21:27,536 train 350 1.297559e-02 0.217632
2019-11-12 21:21:33,935 train 400 1.294230e-02 0.219380
2019-11-12 21:21:40,279 train 450 1.296466e-02 0.224311
2019-11-12 21:21:46,839 train 500 1.294421e-02 0.229525
2019-11-12 21:21:53,362 train 550 1.293896e-02 0.233980
2019-11-12 21:21:59,879 train 600 1.293791e-02 0.235549
2019-11-12 21:22:06,380 train 650 1.291343e-02 0.239361
2019-11-12 21:22:12,889 train 700 1.291908e-02 0.240203
2019-11-12 21:22:19,394 train 750 1.290977e-02 0.242166
2019-11-12 21:22:25,893 train 800 1.289253e-02 0.240394
2019-11-12 21:22:32,404 train 850 1.290167e-02 0.240299
2019-11-12 21:22:34,349 training loss; R2: 1.290058e-02 0.240310
2019-11-12 21:22:34,649 valid 000 5.958723e-01 -37.366878
2019-11-12 21:22:36,293 valid 050 6.105267e-01 -56.093900
2019-11-12 21:22:37,780 validation loss; R2: 6.094081e-01 -53.751925
2019-11-12 21:22:37,795 epoch 9 lr 1.000000e-03
2019-11-12 21:22:38,195 train 000 1.648951e-02 0.307912
2019-11-12 21:22:44,794 train 050 1.318407e-02 0.211012
2019-11-12 21:22:51,317 train 100 1.305012e-02 0.223994
2019-11-12 21:22:57,576 train 150 1.297922e-02 0.239695
2019-11-12 21:23:03,972 train 200 1.288802e-02 0.246637
2019-11-12 21:23:10,581 train 250 1.281109e-02 0.206133
2019-11-12 21:23:16,996 train 300 1.276724e-02 0.212319
2019-11-12 21:23:23,257 train 350 1.282129e-02 0.219160
2019-11-12 21:23:29,503 train 400 1.277812e-02 0.222821
2019-11-12 21:23:35,781 train 450 1.276641e-02 0.229754
2019-11-12 21:23:42,037 train 500 1.279181e-02 0.233321
2019-11-12 21:23:48,300 train 550 1.277015e-02 0.234154
2019-11-12 21:23:54,562 train 600 1.276366e-02 0.236694
2019-11-12 21:24:00,833 train 650 1.275583e-02 0.239652
2019-11-12 21:24:07,101 train 700 1.273063e-02 0.238586
2019-11-12 21:24:13,358 train 750 1.270889e-02 0.238448
2019-11-12 21:24:19,826 train 800 1.270312e-02 0.241486
2019-11-12 21:24:26,089 train 850 1.268555e-02 0.242012
2019-11-12 21:24:27,983 training loss; R2: 1.268892e-02 0.242292
2019-11-12 21:24:28,282 valid 000 2.454071e-02 -0.490623
2019-11-12 21:24:29,937 valid 050 2.539348e-02 -0.692528
2019-11-12 21:24:31,447 validation loss; R2: 2.528623e-02 -0.625604
2019-11-12 21:24:31,462 epoch 10 lr 1.000000e-03
2019-11-12 21:24:31,844 train 000 1.359850e-02 0.175978
2019-11-12 21:24:38,474 train 050 1.217914e-02 0.279717
2019-11-12 21:24:45,093 train 100 1.245541e-02 0.277968
2019-11-12 21:24:51,692 train 150 1.249582e-02 0.272838
2019-11-12 21:24:58,304 train 200 1.255502e-02 0.266044
2019-11-12 21:25:04,897 train 250 1.256239e-02 0.241652
2019-11-12 21:25:11,500 train 300 1.250754e-02 0.245884
2019-11-12 21:25:18,051 train 350 1.249754e-02 0.253757
2019-11-12 21:25:24,292 train 400 1.252888e-02 0.257130
2019-11-12 21:25:30,547 train 450 1.250287e-02 0.262443
2019-11-12 21:25:36,784 train 500 1.246768e-02 0.264077
2019-11-12 21:25:43,031 train 550 1.249669e-02 0.266564
2019-11-12 21:25:49,274 train 600 1.251803e-02 0.265867
2019-11-12 21:25:55,533 train 650 1.250534e-02 0.264384
2019-11-12 21:26:01,777 train 700 1.249968e-02 0.261410
2019-11-12 21:26:08,020 train 750 1.247852e-02 0.260069
2019-11-12 21:26:14,259 train 800 1.247084e-02 0.258114
2019-11-12 21:26:20,583 train 850 1.246256e-02 0.257299
2019-11-12 21:26:22,448 training loss; R2: 1.245432e-02 0.258185
2019-11-12 21:26:22,757 valid 000 2.912410e+00 -138.009989
2019-11-12 21:26:24,410 valid 050 2.927048e+00 -173.502743
2019-11-12 21:26:25,915 validation loss; R2: 2.929586e+00 -169.171757
2019-11-12 21:26:25,936 epoch 11 lr 1.000000e-03
2019-11-12 21:26:26,322 train 000 1.193692e-02 0.327709
2019-11-12 21:26:32,935 train 050 1.262243e-02 0.289112
2019-11-12 21:26:39,645 train 100 1.257196e-02 0.292391
2019-11-12 21:26:46,294 train 150 1.259351e-02 0.277210
2019-11-12 21:26:52,804 train 200 1.247682e-02 0.278486
2019-11-12 21:26:59,405 train 250 1.246604e-02 0.279564
2019-11-12 21:27:06,029 train 300 1.241376e-02 0.276310
2019-11-12 21:27:12,399 train 350 1.237739e-02 0.277530
2019-11-12 21:27:18,659 train 400 1.233166e-02 0.276312
2019-11-12 21:27:24,911 train 450 1.234854e-02 0.274267
2019-11-12 21:27:31,160 train 500 1.229946e-02 0.262810
2019-11-12 21:27:37,410 train 550 1.228010e-02 0.265516
2019-11-12 21:27:43,657 train 600 1.225918e-02 0.265661
2019-11-12 21:27:49,919 train 650 1.224621e-02 0.268082
2019-11-12 21:27:56,163 train 700 1.222488e-02 0.264721
2019-11-12 21:28:02,415 train 750 1.223236e-02 0.266497
2019-11-12 21:28:08,666 train 800 1.223230e-02 0.265762
2019-11-12 21:28:14,916 train 850 1.225360e-02 0.265856
2019-11-12 21:28:16,788 training loss; R2: 1.223945e-02 0.266966
2019-11-12 21:28:17,098 valid 000 6.349826e-01 -38.376989
2019-11-12 21:28:18,796 valid 050 6.137664e-01 -50.098747
2019-11-12 21:28:20,308 validation loss; R2: 6.164996e-01 -50.026032
2019-11-12 21:28:20,323 epoch 12 lr 1.000000e-03
2019-11-12 21:28:20,695 train 000 1.273483e-02 0.265496
2019-11-12 21:28:27,380 train 050 1.199189e-02 0.287123
2019-11-12 21:28:33,831 train 100 1.199921e-02 0.248061
2019-11-12 21:28:40,232 train 150 1.191615e-02 0.271244
2019-11-12 21:28:46,840 train 200 1.192957e-02 0.276579
2019-11-12 21:28:53,453 train 250 1.189754e-02 0.253745
2019-11-12 21:29:00,171 train 300 1.193533e-02 0.249224
2019-11-12 21:29:06,582 train 350 1.200032e-02 0.253784
2019-11-12 21:29:13,093 train 400 1.198890e-02 0.258406
2019-11-12 21:29:19,507 train 450 1.198371e-02 0.261614
2019-11-12 21:29:25,964 train 500 1.199256e-02 0.264422
2019-11-12 21:29:32,536 train 550 1.199910e-02 0.265027
2019-11-12 21:29:39,047 train 600 1.199639e-02 0.266775
2019-11-12 21:29:45,684 train 650 1.200766e-02 0.269053
2019-11-12 21:29:52,191 train 700 1.202233e-02 0.267632
2019-11-12 21:29:58,703 train 750 1.200502e-02 0.269043
2019-11-12 21:30:05,207 train 800 1.199790e-02 0.272600
2019-11-12 21:30:11,713 train 850 1.200258e-02 0.274429
2019-11-12 21:30:13,656 training loss; R2: 1.200534e-02 0.273064
2019-11-12 21:30:13,953 valid 000 2.688827e+00 -161.466120
2019-11-12 21:30:15,617 valid 050 2.680371e+00 -141.090275
2019-11-12 21:30:17,130 validation loss; R2: 2.678264e+00 -133.421230
2019-11-12 21:30:17,146 epoch 13 lr 1.000000e-03
2019-11-12 21:30:17,532 train 000 1.099474e-02 0.204718
2019-11-12 21:30:24,217 train 050 1.207775e-02 0.302104
2019-11-12 21:30:30,730 train 100 1.199192e-02 0.306595
2019-11-12 21:30:36,979 train 150 1.208353e-02 0.296775
2019-11-12 21:30:43,215 train 200 1.197971e-02 0.298917
2019-11-12 21:30:49,466 train 250 1.197831e-02 0.296954
2019-11-12 21:30:55,702 train 300 1.193203e-02 0.297158
2019-11-12 21:31:01,933 train 350 1.189651e-02 0.297226
2019-11-12 21:31:08,165 train 400 1.187418e-02 0.299125
2019-11-12 21:31:14,401 train 450 1.189066e-02 0.297459
2019-11-12 21:31:20,949 train 500 1.188396e-02 0.296016
2019-11-12 21:31:27,526 train 550 1.188419e-02 0.293063
2019-11-12 21:31:33,852 train 600 1.190756e-02 0.292714
2019-11-12 21:31:40,085 train 650 1.189351e-02 0.292355
2019-11-12 21:31:46,314 train 700 1.188427e-02 0.293512
2019-11-12 21:31:52,544 train 750 1.187581e-02 0.292484
2019-11-12 21:31:58,769 train 800 1.187522e-02 0.294524
2019-11-12 21:32:05,021 train 850 1.186283e-02 0.294381
2019-11-12 21:32:06,881 training loss; R2: 1.185918e-02 0.294732
2019-11-12 21:32:07,182 valid 000 2.069759e+00 -122.069304
2019-11-12 21:32:08,862 valid 050 2.124610e+00 -116.871841
2019-11-12 21:32:10,385 validation loss; R2: 2.125981e+00 -111.546887
2019-11-12 21:32:10,405 epoch 14 lr 1.000000e-03
2019-11-12 21:32:10,813 train 000 1.008655e-02 0.378829
2019-11-12 21:32:17,474 train 050 1.170635e-02 0.289619
2019-11-12 21:32:23,936 train 100 1.186314e-02 0.273981
2019-11-12 21:32:30,348 train 150 1.190811e-02 0.262384
2019-11-12 21:32:36,596 train 200 1.197680e-02 0.263071
2019-11-12 21:32:42,981 train 250 1.196192e-02 0.255524
2019-11-12 21:32:49,518 train 300 1.190008e-02 0.264872
2019-11-12 21:32:56,066 train 350 1.185615e-02 0.269310
2019-11-12 21:33:02,622 train 400 1.186296e-02 0.276860
2019-11-12 21:33:09,175 train 450 1.185753e-02 0.269240
2019-11-12 21:33:15,727 train 500 1.185233e-02 0.269487
2019-11-12 21:33:22,250 train 550 1.184917e-02 0.273964
2019-11-12 21:33:28,459 train 600 1.182190e-02 0.276853
2019-11-12 21:33:34,677 train 650 1.180532e-02 0.276460
2019-11-12 21:33:40,889 train 700 1.180371e-02 0.274030
2019-11-12 21:33:47,098 train 750 1.179683e-02 0.275480
2019-11-12 21:33:53,309 train 800 1.178221e-02 0.276008
2019-11-12 21:33:59,519 train 850 1.177686e-02 0.278812
2019-11-12 21:34:01,375 training loss; R2: 1.176937e-02 0.279006
2019-11-12 21:34:01,666 valid 000 1.924149e+01 -2010.769708
2019-11-12 21:34:03,327 valid 050 1.925303e+01 -1447.552575
2019-11-12 21:34:04,830 validation loss; R2: 1.924645e+01 -1485.899325
2019-11-12 21:34:04,851 epoch 15 lr 1.000000e-03
2019-11-12 21:34:05,210 train 000 1.105062e-02 0.229585
2019-11-12 21:34:11,719 train 050 1.192393e-02 0.277597
2019-11-12 21:34:18,340 train 100 1.183554e-02 0.289122
2019-11-12 21:34:24,881 train 150 1.183956e-02 0.295648
2019-11-12 21:34:31,405 train 200 1.179789e-02 0.297614
2019-11-12 21:34:37,934 train 250 1.178871e-02 0.299081
2019-11-12 21:34:44,452 train 300 1.178372e-02 0.302329
2019-11-12 21:34:50,982 train 350 1.175281e-02 0.305495
2019-11-12 21:34:57,506 train 400 1.177510e-02 0.304962
2019-11-12 21:35:04,114 train 450 1.176487e-02 0.302854
2019-11-12 21:35:10,613 train 500 1.175183e-02 0.303699
2019-11-12 21:35:17,252 train 550 1.175550e-02 0.297053
2019-11-12 21:35:23,805 train 600 1.174179e-02 0.294968
2019-11-12 21:35:30,429 train 650 1.172690e-02 0.296989
2019-11-12 21:35:36,687 train 700 1.172907e-02 0.295185
2019-11-12 21:35:43,024 train 750 1.171250e-02 0.294883
2019-11-12 21:35:49,619 train 800 1.170744e-02 0.295076
2019-11-12 21:35:55,863 train 850 1.171544e-02 0.295574
2019-11-12 21:35:57,741 training loss; R2: 1.172156e-02 0.292639
2019-11-12 21:35:58,021 valid 000 1.174081e+01 -10159.782326
2019-11-12 21:35:59,684 valid 050 1.171466e+01 -8773.759597
2019-11-12 21:36:01,201 validation loss; R2: 1.171873e+01 -11574.177303
2019-11-12 21:36:01,221 epoch 16 lr 1.000000e-03
2019-11-12 21:36:01,598 train 000 1.131577e-02 0.061647
2019-11-12 21:36:08,196 train 050 1.174898e-02 0.303658
2019-11-12 21:36:14,813 train 100 1.163916e-02 -2.640903
2019-11-12 21:36:21,184 train 150 1.165272e-02 -1.673638
2019-11-12 21:36:27,547 train 200 1.171334e-02 -1.180785
2019-11-12 21:36:34,125 train 250 1.168739e-02 -0.882779
2019-11-12 21:36:40,589 train 300 1.166205e-02 -0.698274
2019-11-12 21:36:47,071 train 350 1.164564e-02 -0.557358
2019-11-12 21:36:53,701 train 400 1.161043e-02 -0.449442
2019-11-12 21:37:00,185 train 450 1.160122e-02 -0.372879
2019-11-12 21:37:06,703 train 500 1.155013e-02 -0.303781
2019-11-12 21:37:13,165 train 550 1.152444e-02 -0.247055
2019-11-12 21:37:19,623 train 600 1.150962e-02 -0.200977
2019-11-12 21:37:26,118 train 650 1.149663e-02 -0.159930
2019-11-12 21:37:32,447 train 700 1.150648e-02 -0.127076
2019-11-12 21:37:38,874 train 750 1.149482e-02 -0.097069
2019-11-12 21:37:45,483 train 800 1.150583e-02 -0.075179
2019-11-12 21:37:52,087 train 850 1.151495e-02 -0.052498
2019-11-12 21:37:54,063 training loss; R2: 1.152372e-02 -0.046262
2019-11-12 21:37:54,386 valid 000 3.610573e+01 -6092.282415
2019-11-12 21:37:56,082 valid 050 3.612280e+01 -11132.972747
2019-11-12 21:37:57,636 validation loss; R2: 3.612043e+01 -11852.224489
2019-11-12 21:37:57,658 epoch 17 lr 1.000000e-03
2019-11-12 21:37:58,052 train 000 1.099245e-02 0.408860
2019-11-12 21:38:04,780 train 050 1.173306e-02 0.161138
2019-11-12 21:38:11,301 train 100 1.164546e-02 0.195262
2019-11-12 21:38:17,769 train 150 1.153642e-02 0.220860
2019-11-12 21:38:24,143 train 200 1.146262e-02 0.241842
2019-11-12 21:38:30,580 train 250 1.144286e-02 0.257282
2019-11-12 21:38:36,958 train 300 1.142223e-02 0.213147
2019-11-12 21:38:43,323 train 350 1.142135e-02 0.227751
2019-11-12 21:38:49,684 train 400 1.145102e-02 0.225874
2019-11-12 21:38:56,172 train 450 1.143686e-02 0.234761
2019-11-12 21:39:02,820 train 500 1.142750e-02 0.244692
2019-11-12 21:39:09,358 train 550 1.141673e-02 0.248409
2019-11-12 21:39:15,833 train 600 1.139751e-02 0.256141
2019-11-12 21:39:22,347 train 650 1.141862e-02 0.258431
2019-11-12 21:39:28,802 train 700 1.143882e-02 0.260766
2019-11-12 21:39:35,346 train 750 1.142977e-02 0.263456
2019-11-12 21:39:41,882 train 800 1.141764e-02 0.266994
2019-11-12 21:39:48,539 train 850 1.141483e-02 0.267455
2019-11-12 21:39:50,427 training loss; R2: 1.141754e-02 0.268698
2019-11-12 21:39:50,723 valid 000 5.347579e+00 -572.778657
2019-11-12 21:39:52,405 valid 050 5.343001e+00 -784.030662
2019-11-12 21:39:53,946 validation loss; R2: 5.346681e+00 -719.711320
2019-11-12 21:39:53,966 epoch 18 lr 1.000000e-03
2019-11-12 21:39:54,430 train 000 1.134813e-02 0.393017
2019-11-12 21:40:00,911 train 050 1.136714e-02 0.328276
2019-11-12 21:40:07,345 train 100 1.135511e-02 0.255933
2019-11-12 21:40:13,843 train 150 1.135950e-02 0.283581
2019-11-12 21:40:20,423 train 200 1.127884e-02 0.287973
2019-11-12 21:40:26,917 train 250 1.125190e-02 0.274660
2019-11-12 21:40:33,511 train 300 1.122539e-02 0.284067
2019-11-12 21:40:39,976 train 350 1.122760e-02 0.287002
2019-11-12 21:40:46,584 train 400 1.124875e-02 0.289000
2019-11-12 21:40:53,227 train 450 1.126074e-02 0.294295
2019-11-12 21:40:59,677 train 500 1.127676e-02 0.296942
2019-11-12 21:41:06,257 train 550 1.128503e-02 0.296638
2019-11-12 21:41:12,813 train 600 1.131030e-02 0.297095
2019-11-12 21:41:19,383 train 650 1.134344e-02 0.299554
2019-11-12 21:41:25,939 train 700 1.135651e-02 0.300332
2019-11-12 21:41:32,512 train 750 1.133651e-02 0.301254
2019-11-12 21:41:39,085 train 800 1.134569e-02 0.297740
2019-11-12 21:41:45,664 train 850 1.135242e-02 0.298404
2019-11-12 21:41:47,626 training loss; R2: 1.135084e-02 0.298955
2019-11-12 21:41:47,933 valid 000 3.503679e+01 -4096.188541
2019-11-12 21:41:49,576 valid 050 3.520215e+01 -2489.817621
2019-11-12 21:41:51,103 validation loss; R2: 3.520499e+01 -2552.132219
2019-11-12 21:41:51,119 epoch 19 lr 1.000000e-03
2019-11-12 21:41:51,484 train 000 1.093299e-02 0.368240
2019-11-12 21:41:58,146 train 050 1.133799e-02 0.250245
2019-11-12 21:42:04,905 train 100 1.136553e-02 0.292719
2019-11-12 21:42:11,417 train 150 1.143436e-02 0.284551
2019-11-12 21:42:18,009 train 200 1.143888e-02 0.290729
2019-11-12 21:42:24,682 train 250 1.139498e-02 0.296591
2019-11-12 21:42:31,376 train 300 1.138915e-02 0.296617
2019-11-12 21:42:38,050 train 350 1.144126e-02 0.279456
2019-11-12 21:42:44,742 train 400 1.141641e-02 0.279772
2019-11-12 21:42:51,347 train 450 1.142132e-02 0.280814
2019-11-12 21:42:57,944 train 500 1.144234e-02 0.281004
2019-11-12 21:43:04,517 train 550 1.143774e-02 0.285581
2019-11-12 21:43:11,039 train 600 1.141191e-02 0.288029
2019-11-12 21:43:17,453 train 650 1.141928e-02 0.283275
2019-11-12 21:43:23,912 train 700 1.140904e-02 0.285443
2019-11-12 21:43:30,573 train 750 1.137135e-02 0.288456
2019-11-12 21:43:37,315 train 800 1.135896e-02 0.287986
2019-11-12 21:43:43,713 train 850 1.134912e-02 0.289965
2019-11-12 21:43:45,646 training loss; R2: 1.135057e-02 0.285668
2019-11-12 21:43:45,939 valid 000 1.126637e+01 -2138.165057
2019-11-12 21:43:47,603 valid 050 1.136754e+01 -1654.215293
2019-11-12 21:43:49,137 validation loss; R2: 1.135693e+01 -1898.105428
