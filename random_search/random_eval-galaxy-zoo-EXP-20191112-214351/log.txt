2019-11-12 21:43:51,349 gpu device = 1
2019-11-12 21:43:51,349 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name='galaxy-zoo-cleaned', gpu=1, grad_clip=5, gz_dtree=False, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-galaxy-zoo-EXP-20191112-214351', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-11-12 21:44:02,911 param size = 0.197109MB
2019-11-12 21:44:02,915 epoch 0 lr 1.000000e-03
2019-11-12 21:44:04,969 train 000 1.310377e+00 -356.028821
2019-11-12 21:44:10,170 train 050 8.700739e-02 -26.191399
2019-11-12 21:44:15,273 train 100 6.086634e-02 -13.897052
2019-11-12 21:44:20,311 train 150 5.109942e-02 -9.623214
2019-11-12 21:44:25,340 train 200 4.575894e-02 -7.398071
2019-11-12 21:44:30,364 train 250 4.214871e-02 -6.040071
2019-11-12 21:44:35,390 train 300 3.968738e-02 -5.106403
2019-11-12 21:44:40,438 train 350 3.785001e-02 -4.433557
2019-11-12 21:44:45,468 train 400 3.640546e-02 -3.924472
2019-11-12 21:44:50,504 train 450 3.521544e-02 -3.517627
2019-11-12 21:44:55,556 train 500 3.423318e-02 -3.199897
2019-11-12 21:45:00,587 train 550 3.336816e-02 -2.935117
2019-11-12 21:45:05,659 train 600 3.267898e-02 -2.719075
2019-11-12 21:45:10,696 train 650 3.201243e-02 -2.529811
2019-11-12 21:45:15,726 train 700 3.141370e-02 -2.364791
2019-11-12 21:45:20,765 train 750 3.083050e-02 -2.225873
2019-11-12 21:45:25,791 train 800 3.032505e-02 -2.101709
2019-11-12 21:45:30,814 train 850 2.987016e-02 -1.984085
2019-11-12 21:45:32,963 training loss; R2: 2.972187e-02 -1.950409
2019-11-12 21:45:33,268 valid 000 2.366685e-02 -0.116533
2019-11-12 21:45:34,963 valid 050 2.002289e-02 0.059878
2019-11-12 21:45:36,561 validation loss; R2: 2.002208e-02 0.054252
2019-11-12 21:45:36,573 epoch 1 lr 1.000000e-03
2019-11-12 21:45:37,072 train 000 2.408147e-02 -0.039487
2019-11-12 21:45:42,144 train 050 2.190518e-02 -0.189730
2019-11-12 21:45:47,168 train 100 2.198932e-02 -0.161475
2019-11-12 21:45:52,208 train 150 2.193225e-02 -0.171239
2019-11-12 21:45:57,263 train 200 2.180232e-02 -0.141738
2019-11-12 21:46:02,305 train 250 2.168752e-02 -0.130607
2019-11-12 21:46:07,345 train 300 2.156424e-02 -0.118126
2019-11-12 21:46:12,372 train 350 2.137065e-02 -0.108671
2019-11-12 21:46:17,412 train 400 2.127775e-02 -0.107997
2019-11-12 21:46:22,432 train 450 2.120185e-02 -0.105097
2019-11-12 21:46:27,455 train 500 2.109238e-02 -0.093746
2019-11-12 21:46:32,463 train 550 2.105364e-02 -0.085321
2019-11-12 21:46:37,525 train 600 2.093435e-02 -0.074810
2019-11-12 21:46:42,562 train 650 2.081377e-02 -0.071268
2019-11-12 21:46:47,595 train 700 2.075701e-02 -0.070590
2019-11-12 21:46:52,649 train 750 2.068113e-02 -0.066041
2019-11-12 21:46:57,700 train 800 2.059203e-02 -0.059570
2019-11-12 21:47:02,738 train 850 2.051587e-02 -0.054906
2019-11-12 21:47:04,247 training loss; R2: 2.047620e-02 -0.053164
2019-11-12 21:47:04,566 valid 000 1.684063e-02 0.212646
2019-11-12 21:47:06,280 valid 050 1.695864e-02 0.191605
2019-11-12 21:47:07,828 validation loss; R2: 1.709016e-02 0.012568
2019-11-12 21:47:07,838 epoch 2 lr 1.000000e-03
2019-11-12 21:47:08,160 train 000 1.967498e-02 0.083259
2019-11-12 21:47:13,181 train 050 1.885454e-02 -0.035215
2019-11-12 21:47:18,330 train 100 1.931580e-02 -0.011313
2019-11-12 21:47:23,462 train 150 1.942803e-02 0.003296
2019-11-12 21:47:28,616 train 200 1.923590e-02 0.017346
2019-11-12 21:47:33,776 train 250 1.916560e-02 0.020913
2019-11-12 21:47:38,899 train 300 1.908002e-02 0.028781
2019-11-12 21:47:44,048 train 350 1.903135e-02 0.017600
2019-11-12 21:47:49,185 train 400 1.897450e-02 0.022000
2019-11-12 21:47:54,289 train 450 1.886356e-02 0.027470
2019-11-12 21:47:59,337 train 500 1.878421e-02 0.021936
2019-11-12 21:48:04,383 train 550 1.874123e-02 0.027696
2019-11-12 21:48:09,427 train 600 1.865420e-02 0.035097
2019-11-12 21:48:14,483 train 650 1.858984e-02 0.038592
2019-11-12 21:48:19,546 train 700 1.854797e-02 0.035921
2019-11-12 21:48:24,617 train 750 1.851725e-02 0.041846
2019-11-12 21:48:29,673 train 800 1.845306e-02 0.043936
2019-11-12 21:48:34,725 train 850 1.838615e-02 0.046994
2019-11-12 21:48:36,236 training loss; R2: 1.837423e-02 0.048039
2019-11-12 21:48:36,545 valid 000 1.622600e-02 0.181353
2019-11-12 21:48:38,241 valid 050 1.668478e-02 0.195016
2019-11-12 21:48:39,792 validation loss; R2: 1.657303e-02 0.195159
2019-11-12 21:48:39,804 epoch 3 lr 1.000000e-03
2019-11-12 21:48:40,156 train 000 1.611461e-02 0.023869
2019-11-12 21:48:45,276 train 050 1.709198e-02 0.068543
2019-11-12 21:48:50,378 train 100 1.714352e-02 0.089689
2019-11-12 21:48:55,492 train 150 1.719124e-02 0.112436
2019-11-12 21:49:00,587 train 200 1.721630e-02 0.112534
2019-11-12 21:49:05,681 train 250 1.712570e-02 0.118729
2019-11-12 21:49:10,806 train 300 1.715132e-02 0.121417
2019-11-12 21:49:16,006 train 350 1.709806e-02 0.125687
2019-11-12 21:49:21,151 train 400 1.704415e-02 0.119932
2019-11-12 21:49:26,257 train 450 1.703499e-02 0.117471
2019-11-12 21:49:31,418 train 500 1.701701e-02 0.119409
2019-11-12 21:49:36,601 train 550 1.700142e-02 0.121813
2019-11-12 21:49:41,824 train 600 1.693227e-02 0.083113
2019-11-12 21:49:47,055 train 650 1.687412e-02 0.087682
2019-11-12 21:49:52,125 train 700 1.687368e-02 0.093139
2019-11-12 21:49:57,176 train 750 1.683432e-02 0.097563
2019-11-12 21:50:02,339 train 800 1.680770e-02 0.100132
2019-11-12 21:50:07,530 train 850 1.677670e-02 0.104797
2019-11-12 21:50:09,083 training loss; R2: 1.676788e-02 0.104681
2019-11-12 21:50:09,397 valid 000 1.444232e-02 0.289364
2019-11-12 21:50:11,052 valid 050 1.485632e-02 -0.166839
2019-11-12 21:50:12,573 validation loss; R2: 1.465318e-02 0.037049
2019-11-12 21:50:12,586 epoch 4 lr 1.000000e-03
2019-11-12 21:50:12,969 train 000 1.431882e-02 0.145927
2019-11-12 21:50:17,922 train 050 1.608390e-02 0.124238
2019-11-12 21:50:23,173 train 100 1.596022e-02 0.140484
2019-11-12 21:50:28,429 train 150 1.592367e-02 0.145711
2019-11-12 21:50:33,655 train 200 1.588303e-02 0.154757
2019-11-12 21:50:38,890 train 250 1.588170e-02 0.157005
2019-11-12 21:50:44,122 train 300 1.592650e-02 0.157476
2019-11-12 21:50:49,373 train 350 1.585246e-02 0.157319
2019-11-12 21:50:54,600 train 400 1.583492e-02 0.158697
2019-11-12 21:50:59,825 train 450 1.582697e-02 0.159634
2019-11-12 21:51:05,049 train 500 1.578147e-02 0.141482
2019-11-12 21:51:10,287 train 550 1.577832e-02 0.141949
2019-11-12 21:51:15,515 train 600 1.575540e-02 0.146996
2019-11-12 21:51:20,746 train 650 1.570154e-02 0.149492
2019-11-12 21:51:25,972 train 700 1.565788e-02 0.152915
2019-11-12 21:51:31,200 train 750 1.562173e-02 0.154623
2019-11-12 21:51:36,444 train 800 1.559076e-02 0.156492
2019-11-12 21:51:41,668 train 850 1.556542e-02 0.158373
2019-11-12 21:51:43,231 training loss; R2: 1.555453e-02 0.160172
2019-11-12 21:51:43,555 valid 000 1.525711e-02 0.287037
2019-11-12 21:51:45,226 valid 050 1.485361e-02 0.165723
2019-11-12 21:51:46,725 validation loss; R2: 1.507059e-02 0.145489
2019-11-12 21:51:46,735 epoch 5 lr 1.000000e-03
2019-11-12 21:51:47,101 train 000 1.513034e-02 0.256584
2019-11-12 21:51:52,280 train 050 1.502532e-02 0.198721
2019-11-12 21:51:57,446 train 100 1.495148e-02 0.204862
2019-11-12 21:52:02,609 train 150 1.500218e-02 0.191562
2019-11-12 21:52:07,770 train 200 1.510400e-02 0.182064
2019-11-12 21:52:12,915 train 250 1.500033e-02 0.185162
2019-11-12 21:52:18,141 train 300 1.493320e-02 0.187424
2019-11-12 21:52:23,506 train 350 1.496094e-02 0.186461
2019-11-12 21:52:28,794 train 400 1.493888e-02 0.189735
2019-11-12 21:52:33,988 train 450 1.491765e-02 0.192049
2019-11-12 21:52:39,325 train 500 1.491063e-02 0.188160
2019-11-12 21:52:44,433 train 550 1.485309e-02 0.189296
2019-11-12 21:52:49,531 train 600 1.484093e-02 0.187497
2019-11-12 21:52:54,629 train 650 1.479933e-02 0.187260
2019-11-12 21:52:59,723 train 700 1.478905e-02 0.188420
2019-11-12 21:53:04,821 train 750 1.477451e-02 0.187170
2019-11-12 21:53:09,917 train 800 1.477600e-02 0.187868
2019-11-12 21:53:15,040 train 850 1.474853e-02 0.188943
2019-11-12 21:53:16,595 training loss; R2: 1.476001e-02 0.188517
2019-11-12 21:53:16,914 valid 000 1.278782e-02 0.248975
2019-11-12 21:53:18,576 valid 050 1.230744e-02 0.302385
2019-11-12 21:53:20,085 validation loss; R2: 1.261033e-02 0.290161
2019-11-12 21:53:20,096 epoch 6 lr 1.000000e-03
2019-11-12 21:53:20,464 train 000 1.505138e-02 -0.037669
2019-11-12 21:53:25,590 train 050 1.455840e-02 0.197934
2019-11-12 21:53:30,672 train 100 1.460674e-02 0.201063
2019-11-12 21:53:35,719 train 150 1.451576e-02 0.198814
2019-11-12 21:53:40,675 train 200 1.449956e-02 0.196311
2019-11-12 21:53:45,626 train 250 1.449603e-02 0.197123
2019-11-12 21:53:50,591 train 300 1.442305e-02 0.197327
2019-11-12 21:53:55,690 train 350 1.436073e-02 0.200485
2019-11-12 21:54:00,741 train 400 1.435557e-02 0.202975
2019-11-12 21:54:05,790 train 450 1.433948e-02 0.201230
2019-11-12 21:54:10,787 train 500 1.431422e-02 0.193507
2019-11-12 21:54:15,795 train 550 1.428256e-02 0.191836
2019-11-12 21:54:20,864 train 600 1.424186e-02 0.191679
2019-11-12 21:54:25,951 train 650 1.423863e-02 0.190764
2019-11-12 21:54:31,012 train 700 1.422038e-02 0.194055
2019-11-12 21:54:36,088 train 750 1.420685e-02 0.193911
2019-11-12 21:54:41,171 train 800 1.416245e-02 0.197874
2019-11-12 21:54:46,242 train 850 1.416126e-02 0.196020
2019-11-12 21:54:47,760 training loss; R2: 1.414744e-02 0.197387
2019-11-12 21:54:48,098 valid 000 1.372971e-02 0.323961
2019-11-12 21:54:49,714 valid 050 1.251569e-02 0.335544
2019-11-12 21:54:51,241 validation loss; R2: 1.235962e-02 0.340742
2019-11-12 21:54:51,251 epoch 7 lr 1.000000e-03
2019-11-12 21:54:51,600 train 000 1.421373e-02 0.326570
2019-11-12 21:54:56,754 train 050 1.419103e-02 0.241549
2019-11-12 21:55:01,968 train 100 1.397340e-02 0.234349
2019-11-12 21:55:07,333 train 150 1.399033e-02 0.220902
2019-11-12 21:55:12,646 train 200 1.389325e-02 0.221973
2019-11-12 21:55:17,831 train 250 1.383451e-02 0.221994
2019-11-12 21:55:22,978 train 300 1.386229e-02 0.215910
2019-11-12 21:55:28,128 train 350 1.387158e-02 0.213836
2019-11-12 21:55:33,255 train 400 1.382787e-02 0.212148
2019-11-12 21:55:38,425 train 450 1.382659e-02 0.207091
2019-11-12 21:55:43,659 train 500 1.383511e-02 0.209152
2019-11-12 21:55:49,000 train 550 1.382963e-02 0.212064
2019-11-12 21:55:54,322 train 600 1.382465e-02 0.213710
2019-11-12 21:55:59,475 train 650 1.380612e-02 -0.440495
2019-11-12 21:56:04,680 train 700 1.377838e-02 -0.391955
2019-11-12 21:56:09,927 train 750 1.376928e-02 -0.351059
2019-11-12 21:56:15,015 train 800 1.377300e-02 -0.314813
2019-11-12 21:56:20,184 train 850 1.374624e-02 -0.283486
2019-11-12 21:56:21,705 training loss; R2: 1.372932e-02 -0.274778
2019-11-12 21:56:22,040 valid 000 1.159511e-02 0.399981
2019-11-12 21:56:23,712 valid 050 1.164507e-02 0.321881
2019-11-12 21:56:25,226 validation loss; R2: 1.162795e-02 0.321532
2019-11-12 21:56:25,239 epoch 8 lr 1.000000e-03
2019-11-12 21:56:25,621 train 000 1.296868e-02 0.130742
2019-11-12 21:56:30,879 train 050 1.369158e-02 0.199899
2019-11-12 21:56:36,025 train 100 1.359024e-02 0.199282
2019-11-12 21:56:41,119 train 150 1.353686e-02 0.160019
2019-11-12 21:56:46,237 train 200 1.346382e-02 0.183615
2019-11-12 21:56:51,317 train 250 1.345853e-02 0.189342
2019-11-12 21:56:56,449 train 300 1.347258e-02 0.195389
2019-11-12 21:57:01,650 train 350 1.344481e-02 0.201655
2019-11-12 21:57:06,821 train 400 1.342528e-02 0.207802
2019-11-12 21:57:11,967 train 450 1.339462e-02 0.211334
2019-11-12 21:57:17,097 train 500 1.339338e-02 0.208990
2019-11-12 21:57:22,273 train 550 1.338864e-02 0.213461
2019-11-12 21:57:27,465 train 600 1.338032e-02 0.210835
2019-11-12 21:57:32,703 train 650 1.335645e-02 0.167049
2019-11-12 21:57:38,009 train 700 1.334518e-02 0.174479
2019-11-12 21:57:43,160 train 750 1.334701e-02 0.180232
2019-11-12 21:57:48,313 train 800 1.332226e-02 0.184800
2019-11-12 21:57:53,443 train 850 1.330799e-02 0.188542
2019-11-12 21:57:54,979 training loss; R2: 1.331209e-02 0.189807
2019-11-12 21:57:55,305 valid 000 1.120183e-02 0.139615
2019-11-12 21:57:56,993 valid 050 1.190689e-02 0.311187
2019-11-12 21:57:58,496 validation loss; R2: 1.196065e-02 0.305662
2019-11-12 21:57:58,515 epoch 9 lr 1.000000e-03
2019-11-12 21:57:58,891 train 000 1.473478e-02 0.335992
2019-11-12 21:58:04,034 train 050 1.332463e-02 0.258887
2019-11-12 21:58:09,270 train 100 1.321663e-02 0.250891
2019-11-12 21:58:14,463 train 150 1.317421e-02 0.263752
2019-11-12 21:58:19,560 train 200 1.314338e-02 0.253302
2019-11-12 21:58:24,868 train 250 1.307083e-02 0.248141
2019-11-12 21:58:30,208 train 300 1.301235e-02 0.252608
2019-11-12 21:58:35,534 train 350 1.300765e-02 0.247177
2019-11-12 21:58:40,843 train 400 1.304358e-02 0.246188
2019-11-12 21:58:46,164 train 450 1.301120e-02 0.247605
2019-11-12 21:58:51,481 train 500 1.298276e-02 0.246430
2019-11-12 21:58:56,793 train 550 1.299931e-02 0.243522
2019-11-12 21:59:02,069 train 600 1.300393e-02 -0.784097
2019-11-12 21:59:07,183 train 650 1.296911e-02 -0.705984
2019-11-12 21:59:12,299 train 700 1.296063e-02 -0.638338
2019-11-12 21:59:17,419 train 750 1.294368e-02 -0.577771
2019-11-12 21:59:22,685 train 800 1.292859e-02 -0.529148
2019-11-12 21:59:27,999 train 850 1.292233e-02 -0.481334
2019-11-12 21:59:29,586 training loss; R2: 1.292182e-02 -0.468189
2019-11-12 21:59:29,914 valid 000 1.318203e-02 0.372130
2019-11-12 21:59:31,558 valid 050 1.134575e-02 0.353404
2019-11-12 21:59:33,084 validation loss; R2: 1.132501e-02 0.352496
2019-11-12 21:59:33,100 epoch 10 lr 1.000000e-03
2019-11-12 21:59:33,438 train 000 1.327323e-02 0.255955
2019-11-12 21:59:38,509 train 050 1.250330e-02 0.283771
2019-11-12 21:59:43,630 train 100 1.280099e-02 0.225797
2019-11-12 21:59:48,777 train 150 1.289445e-02 0.215705
2019-11-12 21:59:53,904 train 200 1.281119e-02 0.230335
2019-11-12 21:59:59,034 train 250 1.276141e-02 0.237513
2019-11-12 22:00:04,168 train 300 1.271669e-02 0.243668
2019-11-12 22:00:09,315 train 350 1.269266e-02 0.246106
2019-11-12 22:00:14,625 train 400 1.271554e-02 0.245678
2019-11-12 22:00:19,793 train 450 1.274110e-02 0.243432
2019-11-12 22:00:25,122 train 500 1.272179e-02 0.242762
2019-11-12 22:00:30,332 train 550 1.274286e-02 0.242018
2019-11-12 22:00:35,447 train 600 1.276897e-02 -1.379210
2019-11-12 22:00:40,579 train 650 1.276382e-02 -1.251241
2019-11-12 22:00:45,828 train 700 1.275779e-02 -1.143969
2019-11-12 22:00:50,955 train 750 1.275636e-02 -1.055129
2019-11-12 22:00:56,088 train 800 1.275581e-02 -0.972389
2019-11-12 22:01:01,349 train 850 1.273679e-02 -0.901172
2019-11-12 22:01:02,892 training loss; R2: 1.274634e-02 -0.880986
2019-11-12 22:01:03,239 valid 000 1.173989e-02 0.404729
2019-11-12 22:01:04,917 valid 050 1.142228e-02 0.358458
2019-11-12 22:01:06,426 validation loss; R2: 1.151243e-02 0.340158
2019-11-12 22:01:06,436 epoch 11 lr 1.000000e-03
2019-11-12 22:01:06,766 train 000 1.227392e-02 0.286056
2019-11-12 22:01:11,794 train 050 1.284135e-02 0.246500
2019-11-12 22:01:16,927 train 100 1.276991e-02 0.255733
2019-11-12 22:01:22,128 train 150 1.274027e-02 0.257822
2019-11-12 22:01:27,267 train 200 1.264553e-02 0.223296
2019-11-12 22:01:32,411 train 250 1.264056e-02 0.240011
2019-11-12 22:01:37,557 train 300 1.261461e-02 0.246617
2019-11-12 22:01:42,688 train 350 1.264584e-02 0.239985
2019-11-12 22:01:47,800 train 400 1.261501e-02 0.244691
2019-11-12 22:01:52,926 train 450 1.264362e-02 0.248671
2019-11-12 22:01:58,036 train 500 1.262987e-02 0.242053
2019-11-12 22:02:03,164 train 550 1.261636e-02 0.244774
2019-11-12 22:02:08,284 train 600 1.257799e-02 0.246478
2019-11-12 22:02:13,435 train 650 1.255184e-02 0.248682
2019-11-12 22:02:18,636 train 700 1.254022e-02 0.251351
2019-11-12 22:02:23,781 train 750 1.255864e-02 0.251216
2019-11-12 22:02:28,967 train 800 1.256335e-02 0.249718
2019-11-12 22:02:34,107 train 850 1.254639e-02 0.252775
2019-11-12 22:02:35,634 training loss; R2: 1.255469e-02 0.252993
2019-11-12 22:02:35,967 valid 000 1.324716e-02 0.387715
2019-11-12 22:02:37,655 valid 050 1.246701e-02 0.358159
2019-11-12 22:02:39,159 validation loss; R2: 1.246950e-02 0.351169
2019-11-12 22:02:39,169 epoch 12 lr 1.000000e-03
2019-11-12 22:02:39,501 train 000 1.220547e-02 0.311369
2019-11-12 22:02:44,515 train 050 1.246629e-02 0.277679
2019-11-12 22:02:49,687 train 100 1.221557e-02 0.273805
2019-11-12 22:02:55,012 train 150 1.233259e-02 0.267920
2019-11-12 22:03:00,186 train 200 1.232685e-02 0.195195
2019-11-12 22:03:05,518 train 250 1.230034e-02 0.207194
2019-11-12 22:03:10,801 train 300 1.229824e-02 0.222330
2019-11-12 22:03:15,921 train 350 1.230559e-02 0.223214
2019-11-12 22:03:21,042 train 400 1.230448e-02 0.232928
2019-11-12 22:03:26,132 train 450 1.229730e-02 0.236016
2019-11-12 22:03:31,233 train 500 1.232514e-02 0.234885
2019-11-12 22:03:36,329 train 550 1.229967e-02 0.236146
2019-11-12 22:03:41,448 train 600 1.227808e-02 0.239525
2019-11-12 22:03:46,560 train 650 1.228565e-02 0.244541
2019-11-12 22:03:51,689 train 700 1.229849e-02 0.246650
2019-11-12 22:03:56,812 train 750 1.229617e-02 0.246908
2019-11-12 22:04:01,912 train 800 1.232378e-02 0.240141
2019-11-12 22:04:07,011 train 850 1.232412e-02 0.243434
2019-11-12 22:04:08,528 training loss; R2: 1.231798e-02 0.244017
2019-11-12 22:04:08,853 valid 000 1.306944e-02 0.370531
2019-11-12 22:04:10,512 valid 050 1.160467e-02 0.376984
2019-11-12 22:04:12,030 validation loss; R2: 1.159740e-02 0.378182
2019-11-12 22:04:12,040 epoch 13 lr 1.000000e-03
2019-11-12 22:04:12,382 train 000 1.118132e-02 0.254058
2019-11-12 22:04:17,531 train 050 1.194525e-02 0.221437
2019-11-12 22:04:22,677 train 100 1.214378e-02 0.252962
2019-11-12 22:04:27,835 train 150 1.216223e-02 0.256344
2019-11-12 22:04:33,012 train 200 1.220120e-02 0.266128
2019-11-12 22:04:38,225 train 250 1.223893e-02 0.272836
2019-11-12 22:04:43,395 train 300 1.225929e-02 0.255451
2019-11-12 22:04:48,553 train 350 1.224577e-02 0.246948
2019-11-12 22:04:53,750 train 400 1.224821e-02 0.254694
2019-11-12 22:04:59,047 train 450 1.221014e-02 0.259684
2019-11-12 22:05:04,283 train 500 1.218717e-02 0.265069
2019-11-12 22:05:09,564 train 550 1.218531e-02 0.268555
2019-11-12 22:05:14,901 train 600 1.220088e-02 0.270605
2019-11-12 22:05:20,241 train 650 1.221655e-02 -1.890793
2019-11-12 22:05:25,365 train 700 1.219854e-02 -1.735651
2019-11-12 22:05:30,446 train 750 1.219559e-02 -1.601107
2019-11-12 22:05:35,547 train 800 1.218377e-02 -1.484059
2019-11-12 22:05:40,896 train 850 1.216787e-02 -1.380021
2019-11-12 22:05:42,481 training loss; R2: 1.216559e-02 -1.350624
2019-11-12 22:05:42,790 valid 000 1.357010e-02 0.339877
2019-11-12 22:05:44,471 valid 050 1.134414e-02 0.322281
2019-11-12 22:05:45,992 validation loss; R2: 1.124877e-02 0.336220
2019-11-12 22:05:46,010 epoch 14 lr 1.000000e-03
2019-11-12 22:05:46,382 train 000 1.070949e-02 0.305897
2019-11-12 22:05:51,505 train 050 1.200467e-02 0.282462
2019-11-12 22:05:56,644 train 100 1.204592e-02 0.267531
2019-11-12 22:06:01,747 train 150 1.210275e-02 0.267693
2019-11-12 22:06:06,848 train 200 1.208408e-02 0.271989
2019-11-12 22:06:11,963 train 250 1.208449e-02 0.261833
2019-11-12 22:06:17,110 train 300 1.207703e-02 0.269216
2019-11-12 22:06:22,234 train 350 1.204502e-02 0.271881
2019-11-12 22:06:27,359 train 400 1.205909e-02 0.272766
2019-11-12 22:06:32,474 train 450 1.210052e-02 0.272136
2019-11-12 22:06:37,632 train 500 1.209332e-02 0.276119
2019-11-12 22:06:42,782 train 550 1.206289e-02 0.275788
2019-11-12 22:06:47,951 train 600 1.204471e-02 0.273621
2019-11-12 22:06:53,182 train 650 1.201058e-02 0.274859
2019-11-12 22:06:58,294 train 700 1.200386e-02 0.274362
2019-11-12 22:07:03,403 train 750 1.201306e-02 0.275410
2019-11-12 22:07:08,570 train 800 1.201292e-02 0.277841
2019-11-12 22:07:13,758 train 850 1.200311e-02 0.278609
2019-11-12 22:07:15,290 training loss; R2: 1.199996e-02 0.278851
2019-11-12 22:07:15,609 valid 000 9.638466e-03 0.397981
2019-11-12 22:07:17,289 valid 050 1.137483e-02 0.374808
2019-11-12 22:07:18,803 validation loss; R2: 1.133274e-02 0.384303
2019-11-12 22:07:18,813 epoch 15 lr 1.000000e-03
2019-11-12 22:07:19,210 train 000 1.481810e-02 0.219624
2019-11-12 22:07:24,103 train 050 1.201103e-02 0.306310
2019-11-12 22:07:29,023 train 100 1.180320e-02 0.316741
2019-11-12 22:07:34,166 train 150 1.172931e-02 0.305523
2019-11-12 22:07:39,352 train 200 1.178898e-02 0.299183
2019-11-12 22:07:44,590 train 250 1.179624e-02 0.296287
2019-11-12 22:07:49,779 train 300 1.180866e-02 0.291609
2019-11-12 22:07:54,988 train 350 1.187265e-02 0.272929
2019-11-12 22:08:00,237 train 400 1.187569e-02 0.278410
2019-11-12 22:08:05,558 train 450 1.187662e-02 0.272803
2019-11-12 22:08:10,885 train 500 1.189541e-02 0.275955
2019-11-12 22:08:16,216 train 550 1.186760e-02 0.278183
2019-11-12 22:08:21,562 train 600 1.188203e-02 0.278594
2019-11-12 22:08:26,895 train 650 1.187210e-02 0.275626
2019-11-12 22:08:32,238 train 700 1.187227e-02 0.277327
2019-11-12 22:08:37,561 train 750 1.188646e-02 0.278336
2019-11-12 22:08:42,809 train 800 1.189601e-02 0.279303
2019-11-12 22:08:47,929 train 850 1.190741e-02 0.279924
2019-11-12 22:08:49,455 training loss; R2: 1.190028e-02 0.280281
2019-11-12 22:08:49,791 valid 000 1.152190e-02 0.411073
2019-11-12 22:08:51,462 valid 050 1.171140e-02 0.372391
2019-11-12 22:08:52,960 validation loss; R2: 1.160267e-02 0.355930
2019-11-12 22:08:52,975 epoch 16 lr 1.000000e-03
2019-11-12 22:08:53,297 train 000 1.199377e-02 0.299770
2019-11-12 22:08:58,535 train 050 1.205677e-02 0.297390
2019-11-12 22:09:03,686 train 100 1.189714e-02 0.305306
2019-11-12 22:09:08,848 train 150 1.188375e-02 0.295255
2019-11-12 22:09:13,980 train 200 1.190509e-02 0.293451
2019-11-12 22:09:19,131 train 250 1.183735e-02 0.296489
2019-11-12 22:09:24,277 train 300 1.184846e-02 0.293990
2019-11-12 22:09:29,406 train 350 1.183758e-02 0.295803
2019-11-12 22:09:34,542 train 400 1.187283e-02 0.293831
2019-11-12 22:09:39,642 train 450 1.189391e-02 0.288581
2019-11-12 22:09:44,808 train 500 1.190598e-02 0.289171
2019-11-12 22:09:49,931 train 550 1.191501e-02 0.285647
2019-11-12 22:09:55,040 train 600 1.191058e-02 0.287790
2019-11-12 22:10:00,185 train 650 1.189113e-02 0.290645
2019-11-12 22:10:05,327 train 700 1.188302e-02 0.292256
2019-11-12 22:10:10,443 train 750 1.184650e-02 0.209054
2019-11-12 22:10:15,553 train 800 1.182950e-02 0.215240
2019-11-12 22:10:20,654 train 850 1.182875e-02 0.218055
2019-11-12 22:10:22,174 training loss; R2: 1.182819e-02 0.178912
2019-11-12 22:10:22,504 valid 000 1.170554e-02 0.417812
2019-11-12 22:10:24,169 valid 050 1.098075e-02 0.384278
2019-11-12 22:10:25,660 validation loss; R2: 1.091179e-02 0.265122
2019-11-12 22:10:25,671 epoch 17 lr 1.000000e-03
2019-11-12 22:10:26,041 train 000 1.271215e-02 0.322190
2019-11-12 22:10:30,940 train 050 1.161631e-02 0.254279
2019-11-12 22:10:35,979 train 100 1.168405e-02 0.276873
2019-11-12 22:10:41,158 train 150 1.171650e-02 0.226710
2019-11-12 22:10:46,309 train 200 1.162339e-02 0.242235
2019-11-12 22:10:51,494 train 250 1.165720e-02 0.261352
2019-11-12 22:10:56,819 train 300 1.168204e-02 0.263892
2019-11-12 22:11:01,981 train 350 1.164637e-02 0.244494
2019-11-12 22:11:07,252 train 400 1.165723e-02 0.238464
2019-11-12 22:11:12,476 train 450 1.166517e-02 0.229461
2019-11-12 22:11:17,829 train 500 1.167521e-02 0.232833
2019-11-12 22:11:23,145 train 550 1.166087e-02 0.238772
2019-11-12 22:11:28,461 train 600 1.167273e-02 0.242673
2019-11-12 22:11:33,767 train 650 1.166546e-02 0.246634
2019-11-12 22:11:39,085 train 700 1.165795e-02 0.249112
2019-11-12 22:11:44,396 train 750 1.167606e-02 0.250458
2019-11-12 22:11:49,635 train 800 1.168016e-02 0.246335
2019-11-12 22:11:54,762 train 850 1.167962e-02 0.247296
2019-11-12 22:11:56,282 training loss; R2: 1.167842e-02 0.246090
2019-11-12 22:11:56,614 valid 000 1.076799e-02 0.471135
2019-11-12 22:11:58,264 valid 050 1.101431e-02 0.397433
2019-11-12 22:11:59,817 validation loss; R2: 1.102101e-02 0.394352
2019-11-12 22:11:59,827 epoch 18 lr 1.000000e-03
2019-11-12 22:12:00,172 train 000 1.161728e-02 0.344663
2019-11-12 22:12:05,158 train 050 1.155789e-02 0.313472
2019-11-12 22:12:10,293 train 100 1.152250e-02 0.250901
2019-11-12 22:12:15,425 train 150 1.144295e-02 0.281788
2019-11-12 22:12:20,526 train 200 1.145576e-02 0.291949
2019-11-12 22:12:25,601 train 250 1.148496e-02 0.295833
2019-11-12 22:12:30,699 train 300 1.149345e-02 0.296209
2019-11-12 22:12:35,768 train 350 1.150010e-02 0.302328
2019-11-12 22:12:40,883 train 400 1.153248e-02 0.300923
2019-11-12 22:12:45,974 train 450 1.152074e-02 0.299553
2019-11-12 22:12:51,069 train 500 1.152098e-02 0.303393
2019-11-12 22:12:56,176 train 550 1.154813e-02 0.301792
2019-11-12 22:13:01,279 train 600 1.155405e-02 0.300904
2019-11-12 22:13:06,373 train 650 1.156832e-02 0.267773
2019-11-12 22:13:11,471 train 700 1.156093e-02 0.268357
2019-11-12 22:13:16,568 train 750 1.155849e-02 0.272672
2019-11-12 22:13:21,658 train 800 1.157296e-02 0.271856
2019-11-12 22:13:26,740 train 850 1.156372e-02 0.273089
2019-11-12 22:13:28,257 training loss; R2: 1.156549e-02 0.273619
2019-11-12 22:13:28,571 valid 000 1.354474e-02 0.373197
2019-11-12 22:13:30,235 valid 050 1.205415e-02 0.366235
2019-11-12 22:13:31,745 validation loss; R2: 1.196826e-02 0.370963
2019-11-12 22:13:31,754 epoch 19 lr 1.000000e-03
2019-11-12 22:13:32,105 train 000 1.138371e-02 0.383094
2019-11-12 22:13:37,338 train 050 1.140584e-02 0.053175
2019-11-12 22:13:42,664 train 100 1.150823e-02 0.164210
2019-11-12 22:13:47,983 train 150 1.153021e-02 0.211905
2019-11-12 22:13:53,148 train 200 1.154090e-02 0.225621
2019-11-12 22:13:58,251 train 250 1.150538e-02 0.243030
2019-11-12 22:14:03,378 train 300 1.153297e-02 0.254416
2019-11-12 22:14:08,705 train 350 1.153570e-02 0.262028
2019-11-12 22:14:14,026 train 400 1.153370e-02 0.268346
2019-11-12 22:14:19,375 train 450 1.151818e-02 0.264541
2019-11-12 22:14:24,654 train 500 1.152495e-02 0.265547
2019-11-12 22:14:29,773 train 550 1.152751e-02 0.270798
2019-11-12 22:14:34,887 train 600 1.152195e-02 0.272616
2019-11-12 22:14:39,983 train 650 1.152135e-02 0.270402
2019-11-12 22:14:45,078 train 700 1.151902e-02 0.274812
2019-11-12 22:14:50,174 train 750 1.152657e-02 0.275758
2019-11-12 22:14:55,268 train 800 1.151948e-02 0.275329
2019-11-12 22:15:00,357 train 850 1.150215e-02 0.277701
2019-11-12 22:15:01,877 training loss; R2: 1.150708e-02 0.278524
2019-11-12 22:15:02,192 valid 000 2.010763e-02 0.131984
2019-11-12 22:15:03,866 valid 050 1.650055e-02 0.115674
2019-11-12 22:15:05,389 validation loss; R2: 1.625646e-02 0.110805
