2019-12-03 19:48:15,593 gpu device = 1
2019-12-03 19:48:15,594 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=96, cutout=False, cutout_length=16, data='../data', dataset='graphene', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name=None, gpu=1, grad_clip=5, gz_dtree=False, init_channels=36, layers=8, learning_rate=0.002, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-graphene-EXP-20191203-194815', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-12-03 19:48:18,832 param size = 1.218781MB
2019-12-03 19:48:18,835 epoch 0 lr 2.000000e-03
2019-12-03 19:48:21,477 train 000 1.521540e+00 -65.187544
2019-12-03 19:48:35,249 train 050 2.723315e-01 -7.744051
2019-12-03 19:48:48,827 train 100 1.550787e-01 -3.971288
2019-12-03 19:49:02,403 train 150 1.116170e-01 -2.620938
2019-12-03 19:49:15,990 train 200 9.072758e-02 -1.940190
2019-12-03 19:49:23,166 training loss; R2: 8.414514e-02 -1.726650
2019-12-03 19:49:23,294 valid 000 2.041731e-02 0.156796
2019-12-03 19:49:25,011 validation loss; R2: 2.315625e-02 0.251841
2019-12-03 19:49:25,039 epoch 1 lr 2.000000e-03
2019-12-03 19:49:25,579 train 000 2.202680e-02 0.153994
2019-12-03 19:49:38,787 train 050 2.556254e-02 0.194992
2019-12-03 19:49:51,996 train 100 2.267954e-02 0.238303
2019-12-03 19:50:05,204 train 150 2.294992e-02 0.268112
2019-12-03 19:50:18,421 train 200 2.270625e-02 0.279722
2019-12-03 19:50:24,364 training loss; R2: 2.210698e-02 0.284638
2019-12-03 19:50:24,489 valid 000 6.167876e-03 0.752276
2019-12-03 19:50:25,881 validation loss; R2: 1.075546e-02 0.665274
2019-12-03 19:50:25,906 epoch 2 lr 2.000000e-03
2019-12-03 19:50:26,244 train 000 2.054622e-02 0.315794
2019-12-03 19:50:39,459 train 050 2.219671e-02 0.351818
2019-12-03 19:50:52,666 train 100 2.023773e-02 0.363546
2019-12-03 19:51:05,877 train 150 1.842829e-02 0.406204
2019-12-03 19:51:19,089 train 200 1.788481e-02 0.417402
2019-12-03 19:51:25,031 training loss; R2: 1.751507e-02 0.426315
2019-12-03 19:51:25,151 valid 000 1.252901e-02 0.538260
2019-12-03 19:51:26,544 validation loss; R2: 1.402373e-02 0.550396
2019-12-03 19:51:26,570 epoch 3 lr 2.000000e-03
2019-12-03 19:51:26,902 train 000 2.210866e-02 0.568349
2019-12-03 19:51:40,112 train 050 1.360442e-02 0.528607
2019-12-03 19:51:53,323 train 100 1.360120e-02 0.536528
2019-12-03 19:52:06,529 train 150 1.442293e-02 0.519874
2019-12-03 19:52:19,735 train 200 1.473541e-02 0.509874
2019-12-03 19:52:25,679 training loss; R2: 1.478823e-02 0.511587
2019-12-03 19:52:25,799 valid 000 6.329298e-03 0.788465
2019-12-03 19:52:27,193 validation loss; R2: 6.848083e-03 0.767858
2019-12-03 19:52:27,218 epoch 4 lr 2.000000e-03
2019-12-03 19:52:27,547 train 000 7.445472e-03 0.685298
2019-12-03 19:52:40,757 train 050 1.527525e-02 0.506027
2019-12-03 19:52:53,969 train 100 1.373654e-02 0.542905
2019-12-03 19:53:07,182 train 150 1.301175e-02 0.565245
2019-12-03 19:53:20,389 train 200 1.270194e-02 0.578077
2019-12-03 19:53:26,328 training loss; R2: 1.254010e-02 0.583563
2019-12-03 19:53:26,447 valid 000 3.075010e-03 0.856783
2019-12-03 19:53:27,842 validation loss; R2: 5.791805e-03 0.812396
2019-12-03 19:53:27,868 epoch 5 lr 2.000000e-03
2019-12-03 19:53:28,202 train 000 7.941872e-03 0.639982
2019-12-03 19:53:41,410 train 050 1.022167e-02 0.654371
2019-12-03 19:53:54,615 train 100 1.141630e-02 0.637177
2019-12-03 19:54:07,828 train 150 1.062957e-02 0.649620
2019-12-03 19:54:21,036 train 200 1.047860e-02 0.656916
2019-12-03 19:54:26,979 training loss; R2: 1.045448e-02 0.658645
2019-12-03 19:54:27,117 valid 000 3.812353e-03 0.874665
2019-12-03 19:54:28,512 validation loss; R2: 5.182791e-03 0.826806
2019-12-03 19:54:28,538 epoch 6 lr 2.000000e-03
2019-12-03 19:54:28,872 train 000 8.704842e-03 0.733816
2019-12-03 19:54:42,077 train 050 1.068945e-02 0.638174
2019-12-03 19:54:55,279 train 100 9.844468e-03 0.652580
2019-12-03 19:55:08,482 train 150 9.453019e-03 0.674335
2019-12-03 19:55:21,688 train 200 9.158320e-03 0.689217
2019-12-03 19:55:27,630 training loss; R2: 9.306775e-03 0.690952
2019-12-03 19:55:27,749 valid 000 4.966434e-03 0.870937
2019-12-03 19:55:29,144 validation loss; R2: 4.954834e-03 0.830950
2019-12-03 19:55:29,174 epoch 7 lr 2.000000e-03
2019-12-03 19:55:29,514 train 000 6.320568e-03 0.741526
2019-12-03 19:55:43,145 train 050 9.850749e-03 0.691579
2019-12-03 19:55:56,717 train 100 9.238919e-03 0.701880
2019-12-03 19:56:10,287 train 150 8.753369e-03 0.712984
2019-12-03 19:56:23,857 train 200 8.726481e-03 0.707815
2019-12-03 19:56:29,961 training loss; R2: 8.858667e-03 0.708081
2019-12-03 19:56:30,085 valid 000 4.783099e-03 0.872421
2019-12-03 19:56:31,482 validation loss; R2: 5.856422e-03 0.802289
2019-12-03 19:56:31,511 epoch 8 lr 2.000000e-03
2019-12-03 19:56:31,854 train 000 8.106232e-03 0.668297
2019-12-03 19:56:45,419 train 050 7.559297e-03 0.739605
2019-12-03 19:56:58,986 train 100 7.479554e-03 0.742108
2019-12-03 19:57:12,550 train 150 7.765582e-03 0.734920
2019-12-03 19:57:26,114 train 200 7.988740e-03 0.732885
2019-12-03 19:57:32,228 training loss; R2: 8.045440e-03 0.733928
2019-12-03 19:57:32,357 valid 000 5.916018e-03 0.763042
2019-12-03 19:57:33,753 validation loss; R2: 4.317459e-03 0.853724
2019-12-03 19:57:33,778 epoch 9 lr 2.000000e-03
2019-12-03 19:57:34,123 train 000 2.637203e-02 0.607291
2019-12-03 19:57:47,694 train 050 9.178797e-03 0.737227
2019-12-03 19:58:01,265 train 100 8.295752e-03 0.733826
2019-12-03 19:58:14,833 train 150 7.963965e-03 0.740644
2019-12-03 19:58:28,392 train 200 7.764036e-03 0.743375
2019-12-03 19:58:34,493 training loss; R2: 7.710222e-03 0.743847
2019-12-03 19:58:34,613 valid 000 2.813928e-03 0.919577
2019-12-03 19:58:36,008 validation loss; R2: 3.533923e-03 0.881979
2019-12-03 19:58:36,035 epoch 10 lr 2.000000e-03
2019-12-03 19:58:36,381 train 000 7.262997e-03 0.763368
2019-12-03 19:58:49,935 train 050 7.651153e-03 0.741627
2019-12-03 19:59:03,503 train 100 7.475184e-03 0.753276
2019-12-03 19:59:17,063 train 150 7.455383e-03 0.754360
2019-12-03 19:59:30,632 train 200 7.527611e-03 0.752830
2019-12-03 19:59:36,736 training loss; R2: 7.490893e-03 0.753367
2019-12-03 19:59:36,861 valid 000 2.768687e-03 0.902997
2019-12-03 19:59:38,257 validation loss; R2: 3.597208e-03 0.879771
2019-12-03 19:59:38,284 epoch 11 lr 2.000000e-03
2019-12-03 19:59:38,627 train 000 6.058769e-03 0.838438
2019-12-03 19:59:52,181 train 050 7.013475e-03 0.750470
2019-12-03 20:00:05,740 train 100 7.340292e-03 0.746076
2019-12-03 20:00:19,285 train 150 7.106837e-03 0.753087
2019-12-03 20:00:32,844 train 200 6.900514e-03 0.763574
2019-12-03 20:00:38,939 training loss; R2: 6.853559e-03 0.766922
2019-12-03 20:00:39,061 valid 000 2.391115e-03 0.925520
2019-12-03 20:00:40,456 validation loss; R2: 3.514247e-03 0.880665
2019-12-03 20:00:40,483 epoch 12 lr 2.000000e-03
2019-12-03 20:00:40,827 train 000 6.568631e-03 0.630524
2019-12-03 20:00:54,368 train 050 7.611172e-03 0.766383
2019-12-03 20:01:07,909 train 100 6.879925e-03 0.769419
2019-12-03 20:01:21,453 train 150 6.736839e-03 0.775497
2019-12-03 20:01:34,990 train 200 6.578545e-03 0.776516
2019-12-03 20:01:41,081 training loss; R2: 6.643348e-03 0.776432
2019-12-03 20:01:41,207 valid 000 2.603552e-03 0.942607
2019-12-03 20:01:42,603 validation loss; R2: 3.106210e-03 0.890308
2019-12-03 20:01:42,631 epoch 13 lr 2.000000e-03
2019-12-03 20:01:42,978 train 000 4.970836e-03 0.488979
2019-12-03 20:01:56,514 train 050 6.924708e-03 0.763836
2019-12-03 20:02:10,051 train 100 6.788751e-03 0.760774
2019-12-03 20:02:23,437 train 150 6.595269e-03 0.769218
2019-12-03 20:02:36,634 train 200 6.762524e-03 0.769139
2019-12-03 20:02:42,715 training loss; R2: 6.935053e-03 0.761311
2019-12-03 20:02:42,839 valid 000 3.748955e-03 0.856766
2019-12-03 20:02:44,234 validation loss; R2: 3.866274e-03 0.875557
2019-12-03 20:02:44,262 epoch 14 lr 2.000000e-03
2019-12-03 20:02:44,601 train 000 8.805054e-03 0.851549
2019-12-03 20:02:58,146 train 050 6.682795e-03 0.759745
2019-12-03 20:03:11,686 train 100 6.158245e-03 0.783372
2019-12-03 20:03:25,221 train 150 6.252098e-03 0.784552
2019-12-03 20:03:38,750 train 200 6.384661e-03 0.781509
2019-12-03 20:03:44,841 training loss; R2: 6.385411e-03 0.783474
2019-12-03 20:03:44,967 valid 000 5.686399e-03 0.874202
2019-12-03 20:03:46,363 validation loss; R2: 3.394164e-03 0.879815
2019-12-03 20:03:46,388 epoch 15 lr 2.000000e-03
2019-12-03 20:03:46,731 train 000 5.826624e-03 0.799443
2019-12-03 20:04:00,260 train 050 7.602725e-03 0.745199
2019-12-03 20:04:13,783 train 100 6.946294e-03 0.767284
2019-12-03 20:04:27,312 train 150 6.798034e-03 0.777737
2019-12-03 20:04:40,834 train 200 6.630218e-03 0.783073
2019-12-03 20:04:46,919 training loss; R2: 6.503510e-03 0.785017
2019-12-03 20:04:47,052 valid 000 3.395426e-03 0.941886
2019-12-03 20:04:48,447 validation loss; R2: 2.997871e-03 0.895814
2019-12-03 20:04:48,474 epoch 16 lr 2.000000e-03
2019-12-03 20:04:48,815 train 000 4.595133e-03 0.879660
2019-12-03 20:05:02,322 train 050 6.005605e-03 0.798310
2019-12-03 20:05:15,836 train 100 6.219742e-03 0.798138
2019-12-03 20:05:29,343 train 150 6.153649e-03 0.793416
2019-12-03 20:05:42,852 train 200 6.108530e-03 0.794756
2019-12-03 20:05:48,933 training loss; R2: 6.045611e-03 0.798248
2019-12-03 20:05:49,064 valid 000 3.460126e-03 0.852663
2019-12-03 20:05:50,458 validation loss; R2: 4.194133e-03 0.863537
2019-12-03 20:05:50,485 epoch 17 lr 2.000000e-03
2019-12-03 20:05:50,826 train 000 1.761583e-02 0.690754
2019-12-03 20:06:04,343 train 050 7.065328e-03 0.771218
2019-12-03 20:06:17,696 train 100 6.347763e-03 0.794375
2019-12-03 20:06:30,853 train 150 6.058672e-03 0.800040
2019-12-03 20:06:44,007 train 200 5.968941e-03 0.801405
2019-12-03 20:06:49,920 training loss; R2: 5.981185e-03 0.801583
2019-12-03 20:06:50,049 valid 000 4.704734e-02 -0.278287
2019-12-03 20:06:51,444 validation loss; R2: 5.065512e-02 -0.771587
2019-12-03 20:06:51,481 epoch 18 lr 2.000000e-03
2019-12-03 20:06:51,817 train 000 1.259218e-02 0.794750
2019-12-03 20:07:04,985 train 050 6.843237e-03 0.776921
2019-12-03 20:07:18,236 train 100 6.506878e-03 0.783702
2019-12-03 20:07:31,753 train 150 6.581614e-03 0.782486
2019-12-03 20:07:45,260 train 200 6.352705e-03 0.787762
2019-12-03 20:07:51,344 training loss; R2: 6.291058e-03 0.791629
2019-12-03 20:07:51,473 valid 000 1.394701e+00 -55.267083
2019-12-03 20:07:52,867 validation loss; R2: 1.381306e+00 -48.525323
2019-12-03 20:07:52,895 epoch 19 lr 2.000000e-03
2019-12-03 20:07:53,238 train 000 3.548023e-03 0.805850
2019-12-03 20:08:06,742 train 050 6.230838e-03 0.783040
2019-12-03 20:08:20,254 train 100 6.144855e-03 0.791756
2019-12-03 20:08:33,765 train 150 6.102301e-03 0.792686
2019-12-03 20:08:47,272 train 200 6.151962e-03 0.793217
2019-12-03 20:08:53,345 training loss; R2: 6.124006e-03 0.792947
2019-12-03 20:08:53,468 valid 000 2.793781e-01 -4.738446
2019-12-03 20:08:54,861 validation loss; R2: 2.896171e-01 -9.427471
