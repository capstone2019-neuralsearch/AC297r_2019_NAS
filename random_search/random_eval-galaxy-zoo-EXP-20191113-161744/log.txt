2019-11-13 16:17:45,063 gpu device = 1
2019-11-13 16:17:45,064 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name='galaxy-zoo-cleaned', gpu=1, grad_clip=5, gz_dtree=False, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-galaxy-zoo-EXP-20191113-161744', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-11-13 16:17:56,634 param size = 0.211509MB
2019-11-13 16:17:56,637 epoch 0 lr 1.000000e-03
2019-11-13 16:17:58,796 train 000 7.864859e-01 -450.513902
2019-11-13 16:18:04,741 train 050 6.329299e-02 -19.729446
2019-11-13 16:18:10,536 train 100 4.629381e-02 -10.645734
2019-11-13 16:18:16,378 train 150 4.006876e-02 -7.411738
2019-11-13 16:18:22,164 train 200 3.659179e-02 -5.813723
2019-11-13 16:18:27,947 train 250 3.440645e-02 -4.737889
2019-11-13 16:18:33,722 train 300 3.280019e-02 -4.143261
2019-11-13 16:18:39,501 train 350 3.157472e-02 -3.579068
2019-11-13 16:18:45,280 train 400 3.055733e-02 -3.183525
2019-11-13 16:18:51,057 train 450 2.979382e-02 -2.868859
2019-11-13 16:18:56,828 train 500 2.914235e-02 -2.601210
2019-11-13 16:19:02,623 train 550 2.857725e-02 -2.381437
2019-11-13 16:19:08,424 train 600 2.810246e-02 -2.191359
2019-11-13 16:19:14,223 train 650 2.766924e-02 -2.030097
2019-11-13 16:19:20,002 train 700 2.724843e-02 -1.886815
2019-11-13 16:19:25,810 train 750 2.686855e-02 -1.764083
2019-11-13 16:19:31,594 train 800 2.649912e-02 -1.655712
2019-11-13 16:19:37,384 train 850 2.617280e-02 -1.565021
2019-11-13 16:19:39,801 training loss; R2: 2.607333e-02 -1.538598
2019-11-13 16:19:40,138 valid 000 1.791120e-02 0.161429
2019-11-13 16:19:41,845 valid 050 1.932277e-02 -0.354413
2019-11-13 16:19:43,505 validation loss; R2: 1.914232e-02 -0.209898
2019-11-13 16:19:43,522 epoch 1 lr 1.000000e-03
2019-11-13 16:19:44,106 train 000 1.919362e-02 -0.026967
2019-11-13 16:19:49,783 train 050 2.037613e-02 -0.040055
2019-11-13 16:19:55,569 train 100 2.030234e-02 -0.028658
2019-11-13 16:20:01,347 train 150 2.023631e-02 -0.018960
2019-11-13 16:20:07,134 train 200 2.017048e-02 -0.017070
2019-11-13 16:20:12,914 train 250 2.010160e-02 -0.019627
2019-11-13 16:20:18,691 train 300 2.000136e-02 -0.014812
2019-11-13 16:20:24,473 train 350 1.995667e-02 -0.013777
2019-11-13 16:20:30,262 train 400 1.989545e-02 -0.007532
2019-11-13 16:20:36,057 train 450 1.979238e-02 -0.002952
2019-11-13 16:20:41,835 train 500 1.966761e-02 0.003367
2019-11-13 16:20:47,648 train 550 1.959592e-02 0.002499
2019-11-13 16:20:53,468 train 600 1.951760e-02 -0.008323
2019-11-13 16:20:59,254 train 650 1.943787e-02 -0.008713
2019-11-13 16:21:05,048 train 700 1.934745e-02 -0.006756
2019-11-13 16:21:10,828 train 750 1.927700e-02 -0.006979
2019-11-13 16:21:16,609 train 800 1.923974e-02 -0.003474
2019-11-13 16:21:22,390 train 850 1.919173e-02 0.000664
2019-11-13 16:21:24,114 training loss; R2: 1.917415e-02 0.000922
2019-11-13 16:21:24,442 valid 000 1.626865e-02 0.183649
2019-11-13 16:21:26,153 valid 050 1.646249e-02 0.170356
2019-11-13 16:21:27,723 validation loss; R2: 1.643029e-02 0.183620
2019-11-13 16:21:27,741 epoch 2 lr 1.000000e-03
2019-11-13 16:21:28,103 train 000 1.857832e-02 0.147052
2019-11-13 16:21:33,856 train 050 1.802859e-02 0.119402
2019-11-13 16:21:39,747 train 100 1.805315e-02 0.114058
2019-11-13 16:21:45,608 train 150 1.786622e-02 0.122370
2019-11-13 16:21:51,498 train 200 1.774644e-02 0.124488
2019-11-13 16:21:57,496 train 250 1.771220e-02 0.121989
2019-11-13 16:22:03,482 train 300 1.758887e-02 0.115620
2019-11-13 16:22:09,467 train 350 1.756265e-02 0.113711
2019-11-13 16:22:15,372 train 400 1.750923e-02 0.112497
2019-11-13 16:22:21,314 train 450 1.744705e-02 0.113982
2019-11-13 16:22:27,197 train 500 1.743819e-02 0.113579
2019-11-13 16:22:33,124 train 550 1.739064e-02 0.115108
2019-11-13 16:22:39,049 train 600 1.734332e-02 0.114864
2019-11-13 16:22:45,080 train 650 1.731116e-02 0.114826
2019-11-13 16:22:50,861 train 700 1.724835e-02 0.113349
2019-11-13 16:22:56,665 train 750 1.719410e-02 0.114451
2019-11-13 16:23:02,507 train 800 1.715278e-02 0.115932
2019-11-13 16:23:08,325 train 850 1.712060e-02 0.115989
2019-11-13 16:23:10,055 training loss; R2: 1.710175e-02 0.116769
2019-11-13 16:23:10,412 valid 000 1.341442e-02 0.344292
2019-11-13 16:23:12,147 valid 050 1.479909e-02 0.241937
2019-11-13 16:23:13,692 validation loss; R2: 1.479721e-02 0.251328
2019-11-13 16:23:13,704 epoch 3 lr 1.000000e-03
2019-11-13 16:23:14,126 train 000 1.831894e-02 -0.005406
2019-11-13 16:23:19,827 train 050 1.604807e-02 0.103275
2019-11-13 16:23:25,702 train 100 1.629609e-02 0.041303
2019-11-13 16:23:31,566 train 150 1.624279e-02 0.073793
2019-11-13 16:23:37,359 train 200 1.614923e-02 0.104303
2019-11-13 16:23:43,256 train 250 1.609112e-02 0.055256
2019-11-13 16:23:49,141 train 300 1.608179e-02 0.074603
2019-11-13 16:23:55,023 train 350 1.604977e-02 0.090489
2019-11-13 16:24:00,933 train 400 1.600645e-02 0.101983
2019-11-13 16:24:06,792 train 450 1.593895e-02 0.086617
2019-11-13 16:24:12,812 train 500 1.591052e-02 0.095096
2019-11-13 16:24:18,719 train 550 1.587523e-02 0.098546
2019-11-13 16:24:24,639 train 600 1.585209e-02 0.100221
2019-11-13 16:24:30,628 train 650 1.582382e-02 0.103312
2019-11-13 16:24:36,540 train 700 1.579157e-02 0.096486
2019-11-13 16:24:42,481 train 750 1.574140e-02 0.103591
2019-11-13 16:24:48,544 train 800 1.570600e-02 0.108385
2019-11-13 16:24:54,351 train 850 1.565794e-02 0.113235
2019-11-13 16:24:56,070 training loss; R2: 1.563609e-02 0.113246
2019-11-13 16:24:56,409 valid 000 1.157300e-02 0.368821
2019-11-13 16:24:58,075 valid 050 1.388855e-02 0.289581
2019-11-13 16:24:59,598 validation loss; R2: 1.397203e-02 0.287806
2019-11-13 16:24:59,615 epoch 4 lr 1.000000e-03
2019-11-13 16:24:59,991 train 000 1.393588e-02 0.243508
2019-11-13 16:25:05,692 train 050 1.459561e-02 0.215491
2019-11-13 16:25:11,486 train 100 1.484751e-02 0.130166
2019-11-13 16:25:17,247 train 150 1.494238e-02 0.150466
2019-11-13 16:25:22,981 train 200 1.496888e-02 0.162724
2019-11-13 16:25:28,720 train 250 1.498469e-02 0.172080
2019-11-13 16:25:34,458 train 300 1.498613e-02 0.173895
2019-11-13 16:25:40,209 train 350 1.492122e-02 0.176849
2019-11-13 16:25:45,943 train 400 1.484052e-02 0.180184
2019-11-13 16:25:51,689 train 450 1.483893e-02 0.182749
2019-11-13 16:25:57,422 train 500 1.478409e-02 0.187270
2019-11-13 16:26:03,161 train 550 1.478018e-02 0.189353
2019-11-13 16:26:08,883 train 600 1.473438e-02 0.191250
2019-11-13 16:26:14,608 train 650 1.469512e-02 0.192438
2019-11-13 16:26:20,328 train 700 1.467720e-02 0.195867
2019-11-13 16:26:26,055 train 750 1.464942e-02 0.195800
2019-11-13 16:26:31,769 train 800 1.464000e-02 0.194283
2019-11-13 16:26:37,486 train 850 1.459317e-02 0.196018
2019-11-13 16:26:39,199 training loss; R2: 1.458431e-02 0.196670
2019-11-13 16:26:39,536 valid 000 1.336746e-02 0.248418
2019-11-13 16:26:41,197 valid 050 1.281912e-02 0.279419
2019-11-13 16:26:42,726 validation loss; R2: 1.269334e-02 0.287704
2019-11-13 16:26:42,738 epoch 5 lr 1.000000e-03
2019-11-13 16:26:43,130 train 000 1.271283e-02 0.281219
2019-11-13 16:26:49,008 train 050 1.413251e-02 0.179374
2019-11-13 16:26:54,956 train 100 1.410413e-02 0.182506
2019-11-13 16:27:00,859 train 150 1.417582e-02 0.166201
2019-11-13 16:27:06,767 train 200 1.409027e-02 0.159174
2019-11-13 16:27:12,645 train 250 1.402937e-02 0.175581
2019-11-13 16:27:18,533 train 300 1.403397e-02 0.183536
2019-11-13 16:27:24,446 train 350 1.405353e-02 0.180825
2019-11-13 16:27:30,298 train 400 1.403628e-02 0.166797
2019-11-13 16:27:36,176 train 450 1.406834e-02 0.166457
2019-11-13 16:27:42,032 train 500 1.403023e-02 0.173275
2019-11-13 16:27:47,926 train 550 1.399323e-02 0.177209
2019-11-13 16:27:53,860 train 600 1.393646e-02 0.183521
2019-11-13 16:27:59,857 train 650 1.389337e-02 0.188466
2019-11-13 16:28:05,814 train 700 1.388295e-02 0.191846
2019-11-13 16:28:11,736 train 750 1.390027e-02 0.194576
2019-11-13 16:28:17,612 train 800 1.388879e-02 0.191768
2019-11-13 16:28:23,557 train 850 1.387225e-02 0.193324
2019-11-13 16:28:25,350 training loss; R2: 1.386639e-02 0.193633
2019-11-13 16:28:25,694 valid 000 1.233493e-02 0.315381
2019-11-13 16:28:27,413 valid 050 1.175926e-02 0.317883
2019-11-13 16:28:28,974 validation loss; R2: 1.192299e-02 0.323822
2019-11-13 16:28:28,989 epoch 6 lr 1.000000e-03
2019-11-13 16:28:29,430 train 000 1.556856e-02 0.302541
2019-11-13 16:28:35,183 train 050 1.361585e-02 0.261657
2019-11-13 16:28:40,941 train 100 1.339810e-02 0.238197
2019-11-13 16:28:46,669 train 150 1.343730e-02 0.237005
2019-11-13 16:28:52,397 train 200 1.342710e-02 0.232237
2019-11-13 16:28:58,113 train 250 1.332923e-02 0.226672
2019-11-13 16:29:03,830 train 300 1.332652e-02 0.226534
2019-11-13 16:29:09,546 train 350 1.328662e-02 0.182210
2019-11-13 16:29:15,274 train 400 1.327537e-02 0.192129
2019-11-13 16:29:21,046 train 450 1.325050e-02 0.196955
2019-11-13 16:29:26,837 train 500 1.322238e-02 0.204127
2019-11-13 16:29:32,619 train 550 1.325477e-02 0.160982
2019-11-13 16:29:38,436 train 600 1.322405e-02 0.168894
2019-11-13 16:29:44,439 train 650 1.322192e-02 0.170481
2019-11-13 16:29:50,509 train 700 1.320679e-02 0.177646
2019-11-13 16:29:56,562 train 750 1.320175e-02 0.183200
2019-11-13 16:30:02,609 train 800 1.322442e-02 0.187664
2019-11-13 16:30:08,654 train 850 1.321807e-02 0.188532
2019-11-13 16:30:10,458 training loss; R2: 1.322040e-02 0.175648
2019-11-13 16:30:10,779 valid 000 1.052660e-02 0.399457
2019-11-13 16:30:12,434 valid 050 1.190123e-02 0.338221
2019-11-13 16:30:13,973 validation loss; R2: 1.178805e-02 0.328512
2019-11-13 16:30:13,990 epoch 7 lr 1.000000e-03
2019-11-13 16:30:14,384 train 000 1.389866e-02 0.272203
2019-11-13 16:30:20,162 train 050 1.350024e-02 0.209176
2019-11-13 16:30:26,034 train 100 1.319245e-02 0.244015
2019-11-13 16:30:31,865 train 150 1.318921e-02 0.240192
2019-11-13 16:30:37,653 train 200 1.306325e-02 0.237222
2019-11-13 16:30:43,441 train 250 1.309084e-02 0.241159
2019-11-13 16:30:49,218 train 300 1.302757e-02 0.146320
2019-11-13 16:30:55,007 train 350 1.301922e-02 0.157626
2019-11-13 16:31:00,789 train 400 1.307675e-02 0.161436
2019-11-13 16:31:06,559 train 450 1.303089e-02 0.173012
2019-11-13 16:31:12,340 train 500 1.301244e-02 0.181536
2019-11-13 16:31:18,113 train 550 1.299251e-02 0.179944
2019-11-13 16:31:23,890 train 600 1.297468e-02 0.181089
2019-11-13 16:31:29,667 train 650 1.296347e-02 0.188118
2019-11-13 16:31:35,454 train 700 1.293954e-02 0.194735
2019-11-13 16:31:41,231 train 750 1.290042e-02 0.192765
2019-11-13 16:31:47,017 train 800 1.289647e-02 0.031003
2019-11-13 16:31:52,802 train 850 1.287397e-02 0.043447
2019-11-13 16:31:54,526 training loss; R2: 1.286393e-02 0.047688
2019-11-13 16:31:54,872 valid 000 1.123010e-02 0.403877
2019-11-13 16:31:56,586 valid 050 1.128198e-02 0.166135
2019-11-13 16:31:58,114 validation loss; R2: 1.127164e-02 0.259043
2019-11-13 16:31:58,126 epoch 8 lr 1.000000e-03
2019-11-13 16:31:58,513 train 000 1.282739e-02 0.357337
2019-11-13 16:32:04,326 train 050 1.247493e-02 0.268923
2019-11-13 16:32:10,192 train 100 1.256891e-02 0.254548
2019-11-13 16:32:16,169 train 150 1.261002e-02 0.251121
2019-11-13 16:32:22,109 train 200 1.267231e-02 0.258686
2019-11-13 16:32:28,115 train 250 1.263833e-02 0.256340
2019-11-13 16:32:34,094 train 300 1.264959e-02 0.254788
2019-11-13 16:32:40,030 train 350 1.261596e-02 0.245399
2019-11-13 16:32:46,062 train 400 1.267383e-02 0.247210
2019-11-13 16:32:52,013 train 450 1.269252e-02 0.253252
2019-11-13 16:32:57,911 train 500 1.266627e-02 0.246129
2019-11-13 16:33:03,818 train 550 1.266725e-02 0.245639
2019-11-13 16:33:09,818 train 600 1.267705e-02 0.242869
2019-11-13 16:33:15,826 train 650 1.265270e-02 0.245931
2019-11-13 16:33:21,796 train 700 1.262603e-02 0.245745
2019-11-13 16:33:27,581 train 750 1.258721e-02 0.247851
2019-11-13 16:33:33,354 train 800 1.257716e-02 0.251208
2019-11-13 16:33:39,138 train 850 1.257131e-02 0.249865
2019-11-13 16:33:40,860 training loss; R2: 1.256642e-02 0.250338
2019-11-13 16:33:41,208 valid 000 1.321997e-02 -0.008203
2019-11-13 16:33:42,873 valid 050 1.106129e-02 0.317752
2019-11-13 16:33:44,410 validation loss; R2: 1.091970e-02 0.324317
2019-11-13 16:33:44,427 epoch 9 lr 1.000000e-03
2019-11-13 16:33:44,854 train 000 1.114031e-02 0.377949
2019-11-13 16:33:50,568 train 050 1.229060e-02 0.288924
2019-11-13 16:33:56,441 train 100 1.253765e-02 0.275186
2019-11-13 16:34:02,330 train 150 1.255214e-02 0.271460
2019-11-13 16:34:08,187 train 200 1.252453e-02 0.271961
2019-11-13 16:34:14,136 train 250 1.244148e-02 0.275851
2019-11-13 16:34:20,021 train 300 1.241492e-02 0.185980
2019-11-13 16:34:25,922 train 350 1.243738e-02 0.197572
2019-11-13 16:34:31,885 train 400 1.241578e-02 0.209886
2019-11-13 16:34:37,729 train 450 1.239231e-02 0.217192
2019-11-13 16:34:43,575 train 500 1.237937e-02 0.225125
2019-11-13 16:34:49,438 train 550 1.239589e-02 0.226268
2019-11-13 16:34:55,295 train 600 1.236957e-02 0.228780
2019-11-13 16:35:01,147 train 650 1.236728e-02 0.234067
2019-11-13 16:35:07,000 train 700 1.234284e-02 0.218652
2019-11-13 16:35:12,815 train 750 1.232278e-02 0.224695
2019-11-13 16:35:18,708 train 800 1.230386e-02 0.226942
2019-11-13 16:35:24,655 train 850 1.231301e-02 0.230048
2019-11-13 16:35:26,405 training loss; R2: 1.230797e-02 0.231109
2019-11-13 16:35:26,757 valid 000 1.109430e-02 0.303255
2019-11-13 16:35:28,442 valid 050 1.071117e-02 0.369883
2019-11-13 16:35:29,972 validation loss; R2: 1.080693e-02 0.355726
2019-11-13 16:35:29,992 epoch 10 lr 1.000000e-03
2019-11-13 16:35:30,360 train 000 1.137260e-02 0.348964
2019-11-13 16:35:36,152 train 050 1.231894e-02 0.280098
2019-11-13 16:35:41,952 train 100 1.206768e-02 0.281079
2019-11-13 16:35:47,770 train 150 1.204748e-02 0.276663
2019-11-13 16:35:53,555 train 200 1.199402e-02 0.285313
2019-11-13 16:35:59,332 train 250 1.207704e-02 0.286087
2019-11-13 16:36:05,110 train 300 1.206102e-02 0.284623
2019-11-13 16:36:10,886 train 350 1.208166e-02 0.284911
2019-11-13 16:36:16,667 train 400 1.209729e-02 0.276183
2019-11-13 16:36:22,442 train 450 1.211172e-02 0.258872
2019-11-13 16:36:28,217 train 500 1.211462e-02 0.263878
2019-11-13 16:36:33,986 train 550 1.211571e-02 0.264572
2019-11-13 16:36:39,761 train 600 1.212852e-02 0.265895
2019-11-13 16:36:45,556 train 650 1.212368e-02 0.263733
2019-11-13 16:36:51,336 train 700 1.215163e-02 0.261351
2019-11-13 16:36:57,105 train 750 1.214740e-02 0.258305
2019-11-13 16:37:02,881 train 800 1.215498e-02 0.257786
2019-11-13 16:37:08,662 train 850 1.215297e-02 0.259353
2019-11-13 16:37:10,386 training loss; R2: 1.214836e-02 0.258934
2019-11-13 16:37:10,702 valid 000 6.571054e-02 -3.902535
2019-11-13 16:37:12,387 valid 050 7.281433e-02 -3.472973
2019-11-13 16:37:13,911 validation loss; R2: 7.243159e-02 -3.597460
2019-11-13 16:37:13,923 epoch 11 lr 1.000000e-03
2019-11-13 16:37:14,323 train 000 1.062274e-02 0.350223
2019-11-13 16:37:20,066 train 050 1.209581e-02 0.235413
2019-11-13 16:37:25,890 train 100 1.198820e-02 0.263453
2019-11-13 16:37:31,727 train 150 1.200356e-02 0.262551
2019-11-13 16:37:37,527 train 200 1.201243e-02 0.264358
2019-11-13 16:37:43,345 train 250 1.202327e-02 0.262500
2019-11-13 16:37:49,159 train 300 1.199245e-02 0.265111
2019-11-13 16:37:55,011 train 350 1.200784e-02 0.252230
2019-11-13 16:38:00,826 train 400 1.195452e-02 0.239211
2019-11-13 16:38:06,652 train 450 1.192453e-02 0.244478
2019-11-13 16:38:12,471 train 500 1.191432e-02 -0.648803
2019-11-13 16:38:18,268 train 550 1.192064e-02 -0.562253
2019-11-13 16:38:24,072 train 600 1.189007e-02 -0.489667
2019-11-13 16:38:29,877 train 650 1.190975e-02 -0.428340
2019-11-13 16:38:35,726 train 700 1.189611e-02 -0.377487
2019-11-13 16:38:41,561 train 750 1.189080e-02 -0.333963
2019-11-13 16:38:47,394 train 800 1.190788e-02 -0.293631
2019-11-13 16:38:53,221 train 850 1.190106e-02 -0.261608
2019-11-13 16:38:54,952 training loss; R2: 1.190180e-02 -0.251691
2019-11-13 16:38:55,301 valid 000 1.762294e-01 -11.354097
2019-11-13 16:38:56,977 valid 050 1.772336e-01 -13.082972
2019-11-13 16:38:58,525 validation loss; R2: 1.774387e-01 -13.701877
2019-11-13 16:38:58,537 epoch 12 lr 1.000000e-03
2019-11-13 16:38:58,936 train 000 1.082631e-02 0.293229
2019-11-13 16:39:04,794 train 050 1.189942e-02 0.286949
2019-11-13 16:39:10,651 train 100 1.182419e-02 0.298067
2019-11-13 16:39:16,510 train 150 1.174627e-02 0.274171
2019-11-13 16:39:22,372 train 200 1.168196e-02 0.289836
2019-11-13 16:39:28,216 train 250 1.164296e-02 0.294722
2019-11-13 16:39:34,040 train 300 1.167465e-02 0.294188
2019-11-13 16:39:39,858 train 350 1.170135e-02 0.288730
2019-11-13 16:39:45,685 train 400 1.172434e-02 0.287187
2019-11-13 16:39:51,537 train 450 1.170887e-02 0.288201
2019-11-13 16:39:57,422 train 500 1.169548e-02 0.283066
2019-11-13 16:40:03,290 train 550 1.170476e-02 0.285335
2019-11-13 16:40:09,142 train 600 1.166986e-02 0.283180
2019-11-13 16:40:14,990 train 650 1.164280e-02 0.281743
2019-11-13 16:40:20,851 train 700 1.164556e-02 0.284397
2019-11-13 16:40:26,694 train 750 1.164026e-02 0.285999
2019-11-13 16:40:32,519 train 800 1.163923e-02 0.284100
2019-11-13 16:40:38,336 train 850 1.166607e-02 0.284023
2019-11-13 16:40:40,074 training loss; R2: 1.167183e-02 0.282893
2019-11-13 16:40:40,405 valid 000 7.721011e-02 -4.039583
2019-11-13 16:40:42,144 valid 050 7.904252e-02 -5.768022
2019-11-13 16:40:43,703 validation loss; R2: 8.016313e-02 -5.899974
2019-11-13 16:40:43,715 epoch 13 lr 1.000000e-03
2019-11-13 16:40:44,079 train 000 1.094047e-02 0.348554
2019-11-13 16:40:49,901 train 050 1.158144e-02 0.325985
2019-11-13 16:40:55,863 train 100 1.166193e-02 0.306483
2019-11-13 16:41:01,718 train 150 1.162002e-02 0.298434
2019-11-13 16:41:07,559 train 200 1.162113e-02 0.304659
2019-11-13 16:41:13,384 train 250 1.164252e-02 0.285312
2019-11-13 16:41:19,215 train 300 1.165604e-02 0.288983
2019-11-13 16:41:25,035 train 350 1.162021e-02 0.292509
2019-11-13 16:41:30,860 train 400 1.164150e-02 0.292461
2019-11-13 16:41:36,709 train 450 1.159693e-02 0.290279
2019-11-13 16:41:42,525 train 500 1.159213e-02 0.290980
2019-11-13 16:41:48,350 train 550 1.160631e-02 0.255355
2019-11-13 16:41:54,179 train 600 1.158552e-02 0.262843
2019-11-13 16:42:00,004 train 650 1.158133e-02 0.268861
2019-11-13 16:42:05,829 train 700 1.157904e-02 0.273572
2019-11-13 16:42:11,669 train 750 1.154543e-02 0.277261
2019-11-13 16:42:17,505 train 800 1.154013e-02 0.278437
2019-11-13 16:42:23,345 train 850 1.150292e-02 0.281798
2019-11-13 16:42:25,090 training loss; R2: 1.150146e-02 0.283299
2019-11-13 16:42:25,422 valid 000 6.443606e-02 -1.758441
2019-11-13 16:42:27,078 valid 050 5.602609e-02 -1.986412
2019-11-13 16:42:28,595 validation loss; R2: 5.599486e-02 -1.924339
2019-11-13 16:42:28,607 epoch 14 lr 1.000000e-03
2019-11-13 16:42:29,033 train 000 1.208579e-02 0.426916
2019-11-13 16:42:34,602 train 050 1.154264e-02 0.224952
2019-11-13 16:42:40,473 train 100 1.155589e-02 0.271665
2019-11-13 16:42:46,444 train 150 1.166467e-02 0.266830
2019-11-13 16:42:52,335 train 200 1.164207e-02 0.276573
2019-11-13 16:42:58,211 train 250 1.159388e-02 0.290279
2019-11-13 16:43:04,142 train 300 1.153603e-02 0.288116
2019-11-13 16:43:10,058 train 350 1.152535e-02 -0.050259
2019-11-13 16:43:15,983 train 400 1.157364e-02 -0.007864
2019-11-13 16:43:21,934 train 450 1.156524e-02 0.030080
2019-11-13 16:43:27,822 train 500 1.158511e-02 0.058196
2019-11-13 16:43:33,712 train 550 1.160549e-02 0.063315
2019-11-13 16:43:39,625 train 600 1.163046e-02 0.070525
2019-11-13 16:43:45,496 train 650 1.162811e-02 0.087787
2019-11-13 16:43:51,358 train 700 1.159764e-02 0.105419
2019-11-13 16:43:57,227 train 750 1.157191e-02 0.118827
2019-11-13 16:44:03,069 train 800 1.154518e-02 0.133163
2019-11-13 16:44:08,916 train 850 1.150952e-02 0.129525
2019-11-13 16:44:10,653 training loss; R2: 1.151565e-02 0.131845
2019-11-13 16:44:10,993 valid 000 5.053905e-02 -1.855068
2019-11-13 16:44:12,704 valid 050 4.612484e-02 -2.817716
2019-11-13 16:44:14,218 validation loss; R2: 4.632962e-02 -2.754171
2019-11-13 16:44:14,230 epoch 15 lr 1.000000e-03
2019-11-13 16:44:14,651 train 000 1.001631e-02 0.270666
2019-11-13 16:44:20,530 train 050 1.224710e-02 0.258389
2019-11-13 16:44:26,442 train 100 1.207203e-02 0.282614
2019-11-13 16:44:32,493 train 150 1.190403e-02 0.282212
2019-11-13 16:44:38,257 train 200 1.192191e-02 0.282448
2019-11-13 16:44:44,028 train 250 1.196911e-02 0.277402
2019-11-13 16:44:49,802 train 300 1.193317e-02 -0.629810
2019-11-13 16:44:55,569 train 350 1.186232e-02 -0.501103
2019-11-13 16:45:01,321 train 400 1.178262e-02 -0.398566
2019-11-13 16:45:07,073 train 450 1.173684e-02 -0.576219
2019-11-13 16:45:12,824 train 500 1.174428e-02 -0.493828
2019-11-13 16:45:18,565 train 550 1.176241e-02 -0.424266
2019-11-13 16:45:24,302 train 600 1.183372e-02 -0.368350
2019-11-13 16:45:30,054 train 650 1.183588e-02 -0.317575
2019-11-13 16:45:35,816 train 700 1.184554e-02 -0.274271
2019-11-13 16:45:41,592 train 750 1.183332e-02 -0.234894
2019-11-13 16:45:47,346 train 800 1.182292e-02 -0.203392
2019-11-13 16:45:53,088 train 850 1.182387e-02 -0.178047
2019-11-13 16:45:54,798 training loss; R2: 1.182858e-02 -0.169365
2019-11-13 16:45:55,115 valid 000 2.499150e+00 -117.166808
2019-11-13 16:45:56,798 valid 050 2.526966e+00 -131.427319
2019-11-13 16:45:58,293 validation loss; R2: 2.525926e+00 -132.991549
2019-11-13 16:45:58,310 epoch 16 lr 1.000000e-03
2019-11-13 16:45:58,749 train 000 1.174223e-02 0.352618
2019-11-13 16:46:04,675 train 050 1.216056e-02 0.221004
2019-11-13 16:46:10,589 train 100 1.199236e-02 0.226543
2019-11-13 16:46:16,481 train 150 1.194953e-02 0.237048
2019-11-13 16:46:22,420 train 200 1.193076e-02 0.227899
2019-11-13 16:46:28,392 train 250 1.194059e-02 0.225413
2019-11-13 16:46:34,307 train 300 1.192606e-02 0.236332
2019-11-13 16:46:40,204 train 350 1.193526e-02 0.244478
2019-11-13 16:46:46,142 train 400 1.192118e-02 0.251023
2019-11-13 16:46:52,026 train 450 1.186387e-02 0.247961
2019-11-13 16:46:57,993 train 500 1.185831e-02 0.249918
2019-11-13 16:47:03,894 train 550 1.186626e-02 0.253190
2019-11-13 16:47:09,830 train 600 1.189699e-02 0.256016
2019-11-13 16:47:15,832 train 650 1.186284e-02 0.260913
2019-11-13 16:47:21,777 train 700 1.186451e-02 0.263687
2019-11-13 16:47:27,713 train 750 1.185520e-02 0.263632
2019-11-13 16:47:33,576 train 800 1.187464e-02 0.263300
2019-11-13 16:47:39,441 train 850 1.186825e-02 0.263118
2019-11-13 16:47:41,186 training loss; R2: 1.186667e-02 0.263809
2019-11-13 16:47:41,539 valid 000 7.196780e-02 -2.809158
2019-11-13 16:47:43,251 valid 050 6.884205e-02 -3.202756
2019-11-13 16:47:44,770 validation loss; R2: 6.903986e-02 -3.221372
2019-11-13 16:47:44,788 epoch 17 lr 1.000000e-03
2019-11-13 16:47:45,158 train 000 8.883193e-03 0.362139
2019-11-13 16:47:51,119 train 050 1.167620e-02 0.272710
2019-11-13 16:47:57,071 train 100 1.155413e-02 0.273773
2019-11-13 16:48:02,984 train 150 1.168088e-02 0.284578
2019-11-13 16:48:08,870 train 200 1.173929e-02 0.292206
2019-11-13 16:48:14,731 train 250 1.186514e-02 0.290129
2019-11-13 16:48:20,577 train 300 1.190974e-02 0.286524
2019-11-13 16:48:26,580 train 350 1.192822e-02 0.285714
2019-11-13 16:48:32,709 train 400 1.191158e-02 0.285586
2019-11-13 16:48:38,787 train 450 1.195938e-02 0.281850
2019-11-13 16:48:44,893 train 500 1.197465e-02 0.281724
2019-11-13 16:48:50,936 train 550 1.195754e-02 0.281892
2019-11-13 16:48:56,997 train 600 1.193382e-02 0.276909
2019-11-13 16:49:03,068 train 650 1.191306e-02 0.279348
2019-11-13 16:49:09,198 train 700 1.191837e-02 0.278810
2019-11-13 16:49:15,279 train 750 1.191377e-02 0.277091
2019-11-13 16:49:21,158 train 800 1.193079e-02 0.273731
2019-11-13 16:49:26,923 train 850 1.192423e-02 0.273796
2019-11-13 16:49:28,651 training loss; R2: 1.192329e-02 0.274695
2019-11-13 16:49:28,965 valid 000 1.339363e+00 -69.959067
2019-11-13 16:49:30,912 valid 050 1.339876e+00 -80.916680
2019-11-13 16:49:32,593 validation loss; R2: 1.339908e+00 -100.428035
2019-11-13 16:49:32,613 epoch 18 lr 1.000000e-03
2019-11-13 16:49:33,044 train 000 1.215950e-02 0.201992
2019-11-13 16:49:38,924 train 050 1.181094e-02 0.281505
2019-11-13 16:49:44,801 train 100 1.163623e-02 0.212901
2019-11-13 16:49:50,666 train 150 1.169365e-02 0.228913
2019-11-13 16:49:56,527 train 200 1.167191e-02 0.240291
2019-11-13 16:50:02,465 train 250 1.170811e-02 0.244876
2019-11-13 16:50:08,333 train 300 1.166811e-02 0.189162
2019-11-13 16:50:14,237 train 350 1.165212e-02 0.203580
2019-11-13 16:50:20,172 train 400 1.169453e-02 0.212874
2019-11-13 16:50:26,362 train 450 1.172571e-02 0.220786
2019-11-13 16:50:32,364 train 500 1.173290e-02 0.228419
2019-11-13 16:50:38,370 train 550 1.172893e-02 0.233941
2019-11-13 16:50:44,225 train 600 1.173950e-02 0.237211
2019-11-13 16:50:50,105 train 650 1.171618e-02 0.230158
2019-11-13 16:50:55,962 train 700 1.170980e-02 -2.226204
2019-11-13 16:51:01,809 train 750 1.171337e-02 -2.059897
2019-11-13 16:51:07,681 train 800 1.169076e-02 -1.911449
2019-11-13 16:51:13,513 train 850 1.169709e-02 -1.783877
2019-11-13 16:51:15,248 training loss; R2: 1.169245e-02 -1.747799
2019-11-13 16:51:15,556 valid 000 1.314230e-01 -8.021992
2019-11-13 16:51:17,587 valid 050 1.341993e-01 -6.369256
2019-11-13 16:51:19,379 validation loss; R2: 1.343721e-01 -5.920590
2019-11-13 16:51:19,394 epoch 19 lr 1.000000e-03
2019-11-13 16:51:19,842 train 000 1.099677e-02 0.354400
2019-11-13 16:51:25,798 train 050 1.164576e-02 0.307067
2019-11-13 16:51:31,667 train 100 1.147725e-02 0.315848
2019-11-13 16:51:37,508 train 150 1.156488e-02 0.309881
2019-11-13 16:51:43,314 train 200 1.157310e-02 0.311153
2019-11-13 16:51:49,164 train 250 1.155834e-02 0.303466
2019-11-13 16:51:55,005 train 300 1.150054e-02 0.299071
2019-11-13 16:52:00,870 train 350 1.147451e-02 0.303287
2019-11-13 16:52:06,827 train 400 1.144929e-02 0.304274
2019-11-13 16:52:12,639 train 450 1.148777e-02 0.298624
2019-11-13 16:52:18,525 train 500 1.149524e-02 0.299620
2019-11-13 16:52:24,588 train 550 1.148327e-02 0.301158
2019-11-13 16:52:30,435 train 600 1.149145e-02 0.301961
2019-11-13 16:52:36,252 train 650 1.148236e-02 0.259143
2019-11-13 16:52:42,104 train 700 1.146757e-02 0.263779
2019-11-13 16:52:47,956 train 750 1.145543e-02 0.267515
2019-11-13 16:52:53,801 train 800 1.145420e-02 0.269378
2019-11-13 16:52:59,773 train 850 1.144566e-02 0.268277
2019-11-13 16:53:01,512 training loss; R2: 1.144983e-02 0.267684
2019-11-13 16:53:01,815 valid 000 7.307885e-02 -3.616810
2019-11-13 16:53:03,547 valid 050 6.690928e-02 -3.004200
2019-11-13 16:53:05,081 validation loss; R2: 6.641481e-02 -2.950877
