2019-12-04 03:13:46,575 gpu device = 1
2019-12-04 03:13:46,575 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=96, cutout=False, cutout_length=16, data='../data', dataset='graphene', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name=None, gpu=1, grad_clip=5, gz_dtree=False, init_channels=36, layers=8, learning_rate=0.002, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-graphene-EXP-20191204-031346', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-12-04 03:13:49,812 param size = 1.254133MB
2019-12-04 03:13:49,815 epoch 0 lr 2.000000e-03
2019-12-04 03:13:52,556 train 000 8.203683e-01 -46.073204
2019-12-04 03:14:07,029 train 050 1.693182e-01 -4.413080
2019-12-04 03:14:21,277 train 100 1.026203e-01 -2.314666
2019-12-04 03:14:35,535 train 150 7.902592e-02 -1.546891
2019-12-04 03:14:49,789 train 200 6.634344e-02 -1.142759
2019-12-04 03:14:57,324 training loss; R2: 6.255162e-02 -1.016147
2019-12-04 03:14:57,475 valid 000 1.398222e-02 0.309681
2019-12-04 03:14:59,271 validation loss; R2: 2.268553e-02 0.266493
2019-12-04 03:14:59,294 epoch 1 lr 2.000000e-03
2019-12-04 03:14:59,884 train 000 1.495736e-02 0.073729
2019-12-04 03:15:14,160 train 050 2.899133e-02 0.000051
2019-12-04 03:15:28,413 train 100 2.705051e-02 0.104926
2019-12-04 03:15:42,662 train 150 2.589580e-02 0.162759
2019-12-04 03:15:56,915 train 200 2.397584e-02 0.216512
2019-12-04 03:16:03,213 training loss; R2: 2.345106e-02 0.233888
2019-12-04 03:16:03,354 valid 000 8.336407e-03 0.646285
2019-12-04 03:16:04,830 validation loss; R2: 1.248528e-02 0.617567
2019-12-04 03:16:04,853 epoch 2 lr 2.000000e-03
2019-12-04 03:16:05,216 train 000 1.653078e-02 0.135916
2019-12-04 03:16:19,112 train 050 1.944785e-02 0.355950
2019-12-04 03:16:33,010 train 100 1.736810e-02 0.433043
2019-12-04 03:16:46,910 train 150 1.715134e-02 0.443876
2019-12-04 03:17:00,812 train 200 1.615540e-02 0.465676
2019-12-04 03:17:07,062 training loss; R2: 1.582218e-02 0.477867
2019-12-04 03:17:07,212 valid 000 8.691628e-03 0.790394
2019-12-04 03:17:08,690 validation loss; R2: 8.675442e-03 0.718145
2019-12-04 03:17:08,713 epoch 3 lr 2.000000e-03
2019-12-04 03:17:09,075 train 000 7.629767e-03 0.343506
2019-12-04 03:17:22,989 train 050 1.260192e-02 0.590785
2019-12-04 03:17:36,902 train 100 1.240390e-02 0.578626
2019-12-04 03:17:50,815 train 150 1.195044e-02 0.594193
2019-12-04 03:18:04,727 train 200 1.201207e-02 0.600353
2019-12-04 03:18:10,985 training loss; R2: 1.189160e-02 0.605619
2019-12-04 03:18:11,134 valid 000 8.829352e-03 0.734490
2019-12-04 03:18:12,611 validation loss; R2: 6.244481e-03 0.795472
2019-12-04 03:18:12,634 epoch 4 lr 2.000000e-03
2019-12-04 03:18:12,993 train 000 5.877972e-03 0.713056
2019-12-04 03:18:26,909 train 050 1.077607e-02 0.593428
2019-12-04 03:18:40,822 train 100 1.103856e-02 0.612328
2019-12-04 03:18:54,734 train 150 1.075757e-02 0.622027
2019-12-04 03:19:08,653 train 200 1.049701e-02 0.641885
2019-12-04 03:19:14,911 training loss; R2: 1.029246e-02 0.646993
2019-12-04 03:19:15,056 valid 000 8.883557e-03 0.754756
2019-12-04 03:19:16,533 validation loss; R2: 9.353306e-03 0.676786
2019-12-04 03:19:16,558 epoch 5 lr 2.000000e-03
2019-12-04 03:19:16,922 train 000 1.552349e-02 0.753269
2019-12-04 03:19:30,833 train 050 1.003174e-02 0.682485
2019-12-04 03:19:44,744 train 100 1.035957e-02 0.661299
2019-12-04 03:19:58,653 train 150 1.002879e-02 0.670163
2019-12-04 03:20:12,559 train 200 9.656074e-03 0.672622
2019-12-04 03:20:18,810 training loss; R2: 9.776225e-03 0.669862
2019-12-04 03:20:18,948 valid 000 4.727443e-03 0.850314
2019-12-04 03:20:20,425 validation loss; R2: 4.446393e-03 0.849666
2019-12-04 03:20:20,451 epoch 6 lr 2.000000e-03
2019-12-04 03:20:20,815 train 000 8.445705e-03 0.771779
2019-12-04 03:20:34,711 train 050 8.627851e-03 0.715475
2019-12-04 03:20:48,610 train 100 8.344604e-03 0.717213
2019-12-04 03:21:02,511 train 150 8.467310e-03 0.708811
2019-12-04 03:21:16,416 train 200 8.573073e-03 0.713821
2019-12-04 03:21:22,666 training loss; R2: 8.614128e-03 0.710890
2019-12-04 03:21:22,812 valid 000 4.210593e-03 0.800903
2019-12-04 03:21:24,289 validation loss; R2: 6.950193e-03 0.774654
2019-12-04 03:21:24,315 epoch 7 lr 2.000000e-03
2019-12-04 03:21:24,676 train 000 1.076710e-02 0.546571
2019-12-04 03:21:38,590 train 050 1.010839e-02 0.676075
2019-12-04 03:21:52,497 train 100 9.276319e-03 0.693200
2019-12-04 03:22:06,407 train 150 9.015530e-03 0.703290
2019-12-04 03:22:20,315 train 200 8.723097e-03 0.711360
2019-12-04 03:22:26,568 training loss; R2: 8.628268e-03 0.708647
2019-12-04 03:22:26,708 valid 000 7.798368e-03 0.685425
2019-12-04 03:22:28,185 validation loss; R2: 8.145761e-03 0.720660
2019-12-04 03:22:28,210 epoch 8 lr 2.000000e-03
2019-12-04 03:22:28,571 train 000 8.651349e-03 0.759658
2019-12-04 03:22:42,480 train 050 7.432993e-03 0.754674
2019-12-04 03:22:56,386 train 100 7.899353e-03 0.731536
2019-12-04 03:23:10,287 train 150 7.589600e-03 0.735423
2019-12-04 03:23:24,186 train 200 7.536964e-03 0.734634
2019-12-04 03:23:30,436 training loss; R2: 7.508917e-03 0.740205
2019-12-04 03:23:30,577 valid 000 3.622948e-03 0.852749
2019-12-04 03:23:32,054 validation loss; R2: 4.114124e-03 0.861898
2019-12-04 03:23:32,080 epoch 9 lr 2.000000e-03
2019-12-04 03:23:32,442 train 000 7.316880e-03 0.561723
2019-12-04 03:23:46,341 train 050 7.169090e-03 0.730671
2019-12-04 03:24:00,241 train 100 7.647566e-03 0.735625
2019-12-04 03:24:14,141 train 150 7.613605e-03 0.743055
2019-12-04 03:24:28,035 train 200 7.377257e-03 0.749565
2019-12-04 03:24:34,287 training loss; R2: 7.287533e-03 0.752734
2019-12-04 03:24:34,428 valid 000 4.215530e-03 0.906682
2019-12-04 03:24:35,905 validation loss; R2: 3.907214e-03 0.873228
2019-12-04 03:24:35,929 epoch 10 lr 2.000000e-03
2019-12-04 03:24:36,291 train 000 6.489215e-03 0.885391
2019-12-04 03:24:50,186 train 050 7.440706e-03 0.743346
2019-12-04 03:25:04,076 train 100 7.355538e-03 0.741109
2019-12-04 03:25:17,963 train 150 7.469702e-03 0.748999
2019-12-04 03:25:31,852 train 200 7.206203e-03 0.757138
2019-12-04 03:25:38,098 training loss; R2: 7.064337e-03 0.759892
2019-12-04 03:25:38,245 valid 000 2.504573e-03 0.939715
2019-12-04 03:25:39,722 validation loss; R2: 3.382707e-03 0.880674
2019-12-04 03:25:39,753 epoch 11 lr 2.000000e-03
2019-12-04 03:25:40,121 train 000 6.649737e-03 0.740435
2019-12-04 03:25:54,014 train 050 6.846422e-03 0.777525
2019-12-04 03:26:07,905 train 100 6.985823e-03 0.766769
2019-12-04 03:26:21,794 train 150 6.902041e-03 0.769356
2019-12-04 03:26:35,688 train 200 6.841307e-03 0.766145
2019-12-04 03:26:41,932 training loss; R2: 6.954886e-03 0.763749
2019-12-04 03:26:42,071 valid 000 3.370202e-03 0.899841
2019-12-04 03:26:43,547 validation loss; R2: 4.951462e-03 0.826087
2019-12-04 03:26:43,572 epoch 12 lr 2.000000e-03
2019-12-04 03:26:43,934 train 000 9.596948e-03 0.817638
2019-12-04 03:26:57,827 train 050 7.405884e-03 0.762956
2019-12-04 03:27:11,717 train 100 7.161639e-03 0.772177
2019-12-04 03:27:25,600 train 150 6.777653e-03 0.781349
2019-12-04 03:27:39,486 train 200 6.784622e-03 0.779967
2019-12-04 03:27:45,731 training loss; R2: 6.676340e-03 0.781451
2019-12-04 03:27:45,877 valid 000 5.059733e-03 0.899522
2019-12-04 03:27:47,354 validation loss; R2: 3.176317e-03 0.893555
2019-12-04 03:27:47,379 epoch 13 lr 2.000000e-03
2019-12-04 03:27:47,739 train 000 4.888230e-03 0.754631
2019-12-04 03:28:01,623 train 050 5.812735e-03 0.801956
2019-12-04 03:28:15,515 train 100 6.150023e-03 0.786468
2019-12-04 03:28:29,402 train 150 6.475330e-03 0.783311
2019-12-04 03:28:43,285 train 200 6.665627e-03 0.779588
2019-12-04 03:28:49,526 training loss; R2: 6.712765e-03 0.778562
2019-12-04 03:28:49,671 valid 000 1.848109e-03 0.883588
2019-12-04 03:28:51,148 validation loss; R2: 4.369629e-03 0.850495
2019-12-04 03:28:51,173 epoch 14 lr 2.000000e-03
2019-12-04 03:28:51,535 train 000 4.183600e-03 0.835261
2019-12-04 03:29:05,418 train 050 6.671728e-03 0.791516
2019-12-04 03:29:19,301 train 100 6.104206e-03 0.785013
2019-12-04 03:29:33,190 train 150 6.387219e-03 0.774161
2019-12-04 03:29:47,074 train 200 6.679170e-03 0.774659
2019-12-04 03:29:53,316 training loss; R2: 6.822118e-03 0.770470
2019-12-04 03:29:53,467 valid 000 4.108905e-03 0.888200
2019-12-04 03:29:54,944 validation loss; R2: 2.971091e-03 0.889888
2019-12-04 03:29:54,969 epoch 15 lr 2.000000e-03
2019-12-04 03:29:55,330 train 000 5.912010e-03 0.848261
2019-12-04 03:30:09,208 train 050 6.130346e-03 0.804384
2019-12-04 03:30:23,087 train 100 6.315311e-03 0.789791
2019-12-04 03:30:36,968 train 150 6.080065e-03 0.794152
2019-12-04 03:30:50,848 train 200 6.145190e-03 0.792258
2019-12-04 03:30:57,090 training loss; R2: 6.227399e-03 0.791171
2019-12-04 03:30:57,236 valid 000 3.480662e-03 0.896715
2019-12-04 03:30:58,712 validation loss; R2: 4.498909e-03 0.844027
2019-12-04 03:30:58,736 epoch 16 lr 2.000000e-03
2019-12-04 03:30:59,096 train 000 4.406539e-03 0.838185
2019-12-04 03:31:12,976 train 050 6.603204e-03 0.749853
2019-12-04 03:31:26,856 train 100 6.125028e-03 0.781014
2019-12-04 03:31:40,736 train 150 6.373953e-03 0.780967
2019-12-04 03:31:54,612 train 200 6.108236e-03 0.788208
2019-12-04 03:32:00,850 training loss; R2: 6.083124e-03 0.790091
2019-12-04 03:32:00,991 valid 000 6.280432e-03 0.859824
2019-12-04 03:32:02,467 validation loss; R2: 4.380805e-03 0.853890
2019-12-04 03:32:02,491 epoch 17 lr 2.000000e-03
2019-12-04 03:32:02,855 train 000 3.118685e-03 0.828493
2019-12-04 03:32:16,734 train 050 6.895392e-03 0.788230
2019-12-04 03:32:30,604 train 100 6.780884e-03 0.779019
2019-12-04 03:32:44,476 train 150 6.514008e-03 0.787051
2019-12-04 03:32:58,349 train 200 6.431694e-03 0.789156
2019-12-04 03:33:04,588 training loss; R2: 6.388338e-03 0.786107
2019-12-04 03:33:04,728 valid 000 2.126797e-02 0.140053
2019-12-04 03:33:06,203 validation loss; R2: 2.218545e-02 0.213667
2019-12-04 03:33:06,228 epoch 18 lr 2.000000e-03
2019-12-04 03:33:06,599 train 000 4.871303e-03 0.815260
2019-12-04 03:33:20,452 train 050 6.751827e-03 0.788995
2019-12-04 03:33:34,311 train 100 6.680082e-03 0.790633
2019-12-04 03:33:48,163 train 150 6.035886e-03 0.800247
2019-12-04 03:34:02,012 train 200 6.112939e-03 0.797717
2019-12-04 03:34:08,239 training loss; R2: 6.010077e-03 0.800979
2019-12-04 03:34:08,379 valid 000 3.278295e-03 0.894885
2019-12-04 03:34:09,854 validation loss; R2: 3.997268e-03 0.870092
2019-12-04 03:34:09,886 epoch 19 lr 2.000000e-03
2019-12-04 03:34:10,253 train 000 3.693499e-03 0.867809
2019-12-04 03:34:24,095 train 050 5.505962e-03 0.815762
2019-12-04 03:34:37,940 train 100 6.027869e-03 0.798141
2019-12-04 03:34:51,787 train 150 5.911897e-03 0.799686
2019-12-04 03:35:05,627 train 200 5.955757e-03 0.797406
2019-12-04 03:35:11,852 training loss; R2: 6.112767e-03 0.794550
2019-12-04 03:35:11,992 valid 000 2.207504e-02 0.207943
2019-12-04 03:35:13,467 validation loss; R2: 2.445220e-02 0.177013
