2019-11-12 16:19:05,123 gpu device = 1
2019-11-12 16:19:05,123 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name='galaxy-zoo-cleaned', gpu=1, grad_clip=5, gz_dtree=False, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-galaxy-zoo-EXP-20191112-161904', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-11-12 16:19:16,711 param size = 0.244309MB
2019-11-12 16:19:16,715 epoch 0 lr 1.000000e-03
2019-11-12 16:19:18,828 train 000 4.313584e-01 -500.235625
2019-11-12 16:19:24,859 train 050 4.873255e-02 -16.180946
2019-11-12 16:19:31,150 train 100 3.798374e-02 -8.522812
2019-11-12 16:19:37,417 train 150 3.374940e-02 -5.837610
2019-11-12 16:19:43,523 train 200 3.137080e-02 -4.502846
2019-11-12 16:19:49,526 train 250 2.984199e-02 -3.738521
2019-11-12 16:19:55,534 train 300 2.870762e-02 -3.156742
2019-11-12 16:20:01,507 train 350 2.796304e-02 -3.199622
2019-11-12 16:20:07,507 train 400 2.728387e-02 -2.818659
2019-11-12 16:20:13,507 train 450 2.670842e-02 -2.518770
2019-11-12 16:20:19,504 train 500 2.621313e-02 -2.276348
2019-11-12 16:20:25,500 train 550 2.573272e-02 -2.076551
2019-11-12 16:20:31,498 train 600 2.535437e-02 -1.905756
2019-11-12 16:20:37,498 train 650 2.500491e-02 -1.761875
2019-11-12 16:20:43,500 train 700 2.468946e-02 -1.644117
2019-11-12 16:20:49,499 train 750 2.438875e-02 -1.537822
2019-11-12 16:20:55,497 train 800 2.412745e-02 -1.437654
2019-11-12 16:21:01,496 train 850 2.385645e-02 -1.352162
2019-11-12 16:21:03,995 training loss; R2: 2.379423e-02 -1.327608
2019-11-12 16:21:04,265 valid 000 2.161606e-02 -0.532575
2019-11-12 16:21:05,921 valid 050 1.808707e-02 0.092171
2019-11-12 16:21:07,528 validation loss; R2: 1.806322e-02 0.116930
2019-11-12 16:21:07,544 epoch 1 lr 1.000000e-03
2019-11-12 16:21:08,003 train 000 2.385000e-02 0.014432
2019-11-12 16:21:14,009 train 050 2.027118e-02 0.031837
2019-11-12 16:21:20,029 train 100 1.986878e-02 0.015535
2019-11-12 16:21:26,058 train 150 1.957050e-02 -0.005769
2019-11-12 16:21:32,117 train 200 1.951652e-02 -0.005194
2019-11-12 16:21:38,176 train 250 1.937986e-02 0.007162
2019-11-12 16:21:44,222 train 300 1.929909e-02 0.020814
2019-11-12 16:21:50,242 train 350 1.926419e-02 0.028857
2019-11-12 16:21:56,265 train 400 1.917808e-02 0.037318
2019-11-12 16:22:02,287 train 450 1.910622e-02 0.042026
2019-11-12 16:22:08,305 train 500 1.904833e-02 0.044405
2019-11-12 16:22:14,329 train 550 1.899553e-02 0.049534
2019-11-12 16:22:20,355 train 600 1.891495e-02 0.053567
2019-11-12 16:22:26,378 train 650 1.884639e-02 0.059839
2019-11-12 16:22:32,397 train 700 1.872759e-02 0.060315
2019-11-12 16:22:38,411 train 750 1.864276e-02 0.064499
2019-11-12 16:22:44,432 train 800 1.856913e-02 0.065199
2019-11-12 16:22:50,457 train 850 1.847575e-02 0.069421
2019-11-12 16:22:52,257 training loss; R2: 1.845193e-02 0.070460
2019-11-12 16:22:52,543 valid 000 1.811175e-02 0.240528
2019-11-12 16:22:54,207 valid 050 1.511584e-02 0.219227
2019-11-12 16:22:55,729 validation loss; R2: 1.509785e-02 0.215786
2019-11-12 16:22:55,743 epoch 2 lr 1.000000e-03
2019-11-12 16:22:56,084 train 000 1.701369e-02 0.187625
2019-11-12 16:23:02,271 train 050 1.670609e-02 -0.144321
2019-11-12 16:23:08,372 train 100 1.688020e-02 -0.001221
2019-11-12 16:23:14,451 train 150 1.697952e-02 0.018491
2019-11-12 16:23:20,527 train 200 1.681073e-02 0.041870
2019-11-12 16:23:26,585 train 250 1.677837e-02 0.056205
2019-11-12 16:23:32,642 train 300 1.677473e-02 0.064937
2019-11-12 16:23:38,698 train 350 1.669511e-02 0.079797
2019-11-12 16:23:44,717 train 400 1.669346e-02 0.088079
2019-11-12 16:23:50,727 train 450 1.659711e-02 0.096306
2019-11-12 16:23:56,727 train 500 1.655934e-02 0.098944
2019-11-12 16:24:02,731 train 550 1.653240e-02 0.104963
2019-11-12 16:24:08,735 train 600 1.649853e-02 0.101543
2019-11-12 16:24:14,751 train 650 1.647749e-02 0.104107
2019-11-12 16:24:20,762 train 700 1.644175e-02 0.107742
2019-11-12 16:24:26,770 train 750 1.637692e-02 0.111532
2019-11-12 16:24:32,780 train 800 1.635561e-02 0.114813
2019-11-12 16:24:38,789 train 850 1.630086e-02 0.119946
2019-11-12 16:24:40,589 training loss; R2: 1.628505e-02 0.120317
2019-11-12 16:24:40,868 valid 000 1.641729e-02 0.206230
2019-11-12 16:24:42,586 valid 050 1.378113e-02 0.117487
2019-11-12 16:24:44,112 validation loss; R2: 1.394069e-02 0.191179
2019-11-12 16:24:44,126 epoch 3 lr 1.000000e-03
2019-11-12 16:24:44,436 train 000 1.535714e-02 0.178629
2019-11-12 16:24:50,494 train 050 1.568003e-02 0.176633
2019-11-12 16:24:56,541 train 100 1.562314e-02 0.097274
2019-11-12 16:25:02,563 train 150 1.548513e-02 0.124782
2019-11-12 16:25:08,554 train 200 1.543518e-02 0.114281
2019-11-12 16:25:14,560 train 250 1.538458e-02 0.127653
2019-11-12 16:25:20,552 train 300 1.524391e-02 0.138194
2019-11-12 16:25:26,545 train 350 1.517853e-02 0.145288
2019-11-12 16:25:32,551 train 400 1.512840e-02 0.154005
2019-11-12 16:25:38,548 train 450 1.510907e-02 0.157522
2019-11-12 16:25:44,546 train 500 1.508587e-02 0.161182
2019-11-12 16:25:50,549 train 550 1.508580e-02 0.162580
2019-11-12 16:25:56,600 train 600 1.508099e-02 0.163498
2019-11-12 16:26:02,670 train 650 1.505156e-02 0.168758
2019-11-12 16:26:08,717 train 700 1.498383e-02 0.047577
2019-11-12 16:26:14,721 train 750 1.495108e-02 0.056573
2019-11-12 16:26:20,730 train 800 1.491371e-02 -0.451146
2019-11-12 16:26:26,733 train 850 1.487884e-02 -0.410148
2019-11-12 16:26:28,528 training loss; R2: 1.486076e-02 -0.398661
2019-11-12 16:26:28,813 valid 000 1.382805e-02 0.357834
2019-11-12 16:26:30,502 valid 050 1.261223e-02 0.294225
2019-11-12 16:26:32,015 validation loss; R2: 1.251698e-02 0.192215
2019-11-12 16:26:32,038 epoch 4 lr 1.000000e-03
2019-11-12 16:26:32,362 train 000 1.444565e-02 0.295044
2019-11-12 16:26:38,737 train 050 1.429436e-02 0.185991
2019-11-12 16:26:45,093 train 100 1.422889e-02 0.207486
2019-11-12 16:26:51,088 train 150 1.417556e-02 0.197683
2019-11-12 16:26:57,077 train 200 1.420684e-02 0.193901
2019-11-12 16:27:03,082 train 250 1.414644e-02 0.204205
2019-11-12 16:27:09,068 train 300 1.413601e-02 0.210773
2019-11-12 16:27:15,053 train 350 1.413422e-02 0.205419
2019-11-12 16:27:21,040 train 400 1.411925e-02 0.208290
2019-11-12 16:27:27,030 train 450 1.408354e-02 0.209326
2019-11-12 16:27:33,026 train 500 1.405051e-02 0.207929
2019-11-12 16:27:39,024 train 550 1.402866e-02 0.206682
2019-11-12 16:27:45,017 train 600 1.403111e-02 0.208079
2019-11-12 16:27:51,014 train 650 1.401769e-02 0.209578
2019-11-12 16:27:57,017 train 700 1.399385e-02 0.212420
2019-11-12 16:28:03,011 train 750 1.397484e-02 0.210528
2019-11-12 16:28:09,003 train 800 1.396954e-02 0.208613
2019-11-12 16:28:14,996 train 850 1.395279e-02 0.194330
2019-11-12 16:28:16,807 training loss; R2: 1.394195e-02 0.195639
2019-11-12 16:28:17,094 valid 000 1.295929e-02 0.317246
2019-11-12 16:28:18,790 valid 050 1.251540e-02 0.240333
2019-11-12 16:28:20,292 validation loss; R2: 1.241293e-02 0.263039
2019-11-12 16:28:20,312 epoch 5 lr 1.000000e-03
2019-11-12 16:28:20,660 train 000 1.424042e-02 0.274136
2019-11-12 16:28:26,426 train 050 1.353424e-02 0.135007
2019-11-12 16:28:32,182 train 100 1.358132e-02 0.179095
2019-11-12 16:28:37,940 train 150 1.356152e-02 0.205169
2019-11-12 16:28:43,703 train 200 1.355732e-02 0.208720
2019-11-12 16:28:49,473 train 250 1.347807e-02 0.213727
2019-11-12 16:28:55,234 train 300 1.343149e-02 0.211509
2019-11-12 16:29:01,210 train 350 1.345419e-02 0.215684
2019-11-12 16:29:07,275 train 400 1.348836e-02 0.221098
2019-11-12 16:29:13,346 train 450 1.348673e-02 0.226154
2019-11-12 16:29:19,416 train 500 1.349011e-02 0.219475
2019-11-12 16:29:25,447 train 550 1.347264e-02 0.221497
2019-11-12 16:29:31,466 train 600 1.346014e-02 0.226455
2019-11-12 16:29:37,491 train 650 1.342058e-02 0.229315
2019-11-12 16:29:43,530 train 700 1.339735e-02 0.231189
2019-11-12 16:29:49,548 train 750 1.337775e-02 0.231070
2019-11-12 16:29:55,573 train 800 1.338359e-02 0.231980
2019-11-12 16:30:01,595 train 850 1.335587e-02 0.233707
2019-11-12 16:30:03,392 training loss; R2: 1.335013e-02 0.233321
2019-11-12 16:30:03,658 valid 000 1.451336e-02 0.269567
2019-11-12 16:30:05,326 valid 050 1.518614e-02 0.193772
2019-11-12 16:30:06,846 validation loss; R2: 1.519049e-02 0.189903
2019-11-12 16:30:06,866 epoch 6 lr 1.000000e-03
2019-11-12 16:30:07,209 train 000 1.341647e-02 0.301515
2019-11-12 16:30:13,361 train 050 1.339823e-02 0.272866
2019-11-12 16:30:19,399 train 100 1.317726e-02 0.270232
2019-11-12 16:30:25,416 train 150 1.307126e-02 0.263500
2019-11-12 16:30:31,432 train 200 1.311663e-02 0.263053
2019-11-12 16:30:37,443 train 250 1.308279e-02 0.250073
2019-11-12 16:30:43,447 train 300 1.310287e-02 0.251827
2019-11-12 16:30:49,461 train 350 1.303771e-02 0.252746
2019-11-12 16:30:55,473 train 400 1.303524e-02 0.255526
2019-11-12 16:31:01,518 train 450 1.302762e-02 0.254998
2019-11-12 16:31:07,577 train 500 1.300325e-02 0.254812
2019-11-12 16:31:13,631 train 550 1.297635e-02 0.254673
2019-11-12 16:31:19,648 train 600 1.296420e-02 0.256745
2019-11-12 16:31:25,655 train 650 1.295153e-02 0.258491
2019-11-12 16:31:31,662 train 700 1.293723e-02 0.256651
2019-11-12 16:31:37,678 train 750 1.290954e-02 0.241450
2019-11-12 16:31:43,698 train 800 1.288902e-02 0.242258
2019-11-12 16:31:49,707 train 850 1.287308e-02 0.244893
2019-11-12 16:31:51,504 training loss; R2: 1.286449e-02 0.245279
2019-11-12 16:31:51,780 valid 000 3.959212e-02 -2.548760
2019-11-12 16:31:53,482 valid 050 4.381508e-02 -2.784410
2019-11-12 16:31:54,994 validation loss; R2: 4.368243e-02 -2.566589
2019-11-12 16:31:55,014 epoch 7 lr 1.000000e-03
2019-11-12 16:31:55,346 train 000 1.298575e-02 0.383011
2019-11-12 16:32:01,149 train 050 1.263288e-02 0.291465
2019-11-12 16:32:06,946 train 100 1.254078e-02 0.281388
2019-11-12 16:32:12,734 train 150 1.247705e-02 0.287551
2019-11-12 16:32:18,525 train 200 1.247533e-02 0.273068
2019-11-12 16:32:24,315 train 250 1.242275e-02 0.269869
2019-11-12 16:32:30,104 train 300 1.246694e-02 0.259163
2019-11-12 16:32:35,891 train 350 1.251436e-02 0.263304
2019-11-12 16:32:41,685 train 400 1.250810e-02 0.241871
2019-11-12 16:32:47,470 train 450 1.251139e-02 0.238315
2019-11-12 16:32:53,254 train 500 1.249185e-02 0.244270
2019-11-12 16:32:59,068 train 550 1.245461e-02 0.243652
2019-11-12 16:33:04,878 train 600 1.244595e-02 0.248006
2019-11-12 16:33:10,700 train 650 1.243348e-02 0.250838
2019-11-12 16:33:16,492 train 700 1.242535e-02 0.254071
2019-11-12 16:33:22,294 train 750 1.240147e-02 0.250480
2019-11-12 16:33:28,094 train 800 1.241214e-02 0.251402
2019-11-12 16:33:33,913 train 850 1.241041e-02 0.251469
2019-11-12 16:33:35,650 training loss; R2: 1.240248e-02 0.252509
2019-11-12 16:33:35,908 valid 000 3.938785e-01 -25.343604
2019-11-12 16:33:37,587 valid 050 3.887101e-01 -22.493886
2019-11-12 16:33:39,097 validation loss; R2: 3.898521e-01 -25.348946
2019-11-12 16:33:39,111 epoch 8 lr 1.000000e-03
2019-11-12 16:33:39,440 train 000 1.229807e-02 0.395078
2019-11-12 16:33:45,425 train 050 1.228511e-02 0.288238
2019-11-12 16:33:51,651 train 100 1.241764e-02 0.279259
2019-11-12 16:33:57,949 train 150 1.235797e-02 0.271196
2019-11-12 16:34:04,134 train 200 1.225575e-02 0.271727
2019-11-12 16:34:10,252 train 250 1.225896e-02 0.259220
2019-11-12 16:34:16,345 train 300 1.227687e-02 0.262697
2019-11-12 16:34:22,479 train 350 1.225366e-02 0.258986
2019-11-12 16:34:28,635 train 400 1.223813e-02 0.262036
2019-11-12 16:34:34,777 train 450 1.221782e-02 0.263469
2019-11-12 16:34:40,923 train 500 1.220499e-02 0.266155
2019-11-12 16:34:47,107 train 550 1.219891e-02 0.265058
2019-11-12 16:34:53,250 train 600 1.215796e-02 0.266901
2019-11-12 16:34:59,456 train 650 1.214598e-02 0.270271
2019-11-12 16:35:05,643 train 700 1.214603e-02 0.268626
2019-11-12 16:35:11,746 train 750 1.215399e-02 0.270155
2019-11-12 16:35:17,876 train 800 1.213923e-02 0.268397
2019-11-12 16:35:23,978 train 850 1.214499e-02 0.266177
2019-11-12 16:35:25,811 training loss; R2: 1.214926e-02 0.266792
2019-11-12 16:35:26,093 valid 000 1.219349e-02 0.252760
2019-11-12 16:35:27,777 valid 050 1.225838e-02 0.331678
2019-11-12 16:35:29,300 validation loss; R2: 1.204127e-02 0.332142
2019-11-12 16:35:29,320 epoch 9 lr 1.000000e-03
2019-11-12 16:35:29,667 train 000 1.131359e-02 0.412461
2019-11-12 16:35:35,708 train 050 1.171433e-02 0.315931
2019-11-12 16:35:41,526 train 100 1.170495e-02 0.271302
2019-11-12 16:35:47,526 train 150 1.185168e-02 0.283402
2019-11-12 16:35:53,529 train 200 1.189928e-02 0.286462
2019-11-12 16:35:59,535 train 250 1.190970e-02 0.288394
2019-11-12 16:36:05,558 train 300 1.194315e-02 0.286277
2019-11-12 16:36:11,562 train 350 1.193850e-02 0.284837
2019-11-12 16:36:17,569 train 400 1.198844e-02 0.285446
2019-11-12 16:36:23,576 train 450 1.197755e-02 0.285610
2019-11-12 16:36:29,576 train 500 1.197936e-02 0.272539
2019-11-12 16:36:35,581 train 550 1.194689e-02 0.267288
2019-11-12 16:36:41,593 train 600 1.195984e-02 0.266321
2019-11-12 16:36:47,586 train 650 1.194213e-02 0.267725
2019-11-12 16:36:53,567 train 700 1.193342e-02 0.267170
2019-11-12 16:36:59,552 train 750 1.192767e-02 0.270357
2019-11-12 16:37:05,549 train 800 1.191005e-02 0.273622
2019-11-12 16:37:11,542 train 850 1.190906e-02 0.273902
2019-11-12 16:37:13,341 training loss; R2: 1.190324e-02 0.274431
2019-11-12 16:37:13,622 valid 000 3.388015e-01 -28.235338
2019-11-12 16:37:15,300 valid 050 3.260071e-01 -40.818347
2019-11-12 16:37:16,813 validation loss; R2: 3.249224e-01 -41.288707
2019-11-12 16:37:16,833 epoch 10 lr 1.000000e-03
2019-11-12 16:37:17,156 train 000 1.049527e-02 0.404585
2019-11-12 16:37:22,973 train 050 1.153374e-02 0.329311
2019-11-12 16:37:29,131 train 100 1.149541e-02 0.329649
2019-11-12 16:37:35,237 train 150 1.158662e-02 0.324010
2019-11-12 16:37:41,416 train 200 1.164976e-02 0.315006
2019-11-12 16:37:47,523 train 250 1.166190e-02 0.314105
2019-11-12 16:37:53,614 train 300 1.170520e-02 0.309845
2019-11-12 16:37:59,703 train 350 1.173292e-02 0.303823
2019-11-12 16:38:05,848 train 400 1.174545e-02 0.301348
2019-11-12 16:38:11,944 train 450 1.174824e-02 0.304307
2019-11-12 16:38:18,117 train 500 1.176025e-02 0.298836
2019-11-12 16:38:24,165 train 550 1.174986e-02 0.298802
2019-11-12 16:38:30,084 train 600 1.175042e-02 0.299419
2019-11-12 16:38:36,101 train 650 1.175228e-02 0.291424
2019-11-12 16:38:41,906 train 700 1.173344e-02 0.290421
2019-11-12 16:38:47,813 train 750 1.172706e-02 0.289317
2019-11-12 16:38:53,732 train 800 1.173549e-02 0.291246
2019-11-12 16:38:59,708 train 850 1.173036e-02 0.290662
2019-11-12 16:39:01,502 training loss; R2: 1.172705e-02 0.290335
2019-11-12 16:39:01,784 valid 000 1.173753e-01 -9.123992
2019-11-12 16:39:03,504 valid 050 1.197572e-01 -10.106344
2019-11-12 16:39:05,054 validation loss; R2: 1.208533e-01 -10.005762
2019-11-12 16:39:05,068 epoch 11 lr 1.000000e-03
2019-11-12 16:39:05,417 train 000 1.081608e-02 0.366289
2019-11-12 16:39:11,398 train 050 1.162777e-02 0.286478
2019-11-12 16:39:17,668 train 100 1.160778e-02 0.268758
2019-11-12 16:39:23,794 train 150 1.161165e-02 0.284513
2019-11-12 16:39:30,254 train 200 1.164683e-02 0.285337
2019-11-12 16:39:36,695 train 250 1.159668e-02 0.288349
2019-11-12 16:39:43,145 train 300 1.164322e-02 0.287648
2019-11-12 16:39:49,226 train 350 1.162056e-02 0.287888
2019-11-12 16:39:55,312 train 400 1.164572e-02 0.289420
2019-11-12 16:40:01,388 train 450 1.163052e-02 0.272928
2019-11-12 16:40:07,461 train 500 1.163737e-02 0.272242
2019-11-12 16:40:13,536 train 550 1.161936e-02 0.274673
2019-11-12 16:40:19,609 train 600 1.162752e-02 0.274288
2019-11-12 16:40:25,669 train 650 1.160588e-02 0.277869
2019-11-12 16:40:31,736 train 700 1.159019e-02 0.280716
2019-11-12 16:40:37,809 train 750 1.160253e-02 0.282422
2019-11-12 16:40:43,908 train 800 1.158243e-02 0.284438
2019-11-12 16:40:49,994 train 850 1.160393e-02 0.285273
2019-11-12 16:40:51,805 training loss; R2: 1.160127e-02 0.285823
2019-11-12 16:40:52,099 valid 000 5.591127e-02 -2.031323
2019-11-12 16:40:53,767 valid 050 6.069593e-02 -4.000310
2019-11-12 16:40:55,275 validation loss; R2: 6.108997e-02 -3.808569
2019-11-12 16:40:55,288 epoch 12 lr 1.000000e-03
2019-11-12 16:40:55,608 train 000 1.128233e-02 0.387484
2019-11-12 16:41:01,610 train 050 1.162267e-02 0.305268
2019-11-12 16:41:07,870 train 100 1.161997e-02 0.278503
2019-11-12 16:41:14,110 train 150 1.144905e-02 0.284779
2019-11-12 16:41:20,192 train 200 1.143223e-02 0.286112
2019-11-12 16:41:26,268 train 250 1.149975e-02 0.289039
2019-11-12 16:41:32,337 train 300 1.148504e-02 0.292501
2019-11-12 16:41:38,144 train 350 1.147841e-02 0.295892
2019-11-12 16:41:43,912 train 400 1.147868e-02 0.291074
2019-11-12 16:41:49,683 train 450 1.146711e-02 0.293210
2019-11-12 16:41:55,451 train 500 1.148084e-02 0.297243
2019-11-12 16:42:01,224 train 550 1.144618e-02 0.295831
2019-11-12 16:42:06,986 train 600 1.148362e-02 0.297100
2019-11-12 16:42:12,754 train 650 1.147912e-02 0.292011
2019-11-12 16:42:18,517 train 700 1.147532e-02 0.290981
2019-11-12 16:42:24,447 train 750 1.147717e-02 0.289201
2019-11-12 16:42:30,508 train 800 1.145410e-02 0.290093
2019-11-12 16:42:36,900 train 850 1.145269e-02 0.291461
2019-11-12 16:42:38,825 training loss; R2: 1.145647e-02 0.292099
2019-11-12 16:42:39,101 valid 000 1.454590e-02 0.370006
2019-11-12 16:42:40,778 valid 050 1.215461e-02 0.364388
2019-11-12 16:42:42,292 validation loss; R2: 1.208975e-02 0.365216
2019-11-12 16:42:42,318 epoch 13 lr 1.000000e-03
2019-11-12 16:42:42,701 train 000 1.242902e-02 0.373208
2019-11-12 16:42:48,773 train 050 1.133560e-02 0.320309
2019-11-12 16:42:54,908 train 100 1.134224e-02 0.328264
2019-11-12 16:43:00,959 train 150 1.132022e-02 0.320737
2019-11-12 16:43:07,050 train 200 1.129483e-02 0.317020
2019-11-12 16:43:13,234 train 250 1.132203e-02 0.306792
2019-11-12 16:43:19,320 train 300 1.136189e-02 0.285317
2019-11-12 16:43:25,496 train 350 1.136752e-02 0.288722
2019-11-12 16:43:31,541 train 400 1.134258e-02 0.286387
2019-11-12 16:43:37,585 train 450 1.136290e-02 0.286925
2019-11-12 16:43:43,620 train 500 1.137841e-02 0.290403
2019-11-12 16:43:49,661 train 550 1.136764e-02 0.287746
2019-11-12 16:43:55,691 train 600 1.136899e-02 0.286291
2019-11-12 16:44:01,721 train 650 1.138358e-02 0.284148
2019-11-12 16:44:07,749 train 700 1.136824e-02 0.285946
2019-11-12 16:44:13,773 train 750 1.137425e-02 0.288060
2019-11-12 16:44:19,797 train 800 1.136540e-02 0.291020
2019-11-12 16:44:25,826 train 850 1.135978e-02 0.291739
2019-11-12 16:44:27,623 training loss; R2: 1.135892e-02 0.291524
2019-11-12 16:44:27,884 valid 000 2.093366e-02 -2.851370
2019-11-12 16:44:29,554 valid 050 2.218627e-02 -1.463478
2019-11-12 16:44:31,074 validation loss; R2: 2.200955e-02 -1.827713
2019-11-12 16:44:31,092 epoch 14 lr 1.000000e-03
2019-11-12 16:44:31,474 train 000 1.010529e-02 0.444316
2019-11-12 16:44:37,631 train 050 1.165569e-02 0.314072
2019-11-12 16:44:43,647 train 100 1.149068e-02 0.284681
2019-11-12 16:44:49,745 train 150 1.158854e-02 0.291682
2019-11-12 16:44:55,725 train 200 1.157064e-02 0.302233
2019-11-12 16:45:01,677 train 250 1.149691e-02 0.295034
2019-11-12 16:45:07,632 train 300 1.146060e-02 0.293448
2019-11-12 16:45:13,588 train 350 1.145449e-02 0.295258
2019-11-12 16:45:19,528 train 400 1.144281e-02 0.280427
2019-11-12 16:45:25,480 train 450 1.141524e-02 0.285906
2019-11-12 16:45:31,412 train 500 1.137682e-02 0.288294
2019-11-12 16:45:37,350 train 550 1.136058e-02 0.290194
2019-11-12 16:45:43,322 train 600 1.135568e-02 0.289886
2019-11-12 16:45:49,261 train 650 1.134748e-02 0.292083
2019-11-12 16:45:55,200 train 700 1.135905e-02 0.292721
2019-11-12 16:46:01,138 train 750 1.136415e-02 0.294825
2019-11-12 16:46:07,072 train 800 1.133920e-02 0.295937
2019-11-12 16:46:13,009 train 850 1.134678e-02 0.297129
2019-11-12 16:46:14,780 training loss; R2: 1.134871e-02 0.297605
2019-11-12 16:46:15,075 valid 000 3.879102e-02 -1.288052
2019-11-12 16:46:16,728 valid 050 4.329865e-02 -1.483242
2019-11-12 16:46:18,237 validation loss; R2: 4.331327e-02 -1.478428
2019-11-12 16:46:18,252 epoch 15 lr 1.000000e-03
2019-11-12 16:46:18,605 train 000 1.041682e-02 0.401237
2019-11-12 16:46:24,338 train 050 1.143396e-02 0.316537
2019-11-12 16:46:30,469 train 100 1.134929e-02 0.282579
2019-11-12 16:46:36,578 train 150 1.136641e-02 0.182027
2019-11-12 16:46:42,729 train 200 1.137777e-02 0.200722
2019-11-12 16:46:48,805 train 250 1.130241e-02 0.224162
2019-11-12 16:46:54,597 train 300 1.132278e-02 0.240576
2019-11-12 16:47:00,361 train 350 1.128028e-02 0.231648
2019-11-12 16:47:06,160 train 400 1.129770e-02 0.245248
2019-11-12 16:47:12,111 train 450 1.127427e-02 0.253045
2019-11-12 16:47:18,241 train 500 1.126167e-02 0.259786
2019-11-12 16:47:24,372 train 550 1.122784e-02 0.267940
2019-11-12 16:47:30,448 train 600 1.122011e-02 0.266613
2019-11-12 16:47:36,492 train 650 1.121657e-02 0.272430
2019-11-12 16:47:42,583 train 700 1.120729e-02 0.276180
2019-11-12 16:47:48,644 train 750 1.121721e-02 0.279864
2019-11-12 16:47:54,789 train 800 1.121162e-02 0.278687
2019-11-12 16:48:00,899 train 850 1.121947e-02 0.279114
2019-11-12 16:48:02,705 training loss; R2: 1.121735e-02 0.274382
2019-11-12 16:48:03,004 valid 000 1.001342e-02 0.431933
2019-11-12 16:48:04,699 valid 050 1.081633e-02 0.387146
2019-11-12 16:48:06,248 validation loss; R2: 1.073618e-02 0.374152
2019-11-12 16:48:06,266 epoch 16 lr 1.000000e-03
2019-11-12 16:48:06,692 train 000 9.981632e-03 0.338573
2019-11-12 16:48:12,781 train 050 1.091295e-02 0.325875
2019-11-12 16:48:18,841 train 100 1.103776e-02 0.299203
2019-11-12 16:48:24,897 train 150 1.107864e-02 0.302169
2019-11-12 16:48:30,952 train 200 1.118332e-02 0.304416
2019-11-12 16:48:37,011 train 250 1.123357e-02 0.307627
2019-11-12 16:48:43,067 train 300 1.124868e-02 0.310473
2019-11-12 16:48:49,112 train 350 1.124504e-02 0.309514
2019-11-12 16:48:55,156 train 400 1.121177e-02 0.309014
2019-11-12 16:49:00,913 train 450 1.120305e-02 0.306848
2019-11-12 16:49:06,673 train 500 1.115506e-02 0.306212
2019-11-12 16:49:12,430 train 550 1.115665e-02 0.294253
2019-11-12 16:49:18,187 train 600 1.116559e-02 0.298409
2019-11-12 16:49:23,943 train 650 1.117907e-02 0.298563
2019-11-12 16:49:29,707 train 700 1.117040e-02 0.300677
2019-11-12 16:49:35,462 train 750 1.119376e-02 0.300717
2019-11-12 16:49:41,222 train 800 1.119412e-02 0.301417
2019-11-12 16:49:46,977 train 850 1.118870e-02 0.302233
2019-11-12 16:49:48,695 training loss; R2: 1.119434e-02 0.298621
2019-11-12 16:49:48,938 valid 000 3.070799e-01 -36.767036
2019-11-12 16:49:50,641 valid 050 3.037164e-01 -149.269505
2019-11-12 16:49:52,146 validation loss; R2: 3.036201e-01 -192.483738
2019-11-12 16:49:52,166 epoch 17 lr 1.000000e-03
2019-11-12 16:49:52,512 train 000 9.736501e-03 0.357944
2019-11-12 16:49:58,471 train 050 1.125894e-02 0.334004
2019-11-12 16:50:04,408 train 100 1.105247e-02 0.273876
2019-11-12 16:50:10,543 train 150 1.105881e-02 0.294978
2019-11-12 16:50:16,595 train 200 1.103476e-02 0.307473
2019-11-12 16:50:22,556 train 250 1.108458e-02 0.142225
2019-11-12 16:50:28,428 train 300 1.109531e-02 0.171875
2019-11-12 16:50:34,336 train 350 1.107363e-02 0.195701
2019-11-12 16:50:40,655 train 400 1.102602e-02 0.195156
2019-11-12 16:50:46,663 train 450 1.104369e-02 0.210219
2019-11-12 16:50:52,568 train 500 1.107923e-02 0.220781
2019-11-12 16:50:58,692 train 550 1.107600e-02 0.229538
2019-11-12 16:51:04,631 train 600 1.108491e-02 0.229704
2019-11-12 16:51:10,549 train 650 1.106817e-02 0.237951
2019-11-12 16:51:16,419 train 700 1.107757e-02 0.245181
2019-11-12 16:51:22,516 train 750 1.108700e-02 0.250976
2019-11-12 16:51:28,793 train 800 1.108873e-02 0.256059
2019-11-12 16:51:34,923 train 850 1.109245e-02 0.257442
2019-11-12 16:51:36,759 training loss; R2: 1.109728e-02 0.258828
2019-11-12 16:51:37,053 valid 000 3.199694e-01 -60.478394
2019-11-12 16:51:38,739 valid 050 3.260404e-01 -63.790762
2019-11-12 16:51:40,276 validation loss; R2: 3.265060e-01 -64.911056
2019-11-12 16:51:40,302 epoch 18 lr 1.000000e-03
2019-11-12 16:51:40,668 train 000 1.014240e-02 0.372125
2019-11-12 16:51:46,754 train 050 1.081728e-02 0.305739
2019-11-12 16:51:52,865 train 100 1.105511e-02 0.327103
2019-11-12 16:51:58,971 train 150 1.125485e-02 0.312961
2019-11-12 16:52:05,083 train 200 1.112670e-02 0.283760
2019-11-12 16:52:11,179 train 250 1.113416e-02 0.295482
2019-11-12 16:52:17,416 train 300 1.114252e-02 0.291288
2019-11-12 16:52:23,561 train 350 1.115426e-02 0.293954
2019-11-12 16:52:29,597 train 400 1.113320e-02 0.293624
2019-11-12 16:52:35,619 train 450 1.114985e-02 0.295709
2019-11-12 16:52:41,618 train 500 1.117875e-02 0.299752
2019-11-12 16:52:47,664 train 550 1.115041e-02 0.298012
2019-11-12 16:52:53,577 train 600 1.111703e-02 0.300696
2019-11-12 16:52:59,554 train 650 1.109344e-02 0.302158
2019-11-12 16:53:05,592 train 700 1.109107e-02 0.305476
2019-11-12 16:53:11,579 train 750 1.108749e-02 0.305153
2019-11-12 16:53:17,612 train 800 1.107996e-02 0.307234
2019-11-12 16:53:23,417 train 850 1.107715e-02 0.307723
2019-11-12 16:53:25,150 training loss; R2: 1.107184e-02 0.308569
2019-11-12 16:53:25,421 valid 000 3.427261e-01 -27.146851
2019-11-12 16:53:27,113 valid 050 3.522968e-01 -28.930228
2019-11-12 16:53:28,636 validation loss; R2: 3.524543e-01 -28.245210
2019-11-12 16:53:28,654 epoch 19 lr 1.000000e-03
2019-11-12 16:53:29,048 train 000 1.006880e-02 0.308508
2019-11-12 16:53:35,210 train 050 1.079017e-02 0.345970
2019-11-12 16:53:41,390 train 100 1.082593e-02 0.326993
2019-11-12 16:53:47,552 train 150 1.084601e-02 0.323694
2019-11-12 16:53:53,699 train 200 1.083053e-02 0.327188
2019-11-12 16:53:59,824 train 250 1.087991e-02 0.326409
2019-11-12 16:54:05,946 train 300 1.097089e-02 0.326503
2019-11-12 16:54:12,069 train 350 1.092449e-02 0.321085
2019-11-12 16:54:18,197 train 400 1.093589e-02 0.131472
2019-11-12 16:54:24,340 train 450 1.093572e-02 0.153886
2019-11-12 16:54:30,486 train 500 1.094203e-02 0.168181
2019-11-12 16:54:36,603 train 550 1.094433e-02 0.179445
2019-11-12 16:54:42,701 train 600 1.096439e-02 0.192383
2019-11-12 16:54:48,820 train 650 1.096534e-02 0.202513
2019-11-12 16:54:54,956 train 700 1.094101e-02 0.190130
2019-11-12 16:55:01,102 train 750 1.093831e-02 0.200192
2019-11-12 16:55:07,236 train 800 1.094935e-02 0.209320
2019-11-12 16:55:13,373 train 850 1.093557e-02 0.215953
2019-11-12 16:55:15,208 training loss; R2: 1.094112e-02 0.217744
2019-11-12 16:55:15,498 valid 000 2.277663e+00 -900.191863
2019-11-12 16:55:17,169 valid 050 2.282417e+00 -964.817447
2019-11-12 16:55:18,676 validation loss; R2: 2.282578e+00 -947.537488
