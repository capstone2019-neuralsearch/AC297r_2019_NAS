2019-12-04 01:03:06,484 gpu device = 1
2019-12-04 01:03:06,484 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=96, cutout=False, cutout_length=16, data='../data', dataset='graphene', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name=None, gpu=1, grad_clip=5, gz_dtree=False, init_channels=36, layers=8, learning_rate=0.002, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-graphene-EXP-20191204-010306', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-12-04 01:03:09,709 param size = 1.258669MB
2019-12-04 01:03:09,712 epoch 0 lr 2.000000e-03
2019-12-04 01:03:12,461 train 000 1.128615e+00 -51.033422
2019-12-04 01:03:26,134 train 050 2.025366e-01 -5.372175
2019-12-04 01:03:39,690 train 100 1.257457e-01 -2.878710
2019-12-04 01:03:52,944 train 150 9.746671e-02 -2.005235
2019-12-04 01:04:06,175 train 200 8.271978e-02 -1.627752
2019-12-04 01:04:13,251 training loss; R2: 7.777176e-02 -1.473070
2019-12-04 01:04:13,399 valid 000 2.697681e-02 0.401741
2019-12-04 01:04:15,138 validation loss; R2: 2.023514e-02 0.366544
2019-12-04 01:04:15,161 epoch 1 lr 2.000000e-03
2019-12-04 01:04:15,613 train 000 3.281740e-02 -0.123536
2019-12-04 01:04:28,804 train 050 2.511151e-02 0.155254
2019-12-04 01:04:41,997 train 100 2.473198e-02 0.204767
2019-12-04 01:04:55,206 train 150 2.471048e-02 0.211984
2019-12-04 01:05:08,409 train 200 2.343791e-02 0.247768
2019-12-04 01:05:14,351 training loss; R2: 2.283638e-02 0.265855
2019-12-04 01:05:14,490 valid 000 1.591170e-02 0.598211
2019-12-04 01:05:15,908 validation loss; R2: 1.322800e-02 0.583808
2019-12-04 01:05:15,932 epoch 2 lr 2.000000e-03
2019-12-04 01:05:16,280 train 000 3.161712e-02 0.370125
2019-12-04 01:05:29,491 train 050 1.788693e-02 0.445148
2019-12-04 01:05:42,702 train 100 1.797879e-02 0.446844
2019-12-04 01:05:55,913 train 150 1.722286e-02 0.459039
2019-12-04 01:06:09,125 train 200 1.772947e-02 0.429547
2019-12-04 01:06:15,067 training loss; R2: 1.717169e-02 0.439007
2019-12-04 01:06:15,207 valid 000 2.522057e-02 0.552666
2019-12-04 01:06:16,624 validation loss; R2: 1.016310e-02 0.687885
2019-12-04 01:06:16,648 epoch 3 lr 2.000000e-03
2019-12-04 01:06:16,999 train 000 8.809398e-03 0.616686
2019-12-04 01:06:30,210 train 050 1.399615e-02 0.522009
2019-12-04 01:06:43,424 train 100 1.271945e-02 0.543639
2019-12-04 01:06:56,633 train 150 1.301630e-02 0.555022
2019-12-04 01:07:09,841 train 200 1.328538e-02 0.558244
2019-12-04 01:07:15,784 training loss; R2: 1.335780e-02 0.554653
2019-12-04 01:07:15,922 valid 000 6.751633e-03 0.774339
2019-12-04 01:07:17,339 validation loss; R2: 6.310532e-03 0.789125
2019-12-04 01:07:17,363 epoch 4 lr 2.000000e-03
2019-12-04 01:07:17,710 train 000 1.272719e-02 0.317831
2019-12-04 01:07:30,914 train 050 1.112911e-02 0.630910
2019-12-04 01:07:44,115 train 100 1.080725e-02 0.627737
2019-12-04 01:07:57,317 train 150 1.081166e-02 0.624686
2019-12-04 01:08:10,523 train 200 1.076833e-02 0.636054
2019-12-04 01:08:16,463 training loss; R2: 1.097800e-02 0.635907
2019-12-04 01:08:16,603 valid 000 9.419089e-03 0.663899
2019-12-04 01:08:18,020 validation loss; R2: 1.108861e-02 0.647824
2019-12-04 01:08:18,046 epoch 5 lr 2.000000e-03
2019-12-04 01:08:18,397 train 000 6.951530e-03 0.604232
2019-12-04 01:08:31,601 train 050 9.744984e-03 0.662167
2019-12-04 01:08:44,805 train 100 9.772719e-03 0.661020
2019-12-04 01:08:58,010 train 150 9.985074e-03 0.665060
2019-12-04 01:09:11,214 train 200 1.005537e-02 0.667128
2019-12-04 01:09:17,151 training loss; R2: 1.023634e-02 0.658319
2019-12-04 01:09:17,289 valid 000 1.379234e-02 0.744833
2019-12-04 01:09:18,706 validation loss; R2: 9.155344e-03 0.701418
2019-12-04 01:09:18,730 epoch 6 lr 2.000000e-03
2019-12-04 01:09:19,077 train 000 1.706988e-02 0.455477
2019-12-04 01:09:32,281 train 050 1.008545e-02 0.647975
2019-12-04 01:09:45,488 train 100 1.011357e-02 0.660321
2019-12-04 01:09:58,696 train 150 9.550599e-03 0.676016
2019-12-04 01:10:11,907 train 200 1.001236e-02 0.676076
2019-12-04 01:10:17,847 training loss; R2: 9.929379e-03 0.671243
2019-12-04 01:10:17,986 valid 000 8.888000e-03 0.755000
2019-12-04 01:10:19,403 validation loss; R2: 6.850725e-03 0.766597
2019-12-04 01:10:19,429 epoch 7 lr 2.000000e-03
2019-12-04 01:10:19,778 train 000 6.234515e-03 0.787945
2019-12-04 01:10:33,006 train 050 9.000170e-03 0.700342
2019-12-04 01:10:46,237 train 100 8.748444e-03 0.704736
2019-12-04 01:10:59,467 train 150 8.557716e-03 0.709054
2019-12-04 01:11:12,698 train 200 8.929963e-03 0.697632
2019-12-04 01:11:18,745 training loss; R2: 9.271309e-03 0.693721
2019-12-04 01:11:18,886 valid 000 2.227939e-02 0.545746
2019-12-04 01:11:20,305 validation loss; R2: 1.107846e-02 0.639268
2019-12-04 01:11:20,332 epoch 8 lr 2.000000e-03
2019-12-04 01:11:20,692 train 000 9.032433e-03 0.668518
2019-12-04 01:11:34,238 train 050 8.585052e-03 0.718476
2019-12-04 01:11:47,777 train 100 8.483882e-03 0.719438
2019-12-04 01:12:01,319 train 150 8.647956e-03 0.715515
2019-12-04 01:12:14,859 train 200 8.912121e-03 0.707679
2019-12-04 01:12:20,952 training loss; R2: 8.923598e-03 0.707707
2019-12-04 01:12:21,099 valid 000 4.027677e-03 0.844209
2019-12-04 01:12:22,517 validation loss; R2: 4.195435e-03 0.855413
2019-12-04 01:12:22,544 epoch 9 lr 2.000000e-03
2019-12-04 01:12:22,900 train 000 9.300831e-03 0.527767
2019-12-04 01:12:36,458 train 050 8.476567e-03 0.711759
2019-12-04 01:12:50,024 train 100 8.486749e-03 0.706844
2019-12-04 01:13:03,580 train 150 8.724053e-03 0.703091
2019-12-04 01:13:17,144 train 200 8.525895e-03 0.714269
2019-12-04 01:13:23,246 training loss; R2: 8.506452e-03 0.714986
2019-12-04 01:13:23,391 valid 000 5.598361e-03 0.855605
2019-12-04 01:13:24,808 validation loss; R2: 5.898992e-03 0.795546
2019-12-04 01:13:24,834 epoch 10 lr 2.000000e-03
2019-12-04 01:13:25,197 train 000 1.933923e-02 0.764408
2019-12-04 01:13:38,731 train 050 8.442274e-03 0.725221
2019-12-04 01:13:52,267 train 100 8.377880e-03 0.730109
2019-12-04 01:14:05,803 train 150 8.176949e-03 0.722256
2019-12-04 01:14:19,333 train 200 8.210848e-03 0.721479
2019-12-04 01:14:25,425 training loss; R2: 8.146059e-03 0.723139
2019-12-04 01:14:25,575 valid 000 6.896012e-03 0.845090
2019-12-04 01:14:26,992 validation loss; R2: 4.820149e-03 0.835443
2019-12-04 01:14:27,020 epoch 11 lr 2.000000e-03
2019-12-04 01:14:27,377 train 000 1.172591e-02 0.606946
2019-12-04 01:14:40,914 train 050 7.064068e-03 0.757610
2019-12-04 01:14:54,455 train 100 7.335241e-03 0.748849
2019-12-04 01:15:07,992 train 150 7.518820e-03 0.750121
2019-12-04 01:15:21,525 train 200 7.544975e-03 0.745241
2019-12-04 01:15:27,617 training loss; R2: 7.598588e-03 0.745106
2019-12-04 01:15:27,758 valid 000 3.331884e-03 0.852240
2019-12-04 01:15:29,176 validation loss; R2: 4.807474e-03 0.829672
2019-12-04 01:15:29,203 epoch 12 lr 2.000000e-03
2019-12-04 01:15:29,561 train 000 8.094550e-03 0.748955
2019-12-04 01:15:43,106 train 050 7.010049e-03 0.760618
2019-12-04 01:15:56,647 train 100 6.927999e-03 0.764341
2019-12-04 01:16:10,190 train 150 6.738920e-03 0.764635
2019-12-04 01:16:23,556 train 200 6.995102e-03 0.762500
2019-12-04 01:16:29,489 training loss; R2: 7.147321e-03 0.758126
2019-12-04 01:16:29,637 valid 000 5.462904e-03 0.809522
2019-12-04 01:16:31,053 validation loss; R2: 4.156310e-03 0.859223
2019-12-04 01:16:31,078 epoch 13 lr 2.000000e-03
2019-12-04 01:16:31,428 train 000 7.835298e-03 0.490111
2019-12-04 01:16:44,622 train 050 7.719692e-03 0.749582
2019-12-04 01:16:57,813 train 100 7.580065e-03 0.745487
2019-12-04 01:17:11,007 train 150 8.316856e-03 0.737639
2019-12-04 01:17:24,204 train 200 8.008018e-03 0.742708
2019-12-04 01:17:30,142 training loss; R2: 7.890709e-03 0.740933
2019-12-04 01:17:30,282 valid 000 4.497427e-03 0.882586
2019-12-04 01:17:31,698 validation loss; R2: 3.771215e-03 0.871316
2019-12-04 01:17:31,724 epoch 14 lr 2.000000e-03
2019-12-04 01:17:32,075 train 000 6.004347e-03 0.822187
2019-12-04 01:17:45,275 train 050 6.767414e-03 0.764079
2019-12-04 01:17:58,466 train 100 7.080105e-03 0.761714
2019-12-04 01:18:11,653 train 150 7.053629e-03 0.759781
2019-12-04 01:18:24,839 train 200 7.085771e-03 0.760044
2019-12-04 01:18:30,769 training loss; R2: 7.052343e-03 0.762065
2019-12-04 01:18:30,908 valid 000 2.948260e-03 0.884577
2019-12-04 01:18:32,324 validation loss; R2: 4.258650e-03 0.855700
2019-12-04 01:18:32,350 epoch 15 lr 2.000000e-03
2019-12-04 01:18:32,702 train 000 5.377749e-03 0.763726
2019-12-04 01:18:45,890 train 050 6.593976e-03 0.748981
2019-12-04 01:18:59,075 train 100 6.518610e-03 0.760176
2019-12-04 01:19:12,257 train 150 6.666303e-03 0.768333
2019-12-04 01:19:25,454 train 200 6.951335e-03 0.765166
2019-12-04 01:19:31,385 training loss; R2: 7.087883e-03 0.761462
2019-12-04 01:19:31,528 valid 000 5.517324e-03 0.796574
2019-12-04 01:19:32,944 validation loss; R2: 3.764214e-03 0.859569
2019-12-04 01:19:32,970 epoch 16 lr 2.000000e-03
2019-12-04 01:19:33,320 train 000 4.892078e-03 0.820442
2019-12-04 01:19:46,504 train 050 7.727345e-03 0.777865
2019-12-04 01:19:59,686 train 100 7.156742e-03 0.767710
2019-12-04 01:20:12,868 train 150 6.931358e-03 0.774640
2019-12-04 01:20:26,051 train 200 6.726581e-03 0.774850
2019-12-04 01:20:31,980 training loss; R2: 6.761482e-03 0.774468
2019-12-04 01:20:32,123 valid 000 1.644760e-03 0.840000
2019-12-04 01:20:33,539 validation loss; R2: 3.828329e-03 0.857319
2019-12-04 01:20:33,565 epoch 17 lr 2.000000e-03
2019-12-04 01:20:33,916 train 000 6.447134e-03 0.538502
2019-12-04 01:20:47,099 train 050 7.270257e-03 0.748152
2019-12-04 01:21:00,284 train 100 7.128516e-03 0.755035
2019-12-04 01:21:13,465 train 150 7.425738e-03 0.755324
2019-12-04 01:21:26,644 train 200 7.216390e-03 0.760694
2019-12-04 01:21:32,572 training loss; R2: 7.057875e-03 0.763661
2019-12-04 01:21:32,716 valid 000 3.398790e-03 0.895805
2019-12-04 01:21:34,132 validation loss; R2: 4.100053e-03 0.864614
2019-12-04 01:21:34,156 epoch 18 lr 2.000000e-03
2019-12-04 01:21:34,503 train 000 6.134102e-03 0.841085
2019-12-04 01:21:47,685 train 050 6.110123e-03 0.780439
2019-12-04 01:22:00,862 train 100 5.905901e-03 0.793609
2019-12-04 01:22:14,038 train 150 5.888217e-03 0.796582
2019-12-04 01:22:27,213 train 200 5.988003e-03 0.797028
2019-12-04 01:22:33,146 training loss; R2: 6.157406e-03 0.790545
2019-12-04 01:22:33,286 valid 000 4.329834e-01 -7.012901
2019-12-04 01:22:34,702 validation loss; R2: 4.292891e-01 -14.645723
2019-12-04 01:22:34,728 epoch 19 lr 2.000000e-03
2019-12-04 01:22:35,078 train 000 6.962935e-03 0.727128
2019-12-04 01:22:48,257 train 050 6.767777e-03 0.782676
2019-12-04 01:23:01,426 train 100 6.247205e-03 0.781284
2019-12-04 01:23:14,597 train 150 6.359648e-03 0.780745
2019-12-04 01:23:27,767 train 200 6.362858e-03 0.784962
2019-12-04 01:23:33,688 training loss; R2: 6.463227e-03 0.783732
2019-12-04 01:23:33,832 valid 000 7.223861e-03 0.886524
2019-12-04 01:23:35,248 validation loss; R2: 4.126269e-03 0.863592
