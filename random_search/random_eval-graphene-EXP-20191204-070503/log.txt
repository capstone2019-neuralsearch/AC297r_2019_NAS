2019-12-04 07:05:03,569 gpu device = 1
2019-12-04 07:05:03,570 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=96, cutout=False, cutout_length=16, data='../data', dataset='graphene', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name=None, gpu=1, grad_clip=5, gz_dtree=False, init_channels=36, layers=8, learning_rate=0.002, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-graphene-EXP-20191204-070503', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-12-04 07:05:06,779 param size = 1.403677MB
2019-12-04 07:05:06,783 epoch 0 lr 2.000000e-03
2019-12-04 07:05:09,556 train 000 5.481274e-01 -13.264082
2019-12-04 07:05:24,499 train 050 1.229082e-01 -3.740710
2019-12-04 07:05:39,180 train 100 7.402084e-02 -1.805403
2019-12-04 07:05:53,861 train 150 5.735338e-02 -1.116105
2019-12-04 07:06:08,542 train 200 4.798091e-02 -0.749326
2019-12-04 07:06:16,233 training loss; R2: 4.497077e-02 -0.630117
2019-12-04 07:06:16,386 valid 000 1.274031e-02 0.623579
2019-12-04 07:06:18,194 validation loss; R2: 1.289400e-02 0.588615
2019-12-04 07:06:18,220 epoch 1 lr 2.000000e-03
2019-12-04 07:06:18,814 train 000 1.317649e-02 0.112622
2019-12-04 07:06:33,115 train 050 1.595523e-02 0.477237
2019-12-04 07:06:47,390 train 100 1.533767e-02 0.494587
2019-12-04 07:07:01,669 train 150 1.408799e-02 0.539229
2019-12-04 07:07:15,950 train 200 1.339787e-02 0.563557
2019-12-04 07:07:22,376 training loss; R2: 1.307396e-02 0.570274
2019-12-04 07:07:22,518 valid 000 5.247104e-03 0.735556
2019-12-04 07:07:24,020 validation loss; R2: 7.131430e-03 0.759486
2019-12-04 07:07:24,047 epoch 2 lr 2.000000e-03
2019-12-04 07:07:24,421 train 000 9.997972e-03 0.539237
2019-12-04 07:07:38,703 train 050 1.048770e-02 0.687888
2019-12-04 07:07:52,988 train 100 1.057196e-02 0.660076
2019-12-04 07:08:07,276 train 150 1.014099e-02 0.664662
2019-12-04 07:08:21,564 train 200 9.865591e-03 0.678062
2019-12-04 07:08:27,992 training loss; R2: 9.712676e-03 0.680464
2019-12-04 07:08:28,138 valid 000 4.749751e-03 0.805026
2019-12-04 07:08:29,640 validation loss; R2: 5.288648e-03 0.822505
2019-12-04 07:08:29,667 epoch 3 lr 2.000000e-03
2019-12-04 07:08:30,040 train 000 1.100548e-02 0.670879
2019-12-04 07:08:44,336 train 050 8.436908e-03 0.726885
2019-12-04 07:08:58,628 train 100 8.369847e-03 0.715113
2019-12-04 07:09:12,921 train 150 8.490211e-03 0.707517
2019-12-04 07:09:27,220 train 200 8.287440e-03 0.719109
2019-12-04 07:09:33,650 training loss; R2: 8.106062e-03 0.723049
2019-12-04 07:09:33,795 valid 000 6.074257e-03 0.726992
2019-12-04 07:09:35,297 validation loss; R2: 5.367782e-03 0.810435
2019-12-04 07:09:35,326 epoch 4 lr 2.000000e-03
2019-12-04 07:09:35,696 train 000 3.987893e-03 0.874981
2019-12-04 07:09:49,993 train 050 6.947770e-03 0.760677
2019-12-04 07:10:04,282 train 100 7.223832e-03 0.757322
2019-12-04 07:10:18,579 train 150 7.655770e-03 0.746483
2019-12-04 07:10:32,886 train 200 7.461657e-03 0.747371
2019-12-04 07:10:39,318 training loss; R2: 7.478596e-03 0.746570
2019-12-04 07:10:39,461 valid 000 4.402549e-03 0.763059
2019-12-04 07:10:40,963 validation loss; R2: 6.096974e-03 0.797230
2019-12-04 07:10:40,993 epoch 5 lr 2.000000e-03
2019-12-04 07:10:41,370 train 000 6.691711e-03 0.618572
2019-12-04 07:10:55,677 train 050 7.633301e-03 0.766183
2019-12-04 07:11:09,988 train 100 7.160299e-03 0.764636
2019-12-04 07:11:24,296 train 150 6.769662e-03 0.769566
2019-12-04 07:11:38,610 train 200 6.567167e-03 0.773694
2019-12-04 07:11:45,044 training loss; R2: 6.503137e-03 0.777902
2019-12-04 07:11:45,187 valid 000 3.943675e-03 0.898618
2019-12-04 07:11:46,689 validation loss; R2: 4.433873e-03 0.838496
2019-12-04 07:11:46,724 epoch 6 lr 2.000000e-03
2019-12-04 07:11:47,099 train 000 8.064877e-03 0.828589
2019-12-04 07:12:01,391 train 050 6.106813e-03 0.801420
2019-12-04 07:12:15,685 train 100 5.671240e-03 0.800733
2019-12-04 07:12:29,970 train 150 5.887866e-03 0.797724
2019-12-04 07:12:44,263 train 200 5.840102e-03 0.798971
2019-12-04 07:12:50,693 training loss; R2: 5.872764e-03 0.800517
2019-12-04 07:12:50,847 valid 000 2.912938e-03 0.832537
2019-12-04 07:12:52,349 validation loss; R2: 4.973911e-03 0.834465
2019-12-04 07:12:52,377 epoch 7 lr 2.000000e-03
2019-12-04 07:12:52,757 train 000 3.582116e-03 0.803261
2019-12-04 07:13:07,043 train 050 6.036450e-03 0.794159
2019-12-04 07:13:21,331 train 100 5.920425e-03 0.793641
2019-12-04 07:13:35,621 train 150 5.699687e-03 0.803608
2019-12-04 07:13:49,901 train 200 5.526718e-03 0.808704
2019-12-04 07:13:56,323 training loss; R2: 5.472713e-03 0.812013
2019-12-04 07:13:56,480 valid 000 4.379042e-03 0.850516
2019-12-04 07:13:57,981 validation loss; R2: 4.759220e-03 0.829650
2019-12-04 07:13:58,009 epoch 8 lr 2.000000e-03
2019-12-04 07:13:58,382 train 000 3.543191e-03 0.919800
2019-12-04 07:14:12,647 train 050 5.913804e-03 0.788966
2019-12-04 07:14:26,916 train 100 5.656314e-03 0.804054
2019-12-04 07:14:41,191 train 150 5.837707e-03 0.799238
2019-12-04 07:14:55,460 train 200 5.565387e-03 0.807065
2019-12-04 07:15:01,876 training loss; R2: 5.608202e-03 0.808727
2019-12-04 07:15:02,025 valid 000 3.356869e-02 -0.436318
2019-12-04 07:15:03,527 validation loss; R2: 3.664137e-02 -0.304064
2019-12-04 07:15:03,553 epoch 9 lr 2.000000e-03
2019-12-04 07:15:03,928 train 000 4.183555e-03 0.863979
2019-12-04 07:15:18,199 train 050 5.160937e-03 0.819572
2019-12-04 07:15:32,474 train 100 5.298131e-03 0.822978
2019-12-04 07:15:46,744 train 150 5.198032e-03 0.822052
2019-12-04 07:16:01,024 train 200 5.167214e-03 0.822603
2019-12-04 07:16:07,445 training loss; R2: 5.131595e-03 0.824199
2019-12-04 07:16:07,589 valid 000 4.908988e-03 0.779319
2019-12-04 07:16:09,091 validation loss; R2: 5.388745e-03 0.811758
2019-12-04 07:16:09,119 epoch 10 lr 2.000000e-03
2019-12-04 07:16:09,494 train 000 4.656565e-03 0.900012
2019-12-04 07:16:23,770 train 050 4.704439e-03 0.852951
2019-12-04 07:16:38,045 train 100 5.020169e-03 0.832390
2019-12-04 07:16:52,316 train 150 4.867249e-03 0.831649
2019-12-04 07:17:06,592 train 200 4.834213e-03 0.829160
2019-12-04 07:17:13,011 training loss; R2: 4.867135e-03 0.830402
2019-12-04 07:17:13,159 valid 000 8.716974e-01 -53.697342
2019-12-04 07:17:14,660 validation loss; R2: 8.168877e-01 -28.540865
2019-12-04 07:17:14,687 epoch 11 lr 2.000000e-03
2019-12-04 07:17:15,061 train 000 1.372606e-02 0.777877
2019-12-04 07:17:29,324 train 050 5.356368e-03 0.821641
2019-12-04 07:17:43,587 train 100 5.173234e-03 0.829809
2019-12-04 07:17:57,849 train 150 4.842520e-03 0.836720
2019-12-04 07:18:12,111 train 200 4.892700e-03 0.836362
2019-12-04 07:18:18,525 training loss; R2: 4.791380e-03 0.838329
2019-12-04 07:18:18,671 valid 000 6.638329e-02 -0.042371
2019-12-04 07:18:20,172 validation loss; R2: 3.697594e-02 -0.239313
2019-12-04 07:18:20,200 epoch 12 lr 2.000000e-03
2019-12-04 07:18:20,574 train 000 3.183600e-03 0.897318
2019-12-04 07:18:34,845 train 050 4.467711e-03 0.844917
2019-12-04 07:18:49,113 train 100 4.866442e-03 0.838931
2019-12-04 07:19:03,384 train 150 4.749053e-03 0.839585
2019-12-04 07:19:17,649 train 200 4.658998e-03 0.839900
2019-12-04 07:19:24,066 training loss; R2: 4.767992e-03 0.838412
2019-12-04 07:19:24,214 valid 000 5.298289e-02 -1.589078
2019-12-04 07:19:25,713 validation loss; R2: 5.957777e-02 -1.037042
2019-12-04 07:19:25,741 epoch 13 lr 2.000000e-03
2019-12-04 07:19:26,117 train 000 4.212796e-03 0.795493
2019-12-04 07:19:40,376 train 050 4.375075e-03 0.851375
2019-12-04 07:19:54,637 train 100 4.354080e-03 0.852234
2019-12-04 07:20:08,895 train 150 4.311578e-03 0.851228
2019-12-04 07:20:23,152 train 200 4.397243e-03 0.849372
2019-12-04 07:20:29,563 training loss; R2: 4.337909e-03 0.853024
2019-12-04 07:20:29,713 valid 000 7.090883e-02 -1.118754
2019-12-04 07:20:31,212 validation loss; R2: 6.125402e-02 -1.084364
2019-12-04 07:20:31,239 epoch 14 lr 2.000000e-03
2019-12-04 07:20:31,613 train 000 2.787490e-03 0.873552
2019-12-04 07:20:45,866 train 050 4.817410e-03 0.857391
2019-12-04 07:21:00,116 train 100 4.569283e-03 0.857841
2019-12-04 07:21:14,370 train 150 4.336037e-03 0.852288
2019-12-04 07:21:28,627 train 200 4.296944e-03 0.853289
2019-12-04 07:21:35,037 training loss; R2: 4.259109e-03 0.853378
2019-12-04 07:21:35,181 valid 000 1.367391e-01 -3.062019
2019-12-04 07:21:36,682 validation loss; R2: 1.345238e-01 -3.870721
2019-12-04 07:21:36,710 epoch 15 lr 2.000000e-03
2019-12-04 07:21:37,086 train 000 3.210475e-03 0.892005
2019-12-04 07:21:51,336 train 050 4.510136e-03 0.828688
2019-12-04 07:22:05,589 train 100 4.601627e-03 0.829942
2019-12-04 07:22:19,844 train 150 4.484976e-03 0.836152
2019-12-04 07:22:34,084 train 200 4.643654e-03 0.839072
2019-12-04 07:22:40,490 training loss; R2: 4.606609e-03 0.839767
2019-12-04 07:22:40,637 valid 000 6.596311e+00 -292.001708
2019-12-04 07:22:42,138 validation loss; R2: 6.508558e+00 -237.733225
2019-12-04 07:22:42,166 epoch 16 lr 2.000000e-03
2019-12-04 07:22:42,540 train 000 4.134994e-03 0.894969
2019-12-04 07:22:56,772 train 050 4.192999e-03 0.850302
2019-12-04 07:23:11,004 train 100 4.219340e-03 0.853683
2019-12-04 07:23:25,229 train 150 4.173582e-03 0.856072
2019-12-04 07:23:39,455 train 200 4.172349e-03 0.857018
2019-12-04 07:23:45,856 training loss; R2: 4.268384e-03 0.851632
2019-12-04 07:23:46,002 valid 000 4.320574e-02 -0.849319
2019-12-04 07:23:47,502 validation loss; R2: 4.728165e-02 -0.637830
2019-12-04 07:23:47,529 epoch 17 lr 2.000000e-03
2019-12-04 07:23:47,902 train 000 9.122486e-03 0.813370
2019-12-04 07:24:02,139 train 050 4.226875e-03 0.859689
2019-12-04 07:24:16,368 train 100 4.573109e-03 0.846261
2019-12-04 07:24:30,594 train 150 4.737632e-03 0.838316
2019-12-04 07:24:44,813 train 200 4.778396e-03 0.836031
2019-12-04 07:24:51,208 training loss; R2: 4.707914e-03 0.836853
2019-12-04 07:24:51,351 valid 000 3.529868e-01 -4.329463
2019-12-04 07:24:52,851 validation loss; R2: 2.741568e-01 -9.199213
2019-12-04 07:24:52,880 epoch 18 lr 2.000000e-03
2019-12-04 07:24:53,265 train 000 4.461376e-03 0.870505
2019-12-04 07:25:07,868 train 050 4.057635e-03 0.868442
2019-12-04 07:25:22,462 train 100 4.202951e-03 0.855434
2019-12-04 07:25:37,056 train 150 4.235397e-03 0.853342
2019-12-04 07:25:51,649 train 200 4.214926e-03 0.854107
2019-12-04 07:25:58,216 training loss; R2: 4.244918e-03 0.854360
2019-12-04 07:25:58,360 valid 000 6.972336e-02 -0.825610
2019-12-04 07:25:59,860 validation loss; R2: 6.884859e-02 -1.392784
2019-12-04 07:25:59,889 epoch 19 lr 2.000000e-03
2019-12-04 07:26:00,271 train 000 4.410050e-03 0.781612
2019-12-04 07:26:14,862 train 050 4.376097e-03 0.850521
2019-12-04 07:26:29,445 train 100 4.127585e-03 0.855792
2019-12-04 07:26:44,039 train 150 4.175495e-03 0.854466
2019-12-04 07:26:58,625 train 200 4.124144e-03 0.857775
2019-12-04 07:27:05,188 training loss; R2: 4.134961e-03 0.858151
2019-12-04 07:27:05,336 valid 000 8.934708e-01 -43.017443
2019-12-04 07:27:06,836 validation loss; R2: 8.826713e-01 -32.544044
