2019-12-04 06:29:12,706 gpu device = 1
2019-12-04 06:29:12,706 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=96, cutout=False, cutout_length=16, data='../data', dataset='graphene', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name=None, gpu=1, grad_clip=5, gz_dtree=False, init_channels=36, layers=8, learning_rate=0.002, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-graphene-EXP-20191204-062912', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-12-04 06:29:15,905 param size = 1.334701MB
2019-12-04 06:29:15,908 epoch 0 lr 2.000000e-03
2019-12-04 06:29:18,700 train 000 6.274180e-01 -35.088930
2019-12-04 06:29:33,309 train 050 1.747733e-01 -6.056566
2019-12-04 06:29:47,813 train 100 1.033514e-01 -3.057536
2019-12-04 06:30:02,313 train 150 7.805496e-02 -1.995729
2019-12-04 06:30:16,810 train 200 6.429995e-02 -1.430131
2019-12-04 06:30:24,483 training loss; R2: 6.053205e-02 -1.269144
2019-12-04 06:30:24,637 valid 000 7.362229e-03 0.575235
2019-12-04 06:30:26,460 validation loss; R2: 1.533013e-02 0.522009
2019-12-04 06:30:26,489 epoch 1 lr 2.000000e-03
2019-12-04 06:30:26,981 train 000 3.106906e-02 0.178123
2019-12-04 06:30:41,114 train 050 2.214699e-02 0.330997
2019-12-04 06:30:55,247 train 100 2.052258e-02 0.350971
2019-12-04 06:31:09,363 train 150 2.083562e-02 0.333949
2019-12-04 06:31:23,487 train 200 2.004392e-02 0.355441
2019-12-04 06:31:29,842 training loss; R2: 1.953067e-02 0.369529
2019-12-04 06:31:29,989 valid 000 8.843829e-03 0.656511
2019-12-04 06:31:31,481 validation loss; R2: 1.007070e-02 0.681389
2019-12-04 06:31:31,509 epoch 2 lr 2.000000e-03
2019-12-04 06:31:31,878 train 000 8.498634e-03 0.633845
2019-12-04 06:31:46,009 train 050 1.360424e-02 0.515453
2019-12-04 06:32:00,138 train 100 1.318099e-02 0.546869
2019-12-04 06:32:14,261 train 150 1.400041e-02 0.523714
2019-12-04 06:32:28,381 train 200 1.405842e-02 0.528054
2019-12-04 06:32:34,734 training loss; R2: 1.385892e-02 0.536092
2019-12-04 06:32:34,876 valid 000 5.461109e-03 0.793559
2019-12-04 06:32:36,368 validation loss; R2: 7.648279e-03 0.749049
2019-12-04 06:32:36,395 epoch 3 lr 2.000000e-03
2019-12-04 06:32:36,764 train 000 9.036929e-03 0.754249
2019-12-04 06:32:50,890 train 050 1.153379e-02 0.586690
2019-12-04 06:33:05,021 train 100 1.116549e-02 0.611068
2019-12-04 06:33:19,147 train 150 1.119474e-02 0.626241
2019-12-04 06:33:33,276 train 200 1.099487e-02 0.631553
2019-12-04 06:33:39,629 training loss; R2: 1.088579e-02 0.639099
2019-12-04 06:33:39,771 valid 000 7.866400e-03 0.501620
2019-12-04 06:33:41,263 validation loss; R2: 1.195049e-02 0.603600
2019-12-04 06:33:41,290 epoch 4 lr 2.000000e-03
2019-12-04 06:33:41,658 train 000 1.371275e-02 0.652414
2019-12-04 06:33:55,778 train 050 9.147167e-03 0.672743
2019-12-04 06:34:09,908 train 100 9.429960e-03 0.659296
2019-12-04 06:34:24,040 train 150 9.548750e-03 0.663719
2019-12-04 06:34:38,172 train 200 9.486895e-03 0.676914
2019-12-04 06:34:44,526 training loss; R2: 9.460233e-03 0.682681
2019-12-04 06:34:44,667 valid 000 5.652244e-03 0.836118
2019-12-04 06:34:46,160 validation loss; R2: 6.338766e-03 0.799645
2019-12-04 06:34:46,187 epoch 5 lr 2.000000e-03
2019-12-04 06:34:46,557 train 000 2.250603e-02 0.598760
2019-12-04 06:35:00,687 train 050 9.518241e-03 0.657944
2019-12-04 06:35:14,824 train 100 8.832083e-03 0.698298
2019-12-04 06:35:28,945 train 150 9.120380e-03 0.679779
2019-12-04 06:35:43,082 train 200 8.920364e-03 0.694348
2019-12-04 06:35:49,441 training loss; R2: 8.836175e-03 0.697729
2019-12-04 06:35:49,582 valid 000 4.111115e-03 0.872044
2019-12-04 06:35:51,075 validation loss; R2: 4.956411e-03 0.839942
2019-12-04 06:35:51,102 epoch 6 lr 2.000000e-03
2019-12-04 06:35:51,469 train 000 8.278993e-03 0.594288
2019-12-04 06:36:05,591 train 050 7.541696e-03 0.752464
2019-12-04 06:36:19,721 train 100 7.526741e-03 0.745836
2019-12-04 06:36:33,850 train 150 7.773912e-03 0.737991
2019-12-04 06:36:47,971 train 200 7.855293e-03 0.736662
2019-12-04 06:36:54,328 training loss; R2: 7.794901e-03 0.737488
2019-12-04 06:36:54,475 valid 000 6.693572e-03 0.785887
2019-12-04 06:36:55,968 validation loss; R2: 5.854561e-03 0.801063
2019-12-04 06:36:55,995 epoch 7 lr 2.000000e-03
2019-12-04 06:36:56,363 train 000 8.797267e-03 0.664029
2019-12-04 06:37:10,494 train 050 7.160170e-03 0.741447
2019-12-04 06:37:24,625 train 100 7.583764e-03 0.745197
2019-12-04 06:37:38,749 train 150 7.759028e-03 0.739033
2019-12-04 06:37:52,884 train 200 7.896581e-03 0.733641
2019-12-04 06:37:59,241 training loss; R2: 7.796990e-03 0.738112
2019-12-04 06:37:59,384 valid 000 2.485026e-03 0.742527
2019-12-04 06:38:00,877 validation loss; R2: 4.032973e-03 0.862671
2019-12-04 06:38:00,910 epoch 8 lr 2.000000e-03
2019-12-04 06:38:01,281 train 000 1.233189e-02 0.733610
2019-12-04 06:38:15,422 train 050 7.366546e-03 0.743685
2019-12-04 06:38:29,553 train 100 7.309256e-03 0.752194
2019-12-04 06:38:43,689 train 150 7.333874e-03 0.749823
2019-12-04 06:38:57,809 train 200 7.328945e-03 0.746997
2019-12-04 06:39:04,166 training loss; R2: 7.353455e-03 0.746610
2019-12-04 06:39:04,308 valid 000 3.033628e-02 0.508932
2019-12-04 06:39:05,801 validation loss; R2: 1.859870e-02 0.384332
2019-12-04 06:39:05,827 epoch 9 lr 2.000000e-03
2019-12-04 06:39:06,195 train 000 9.201734e-03 0.753285
2019-12-04 06:39:20,321 train 050 6.955043e-03 0.760942
2019-12-04 06:39:34,453 train 100 6.608411e-03 0.773455
2019-12-04 06:39:48,580 train 150 6.594856e-03 0.778667
2019-12-04 06:40:02,714 train 200 6.520558e-03 0.783829
2019-12-04 06:40:09,065 training loss; R2: 6.504697e-03 0.784776
2019-12-04 06:40:09,207 valid 000 4.315827e-03 0.799689
2019-12-04 06:40:10,700 validation loss; R2: 7.831915e-03 0.750017
2019-12-04 06:40:10,727 epoch 10 lr 2.000000e-03
2019-12-04 06:40:11,100 train 000 1.107586e-02 0.724171
2019-12-04 06:40:25,235 train 050 6.699461e-03 0.732378
2019-12-04 06:40:39,371 train 100 6.687765e-03 0.754055
2019-12-04 06:40:53,505 train 150 6.543701e-03 0.764991
2019-12-04 06:41:07,636 train 200 6.682994e-03 0.766355
2019-12-04 06:41:13,992 training loss; R2: 6.703644e-03 0.767459
2019-12-04 06:41:14,136 valid 000 5.767799e-03 0.839520
2019-12-04 06:41:15,629 validation loss; R2: 3.830584e-03 0.869111
2019-12-04 06:41:15,656 epoch 11 lr 2.000000e-03
2019-12-04 06:41:16,026 train 000 1.285911e-02 0.748660
2019-12-04 06:41:30,161 train 050 6.864747e-03 0.782346
2019-12-04 06:41:44,296 train 100 6.750805e-03 0.780223
2019-12-04 06:41:58,418 train 150 6.515274e-03 0.778900
2019-12-04 06:42:12,542 train 200 6.418895e-03 0.781253
2019-12-04 06:42:18,891 training loss; R2: 6.475341e-03 0.783958
2019-12-04 06:42:19,034 valid 000 1.016841e-02 0.642895
2019-12-04 06:42:20,526 validation loss; R2: 1.024396e-02 0.666627
2019-12-04 06:42:20,553 epoch 12 lr 2.000000e-03
2019-12-04 06:42:20,922 train 000 5.827357e-03 0.849018
2019-12-04 06:42:35,042 train 050 6.675987e-03 0.788497
2019-12-04 06:42:49,162 train 100 6.324636e-03 0.786638
2019-12-04 06:43:03,278 train 150 6.740305e-03 0.771824
2019-12-04 06:43:17,395 train 200 6.600238e-03 0.776752
2019-12-04 06:43:23,746 training loss; R2: 6.531065e-03 0.777248
2019-12-04 06:43:23,888 valid 000 4.863957e-03 0.911450
2019-12-04 06:43:25,381 validation loss; R2: 3.704323e-03 0.874593
2019-12-04 06:43:25,410 epoch 13 lr 2.000000e-03
2019-12-04 06:43:25,776 train 000 6.131345e-03 0.685108
2019-12-04 06:43:39,897 train 050 5.384825e-03 0.811690
2019-12-04 06:43:54,023 train 100 5.474255e-03 0.805317
2019-12-04 06:44:08,146 train 150 6.140060e-03 0.790690
2019-12-04 06:44:22,271 train 200 6.044969e-03 0.799237
2019-12-04 06:44:28,617 training loss; R2: 5.969007e-03 0.799939
2019-12-04 06:44:28,764 valid 000 2.951581e-03 0.891315
2019-12-04 06:44:30,256 validation loss; R2: 3.852108e-03 0.874904
2019-12-04 06:44:30,282 epoch 14 lr 2.000000e-03
2019-12-04 06:44:30,650 train 000 6.918656e-03 0.855891
2019-12-04 06:44:44,762 train 050 7.190311e-03 0.776033
2019-12-04 06:44:58,877 train 100 6.659262e-03 0.777005
2019-12-04 06:45:12,980 train 150 6.353037e-03 0.788612
2019-12-04 06:45:27,083 train 200 6.360816e-03 0.783772
2019-12-04 06:45:33,430 training loss; R2: 6.348758e-03 0.786045
2019-12-04 06:45:33,570 valid 000 4.282171e-03 0.874460
2019-12-04 06:45:35,062 validation loss; R2: 4.136534e-03 0.853849
2019-12-04 06:45:35,089 epoch 15 lr 2.000000e-03
2019-12-04 06:45:35,457 train 000 5.607560e-03 0.879959
2019-12-04 06:45:49,548 train 050 5.825031e-03 0.815127
2019-12-04 06:46:03,641 train 100 5.926047e-03 0.807486
2019-12-04 06:46:17,734 train 150 5.853864e-03 0.803513
2019-12-04 06:46:31,824 train 200 5.778574e-03 0.801804
2019-12-04 06:46:38,163 training loss; R2: 5.987070e-03 0.796041
2019-12-04 06:46:38,307 valid 000 4.185298e-03 0.888922
2019-12-04 06:46:39,799 validation loss; R2: 3.923538e-03 0.870369
2019-12-04 06:46:39,833 epoch 16 lr 2.000000e-03
2019-12-04 06:46:40,204 train 000 3.838025e-03 0.901041
2019-12-04 06:46:54,656 train 050 5.594157e-03 0.807473
2019-12-04 06:47:09,119 train 100 5.651042e-03 0.803592
2019-12-04 06:47:23,578 train 150 5.991888e-03 0.802803
2019-12-04 06:47:38,030 train 200 6.011654e-03 0.801786
2019-12-04 06:47:44,531 training loss; R2: 6.049622e-03 0.798717
2019-12-04 06:47:44,673 valid 000 2.089647e-03 0.871972
2019-12-04 06:47:46,165 validation loss; R2: 3.453017e-03 0.887165
2019-12-04 06:47:46,192 epoch 17 lr 2.000000e-03
2019-12-04 06:47:46,567 train 000 5.507968e-03 0.853483
2019-12-04 06:48:00,804 train 050 6.231368e-03 0.806707
2019-12-04 06:48:14,873 train 100 5.854874e-03 0.802325
2019-12-04 06:48:28,933 train 150 5.631793e-03 0.808396
2019-12-04 06:48:42,995 train 200 5.511420e-03 0.810010
2019-12-04 06:48:49,318 training loss; R2: 5.548243e-03 0.810910
2019-12-04 06:48:49,462 valid 000 3.992293e-03 0.788001
2019-12-04 06:48:50,952 validation loss; R2: 3.685558e-03 0.873417
2019-12-04 06:48:50,979 epoch 18 lr 2.000000e-03
2019-12-04 06:48:51,346 train 000 6.068893e-03 0.818561
2019-12-04 06:49:05,416 train 050 5.606081e-03 0.807942
2019-12-04 06:49:19,477 train 100 5.592351e-03 0.813680
2019-12-04 06:49:33,537 train 150 5.457024e-03 0.814740
2019-12-04 06:49:47,598 train 200 5.588933e-03 0.811722
2019-12-04 06:49:53,921 training loss; R2: 5.579462e-03 0.811664
2019-12-04 06:49:54,063 valid 000 3.733080e-03 0.857523
2019-12-04 06:49:55,553 validation loss; R2: 4.050680e-03 0.861627
2019-12-04 06:49:55,578 epoch 19 lr 2.000000e-03
2019-12-04 06:49:55,948 train 000 5.631462e-03 0.800005
2019-12-04 06:50:09,992 train 050 6.019400e-03 0.780400
2019-12-04 06:50:24,039 train 100 5.986084e-03 0.787614
2019-12-04 06:50:38,078 train 150 6.091719e-03 0.789109
2019-12-04 06:50:52,114 train 200 6.063556e-03 0.792039
2019-12-04 06:50:58,428 training loss; R2: 5.973514e-03 0.795519
2019-12-04 06:50:58,573 valid 000 7.070735e-01 -28.291610
2019-12-04 06:51:00,064 validation loss; R2: 6.999499e-01 -24.930795
