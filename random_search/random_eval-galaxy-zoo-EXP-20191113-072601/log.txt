2019-11-13 07:26:01,414 gpu device = 1
2019-11-13 07:26:01,415 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name='galaxy-zoo-cleaned', gpu=1, grad_clip=5, gz_dtree=False, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-galaxy-zoo-EXP-20191113-072601', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-11-13 07:26:12,955 param size = 0.325045MB
2019-11-13 07:26:12,960 epoch 0 lr 1.000000e-03
2019-11-13 07:26:15,196 train 000 1.830745e-01 -157.802781
2019-11-13 07:26:23,151 train 050 4.617117e-02 -5.505675
2019-11-13 07:26:30,868 train 100 3.542869e-02 -27.425948
2019-11-13 07:26:38,647 train 150 3.163416e-02 -18.447119
2019-11-13 07:26:46,508 train 200 2.949551e-02 -13.961057
2019-11-13 07:26:54,173 train 250 2.791664e-02 -11.201958
2019-11-13 07:27:01,818 train 300 2.672363e-02 -9.347943
2019-11-13 07:27:09,526 train 350 2.584399e-02 -8.017519
2019-11-13 07:27:17,141 train 400 2.518844e-02 -7.013556
2019-11-13 07:27:24,751 train 450 2.461581e-02 -6.254294
2019-11-13 07:27:32,357 train 500 2.408477e-02 -5.626062
2019-11-13 07:27:39,967 train 550 2.366917e-02 -5.112629
2019-11-13 07:27:47,571 train 600 2.331341e-02 -4.684271
2019-11-13 07:27:55,178 train 650 2.297524e-02 -4.316587
2019-11-13 07:28:02,775 train 700 2.263585e-02 -4.002235
2019-11-13 07:28:10,378 train 750 2.234678e-02 -3.728607
2019-11-13 07:28:18,011 train 800 2.207618e-02 -3.490387
2019-11-13 07:28:25,632 train 850 2.181541e-02 -3.280749
2019-11-13 07:28:28,732 training loss; R2: 2.172633e-02 -3.222254
2019-11-13 07:28:29,046 valid 000 1.725274e-02 -0.137255
2019-11-13 07:28:30,725 valid 050 1.721523e-02 0.142375
2019-11-13 07:28:32,327 validation loss; R2: 1.733982e-02 0.137611
2019-11-13 07:28:32,347 epoch 1 lr 1.000000e-03
2019-11-13 07:28:32,950 train 000 1.604081e-02 0.158300
2019-11-13 07:28:40,559 train 050 1.724826e-02 0.102410
2019-11-13 07:28:48,179 train 100 1.693613e-02 0.121381
2019-11-13 07:28:55,798 train 150 1.676431e-02 0.133439
2019-11-13 07:29:03,414 train 200 1.669851e-02 0.133765
2019-11-13 07:29:11,029 train 250 1.666440e-02 0.099305
2019-11-13 07:29:18,650 train 300 1.663741e-02 0.098539
2019-11-13 07:29:26,262 train 350 1.666059e-02 0.106937
2019-11-13 07:29:33,892 train 400 1.663346e-02 0.113101
2019-11-13 07:29:41,510 train 450 1.656645e-02 0.120275
2019-11-13 07:29:49,126 train 500 1.648419e-02 0.124393
2019-11-13 07:29:56,740 train 550 1.644014e-02 0.125772
2019-11-13 07:30:04,359 train 600 1.637680e-02 0.131048
2019-11-13 07:30:11,972 train 650 1.632116e-02 0.137173
2019-11-13 07:30:19,583 train 700 1.627159e-02 0.141541
2019-11-13 07:30:27,196 train 750 1.617983e-02 0.146009
2019-11-13 07:30:34,807 train 800 1.612169e-02 0.146005
2019-11-13 07:30:42,416 train 850 1.605835e-02 0.148289
2019-11-13 07:30:44,695 training loss; R2: 1.604331e-02 0.146768
2019-11-13 07:30:44,990 valid 000 1.771969e-02 -0.091367
2019-11-13 07:30:46,678 valid 050 1.564681e-02 0.177410
2019-11-13 07:30:48,240 validation loss; R2: 1.552253e-02 0.168610
2019-11-13 07:30:48,261 epoch 2 lr 1.000000e-03
2019-11-13 07:30:48,686 train 000 1.400898e-02 0.239590
2019-11-13 07:30:56,343 train 050 1.468803e-02 0.204312
2019-11-13 07:31:03,963 train 100 1.489186e-02 0.211952
2019-11-13 07:31:11,620 train 150 1.476800e-02 0.212030
2019-11-13 07:31:19,247 train 200 1.471909e-02 0.150978
2019-11-13 07:31:26,873 train 250 1.467652e-02 0.161513
2019-11-13 07:31:34,499 train 300 1.463704e-02 0.172950
2019-11-13 07:31:42,121 train 350 1.460903e-02 0.156691
2019-11-13 07:31:49,743 train 400 1.457367e-02 0.165310
2019-11-13 07:31:57,369 train 450 1.452319e-02 0.174282
2019-11-13 07:32:04,988 train 500 1.449330e-02 0.180892
2019-11-13 07:32:12,608 train 550 1.450102e-02 0.184248
2019-11-13 07:32:20,229 train 600 1.450174e-02 0.186141
2019-11-13 07:32:27,863 train 650 1.447849e-02 0.189949
2019-11-13 07:32:35,494 train 700 1.444588e-02 0.189650
2019-11-13 07:32:43,122 train 750 1.439586e-02 0.192758
2019-11-13 07:32:50,748 train 800 1.434339e-02 0.195010
2019-11-13 07:32:58,374 train 850 1.430438e-02 0.170433
2019-11-13 07:33:00,656 training loss; R2: 1.429151e-02 0.171172
2019-11-13 07:33:00,964 valid 000 1.294482e-02 0.347013
2019-11-13 07:33:02,675 valid 050 1.237901e-02 0.272616
2019-11-13 07:33:04,233 validation loss; R2: 1.251194e-02 0.261572
2019-11-13 07:33:04,253 epoch 3 lr 1.000000e-03
2019-11-13 07:33:04,662 train 000 1.288541e-02 0.278904
2019-11-13 07:33:12,457 train 050 1.357435e-02 0.170871
2019-11-13 07:33:20,139 train 100 1.349704e-02 0.214299
2019-11-13 07:33:27,773 train 150 1.357310e-02 0.206458
2019-11-13 07:33:35,394 train 200 1.360342e-02 0.212587
2019-11-13 07:33:43,010 train 250 1.358122e-02 0.219265
2019-11-13 07:33:50,626 train 300 1.353849e-02 0.227483
2019-11-13 07:33:58,247 train 350 1.350172e-02 0.230135
2019-11-13 07:34:05,863 train 400 1.346668e-02 0.232374
2019-11-13 07:34:13,478 train 450 1.343451e-02 0.227767
2019-11-13 07:34:21,091 train 500 1.342401e-02 0.229308
2019-11-13 07:34:28,716 train 550 1.340712e-02 0.231932
2019-11-13 07:34:36,337 train 600 1.335977e-02 0.232966
2019-11-13 07:34:43,949 train 650 1.332428e-02 0.230612
2019-11-13 07:34:51,568 train 700 1.332080e-02 0.233761
2019-11-13 07:34:59,225 train 750 1.334326e-02 0.234054
2019-11-13 07:35:06,839 train 800 1.333266e-02 0.234194
2019-11-13 07:35:14,450 train 850 1.329418e-02 0.228933
2019-11-13 07:35:16,725 training loss; R2: 1.328976e-02 0.230325
2019-11-13 07:35:17,032 valid 000 1.017117e-02 0.379592
2019-11-13 07:35:18,732 valid 050 1.237131e-02 0.328153
2019-11-13 07:35:20,277 validation loss; R2: 1.238771e-02 0.324440
2019-11-13 07:35:20,297 epoch 4 lr 1.000000e-03
2019-11-13 07:35:20,699 train 000 1.430295e-02 0.349398
2019-11-13 07:35:28,491 train 050 1.286699e-02 0.262290
2019-11-13 07:35:36,116 train 100 1.268459e-02 0.272861
2019-11-13 07:35:43,737 train 150 1.262570e-02 0.274478
2019-11-13 07:35:51,352 train 200 1.266995e-02 0.272806
2019-11-13 07:35:58,963 train 250 1.268021e-02 0.266675
2019-11-13 07:36:06,572 train 300 1.269008e-02 0.270255
2019-11-13 07:36:14,184 train 350 1.268074e-02 0.272658
2019-11-13 07:36:21,791 train 400 1.267663e-02 0.259045
2019-11-13 07:36:29,398 train 450 1.266305e-02 0.260461
2019-11-13 07:36:37,005 train 500 1.267529e-02 0.257875
2019-11-13 07:36:44,607 train 550 1.268619e-02 0.260634
2019-11-13 07:36:52,214 train 600 1.267893e-02 0.256478
2019-11-13 07:36:59,822 train 650 1.268110e-02 0.253855
2019-11-13 07:37:07,431 train 700 1.268418e-02 0.254798
2019-11-13 07:37:15,038 train 750 1.264554e-02 0.255003
2019-11-13 07:37:22,644 train 800 1.264243e-02 0.256926
2019-11-13 07:37:30,254 train 850 1.261649e-02 0.257482
2019-11-13 07:37:32,530 training loss; R2: 1.260852e-02 0.257609
2019-11-13 07:37:32,836 valid 000 1.597241e-02 0.171099
2019-11-13 07:37:34,549 valid 050 1.717527e-02 -0.046175
2019-11-13 07:37:36,072 validation loss; R2: 1.697690e-02 0.036328
2019-11-13 07:37:36,093 epoch 5 lr 1.000000e-03
2019-11-13 07:37:36,505 train 000 1.209336e-02 0.360166
2019-11-13 07:37:44,145 train 050 1.243659e-02 0.117034
2019-11-13 07:37:51,755 train 100 1.255869e-02 0.197192
2019-11-13 07:37:59,368 train 150 1.245218e-02 0.223923
2019-11-13 07:38:06,984 train 200 1.237728e-02 0.236468
2019-11-13 07:38:14,597 train 250 1.235380e-02 0.243743
2019-11-13 07:38:22,212 train 300 1.232254e-02 0.253139
2019-11-13 07:38:29,860 train 350 1.231329e-02 0.254831
2019-11-13 07:38:37,469 train 400 1.228139e-02 0.249546
2019-11-13 07:38:45,078 train 450 1.227775e-02 0.242268
2019-11-13 07:38:52,680 train 500 1.226199e-02 0.246604
2019-11-13 07:39:00,289 train 550 1.221417e-02 0.252612
2019-11-13 07:39:07,902 train 600 1.220607e-02 0.254336
2019-11-13 07:39:15,506 train 650 1.219724e-02 0.248530
2019-11-13 07:39:23,112 train 700 1.219476e-02 0.251826
2019-11-13 07:39:30,717 train 750 1.217732e-02 0.255914
2019-11-13 07:39:38,339 train 800 1.216767e-02 0.256902
2019-11-13 07:39:45,947 train 850 1.214841e-02 0.249261
2019-11-13 07:39:48,223 training loss; R2: 1.214402e-02 0.123923
2019-11-13 07:39:48,521 valid 000 1.391352e-02 0.293416
2019-11-13 07:39:50,244 valid 050 1.242795e-02 0.248934
2019-11-13 07:39:51,780 validation loss; R2: 1.237499e-02 0.248724
2019-11-13 07:39:51,801 epoch 6 lr 1.000000e-03
2019-11-13 07:39:52,212 train 000 1.139745e-02 0.312331
2019-11-13 07:39:59,847 train 050 1.217404e-02 0.261493
2019-11-13 07:40:07,454 train 100 1.192914e-02 0.274077
2019-11-13 07:40:15,055 train 150 1.188883e-02 0.233940
2019-11-13 07:40:22,653 train 200 1.187191e-02 0.237379
2019-11-13 07:40:30,257 train 250 1.184786e-02 0.247444
2019-11-13 07:40:37,856 train 300 1.183878e-02 0.249606
2019-11-13 07:40:45,456 train 350 1.182320e-02 0.251276
2019-11-13 07:40:53,055 train 400 1.184563e-02 0.259316
2019-11-13 07:41:00,654 train 450 1.179660e-02 0.250265
2019-11-13 07:41:08,258 train 500 1.181492e-02 0.255753
2019-11-13 07:41:15,854 train 550 1.182983e-02 0.258659
2019-11-13 07:41:23,506 train 600 1.181547e-02 0.262765
2019-11-13 07:41:31,103 train 650 1.180972e-02 0.263469
2019-11-13 07:41:38,709 train 700 1.180844e-02 0.266963
2019-11-13 07:41:46,314 train 750 1.180656e-02 0.264397
2019-11-13 07:41:53,912 train 800 1.183470e-02 0.261448
2019-11-13 07:42:01,508 train 850 1.182002e-02 0.060729
2019-11-13 07:42:03,787 training loss; R2: 1.182126e-02 0.064899
2019-11-13 07:42:04,085 valid 000 1.231865e-02 0.302037
2019-11-13 07:42:05,779 valid 050 1.206370e-02 0.265383
2019-11-13 07:42:07,321 validation loss; R2: 1.223473e-02 0.258441
2019-11-13 07:42:07,342 epoch 7 lr 1.000000e-03
2019-11-13 07:42:07,751 train 000 1.207367e-02 0.368746
2019-11-13 07:42:15,552 train 050 1.175945e-02 0.313994
2019-11-13 07:42:23,171 train 100 1.162417e-02 0.320881
2019-11-13 07:42:30,776 train 150 1.171757e-02 0.314221
2019-11-13 07:42:38,371 train 200 1.176657e-02 0.306975
2019-11-13 07:42:45,972 train 250 1.170972e-02 0.308197
2019-11-13 07:42:53,570 train 300 1.163550e-02 0.301439
2019-11-13 07:43:01,169 train 350 1.161300e-02 0.304335
2019-11-13 07:43:08,763 train 400 1.161261e-02 0.305109
2019-11-13 07:43:16,361 train 450 1.159688e-02 0.305110
2019-11-13 07:43:23,961 train 500 1.156098e-02 0.305047
2019-11-13 07:43:31,554 train 550 1.155789e-02 0.305000
2019-11-13 07:43:39,152 train 600 1.154603e-02 0.304970
2019-11-13 07:43:46,742 train 650 1.153842e-02 0.299980
2019-11-13 07:43:54,340 train 700 1.151060e-02 0.297789
2019-11-13 07:44:01,937 train 750 1.149387e-02 0.296219
2019-11-13 07:44:09,530 train 800 1.149047e-02 0.293591
2019-11-13 07:44:17,130 train 850 1.148226e-02 0.295728
2019-11-13 07:44:19,402 training loss; R2: 1.148051e-02 0.296059
2019-11-13 07:44:19,713 valid 000 9.787242e-03 0.366454
2019-11-13 07:44:21,423 valid 050 1.046569e-02 -1.094094
2019-11-13 07:44:22,949 validation loss; R2: 1.038313e-02 -0.436991
2019-11-13 07:44:22,976 epoch 8 lr 1.000000e-03
2019-11-13 07:44:23,368 train 000 1.278617e-02 0.300658
2019-11-13 07:44:31,199 train 050 1.146207e-02 0.281329
2019-11-13 07:44:38,815 train 100 1.147286e-02 0.298430
2019-11-13 07:44:46,426 train 150 1.149926e-02 0.307200
2019-11-13 07:44:54,035 train 200 1.144900e-02 0.304823
2019-11-13 07:45:01,643 train 250 1.137621e-02 0.312397
2019-11-13 07:45:09,258 train 300 1.126995e-02 0.318393
2019-11-13 07:45:16,858 train 350 1.124487e-02 0.318051
2019-11-13 07:45:24,464 train 400 1.124450e-02 0.315057
2019-11-13 07:45:32,072 train 450 1.126529e-02 0.314297
2019-11-13 07:45:39,679 train 500 1.127008e-02 0.314434
2019-11-13 07:45:47,281 train 550 1.125172e-02 0.313623
2019-11-13 07:45:54,884 train 600 1.127858e-02 0.310866
2019-11-13 07:46:02,493 train 650 1.128943e-02 0.305697
2019-11-13 07:46:10,096 train 700 1.131629e-02 0.304762
2019-11-13 07:46:17,701 train 750 1.131353e-02 0.301662
2019-11-13 07:46:25,305 train 800 1.133128e-02 0.301360
2019-11-13 07:46:32,914 train 850 1.132782e-02 0.301261
2019-11-13 07:46:35,193 training loss; R2: 1.132588e-02 0.302368
2019-11-13 07:46:35,500 valid 000 9.928836e-03 0.413820
2019-11-13 07:46:37,188 valid 050 1.010711e-02 0.337343
2019-11-13 07:46:38,727 validation loss; R2: 1.014191e-02 0.324793
2019-11-13 07:46:38,759 epoch 9 lr 1.000000e-03
2019-11-13 07:46:39,180 train 000 1.183838e-02 0.361162
2019-11-13 07:46:46,853 train 050 1.092777e-02 0.294862
2019-11-13 07:46:54,455 train 100 1.107309e-02 0.276993
2019-11-13 07:47:02,056 train 150 1.103592e-02 0.292277
2019-11-13 07:47:09,650 train 200 1.115150e-02 0.281567
2019-11-13 07:47:17,299 train 250 1.116132e-02 0.290032
2019-11-13 07:47:24,888 train 300 1.115806e-02 0.243955
2019-11-13 07:47:32,483 train 350 1.115996e-02 0.255692
2019-11-13 07:47:40,077 train 400 1.114959e-02 0.264103
2019-11-13 07:47:47,667 train 450 1.114589e-02 0.271348
2019-11-13 07:47:55,257 train 500 1.112780e-02 0.277219
2019-11-13 07:48:02,847 train 550 1.112549e-02 0.274374
2019-11-13 07:48:10,435 train 600 1.113123e-02 0.277787
2019-11-13 07:48:18,024 train 650 1.111545e-02 0.283325
2019-11-13 07:48:25,606 train 700 1.109880e-02 0.285987
2019-11-13 07:48:33,194 train 750 1.108576e-02 0.289624
2019-11-13 07:48:40,777 train 800 1.109317e-02 0.291677
2019-11-13 07:48:48,390 train 850 1.109850e-02 0.293722
2019-11-13 07:48:50,656 training loss; R2: 1.110030e-02 0.294182
2019-11-13 07:48:50,966 valid 000 3.728035e-02 -3.378751
2019-11-13 07:48:52,674 valid 050 3.800812e-02 -4.703689
2019-11-13 07:48:54,224 validation loss; R2: 3.798081e-02 -5.061707
2019-11-13 07:48:54,246 epoch 10 lr 1.000000e-03
2019-11-13 07:48:54,624 train 000 1.245172e-02 -0.724055
2019-11-13 07:49:02,495 train 050 1.108126e-02 0.281206
2019-11-13 07:49:10,163 train 100 1.122943e-02 0.303096
2019-11-13 07:49:17,818 train 150 1.119056e-02 0.297111
2019-11-13 07:49:25,429 train 200 1.118379e-02 0.296779
2019-11-13 07:49:33,027 train 250 1.115317e-02 0.302705
2019-11-13 07:49:40,630 train 300 1.107792e-02 0.302384
2019-11-13 07:49:48,226 train 350 1.109364e-02 0.060543
2019-11-13 07:49:55,817 train 400 1.115029e-02 0.086175
2019-11-13 07:50:03,409 train 450 1.114905e-02 0.108881
2019-11-13 07:50:10,999 train 500 1.113538e-02 0.115668
2019-11-13 07:50:18,587 train 550 1.114527e-02 0.127169
2019-11-13 07:50:26,175 train 600 1.114433e-02 0.139686
2019-11-13 07:50:33,783 train 650 1.114449e-02 0.153673
2019-11-13 07:50:41,377 train 700 1.111401e-02 0.166871
2019-11-13 07:50:48,973 train 750 1.109378e-02 0.175895
2019-11-13 07:50:56,574 train 800 1.108003e-02 0.185186
2019-11-13 07:51:04,164 train 850 1.108665e-02 0.194239
2019-11-13 07:51:06,434 training loss; R2: 1.108018e-02 0.195764
2019-11-13 07:51:06,750 valid 000 2.431147e-01 -64.950465
2019-11-13 07:51:08,469 valid 050 2.433389e-01 -144.022141
2019-11-13 07:51:10,009 validation loss; R2: 2.434683e-01 -130.756982
2019-11-13 07:51:10,043 epoch 11 lr 1.000000e-03
2019-11-13 07:51:10,451 train 000 1.164606e-02 0.340853
2019-11-13 07:51:18,095 train 050 1.068952e-02 0.340177
2019-11-13 07:51:25,793 train 100 1.075115e-02 0.339166
2019-11-13 07:51:33,572 train 150 1.072375e-02 0.329646
2019-11-13 07:51:41,256 train 200 1.073470e-02 0.337178
2019-11-13 07:51:48,888 train 250 1.083474e-02 0.330253
2019-11-13 07:51:56,515 train 300 1.084236e-02 0.320080
2019-11-13 07:52:04,137 train 350 1.083035e-02 0.321356
2019-11-13 07:52:11,780 train 400 1.084799e-02 0.317782
2019-11-13 07:52:19,402 train 450 1.087203e-02 0.316378
2019-11-13 07:52:27,146 train 500 1.086557e-02 0.313843
2019-11-13 07:52:34,787 train 550 1.088627e-02 0.312798
2019-11-13 07:52:42,418 train 600 1.089282e-02 0.310712
2019-11-13 07:52:50,051 train 650 1.090093e-02 0.310180
2019-11-13 07:52:57,746 train 700 1.090744e-02 0.311067
2019-11-13 07:53:05,458 train 750 1.089025e-02 0.311800
2019-11-13 07:53:13,214 train 800 1.089456e-02 0.311427
2019-11-13 07:53:20,835 train 850 1.091479e-02 0.312800
2019-11-13 07:53:23,111 training loss; R2: 1.092450e-02 0.312281
2019-11-13 07:53:23,421 valid 000 1.066689e-01 -2.801282
2019-11-13 07:53:25,154 valid 050 1.033768e-01 -3.016322
2019-11-13 07:53:26,709 validation loss; R2: 1.028816e-01 -3.086463
2019-11-13 07:53:26,735 epoch 12 lr 1.000000e-03
2019-11-13 07:53:27,189 train 000 9.408524e-03 0.384437
2019-11-13 07:53:34,874 train 050 1.077202e-02 0.339448
2019-11-13 07:53:42,699 train 100 1.085222e-02 0.310870
2019-11-13 07:53:50,447 train 150 1.091456e-02 0.311941
2019-11-13 07:53:58,194 train 200 1.094034e-02 0.320429
2019-11-13 07:54:05,923 train 250 1.096108e-02 0.313130
2019-11-13 07:54:13,814 train 300 1.091384e-02 0.308110
2019-11-13 07:54:21,512 train 350 1.086588e-02 0.312499
2019-11-13 07:54:29,279 train 400 1.085334e-02 0.314432
2019-11-13 07:54:37,014 train 450 1.083876e-02 0.316480
2019-11-13 07:54:44,717 train 500 1.082909e-02 0.305362
2019-11-13 07:54:52,462 train 550 1.080099e-02 0.308110
2019-11-13 07:55:00,137 train 600 1.077480e-02 0.311183
2019-11-13 07:55:07,817 train 650 1.077240e-02 0.314932
2019-11-13 07:55:15,514 train 700 1.076585e-02 0.316843
2019-11-13 07:55:23,247 train 750 1.074257e-02 0.314165
2019-11-13 07:55:31,011 train 800 1.075035e-02 0.315640
2019-11-13 07:55:38,816 train 850 1.074092e-02 0.318570
2019-11-13 07:55:41,105 training loss; R2: 1.073119e-02 0.318043
2019-11-13 07:55:41,409 valid 000 3.474764e-02 -1.360137
2019-11-13 07:55:43,104 valid 050 3.166688e-02 -2.058411
2019-11-13 07:55:44,648 validation loss; R2: 3.153141e-02 -1.979686
2019-11-13 07:55:44,669 epoch 13 lr 1.000000e-03
2019-11-13 07:55:45,079 train 000 1.061639e-02 -0.089594
2019-11-13 07:55:52,860 train 050 1.048708e-02 0.305511
2019-11-13 07:56:00,600 train 100 1.044995e-02 0.315696
2019-11-13 07:56:08,325 train 150 1.044117e-02 0.317420
2019-11-13 07:56:16,054 train 200 1.046357e-02 0.323576
2019-11-13 07:56:23,795 train 250 1.048142e-02 0.324085
2019-11-13 07:56:31,521 train 300 1.052997e-02 0.322413
2019-11-13 07:56:39,269 train 350 1.054618e-02 0.317497
2019-11-13 07:56:46,984 train 400 1.052695e-02 0.255766
2019-11-13 07:56:54,671 train 450 1.056222e-02 0.259579
2019-11-13 07:57:02,345 train 500 1.057823e-02 0.266104
2019-11-13 07:57:10,065 train 550 1.061433e-02 0.264962
2019-11-13 07:57:17,875 train 600 1.063349e-02 0.264230
2019-11-13 07:57:25,776 train 650 1.062036e-02 0.269881
2019-11-13 07:57:33,516 train 700 1.058608e-02 0.274682
2019-11-13 07:57:41,285 train 750 1.057148e-02 0.278082
2019-11-13 07:57:49,049 train 800 1.056913e-02 0.281444
2019-11-13 07:57:56,780 train 850 1.057181e-02 0.283881
2019-11-13 07:57:59,081 training loss; R2: 1.057019e-02 0.285302
2019-11-13 07:57:59,404 valid 000 1.345586e+01 -585.392316
2019-11-13 07:58:01,131 valid 050 1.347715e+01 -699.498990
2019-11-13 07:58:02,670 validation loss; R2: 1.348185e+01 -718.872332
2019-11-13 07:58:02,691 epoch 14 lr 1.000000e-03
2019-11-13 07:58:03,120 train 000 1.256624e-02 0.356850
2019-11-13 07:58:10,957 train 050 1.054718e-02 0.298850
2019-11-13 07:58:18,716 train 100 1.058202e-02 0.310776
2019-11-13 07:58:26,418 train 150 1.061040e-02 0.327256
2019-11-13 07:58:34,283 train 200 1.060588e-02 0.329138
2019-11-13 07:58:42,003 train 250 1.058175e-02 0.311605
2019-11-13 07:58:49,733 train 300 1.054512e-02 0.320540
2019-11-13 07:58:57,393 train 350 1.174569e-02 -16.471297
2019-11-13 07:59:05,099 train 400 1.214436e-02 -14.438975
2019-11-13 07:59:12,749 train 450 1.220893e-02 -12.872712
2019-11-13 07:59:20,531 train 500 1.216506e-02 -11.566813
2019-11-13 07:59:28,478 train 550 1.207207e-02 -10.491126
2019-11-13 07:59:36,251 train 600 1.198501e-02 -9.838315
2019-11-13 07:59:44,113 train 650 1.195327e-02 -9.060941
2019-11-13 07:59:51,929 train 700 1.188212e-02 -8.395299
2019-11-13 07:59:59,668 train 750 1.182102e-02 -7.815241
2019-11-13 08:00:07,365 train 800 1.176825e-02 -7.306148
2019-11-13 08:00:15,076 train 850 1.170881e-02 -6.859689
2019-11-13 08:00:17,373 training loss; R2: 1.169425e-02 -6.735833
2019-11-13 08:00:17,680 valid 000 3.321226e+02 -34396.407945
2019-11-13 08:00:19,394 valid 050 3.323113e+02 -87055.721652
2019-11-13 08:00:20,943 validation loss; R2: 3.323509e+02 -73822.302944
2019-11-13 08:00:20,975 epoch 15 lr 1.000000e-03
2019-11-13 08:00:21,440 train 000 1.134064e-02 0.394087
2019-11-13 08:00:29,420 train 050 1.077705e-02 0.333092
2019-11-13 08:00:37,255 train 100 1.075504e-02 0.307286
2019-11-13 08:00:45,000 train 150 1.067029e-02 0.314376
2019-11-13 08:00:52,720 train 200 1.068865e-02 0.321184
2019-11-13 08:01:00,569 train 250 1.072779e-02 0.307179
2019-11-13 08:01:08,396 train 300 1.072621e-02 0.305528
2019-11-13 08:01:16,169 train 350 1.070490e-02 0.313005
2019-11-13 08:01:24,028 train 400 1.068673e-02 0.312195
2019-11-13 08:01:31,893 train 450 1.067854e-02 0.315884
2019-11-13 08:01:39,632 train 500 1.065579e-02 0.319307
2019-11-13 08:01:47,443 train 550 1.065789e-02 0.304578
2019-11-13 08:01:55,192 train 600 1.064239e-02 0.304167
2019-11-13 08:02:02,998 train 650 1.070215e-02 0.293966
2019-11-13 08:02:10,770 train 700 1.076106e-02 0.293773
2019-11-13 08:02:18,542 train 750 1.078234e-02 0.295685
2019-11-13 08:02:26,272 train 800 1.079750e-02 0.294780
2019-11-13 08:02:33,949 train 850 1.081316e-02 0.295442
2019-11-13 08:02:36,239 training loss; R2: 1.081788e-02 0.295059
2019-11-13 08:02:36,572 valid 000 1.754808e+02 -12097.174653
2019-11-13 08:02:38,229 valid 050 1.754197e+02 -14297.553480
2019-11-13 08:02:39,745 validation loss; R2: 1.754366e+02 -14675.079580
2019-11-13 08:02:39,766 epoch 16 lr 1.000000e-03
2019-11-13 08:02:40,219 train 000 1.115225e-02 0.307216
2019-11-13 08:02:47,919 train 050 1.087429e-02 0.319001
2019-11-13 08:02:55,577 train 100 1.051710e-02 0.321539
2019-11-13 08:03:03,404 train 150 1.053920e-02 0.319707
2019-11-13 08:03:11,065 train 200 1.058637e-02 0.324154
2019-11-13 08:03:18,731 train 250 1.057884e-02 0.319093
2019-11-13 08:03:26,373 train 300 1.076854e-02 0.302500
2019-11-13 08:03:34,019 train 350 1.079464e-02 0.303430
2019-11-13 08:03:41,700 train 400 1.075870e-02 0.302408
2019-11-13 08:03:49,415 train 450 1.073889e-02 0.302430
2019-11-13 08:03:57,250 train 500 1.075173e-02 0.302543
2019-11-13 08:04:04,962 train 550 1.072013e-02 0.290460
2019-11-13 08:04:12,703 train 600 1.068313e-02 0.293307
2019-11-13 08:04:20,429 train 650 1.068761e-02 0.292397
2019-11-13 08:04:28,146 train 700 1.067536e-02 0.292207
2019-11-13 08:04:35,845 train 750 1.065379e-02 0.295088
2019-11-13 08:04:43,650 train 800 1.065138e-02 0.295504
2019-11-13 08:04:51,363 train 850 1.063742e-02 0.296847
2019-11-13 08:04:53,652 training loss; R2: 1.064368e-02 0.296081
2019-11-13 08:04:53,964 valid 000 6.045439e+02 -25351.176740
2019-11-13 08:04:55,677 valid 050 6.048021e+02 -26346.694231
2019-11-13 08:04:57,214 validation loss; R2: 6.048520e+02 -28402.771745
2019-11-13 08:04:57,245 epoch 17 lr 1.000000e-03
2019-11-13 08:04:57,699 train 000 1.129227e-02 0.258022
2019-11-13 08:05:05,600 train 050 1.152997e-02 0.072915
2019-11-13 08:05:13,345 train 100 1.115854e-02 0.133801
2019-11-13 08:05:20,993 train 150 1.114842e-02 0.187494
2019-11-13 08:05:28,608 train 200 1.099066e-02 0.219126
2019-11-13 08:05:36,212 train 250 1.082905e-02 0.237596
2019-11-13 08:05:43,830 train 300 1.105159e-02 0.138396
2019-11-13 08:05:51,418 train 350 2.066045e-02 -191.282022
2019-11-13 08:05:58,997 train 400 2.070940e-02 -167.532769
2019-11-13 08:06:06,587 train 450 2.049163e-02 -149.005401
2019-11-13 08:06:14,193 train 500 2.013933e-02 -134.124165
2019-11-13 08:06:21,826 train 550 1.968928e-02 -121.947973
2019-11-13 08:06:29,452 train 600 1.930591e-02 -111.805312
2019-11-13 08:06:37,083 train 650 1.897764e-02 -103.209832
2019-11-13 08:06:44,700 train 700 1.872575e-02 -95.850161
2019-11-13 08:06:52,334 train 750 1.852985e-02 -89.475190
2019-11-13 08:06:59,944 train 800 1.825698e-02 -83.884126
2019-11-13 08:07:07,577 train 850 1.808119e-02 -78.946321
2019-11-13 08:07:09,862 training loss; R2: 1.800277e-02 -77.582390
2019-11-13 08:07:10,159 valid 000 1.440910e+02 -13900.337916
2019-11-13 08:07:11,821 valid 050 1.440730e+02 -15470.667767
2019-11-13 08:07:13,330 validation loss; R2: 1.440649e+02 -18046.912032
2019-11-13 08:07:13,350 epoch 18 lr 1.000000e-03
2019-11-13 08:07:13,748 train 000 1.108439e-02 0.170526
2019-11-13 08:07:21,613 train 050 1.282427e-02 0.204241
2019-11-13 08:07:29,437 train 100 1.314664e-02 -0.637550
2019-11-13 08:07:37,328 train 150 1.294609e-02 -0.394269
2019-11-13 08:07:45,091 train 200 1.288334e-02 -0.243936
2019-11-13 08:07:53,049 train 250 1.275330e-02 -0.152063
2019-11-13 08:08:00,891 train 300 1.263808e-02 -0.086904
2019-11-13 08:08:08,745 train 350 1.249904e-02 -0.037119
2019-11-13 08:08:16,754 train 400 1.238826e-02 -0.010160
2019-11-13 08:08:24,618 train 450 1.241916e-02 -0.004262
2019-11-13 08:08:32,343 train 500 1.234072e-02 0.011884
2019-11-13 08:08:40,059 train 550 1.224645e-02 0.038094
2019-11-13 08:08:47,741 train 600 1.268231e-02 -0.257307
2019-11-13 08:08:55,424 train 650 1.278592e-02 -0.247659
2019-11-13 08:09:03,421 train 700 1.283642e-02 -0.316907
2019-11-13 08:09:11,259 train 750 1.279708e-02 -0.283494
2019-11-13 08:09:19,259 train 800 1.272282e-02 -0.254235
2019-11-13 08:09:27,117 train 850 1.275510e-02 -0.309507
2019-11-13 08:09:29,446 training loss; R2: 1.277559e-02 -0.300867
2019-11-13 08:09:29,759 valid 000 2.667842e+02 -66068.478042
2019-11-13 08:09:31,457 valid 050 2.668125e+02 -127636.282539
2019-11-13 08:09:32,965 validation loss; R2: 2.668205e+02 -108551.759804
2019-11-13 08:09:32,985 epoch 19 lr 1.000000e-03
2019-11-13 08:09:33,402 train 000 1.391795e-02 0.159006
2019-11-13 08:09:41,286 train 050 1.264998e-02 0.165634
2019-11-13 08:09:49,050 train 100 1.249467e-02 0.183835
2019-11-13 08:09:56,775 train 150 1.226996e-02 0.204335
2019-11-13 08:10:04,888 train 200 1.212336e-02 0.103649
2019-11-13 08:10:12,986 train 250 1.199122e-02 0.079099
2019-11-13 08:10:20,815 train 300 1.186068e-02 0.103432
2019-11-13 08:10:28,887 train 350 1.177130e-02 0.124080
2019-11-13 08:10:36,898 train 400 1.182334e-02 0.114249
2019-11-13 08:10:44,753 train 450 1.180962e-02 0.083675
2019-11-13 08:10:52,476 train 500 1.181413e-02 0.098581
2019-11-13 08:11:00,227 train 550 1.177798e-02 0.112366
2019-11-13 08:11:07,936 train 600 1.175926e-02 0.114536
2019-11-13 08:11:15,601 train 650 1.194532e-02 0.081876
2019-11-13 08:11:23,269 train 700 1.187007e-02 0.099902
2019-11-13 08:11:30,901 train 750 1.183108e-02 0.107677
2019-11-13 08:11:38,541 train 800 1.177303e-02 0.113327
2019-11-13 08:11:46,178 train 850 1.174852e-02 0.124253
2019-11-13 08:11:48,446 training loss; R2: 1.173566e-02 0.127066
2019-11-13 08:11:48,752 valid 000 5.078704e+02 -52490.798276
2019-11-13 08:11:50,440 valid 050 5.093400e+02 -170698.277523
2019-11-13 08:11:51,975 validation loss; R2: 5.095555e+02 -157056.191902
