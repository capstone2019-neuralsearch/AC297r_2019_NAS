2019-11-13 08:11:54,441 gpu device = 1
2019-11-13 08:11:54,441 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name='galaxy-zoo-cleaned', gpu=1, grad_clip=5, gz_dtree=False, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-galaxy-zoo-EXP-20191113-081154', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-11-13 08:12:06,013 param size = 0.282325MB
2019-11-13 08:12:06,017 epoch 0 lr 1.000000e-03
2019-11-13 08:12:08,243 train 000 1.145704e+00 -655.609248
2019-11-13 08:12:15,798 train 050 7.414467e-02 -23.681154
2019-11-13 08:12:23,213 train 100 5.269524e-02 -12.794724
2019-11-13 08:12:30,644 train 150 4.434108e-02 -8.829321
2019-11-13 08:12:38,080 train 200 3.993385e-02 -6.794302
2019-11-13 08:12:45,398 train 250 3.702765e-02 -5.584947
2019-11-13 08:12:52,714 train 300 3.506036e-02 -4.718293
2019-11-13 08:13:00,158 train 350 3.361283e-02 -4.127306
2019-11-13 08:13:07,380 train 400 3.250014e-02 -3.643295
2019-11-13 08:13:14,684 train 450 3.154352e-02 -3.264799
2019-11-13 08:13:21,821 train 500 3.073026e-02 -2.962465
2019-11-13 08:13:28,980 train 550 3.000874e-02 -2.710291
2019-11-13 08:13:36,377 train 600 2.938375e-02 -2.503229
2019-11-13 08:13:43,807 train 650 2.882913e-02 -2.322962
2019-11-13 08:13:51,248 train 700 2.832916e-02 -2.242811
2019-11-13 08:13:58,691 train 750 2.790912e-02 -2.106218
2019-11-13 08:14:06,154 train 800 2.751747e-02 -1.977882
2019-11-13 08:14:13,586 train 850 2.715846e-02 -1.865916
2019-11-13 08:14:16,625 training loss; R2: 2.703968e-02 -1.834287
2019-11-13 08:14:16,910 valid 000 2.053543e-02 -0.008862
2019-11-13 08:14:18,590 valid 050 2.388504e-02 -0.060107
2019-11-13 08:14:20,166 validation loss; R2: 2.360914e-02 -0.047341
2019-11-13 08:14:20,182 epoch 1 lr 1.000000e-03
2019-11-13 08:14:20,737 train 000 2.128131e-02 0.014840
2019-11-13 08:14:28,057 train 050 2.143889e-02 -0.061119
2019-11-13 08:14:35,325 train 100 2.108482e-02 -0.189262
2019-11-13 08:14:42,572 train 150 2.107523e-02 -0.155711
2019-11-13 08:14:49,802 train 200 2.102546e-02 -0.152117
2019-11-13 08:14:56,963 train 250 2.074608e-02 -0.122135
2019-11-13 08:15:04,075 train 300 2.063411e-02 -0.104162
2019-11-13 08:15:11,166 train 350 2.047708e-02 -0.112391
2019-11-13 08:15:18,259 train 400 2.039726e-02 -0.098562
2019-11-13 08:15:25,342 train 450 2.033928e-02 -0.088694
2019-11-13 08:15:32,432 train 500 2.023269e-02 -0.076447
2019-11-13 08:15:39,524 train 550 2.018253e-02 -0.139157
2019-11-13 08:15:46,608 train 600 2.013648e-02 -0.138611
2019-11-13 08:15:53,699 train 650 2.011526e-02 -0.123395
2019-11-13 08:16:00,786 train 700 2.005558e-02 -0.113784
2019-11-13 08:16:07,881 train 750 1.997046e-02 -0.102184
2019-11-13 08:16:14,969 train 800 1.989123e-02 -0.092264
2019-11-13 08:16:22,053 train 850 1.985732e-02 -0.111174
2019-11-13 08:16:24,174 training loss; R2: 1.983074e-02 -0.110153
2019-11-13 08:16:24,484 valid 000 2.255081e-02 -0.765360
2019-11-13 08:16:26,181 valid 050 2.144018e-02 0.013231
2019-11-13 08:16:27,696 validation loss; R2: 2.133411e-02 -0.127180
2019-11-13 08:16:27,712 epoch 2 lr 1.000000e-03
2019-11-13 08:16:28,106 train 000 2.186425e-02 0.044204
2019-11-13 08:16:35,208 train 050 1.880047e-02 0.062578
2019-11-13 08:16:42,302 train 100 1.872234e-02 0.014714
2019-11-13 08:16:49,395 train 150 1.854801e-02 0.027723
2019-11-13 08:16:56,481 train 200 1.855947e-02 0.035390
2019-11-13 08:17:03,580 train 250 1.855856e-02 0.044891
2019-11-13 08:17:10,679 train 300 1.857673e-02 0.049720
2019-11-13 08:17:17,773 train 350 1.851659e-02 0.055629
2019-11-13 08:17:24,866 train 400 1.844433e-02 0.064084
2019-11-13 08:17:31,957 train 450 1.833441e-02 0.066903
2019-11-13 08:17:39,048 train 500 1.830949e-02 0.057644
2019-11-13 08:17:46,146 train 550 1.828148e-02 0.063010
2019-11-13 08:17:53,233 train 600 1.825676e-02 0.065593
2019-11-13 08:18:00,321 train 650 1.820703e-02 0.069235
2019-11-13 08:18:07,415 train 700 1.818573e-02 0.071304
2019-11-13 08:18:14,513 train 750 1.818112e-02 0.070807
2019-11-13 08:18:21,601 train 800 1.812969e-02 0.073085
2019-11-13 08:18:28,699 train 850 1.805913e-02 0.075184
2019-11-13 08:18:30,821 training loss; R2: 1.803758e-02 0.076244
2019-11-13 08:18:31,116 valid 000 1.466816e-02 0.246363
2019-11-13 08:18:32,833 valid 050 1.600948e-02 0.191103
2019-11-13 08:18:34,372 validation loss; R2: 1.590547e-02 0.198701
2019-11-13 08:18:34,389 epoch 3 lr 1.000000e-03
2019-11-13 08:18:34,772 train 000 1.793643e-02 -0.017162
2019-11-13 08:18:41,938 train 050 1.766193e-02 0.094409
2019-11-13 08:18:49,235 train 100 1.733108e-02 0.070763
2019-11-13 08:18:56,472 train 150 1.730786e-02 0.078169
2019-11-13 08:19:03,650 train 200 1.730867e-02 0.074245
2019-11-13 08:19:10,825 train 250 1.731767e-02 0.073346
2019-11-13 08:19:17,984 train 300 1.730263e-02 0.080394
2019-11-13 08:19:25,131 train 350 1.728031e-02 0.088511
2019-11-13 08:19:32,377 train 400 1.721393e-02 0.094785
2019-11-13 08:19:39,521 train 450 1.719165e-02 0.098342
2019-11-13 08:19:46,644 train 500 1.717170e-02 0.103449
2019-11-13 08:19:53,738 train 550 1.717331e-02 0.097792
2019-11-13 08:20:00,838 train 600 1.712925e-02 0.100812
2019-11-13 08:20:07,924 train 650 1.710499e-02 0.097601
2019-11-13 08:20:15,019 train 700 1.706251e-02 0.101138
2019-11-13 08:20:22,108 train 750 1.700716e-02 0.105450
2019-11-13 08:20:29,199 train 800 1.697525e-02 0.106097
2019-11-13 08:20:36,295 train 850 1.693155e-02 0.110533
2019-11-13 08:20:38,417 training loss; R2: 1.691094e-02 0.109703
2019-11-13 08:20:38,722 valid 000 1.940781e-02 0.246230
2019-11-13 08:20:40,449 valid 050 1.600934e-02 0.161835
2019-11-13 08:20:42,003 validation loss; R2: 1.602120e-02 0.167474
2019-11-13 08:20:42,022 epoch 4 lr 1.000000e-03
2019-11-13 08:20:42,413 train 000 1.798439e-02 0.118234
2019-11-13 08:20:49,656 train 050 1.640193e-02 0.077549
2019-11-13 08:20:56,845 train 100 1.622939e-02 -12.144131
2019-11-13 08:21:04,032 train 150 1.637407e-02 -8.075126
2019-11-13 08:21:11,187 train 200 1.627781e-02 -6.042750
2019-11-13 08:21:18,328 train 250 1.625273e-02 -4.807344
2019-11-13 08:21:25,467 train 300 1.620125e-02 -4.110849
2019-11-13 08:21:32,648 train 350 1.614859e-02 -3.502582
2019-11-13 08:21:39,873 train 400 1.611820e-02 -3.046874
2019-11-13 08:21:47,076 train 450 1.607342e-02 -2.690694
2019-11-13 08:21:54,273 train 500 1.606419e-02 -2.406263
2019-11-13 08:22:01,418 train 550 1.602805e-02 -2.174599
2019-11-13 08:22:08,542 train 600 1.597537e-02 -1.977858
2019-11-13 08:22:15,666 train 650 1.594962e-02 -1.816440
2019-11-13 08:22:22,787 train 700 1.592810e-02 -1.674995
2019-11-13 08:22:29,936 train 750 1.590787e-02 -1.551185
2019-11-13 08:22:37,054 train 800 1.587406e-02 -1.445985
2019-11-13 08:22:44,391 train 850 1.583753e-02 -1.351220
2019-11-13 08:22:46,601 training loss; R2: 1.582235e-02 -1.324085
2019-11-13 08:22:46,885 valid 000 1.324212e-02 0.216620
2019-11-13 08:22:48,550 valid 050 1.432727e-02 0.236209
2019-11-13 08:22:50,084 validation loss; R2: 1.443219e-02 0.231053
2019-11-13 08:22:50,101 epoch 5 lr 1.000000e-03
2019-11-13 08:22:50,513 train 000 1.367621e-02 0.149010
2019-11-13 08:22:57,642 train 050 1.525417e-02 0.178306
2019-11-13 08:23:04,824 train 100 1.527952e-02 0.183780
2019-11-13 08:23:11,965 train 150 1.504203e-02 0.192579
2019-11-13 08:23:19,123 train 200 1.504170e-02 0.114500
2019-11-13 08:23:26,240 train 250 1.507102e-02 0.124103
2019-11-13 08:23:33,362 train 300 1.501800e-02 0.125481
2019-11-13 08:23:40,480 train 350 1.504297e-02 0.132753
2019-11-13 08:23:47,595 train 400 1.507670e-02 0.140141
2019-11-13 08:23:54,718 train 450 1.507704e-02 0.143280
2019-11-13 08:24:01,840 train 500 1.511096e-02 0.149760
2019-11-13 08:24:08,951 train 550 1.509475e-02 0.150596
2019-11-13 08:24:16,063 train 600 1.504516e-02 0.135693
2019-11-13 08:24:23,172 train 650 1.499773e-02 0.139867
2019-11-13 08:24:30,294 train 700 1.496650e-02 0.143887
2019-11-13 08:24:37,403 train 750 1.495720e-02 0.148332
2019-11-13 08:24:44,515 train 800 1.493065e-02 0.152229
2019-11-13 08:24:51,645 train 850 1.489759e-02 0.155511
2019-11-13 08:24:53,774 training loss; R2: 1.488252e-02 0.157177
2019-11-13 08:24:54,079 valid 000 1.432770e-02 0.235823
2019-11-13 08:24:55,816 valid 050 1.275262e-02 0.201151
2019-11-13 08:24:57,361 validation loss; R2: 1.266791e-02 0.237945
2019-11-13 08:24:57,377 epoch 6 lr 1.000000e-03
2019-11-13 08:24:57,781 train 000 1.257037e-02 0.357354
2019-11-13 08:25:04,918 train 050 1.490054e-02 0.191847
2019-11-13 08:25:12,208 train 100 1.481716e-02 0.190053
2019-11-13 08:25:19,458 train 150 1.469318e-02 0.174378
2019-11-13 08:25:26,699 train 200 1.461383e-02 0.180479
2019-11-13 08:25:34,093 train 250 1.461043e-02 0.188426
2019-11-13 08:25:41,255 train 300 1.458506e-02 0.193484
2019-11-13 08:25:48,470 train 350 1.455104e-02 0.193343
2019-11-13 08:25:55,660 train 400 1.449307e-02 0.190876
2019-11-13 08:26:02,866 train 450 1.444655e-02 0.196382
2019-11-13 08:26:10,071 train 500 1.444638e-02 0.197164
2019-11-13 08:26:17,240 train 550 1.442454e-02 0.196971
2019-11-13 08:26:24,427 train 600 1.441560e-02 0.198545
2019-11-13 08:26:31,712 train 650 1.436624e-02 0.200370
2019-11-13 08:26:38,920 train 700 1.433433e-02 0.197814
2019-11-13 08:26:46,109 train 750 1.429995e-02 0.199065
2019-11-13 08:26:53,258 train 800 1.426551e-02 0.200404
2019-11-13 08:27:00,497 train 850 1.425588e-02 0.202675
2019-11-13 08:27:02,647 training loss; R2: 1.426387e-02 0.203506
2019-11-13 08:27:02,967 valid 000 1.212362e-02 0.366274
2019-11-13 08:27:04,691 valid 050 1.306812e-02 0.280906
2019-11-13 08:27:06,246 validation loss; R2: 1.291514e-02 0.268467
2019-11-13 08:27:06,263 epoch 7 lr 1.000000e-03
2019-11-13 08:27:06,668 train 000 1.311547e-02 0.309233
2019-11-13 08:27:13,925 train 050 1.418523e-02 0.226421
2019-11-13 08:27:21,166 train 100 1.407796e-02 0.237455
2019-11-13 08:27:28,374 train 150 1.398372e-02 0.232741
2019-11-13 08:27:35,559 train 200 1.395395e-02 0.234509
2019-11-13 08:27:42,912 train 250 1.391561e-02 0.230568
2019-11-13 08:27:50,212 train 300 1.390475e-02 0.217410
2019-11-13 08:27:57,470 train 350 1.390636e-02 0.218112
2019-11-13 08:28:04,724 train 400 1.388022e-02 0.218689
2019-11-13 08:28:11,970 train 450 1.383525e-02 0.219711
2019-11-13 08:28:19,191 train 500 1.383288e-02 0.222603
2019-11-13 08:28:26,378 train 550 1.380061e-02 0.218737
2019-11-13 08:28:33,568 train 600 1.377293e-02 0.218995
2019-11-13 08:28:40,778 train 650 1.371219e-02 0.221145
2019-11-13 08:28:47,942 train 700 1.368612e-02 0.221243
2019-11-13 08:28:55,172 train 750 1.370253e-02 0.221343
2019-11-13 08:29:02,346 train 800 1.369538e-02 -0.407841
2019-11-13 08:29:09,588 train 850 1.368767e-02 -0.368386
2019-11-13 08:29:11,732 training loss; R2: 1.368312e-02 -0.357374
2019-11-13 08:29:12,048 valid 000 1.329388e-02 0.327224
2019-11-13 08:29:13,756 valid 050 1.232463e-02 0.308780
2019-11-13 08:29:15,319 validation loss; R2: 1.214512e-02 0.305722
2019-11-13 08:29:15,335 epoch 8 lr 1.000000e-03
2019-11-13 08:29:15,730 train 000 1.351364e-02 0.269588
2019-11-13 08:29:22,924 train 050 1.343498e-02 0.252360
2019-11-13 08:29:30,150 train 100 1.342834e-02 0.203196
2019-11-13 08:29:37,354 train 150 1.338780e-02 0.206903
2019-11-13 08:29:44,590 train 200 1.339628e-02 0.218148
2019-11-13 08:29:51,776 train 250 1.331838e-02 0.224866
2019-11-13 08:29:58,972 train 300 1.329880e-02 0.230217
2019-11-13 08:30:06,196 train 350 1.331875e-02 0.219783
2019-11-13 08:30:13,391 train 400 1.332638e-02 0.187952
2019-11-13 08:30:20,611 train 450 1.333710e-02 0.193979
2019-11-13 08:30:27,805 train 500 1.333614e-02 0.199524
2019-11-13 08:30:34,963 train 550 1.335150e-02 0.203307
2019-11-13 08:30:42,341 train 600 1.330851e-02 0.208551
2019-11-13 08:30:49,723 train 650 1.328892e-02 0.211786
2019-11-13 08:30:57,085 train 700 1.328539e-02 0.214557
2019-11-13 08:31:04,437 train 750 1.328564e-02 0.216913
2019-11-13 08:31:11,801 train 800 1.327893e-02 0.219714
2019-11-13 08:31:19,153 train 850 1.326394e-02 0.220837
2019-11-13 08:31:21,352 training loss; R2: 1.326289e-02 0.221881
2019-11-13 08:31:21,666 valid 000 1.053914e-02 0.407617
2019-11-13 08:31:23,351 valid 050 1.166896e-02 0.225475
2019-11-13 08:31:24,870 validation loss; R2: 1.159315e-02 0.172224
2019-11-13 08:31:24,892 epoch 9 lr 1.000000e-03
2019-11-13 08:31:25,344 train 000 1.463082e-02 0.244768
2019-11-13 08:31:32,695 train 050 1.318845e-02 0.170551
2019-11-13 08:31:40,038 train 100 1.328122e-02 0.193202
2019-11-13 08:31:47,197 train 150 1.323359e-02 0.215080
2019-11-13 08:31:54,509 train 200 1.326044e-02 0.218702
2019-11-13 08:32:01,931 train 250 1.328665e-02 0.222809
2019-11-13 08:32:09,327 train 300 1.322445e-02 0.227695
2019-11-13 08:32:16,592 train 350 1.321723e-02 -2.016180
2019-11-13 08:32:23,734 train 400 1.316022e-02 -1.731630
2019-11-13 08:32:31,189 train 450 1.314232e-02 -1.515759
2019-11-13 08:32:38,360 train 500 1.311516e-02 -1.337695
2019-11-13 08:32:45,736 train 550 1.309429e-02 -1.192358
2019-11-13 08:32:53,082 train 600 1.307435e-02 -1.074045
2019-11-13 08:33:00,247 train 650 1.303955e-02 -0.979500
2019-11-13 08:33:07,371 train 700 1.303828e-02 -0.892803
2019-11-13 08:33:14,588 train 750 1.303922e-02 -0.817449
2019-11-13 08:33:21,738 train 800 1.303220e-02 -0.756786
2019-11-13 08:33:29,205 train 850 1.302343e-02 -0.697378
2019-11-13 08:33:31,419 training loss; R2: 1.302909e-02 -0.680517
2019-11-13 08:33:31,706 valid 000 1.242505e-02 0.276429
2019-11-13 08:33:33,383 valid 050 1.169506e-02 0.332351
2019-11-13 08:33:34,908 validation loss; R2: 1.159372e-02 0.335765
2019-11-13 08:33:34,923 epoch 10 lr 1.000000e-03
2019-11-13 08:33:35,316 train 000 1.089583e-02 0.299554
2019-11-13 08:33:42,415 train 050 1.288732e-02 0.285727
2019-11-13 08:33:49,512 train 100 1.260610e-02 0.261860
2019-11-13 08:33:56,602 train 150 1.266810e-02 0.236281
2019-11-13 08:34:03,694 train 200 1.278216e-02 0.242225
2019-11-13 08:34:10,793 train 250 1.276892e-02 0.248672
2019-11-13 08:34:17,881 train 300 1.272735e-02 0.253762
2019-11-13 08:34:24,968 train 350 1.272610e-02 0.253252
2019-11-13 08:34:32,240 train 400 1.272430e-02 0.241093
2019-11-13 08:34:39,629 train 450 1.270425e-02 0.245379
2019-11-13 08:34:46,861 train 500 1.271623e-02 0.247089
2019-11-13 08:34:54,301 train 550 1.272653e-02 0.250991
2019-11-13 08:35:01,698 train 600 1.274039e-02 0.250458
2019-11-13 08:35:09,091 train 650 1.273473e-02 0.250033
2019-11-13 08:35:16,505 train 700 1.271442e-02 0.250755
2019-11-13 08:35:23,898 train 750 1.270192e-02 0.250539
2019-11-13 08:35:31,288 train 800 1.270372e-02 0.252149
2019-11-13 08:35:38,677 train 850 1.270252e-02 0.252693
2019-11-13 08:35:40,887 training loss; R2: 1.270921e-02 0.252802
2019-11-13 08:35:41,166 valid 000 1.259496e-02 0.375455
2019-11-13 08:35:42,871 valid 050 1.228426e-02 0.327358
2019-11-13 08:35:44,396 validation loss; R2: 1.219888e-02 0.319198
2019-11-13 08:35:44,412 epoch 11 lr 1.000000e-03
2019-11-13 08:35:44,805 train 000 1.103993e-02 0.360090
2019-11-13 08:35:51,904 train 050 1.249168e-02 0.213795
2019-11-13 08:35:58,993 train 100 1.252501e-02 0.238985
2019-11-13 08:36:06,082 train 150 1.257523e-02 0.249960
2019-11-13 08:36:13,181 train 200 1.251299e-02 0.247179
2019-11-13 08:36:20,281 train 250 1.256032e-02 0.255213
2019-11-13 08:36:27,568 train 300 1.256157e-02 0.257526
2019-11-13 08:36:34,971 train 350 1.255511e-02 0.253771
2019-11-13 08:36:42,384 train 400 1.256875e-02 0.255259
2019-11-13 08:36:49,787 train 450 1.261517e-02 0.251430
2019-11-13 08:36:57,200 train 500 1.259503e-02 0.250456
2019-11-13 08:37:04,599 train 550 1.257474e-02 0.253043
2019-11-13 08:37:11,994 train 600 1.255580e-02 0.255059
2019-11-13 08:37:19,391 train 650 1.253431e-02 0.257633
2019-11-13 08:37:26,789 train 700 1.251498e-02 0.260005
2019-11-13 08:37:34,217 train 750 1.250412e-02 0.260940
2019-11-13 08:37:41,619 train 800 1.251674e-02 0.259932
2019-11-13 08:37:49,020 train 850 1.250538e-02 0.259190
2019-11-13 08:37:51,227 training loss; R2: 1.250740e-02 0.255564
2019-11-13 08:37:51,554 valid 000 1.010842e-02 0.434129
2019-11-13 08:37:53,233 valid 050 1.087096e-02 0.366955
2019-11-13 08:37:54,761 validation loss; R2: 1.085354e-02 0.355169
2019-11-13 08:37:54,777 epoch 12 lr 1.000000e-03
2019-11-13 08:37:55,171 train 000 1.217879e-02 0.389567
2019-11-13 08:38:02,605 train 050 1.225469e-02 0.305312
2019-11-13 08:38:10,004 train 100 1.221450e-02 0.299010
2019-11-13 08:38:17,336 train 150 1.231579e-02 0.293691
2019-11-13 08:38:24,445 train 200 1.228163e-02 0.294457
2019-11-13 08:38:31,552 train 250 1.232537e-02 0.286289
2019-11-13 08:38:38,654 train 300 1.230982e-02 0.273947
2019-11-13 08:38:45,751 train 350 1.232276e-02 0.275037
2019-11-13 08:38:52,852 train 400 1.228311e-02 0.279345
2019-11-13 08:38:59,945 train 450 1.226700e-02 0.275810
2019-11-13 08:39:07,126 train 500 1.227519e-02 0.274084
2019-11-13 08:39:14,545 train 550 1.227959e-02 0.269300
2019-11-13 08:39:21,954 train 600 1.225219e-02 0.264411
2019-11-13 08:39:29,359 train 650 1.224111e-02 0.266155
2019-11-13 08:39:36,774 train 700 1.226337e-02 0.268040
2019-11-13 08:39:44,165 train 750 1.226301e-02 0.266661
2019-11-13 08:39:51,575 train 800 1.225995e-02 0.265139
2019-11-13 08:39:58,972 train 850 1.227138e-02 0.265973
2019-11-13 08:40:01,181 training loss; R2: 1.228158e-02 0.266137
2019-11-13 08:40:01,506 valid 000 5.876794e+00 -725.791275
2019-11-13 08:40:03,231 valid 050 5.855682e+00 -481.612376
2019-11-13 08:40:04,746 validation loss; R2: 5.855388e+00 -469.141198
2019-11-13 08:40:04,762 epoch 13 lr 1.000000e-03
2019-11-13 08:40:05,156 train 000 1.146332e-02 0.302401
2019-11-13 08:40:12,579 train 050 1.190393e-02 0.303395
2019-11-13 08:40:20,017 train 100 1.202210e-02 0.298274
2019-11-13 08:40:27,432 train 150 1.214774e-02 0.286431
2019-11-13 08:40:34,866 train 200 1.212564e-02 0.283509
2019-11-13 08:40:42,278 train 250 1.213096e-02 0.202463
2019-11-13 08:40:49,696 train 300 1.216384e-02 0.214219
2019-11-13 08:40:57,039 train 350 1.211066e-02 0.225834
2019-11-13 08:41:04,198 train 400 1.209521e-02 0.234311
2019-11-13 08:41:11,371 train 450 1.210701e-02 0.238583
2019-11-13 08:41:18,582 train 500 1.210089e-02 0.245130
2019-11-13 08:41:25,968 train 550 1.211686e-02 0.243612
2019-11-13 08:41:33,156 train 600 1.213287e-02 0.240932
2019-11-13 08:41:40,357 train 650 1.213081e-02 0.242858
2019-11-13 08:41:47,532 train 700 1.215435e-02 0.247158
2019-11-13 08:41:54,722 train 750 1.215803e-02 0.250117
2019-11-13 08:42:02,032 train 800 1.214466e-02 0.249305
2019-11-13 08:42:09,237 train 850 1.213895e-02 0.250797
2019-11-13 08:42:11,419 training loss; R2: 1.213117e-02 0.251651
2019-11-13 08:42:11,717 valid 000 2.469102e+00 -122.011514
2019-11-13 08:42:13,400 valid 050 2.455975e+00 -124.950345
2019-11-13 08:42:14,922 validation loss; R2: 2.458402e+00 -122.584994
2019-11-13 08:42:14,938 epoch 14 lr 1.000000e-03
2019-11-13 08:42:15,319 train 000 1.147475e-02 0.332149
2019-11-13 08:42:22,527 train 050 1.184836e-02 0.260224
2019-11-13 08:42:29,649 train 100 1.209722e-02 0.275088
2019-11-13 08:42:36,766 train 150 1.194653e-02 0.011762
2019-11-13 08:42:43,870 train 200 1.196122e-02 0.084064
2019-11-13 08:42:50,995 train 250 1.188142e-02 0.125175
2019-11-13 08:42:58,340 train 300 1.193622e-02 0.144858
2019-11-13 08:43:05,509 train 350 1.190847e-02 0.168323
2019-11-13 08:43:12,613 train 400 1.188063e-02 0.185392
2019-11-13 08:43:19,774 train 450 1.191793e-02 0.197087
2019-11-13 08:43:27,144 train 500 1.191606e-02 0.191970
2019-11-13 08:43:34,252 train 550 1.189242e-02 0.200968
2019-11-13 08:43:41,368 train 600 1.190331e-02 0.204668
2019-11-13 08:43:48,491 train 650 1.189231e-02 0.214206
2019-11-13 08:43:55,591 train 700 1.188422e-02 0.221078
2019-11-13 08:44:02,690 train 750 1.191250e-02 0.223620
2019-11-13 08:44:09,796 train 800 1.190549e-02 0.227049
2019-11-13 08:44:16,903 train 850 1.189975e-02 0.226055
2019-11-13 08:44:19,024 training loss; R2: 1.190620e-02 0.226555
2019-11-13 08:44:19,304 valid 000 3.191627e+00 -560.440524
2019-11-13 08:44:20,977 valid 050 3.189299e+00 -503.420337
2019-11-13 08:44:22,493 validation loss; R2: 3.193132e+00 -514.095497
2019-11-13 08:44:22,509 epoch 15 lr 1.000000e-03
2019-11-13 08:44:22,901 train 000 1.157933e-02 0.303741
2019-11-13 08:44:30,010 train 050 1.177283e-02 0.317193
2019-11-13 08:44:37,105 train 100 1.196240e-02 0.312691
2019-11-13 08:44:44,219 train 150 1.210322e-02 0.272695
2019-11-13 08:44:51,314 train 200 1.208852e-02 0.275418
2019-11-13 08:44:58,475 train 250 1.202964e-02 0.268117
2019-11-13 08:45:05,598 train 300 1.202338e-02 0.267635
2019-11-13 08:45:12,716 train 350 1.198006e-02 0.241163
2019-11-13 08:45:20,024 train 400 1.201928e-02 0.244568
2019-11-13 08:45:27,342 train 450 1.197247e-02 0.247367
2019-11-13 08:45:34,438 train 500 1.194209e-02 0.253036
2019-11-13 08:45:41,683 train 550 1.196436e-02 0.255452
2019-11-13 08:45:48,785 train 600 1.195554e-02 0.258491
2019-11-13 08:45:56,142 train 650 1.196295e-02 0.258601
2019-11-13 08:46:03,556 train 700 1.196692e-02 0.258862
2019-11-13 08:46:10,886 train 750 1.195779e-02 0.261337
2019-11-13 08:46:18,122 train 800 1.194088e-02 0.263069
2019-11-13 08:46:25,593 train 850 1.192786e-02 0.266038
2019-11-13 08:46:27,780 training loss; R2: 1.192753e-02 0.265924
2019-11-13 08:46:28,069 valid 000 7.122550e-01 -38.097365
2019-11-13 08:46:29,783 valid 050 7.277647e-01 -42.926746
2019-11-13 08:46:31,307 validation loss; R2: 7.300097e-01 -41.939792
2019-11-13 08:46:31,328 epoch 16 lr 1.000000e-03
2019-11-13 08:46:31,737 train 000 1.333760e-02 0.272020
2019-11-13 08:46:38,884 train 050 1.202260e-02 -25.557821
2019-11-13 08:46:46,210 train 100 1.198155e-02 -12.762337
2019-11-13 08:46:53,510 train 150 1.202508e-02 -10.225152
2019-11-13 08:47:00,935 train 200 1.198453e-02 -7.609439
2019-11-13 08:47:08,337 train 250 1.197199e-02 -6.038574
2019-11-13 08:47:15,717 train 300 1.194647e-02 -4.992822
2019-11-13 08:47:22,838 train 350 1.188643e-02 -4.239519
2019-11-13 08:47:29,956 train 400 1.191805e-02 -3.673075
2019-11-13 08:47:37,127 train 450 1.188365e-02 -3.233653
2019-11-13 08:47:44,267 train 500 1.185828e-02 -2.883340
2019-11-13 08:47:51,510 train 550 1.185414e-02 -2.599310
2019-11-13 08:47:58,642 train 600 1.183231e-02 -2.357684
2019-11-13 08:48:05,794 train 650 1.183053e-02 -2.159276
2019-11-13 08:48:13,244 train 700 1.181947e-02 -1.988105
2019-11-13 08:48:20,685 train 750 1.179814e-02 -1.837584
2019-11-13 08:48:28,103 train 800 1.179931e-02 -1.703801
2019-11-13 08:48:35,546 train 850 1.177453e-02 -1.585276
2019-11-13 08:48:37,783 training loss; R2: 1.178136e-02 -1.552210
2019-11-13 08:48:38,068 valid 000 2.411242e+01 -5546.898126
2019-11-13 08:48:39,751 valid 050 2.420373e+01 -1864.368209
2019-11-13 08:48:41,278 validation loss; R2: 2.420562e+01 -1900.295889
2019-11-13 08:48:41,293 epoch 17 lr 1.000000e-03
2019-11-13 08:48:41,692 train 000 1.218177e-02 0.262907
2019-11-13 08:48:48,814 train 050 1.193703e-02 0.305781
2019-11-13 08:48:55,950 train 100 1.170327e-02 0.307395
2019-11-13 08:49:03,190 train 150 1.177223e-02 0.298735
2019-11-13 08:49:10,643 train 200 1.178702e-02 0.301830
2019-11-13 08:49:18,074 train 250 1.175796e-02 0.294836
2019-11-13 08:49:25,508 train 300 1.170632e-02 -0.100942
2019-11-13 08:49:32,944 train 350 1.172897e-02 -0.042991
2019-11-13 08:49:40,377 train 400 1.174331e-02 -0.003766
2019-11-13 08:49:47,806 train 450 1.174759e-02 0.034969
2019-11-13 08:49:55,237 train 500 1.174251e-02 0.064009
2019-11-13 08:50:02,636 train 550 1.174646e-02 0.083975
2019-11-13 08:50:09,775 train 600 1.176968e-02 0.102750
2019-11-13 08:50:16,915 train 650 1.176224e-02 0.117323
2019-11-13 08:50:24,046 train 700 1.174058e-02 0.127716
2019-11-13 08:50:31,193 train 750 1.172168e-02 0.135252
2019-11-13 08:50:38,392 train 800 1.172307e-02 0.147375
2019-11-13 08:50:45,695 train 850 1.170792e-02 0.156320
2019-11-13 08:50:47,841 training loss; R2: 1.170385e-02 0.159648
2019-11-13 08:50:48,166 valid 000 4.639320e-01 -32.085934
2019-11-13 08:50:49,870 valid 050 4.599718e-01 -36.250101
2019-11-13 08:50:51,414 validation loss; R2: 4.595174e-01 -41.088497
2019-11-13 08:50:51,432 epoch 18 lr 1.000000e-03
2019-11-13 08:50:51,853 train 000 1.118723e-02 0.359298
2019-11-13 08:50:59,012 train 050 1.168934e-02 0.289407
2019-11-13 08:51:06,329 train 100 1.158639e-02 0.287986
2019-11-13 08:51:13,555 train 150 1.153347e-02 0.298899
2019-11-13 08:51:20,676 train 200 1.153931e-02 0.295637
2019-11-13 08:51:27,865 train 250 1.153846e-02 0.286492
2019-11-13 08:51:35,013 train 300 1.152191e-02 0.289322
2019-11-13 08:51:42,320 train 350 1.154064e-02 0.294160
2019-11-13 08:51:49,458 train 400 1.156098e-02 0.286741
2019-11-13 08:51:56,667 train 450 1.154704e-02 0.291787
2019-11-13 08:52:03,821 train 500 1.153269e-02 0.292927
2019-11-13 08:52:10,938 train 550 1.154402e-02 0.291095
2019-11-13 08:52:18,070 train 600 1.158027e-02 0.292871
2019-11-13 08:52:25,188 train 650 1.158048e-02 0.294734
2019-11-13 08:52:32,312 train 700 1.157509e-02 0.292072
2019-11-13 08:52:39,541 train 750 1.158999e-02 0.294033
2019-11-13 08:52:46,669 train 800 1.158924e-02 0.294201
2019-11-13 08:52:53,801 train 850 1.160039e-02 0.294670
2019-11-13 08:52:55,924 training loss; R2: 1.159674e-02 0.294584
2019-11-13 08:52:56,235 valid 000 1.301148e-01 -7.193799
2019-11-13 08:52:57,960 valid 050 1.301358e-01 -7.161258
2019-11-13 08:52:59,522 validation loss; R2: 1.300394e-01 -7.223785
2019-11-13 08:52:59,539 epoch 19 lr 1.000000e-03
2019-11-13 08:52:59,963 train 000 1.144195e-02 0.326046
2019-11-13 08:53:07,103 train 050 1.190184e-02 0.232767
2019-11-13 08:53:14,326 train 100 1.176252e-02 0.258201
2019-11-13 08:53:21,461 train 150 1.164624e-02 0.238134
2019-11-13 08:53:28,630 train 200 1.157278e-02 0.257543
2019-11-13 08:53:35,833 train 250 1.152825e-02 0.271047
2019-11-13 08:53:42,956 train 300 1.158014e-02 0.278868
2019-11-13 08:53:50,082 train 350 1.153603e-02 0.288769
2019-11-13 08:53:57,199 train 400 1.153185e-02 0.288773
2019-11-13 08:54:04,322 train 450 1.149623e-02 0.288077
2019-11-13 08:54:11,440 train 500 1.147111e-02 -0.251949
2019-11-13 08:54:18,549 train 550 1.148512e-02 -0.202356
2019-11-13 08:54:25,652 train 600 1.146601e-02 -0.163756
2019-11-13 08:54:32,772 train 650 1.146759e-02 -0.132680
2019-11-13 08:54:39,879 train 700 1.148547e-02 -0.105962
2019-11-13 08:54:47,034 train 750 1.150043e-02 -0.081210
2019-11-13 08:54:54,165 train 800 1.148156e-02 -0.067619
2019-11-13 08:55:01,298 train 850 1.146667e-02 -0.058205
2019-11-13 08:55:03,424 training loss; R2: 1.147194e-02 -0.052125
2019-11-13 08:55:03,712 valid 000 1.316401e+00 -61.823923
2019-11-13 08:55:05,481 valid 050 1.305773e+00 -177.439043
2019-11-13 08:55:07,047 validation loss; R2: 1.306174e+00 -125.973802
