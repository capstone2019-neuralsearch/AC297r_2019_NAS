2019-11-12 11:15:12,445 gpu device = 0
2019-11-12 11:15:12,445 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.2, epochs=10, fc1_size=1024, fc2_size=1024, folder_name='galaxy-zoo-cleaned', gpu=0, grad_clip=5, gz_dtree=False, init_channels=8, layers=10, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-galaxy-zoo-EXP-20191112-111512', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-11-12 11:15:24,161 param size = 0.117317MB
2019-11-12 11:15:24,167 epoch 0 lr 1.000000e-03
2019-11-12 11:15:26,462 train 000 5.545694e-01 -221.947150
2019-11-12 11:15:32,992 train 050 5.916772e-02 -15.318626
2019-11-12 11:15:39,193 train 100 4.425699e-02 -8.656323
2019-11-12 11:15:45,150 train 150 3.874120e-02 -6.447438
2019-11-12 11:15:51,085 train 200 3.560398e-02 -5.169163
2019-11-12 11:15:57,522 train 250 3.349432e-02 -4.247547
2019-11-12 11:16:03,797 train 300 3.214044e-02 -3.607680
2019-11-12 11:16:10,046 train 350 3.102025e-02 -3.146320
2019-11-12 11:16:16,310 train 400 3.015136e-02 -2.792072
2019-11-12 11:16:22,551 train 450 2.944849e-02 -2.518083
2019-11-12 11:16:28,729 train 500 2.879708e-02 -2.281735
2019-11-12 11:16:34,902 train 550 2.824722e-02 -2.097944
2019-11-12 11:16:40,610 train 600 2.777820e-02 -1.934968
2019-11-12 11:16:46,077 train 650 2.734022e-02 -1.797499
2019-11-12 11:16:51,612 train 700 2.692346e-02 -1.672558
2019-11-12 11:16:57,197 train 750 2.656917e-02 -1.566192
2019-11-12 11:17:02,789 train 800 2.624303e-02 -1.470713
2019-11-12 11:17:08,416 train 850 2.593308e-02 -1.384695
2019-11-12 11:17:10,919 training loss; R2: 2.583793e-02 -1.362327
2019-11-12 11:17:11,206 valid 000 1.670441e-02 0.086651
2019-11-12 11:17:12,956 valid 050 1.944691e-02 -0.077014
2019-11-12 11:17:14,597 validation loss; R2: 1.943582e-02 -0.015105
2019-11-12 11:17:14,616 epoch 1 lr 1.000000e-03
2019-11-12 11:17:15,169 train 000 2.073221e-02 0.012120
2019-11-12 11:17:20,846 train 050 2.034820e-02 -0.014666
2019-11-12 11:17:26,544 train 100 2.024324e-02 -0.042456
2019-11-12 11:17:32,272 train 150 1.999190e-02 -0.021738
2019-11-12 11:17:38,045 train 200 1.980821e-02 -0.011401
2019-11-12 11:17:43,844 train 250 1.971572e-02 -0.006590
2019-11-12 11:17:49,692 train 300 1.964563e-02 -0.017017
2019-11-12 11:17:55,544 train 350 1.952877e-02 -0.006507
2019-11-12 11:18:01,405 train 400 1.937881e-02 -0.017888
2019-11-12 11:18:07,262 train 450 1.930935e-02 -0.013590
2019-11-12 11:18:13,150 train 500 1.922811e-02 -0.008132
2019-11-12 11:18:19,058 train 550 1.912989e-02 0.000056
2019-11-12 11:18:24,946 train 600 1.905732e-02 0.004733
2019-11-12 11:18:30,856 train 650 1.899285e-02 0.009470
2019-11-12 11:18:36,757 train 700 1.893748e-02 0.014251
2019-11-12 11:18:42,673 train 750 1.884997e-02 0.019835
2019-11-12 11:18:48,599 train 800 1.877252e-02 0.022116
2019-11-12 11:18:54,510 train 850 1.870329e-02 0.020530
2019-11-12 11:18:56,280 training loss; R2: 1.868144e-02 0.021332
2019-11-12 11:18:56,536 valid 000 1.662453e-02 0.236464
2019-11-12 11:18:58,257 valid 050 1.666332e-02 0.188689
2019-11-12 11:18:59,884 validation loss; R2: 1.691589e-02 0.180322
2019-11-12 11:18:59,903 epoch 2 lr 1.000000e-03
2019-11-12 11:19:00,240 train 000 1.646082e-02 0.213894
2019-11-12 11:19:06,636 train 050 1.744335e-02 0.103098
