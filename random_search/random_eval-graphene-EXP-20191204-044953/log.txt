2019-12-04 04:49:53,648 gpu device = 1
2019-12-04 04:49:53,648 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=96, cutout=False, cutout_length=16, data='../data', dataset='graphene', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name=None, gpu=1, grad_clip=5, gz_dtree=False, init_channels=36, layers=8, learning_rate=0.002, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-graphene-EXP-20191204-044953', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-12-04 04:49:56,859 param size = 1.262557MB
2019-12-04 04:49:56,862 epoch 0 lr 2.000000e-03
2019-12-04 04:49:59,709 train 000 3.379094e-01 -12.528561
2019-12-04 04:50:13,924 train 050 1.121935e-01 -3.323491
2019-12-04 04:50:28,031 train 100 7.335535e-02 -1.746588
2019-12-04 04:50:42,143 train 150 5.783985e-02 -1.101639
2019-12-04 04:50:56,255 train 200 4.949289e-02 -0.756990
2019-12-04 04:51:03,775 training loss; R2: 4.630770e-02 -0.649633
2019-12-04 04:51:03,920 valid 000 1.643626e-02 0.400036
2019-12-04 04:51:05,721 validation loss; R2: 1.891098e-02 0.397111
2019-12-04 04:51:05,745 epoch 1 lr 2.000000e-03
2019-12-04 04:51:06,191 train 000 2.254181e-02 0.394212
2019-12-04 04:51:20,300 train 050 1.996511e-02 0.328317
2019-12-04 04:51:34,413 train 100 1.955118e-02 0.371704
2019-12-04 04:51:48,528 train 150 1.849308e-02 0.417941
2019-12-04 04:52:02,641 train 200 1.719932e-02 0.449627
2019-12-04 04:52:08,993 training loss; R2: 1.658457e-02 0.469527
2019-12-04 04:52:09,134 valid 000 5.448973e-03 0.789604
2019-12-04 04:52:10,602 validation loss; R2: 7.684699e-03 0.753033
2019-12-04 04:52:10,627 epoch 2 lr 2.000000e-03
2019-12-04 04:52:11,009 train 000 1.020791e-02 0.503622
2019-12-04 04:52:25,128 train 050 1.053574e-02 0.597395
2019-12-04 04:52:39,237 train 100 1.044939e-02 0.626147
2019-12-04 04:52:53,346 train 150 1.061037e-02 0.645868
2019-12-04 04:53:07,456 train 200 1.034988e-02 0.645252
2019-12-04 04:53:13,806 training loss; R2: 1.014019e-02 0.653521
2019-12-04 04:53:13,952 valid 000 1.103391e-02 0.812560
2019-12-04 04:53:15,421 validation loss; R2: 6.239123e-03 0.788482
2019-12-04 04:53:15,446 epoch 3 lr 2.000000e-03
2019-12-04 04:53:15,816 train 000 7.945166e-03 0.833468
2019-12-04 04:53:29,925 train 050 8.881740e-03 0.696293
2019-12-04 04:53:44,030 train 100 8.949292e-03 0.708024
2019-12-04 04:53:58,138 train 150 8.870054e-03 0.706689
2019-12-04 04:54:12,247 train 200 8.864127e-03 0.708386
2019-12-04 04:54:18,595 training loss; R2: 8.765249e-03 0.710361
2019-12-04 04:54:18,742 valid 000 3.721328e-03 0.852396
2019-12-04 04:54:20,211 validation loss; R2: 5.950628e-03 0.801474
2019-12-04 04:54:20,235 epoch 4 lr 2.000000e-03
2019-12-04 04:54:20,605 train 000 8.449650e-03 0.686719
2019-12-04 04:54:34,722 train 050 6.820953e-03 0.760535
2019-12-04 04:54:48,839 train 100 7.184476e-03 0.756066
2019-12-04 04:55:02,958 train 150 7.456295e-03 0.751050
2019-12-04 04:55:17,077 train 200 7.307904e-03 0.751977
2019-12-04 04:55:23,434 training loss; R2: 7.252833e-03 0.751644
2019-12-04 04:55:23,582 valid 000 4.482334e-03 0.798167
2019-12-04 04:55:25,051 validation loss; R2: 4.972296e-03 0.827543
2019-12-04 04:55:25,077 epoch 5 lr 2.000000e-03
2019-12-04 04:55:25,446 train 000 6.306215e-03 0.531621
2019-12-04 04:55:39,559 train 050 7.381687e-03 0.753104
2019-12-04 04:55:53,672 train 100 6.824687e-03 0.767869
2019-12-04 04:56:07,783 train 150 6.860279e-03 0.766406
2019-12-04 04:56:21,899 train 200 6.819964e-03 0.765778
2019-12-04 04:56:28,254 training loss; R2: 6.772086e-03 0.768273
2019-12-04 04:56:28,400 valid 000 9.228255e-03 0.748797
2019-12-04 04:56:29,869 validation loss; R2: 7.253697e-03 0.753715
2019-12-04 04:56:29,894 epoch 6 lr 2.000000e-03
2019-12-04 04:56:30,265 train 000 6.255094e-03 0.731629
2019-12-04 04:56:44,269 train 050 6.595468e-03 0.777420
2019-12-04 04:56:58,034 train 100 6.270738e-03 0.782702
2019-12-04 04:57:11,800 train 150 6.081922e-03 0.791935
2019-12-04 04:57:25,565 train 200 5.996492e-03 0.794052
2019-12-04 04:57:31,753 training loss; R2: 6.033457e-03 0.793034
2019-12-04 04:57:31,896 valid 000 5.710656e-03 0.889753
2019-12-04 04:57:33,363 validation loss; R2: 4.976633e-03 0.829481
2019-12-04 04:57:33,389 epoch 7 lr 2.000000e-03
2019-12-04 04:57:33,753 train 000 4.383153e-03 0.898938
2019-12-04 04:57:47,517 train 050 6.019238e-03 0.799450
2019-12-04 04:58:01,287 train 100 6.367672e-03 0.778685
2019-12-04 04:58:15,054 train 150 6.297824e-03 0.782750
2019-12-04 04:58:28,826 train 200 6.309587e-03 0.784014
2019-12-04 04:58:35,019 training loss; R2: 6.221269e-03 0.787080
2019-12-04 04:58:35,160 valid 000 5.144414e-03 0.826721
2019-12-04 04:58:36,628 validation loss; R2: 4.723743e-03 0.820656
2019-12-04 04:58:36,654 epoch 8 lr 2.000000e-03
2019-12-04 04:58:37,015 train 000 6.462906e-03 0.690748
2019-12-04 04:58:50,784 train 050 5.146433e-03 0.815913
2019-12-04 04:59:04,553 train 100 5.427266e-03 0.817381
2019-12-04 04:59:18,326 train 150 5.501353e-03 0.811518
2019-12-04 04:59:32,092 train 200 5.561611e-03 0.810892
2019-12-04 04:59:38,282 training loss; R2: 5.548718e-03 0.809337
2019-12-04 04:59:38,436 valid 000 3.206626e-03 0.788177
2019-12-04 04:59:39,904 validation loss; R2: 3.888297e-03 0.859368
2019-12-04 04:59:39,930 epoch 9 lr 2.000000e-03
2019-12-04 04:59:40,290 train 000 3.639573e-03 0.778055
2019-12-04 04:59:54,056 train 050 5.214117e-03 0.816724
2019-12-04 05:00:07,818 train 100 5.267608e-03 0.820092
2019-12-04 05:00:21,582 train 150 5.362554e-03 0.819879
2019-12-04 05:00:35,347 train 200 5.376481e-03 0.818108
2019-12-04 05:00:41,536 training loss; R2: 5.291844e-03 0.820547
2019-12-04 05:00:41,678 valid 000 2.923001e-03 0.887867
2019-12-04 05:00:43,146 validation loss; R2: 3.289104e-03 0.885303
2019-12-04 05:00:43,179 epoch 10 lr 2.000000e-03
2019-12-04 05:00:43,543 train 000 5.158391e-03 0.851871
2019-12-04 05:00:57,300 train 050 4.988598e-03 0.833137
2019-12-04 05:01:11,054 train 100 5.146317e-03 0.822586
2019-12-04 05:01:24,810 train 150 5.120639e-03 0.815182
2019-12-04 05:01:38,566 train 200 5.098664e-03 0.820039
2019-12-04 05:01:44,749 training loss; R2: 5.156174e-03 0.820535
2019-12-04 05:01:44,892 valid 000 2.655200e-03 0.901217
2019-12-04 05:01:46,360 validation loss; R2: 3.373967e-03 0.890606
2019-12-04 05:01:46,385 epoch 11 lr 2.000000e-03
2019-12-04 05:01:46,747 train 000 4.800398e-03 0.738780
2019-12-04 05:02:00,506 train 050 6.020466e-03 0.806443
2019-12-04 05:02:14,258 train 100 5.597552e-03 0.810779
2019-12-04 05:02:28,015 train 150 5.240130e-03 0.820601
2019-12-04 05:02:41,769 train 200 5.088823e-03 0.823940
2019-12-04 05:02:47,953 training loss; R2: 5.128785e-03 0.821913
2019-12-04 05:02:48,097 valid 000 6.299103e-03 0.821212
2019-12-04 05:02:49,565 validation loss; R2: 4.325884e-03 0.854169
2019-12-04 05:02:49,590 epoch 12 lr 2.000000e-03
2019-12-04 05:02:49,952 train 000 4.554710e-03 0.869435
2019-12-04 05:03:03,702 train 050 4.744424e-03 0.830287
2019-12-04 05:03:17,453 train 100 4.532229e-03 0.834341
2019-12-04 05:03:31,200 train 150 4.712477e-03 0.836852
2019-12-04 05:03:44,945 train 200 4.794881e-03 0.834219
2019-12-04 05:03:51,127 training loss; R2: 4.892978e-03 0.834919
2019-12-04 05:03:51,273 valid 000 4.854939e-03 0.831695
2019-12-04 05:03:52,740 validation loss; R2: 5.228561e-03 0.808008
2019-12-04 05:03:52,764 epoch 13 lr 2.000000e-03
2019-12-04 05:03:53,126 train 000 6.661983e-03 0.762598
2019-12-04 05:04:06,875 train 050 4.822823e-03 0.843939
2019-12-04 05:04:20,618 train 100 4.928462e-03 0.832002
2019-12-04 05:04:34,361 train 150 4.832495e-03 0.838717
2019-12-04 05:04:48,106 train 200 4.836612e-03 0.834207
2019-12-04 05:04:54,285 training loss; R2: 4.817704e-03 0.833960
2019-12-04 05:04:54,427 valid 000 5.024413e-03 0.933788
2019-12-04 05:04:55,894 validation loss; R2: 4.094562e-03 0.851690
2019-12-04 05:04:55,925 epoch 14 lr 2.000000e-03
2019-12-04 05:04:56,287 train 000 3.139083e-03 0.910248
2019-12-04 05:05:10,025 train 050 4.063311e-03 0.847951
2019-12-04 05:05:23,762 train 100 4.080873e-03 0.846439
2019-12-04 05:05:37,504 train 150 4.330913e-03 0.844810
2019-12-04 05:05:51,244 train 200 4.402820e-03 0.848115
2019-12-04 05:05:57,424 training loss; R2: 4.412819e-03 0.847741
2019-12-04 05:05:57,571 valid 000 6.346353e-01 -24.167229
2019-12-04 05:05:59,039 validation loss; R2: 6.076202e-01 -20.528363
2019-12-04 05:05:59,065 epoch 15 lr 2.000000e-03
2019-12-04 05:05:59,434 train 000 5.398028e-03 0.893425
2019-12-04 05:06:13,527 train 050 5.225782e-03 0.833851
2019-12-04 05:06:27,623 train 100 4.860684e-03 0.834692
2019-12-04 05:06:41,725 train 150 4.896341e-03 0.833760
2019-12-04 05:06:55,822 train 200 4.683008e-03 0.838225
2019-12-04 05:07:02,080 training loss; R2: 4.658016e-03 0.838354
2019-12-04 05:07:02,224 valid 000 1.389864e-01 -9.060722
2019-12-04 05:07:03,690 validation loss; R2: 1.327156e-01 -4.028742
2019-12-04 05:07:03,714 epoch 16 lr 2.000000e-03
2019-12-04 05:07:04,075 train 000 3.482426e-03 0.850387
2019-12-04 05:07:17,810 train 050 4.494091e-03 0.845845
2019-12-04 05:07:31,546 train 100 4.471996e-03 0.842304
2019-12-04 05:07:45,280 train 150 4.417826e-03 0.849008
2019-12-04 05:07:59,010 train 200 4.534406e-03 0.845222
2019-12-04 05:08:05,181 training loss; R2: 4.500086e-03 0.845386
2019-12-04 05:08:05,332 valid 000 1.029082e-02 0.760218
2019-12-04 05:08:06,798 validation loss; R2: 7.889041e-03 0.723770
2019-12-04 05:08:06,823 epoch 17 lr 2.000000e-03
2019-12-04 05:08:07,184 train 000 3.229912e-03 0.875044
2019-12-04 05:08:20,913 train 050 4.042439e-03 0.851942
2019-12-04 05:08:34,639 train 100 4.672173e-03 0.840500
2019-12-04 05:08:48,375 train 150 4.697565e-03 0.836899
2019-12-04 05:09:02,102 train 200 4.765089e-03 0.837318
2019-12-04 05:09:08,271 training loss; R2: 4.776498e-03 0.838590
2019-12-04 05:09:08,415 valid 000 1.147910e-01 -3.833331
2019-12-04 05:09:09,882 validation loss; R2: 1.220161e-01 -3.322904
2019-12-04 05:09:09,908 epoch 18 lr 2.000000e-03
2019-12-04 05:09:10,269 train 000 4.180812e-03 0.882883
2019-12-04 05:09:23,993 train 050 4.467853e-03 0.861600
2019-12-04 05:09:37,713 train 100 4.187571e-03 0.847535
2019-12-04 05:09:51,439 train 150 4.162764e-03 0.854700
2019-12-04 05:10:05,158 train 200 4.184899e-03 0.857314
2019-12-04 05:10:11,325 training loss; R2: 4.197901e-03 0.856593
2019-12-04 05:10:11,470 valid 000 1.340198e+00 -30.292186
2019-12-04 05:10:12,935 validation loss; R2: 1.385595e+00 -49.302684
2019-12-04 05:10:12,960 epoch 19 lr 2.000000e-03
2019-12-04 05:10:13,322 train 000 3.462898e-03 0.872964
2019-12-04 05:10:27,037 train 050 4.363438e-03 0.862943
2019-12-04 05:10:40,745 train 100 4.324461e-03 0.857532
2019-12-04 05:10:54,453 train 150 4.271444e-03 0.858917
2019-12-04 05:11:08,161 train 200 4.139756e-03 0.861654
2019-12-04 05:11:14,325 training loss; R2: 4.204387e-03 0.859584
2019-12-04 05:11:14,470 valid 000 8.272135e-03 0.513540
2019-12-04 05:11:15,937 validation loss; R2: 9.727732e-03 0.654193
