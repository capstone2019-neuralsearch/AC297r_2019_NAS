2019-12-04 02:41:00,864 gpu device = 1
2019-12-04 02:41:00,864 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=96, cutout=False, cutout_length=16, data='../data', dataset='graphene', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name=None, gpu=1, grad_clip=5, gz_dtree=False, init_channels=36, layers=8, learning_rate=0.002, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-graphene-EXP-20191204-024100', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-12-04 02:41:04,035 param size = 1.025173MB
2019-12-04 02:41:04,038 epoch 0 lr 2.000000e-03
2019-12-04 02:41:06,500 train 000 1.419534e+00 -26.729261
2019-12-04 02:41:17,534 train 050 2.258995e-01 -6.175267
2019-12-04 02:41:28,495 train 100 1.286245e-01 -3.107667
2019-12-04 02:41:39,457 train 150 9.517345e-02 -2.032075
2019-12-04 02:41:50,472 train 200 7.810503e-02 -1.489294
2019-12-04 02:41:56,547 training loss; R2: 7.262858e-02 -1.321721
2019-12-04 02:41:56,684 valid 000 2.058540e-02 0.281946
2019-12-04 02:41:58,193 validation loss; R2: 2.292770e-02 0.284280
2019-12-04 02:41:58,213 epoch 1 lr 2.000000e-03
2019-12-04 02:41:58,583 train 000 3.523586e-02 -0.211754
2019-12-04 02:42:09,788 train 050 2.444175e-02 0.136273
2019-12-04 02:42:21,003 train 100 2.271741e-02 0.216884
2019-12-04 02:42:32,221 train 150 2.176099e-02 0.273288
2019-12-04 02:42:43,439 train 200 2.027785e-02 0.319905
2019-12-04 02:42:48,486 training loss; R2: 2.018407e-02 0.329606
2019-12-04 02:42:48,615 valid 000 9.174325e-03 0.664981
2019-12-04 02:42:49,816 validation loss; R2: 8.949670e-03 0.692448
2019-12-04 02:42:49,835 epoch 2 lr 2.000000e-03
2019-12-04 02:42:50,142 train 000 1.636379e-02 0.726439
2019-12-04 02:43:01,364 train 050 1.514097e-02 0.484104
2019-12-04 02:43:12,584 train 100 1.396514e-02 0.526137
2019-12-04 02:43:23,806 train 150 1.359693e-02 0.541772
2019-12-04 02:43:35,033 train 200 1.341984e-02 0.553860
2019-12-04 02:43:40,079 training loss; R2: 1.337187e-02 0.548952
2019-12-04 02:43:40,210 valid 000 6.633252e-03 0.692927
2019-12-04 02:43:41,412 validation loss; R2: 9.998306e-03 0.682509
2019-12-04 02:43:41,431 epoch 3 lr 2.000000e-03
2019-12-04 02:43:41,742 train 000 8.028512e-03 0.557477
2019-12-04 02:43:52,952 train 050 1.297026e-02 0.585618
2019-12-04 02:44:04,167 train 100 1.192757e-02 0.612303
2019-12-04 02:44:15,380 train 150 1.182145e-02 0.609609
2019-12-04 02:44:26,590 train 200 1.148135e-02 0.622947
2019-12-04 02:44:31,632 training loss; R2: 1.132248e-02 0.623923
2019-12-04 02:44:31,763 valid 000 5.743728e-03 0.845517
2019-12-04 02:44:32,964 validation loss; R2: 5.347366e-03 0.818073
2019-12-04 02:44:32,985 epoch 4 lr 2.000000e-03
2019-12-04 02:44:33,294 train 000 7.803337e-03 0.701294
2019-12-04 02:44:44,507 train 050 1.139163e-02 0.635334
2019-12-04 02:44:55,718 train 100 1.134249e-02 0.631997
2019-12-04 02:45:06,930 train 150 1.110810e-02 0.642074
2019-12-04 02:45:18,139 train 200 1.065697e-02 0.648394
2019-12-04 02:45:23,183 training loss; R2: 1.042135e-02 0.654920
2019-12-04 02:45:23,312 valid 000 4.762045e-03 0.873629
2019-12-04 02:45:24,515 validation loss; R2: 5.170680e-03 0.816886
2019-12-04 02:45:24,534 epoch 5 lr 2.000000e-03
2019-12-04 02:45:24,842 train 000 8.296099e-03 0.773872
2019-12-04 02:45:36,046 train 050 8.442539e-03 0.704710
2019-12-04 02:45:47,030 train 100 9.399745e-03 0.690478
2019-12-04 02:45:57,996 train 150 9.267647e-03 0.688843
2019-12-04 02:46:08,961 train 200 9.115693e-03 0.691238
2019-12-04 02:46:13,889 training loss; R2: 9.000731e-03 0.694011
2019-12-04 02:46:14,019 valid 000 2.877359e-03 0.811017
2019-12-04 02:46:15,218 validation loss; R2: 6.234596e-03 0.799643
2019-12-04 02:46:15,238 epoch 6 lr 2.000000e-03
2019-12-04 02:46:15,542 train 000 7.452026e-03 0.540670
2019-12-04 02:46:26,507 train 050 9.379729e-03 0.697158
2019-12-04 02:46:37,472 train 100 9.479235e-03 0.703935
2019-12-04 02:46:48,439 train 150 9.030878e-03 0.694872
2019-12-04 02:46:59,404 train 200 8.965909e-03 0.692000
2019-12-04 02:47:04,333 training loss; R2: 9.021600e-03 0.687669
2019-12-04 02:47:04,461 valid 000 4.448901e-03 0.827361
2019-12-04 02:47:05,662 validation loss; R2: 6.007355e-03 0.798436
2019-12-04 02:47:05,681 epoch 7 lr 2.000000e-03
2019-12-04 02:47:05,984 train 000 7.610947e-03 0.691603
2019-12-04 02:47:16,947 train 050 8.760334e-03 0.696092
2019-12-04 02:47:27,908 train 100 8.755802e-03 0.706521
2019-12-04 02:47:38,873 train 150 8.656748e-03 0.708881
2019-12-04 02:47:49,836 train 200 8.486143e-03 0.718261
2019-12-04 02:47:54,765 training loss; R2: 8.346659e-03 0.718299
2019-12-04 02:47:54,895 valid 000 7.282658e-03 0.846421
2019-12-04 02:47:56,095 validation loss; R2: 4.230557e-03 0.858744
2019-12-04 02:47:56,115 epoch 8 lr 2.000000e-03
2019-12-04 02:47:56,422 train 000 8.203645e-03 0.537213
2019-12-04 02:48:07,388 train 050 9.472285e-03 0.689109
2019-12-04 02:48:18,353 train 100 1.054222e-02 0.661405
2019-12-04 02:48:29,315 train 150 1.010740e-02 0.668842
2019-12-04 02:48:40,277 train 200 9.482079e-03 0.686928
2019-12-04 02:48:45,204 training loss; R2: 9.372648e-03 0.688981
2019-12-04 02:48:45,337 valid 000 1.656735e-02 0.814040
2019-12-04 02:48:46,536 validation loss; R2: 4.559964e-03 0.851057
2019-12-04 02:48:46,554 epoch 9 lr 2.000000e-03
2019-12-04 02:48:46,857 train 000 1.092679e-02 0.769208
2019-12-04 02:48:57,815 train 050 8.505253e-03 0.728382
2019-12-04 02:49:08,776 train 100 8.467939e-03 0.719478
2019-12-04 02:49:19,740 train 150 8.855641e-03 0.715421
2019-12-04 02:49:30,701 train 200 8.785938e-03 0.708741
2019-12-04 02:49:35,628 training loss; R2: 8.644483e-03 0.710796
2019-12-04 02:49:35,760 valid 000 9.102928e-03 0.629006
2019-12-04 02:49:36,961 validation loss; R2: 7.758272e-03 0.720150
2019-12-04 02:49:36,979 epoch 10 lr 2.000000e-03
2019-12-04 02:49:37,282 train 000 6.443656e-03 0.691165
2019-12-04 02:49:48,242 train 050 8.111867e-03 0.730148
2019-12-04 02:49:59,201 train 100 7.636746e-03 0.746329
2019-12-04 02:50:10,157 train 150 7.614021e-03 0.743035
2019-12-04 02:50:21,115 train 200 7.538232e-03 0.742618
2019-12-04 02:50:26,043 training loss; R2: 7.474339e-03 0.747515
2019-12-04 02:50:26,177 valid 000 4.253589e-03 0.740474
2019-12-04 02:50:27,378 validation loss; R2: 5.064314e-03 0.824754
2019-12-04 02:50:27,397 epoch 11 lr 2.000000e-03
2019-12-04 02:50:27,695 train 000 1.297238e-02 0.722945
2019-12-04 02:50:38,654 train 050 8.504630e-03 0.735232
2019-12-04 02:50:49,613 train 100 7.778171e-03 0.740345
2019-12-04 02:51:00,568 train 150 7.253830e-03 0.753498
2019-12-04 02:51:11,527 train 200 7.265695e-03 0.756008
2019-12-04 02:51:16,454 training loss; R2: 7.208864e-03 0.756728
2019-12-04 02:51:16,585 valid 000 5.646724e-03 0.878517
2019-12-04 02:51:17,786 validation loss; R2: 4.019708e-03 0.866161
2019-12-04 02:51:17,812 epoch 12 lr 2.000000e-03
2019-12-04 02:51:18,115 train 000 6.558750e-03 0.832824
2019-12-04 02:51:29,073 train 050 6.216573e-03 0.789297
2019-12-04 02:51:40,037 train 100 6.766725e-03 0.771901
2019-12-04 02:51:50,997 train 150 6.870219e-03 0.769981
2019-12-04 02:52:01,956 train 200 6.788597e-03 0.770025
2019-12-04 02:52:06,883 training loss; R2: 6.753814e-03 0.771808
2019-12-04 02:52:07,023 valid 000 5.177743e-02 -0.102895
2019-12-04 02:52:08,223 validation loss; R2: 3.653566e-02 -0.204146
2019-12-04 02:52:08,241 epoch 13 lr 2.000000e-03
2019-12-04 02:52:08,542 train 000 7.559534e-03 0.851009
2019-12-04 02:52:19,496 train 050 6.702548e-03 0.785785
2019-12-04 02:52:30,452 train 100 6.446231e-03 0.784075
2019-12-04 02:52:41,405 train 150 6.391592e-03 0.787329
2019-12-04 02:52:52,359 train 200 6.574723e-03 0.782001
2019-12-04 02:52:57,288 training loss; R2: 6.524118e-03 0.781358
2019-12-04 02:52:57,425 valid 000 4.623620e-02 -0.394928
2019-12-04 02:52:58,625 validation loss; R2: 4.569563e-02 -0.638363
2019-12-04 02:52:58,645 epoch 14 lr 2.000000e-03
2019-12-04 02:52:58,946 train 000 6.067714e-03 0.785509
2019-12-04 02:53:09,904 train 050 7.012211e-03 0.760462
2019-12-04 02:53:20,867 train 100 7.288504e-03 0.761942
2019-12-04 02:53:31,822 train 150 7.042067e-03 0.765987
2019-12-04 02:53:42,777 train 200 6.821831e-03 0.770045
2019-12-04 02:53:47,701 training loss; R2: 6.889018e-03 0.771068
2019-12-04 02:53:47,832 valid 000 4.413820e-02 -0.569932
2019-12-04 02:53:49,032 validation loss; R2: 4.764618e-02 -0.664215
2019-12-04 02:53:49,050 epoch 15 lr 2.000000e-03
2019-12-04 02:53:49,351 train 000 5.240227e-03 0.730674
2019-12-04 02:54:00,303 train 050 5.341290e-03 0.794971
2019-12-04 02:54:11,249 train 100 5.598825e-03 0.783239
2019-12-04 02:54:22,197 train 150 5.970203e-03 0.785022
2019-12-04 02:54:33,142 train 200 6.233071e-03 0.784476
2019-12-04 02:54:38,066 training loss; R2: 6.348756e-03 0.780334
2019-12-04 02:54:38,196 valid 000 5.195721e-03 0.874189
2019-12-04 02:54:39,396 validation loss; R2: 4.014146e-03 0.862298
2019-12-04 02:54:39,415 epoch 16 lr 2.000000e-03
2019-12-04 02:54:39,719 train 000 7.289607e-03 0.734731
2019-12-04 02:54:50,668 train 050 6.758633e-03 0.772457
2019-12-04 02:55:01,616 train 100 6.600739e-03 0.775750
2019-12-04 02:55:12,561 train 150 6.674933e-03 0.781038
2019-12-04 02:55:23,504 train 200 6.622977e-03 0.777447
2019-12-04 02:55:28,429 training loss; R2: 6.592761e-03 0.778907
2019-12-04 02:55:28,560 valid 000 2.483448e-02 -0.896181
2019-12-04 02:55:29,759 validation loss; R2: 3.723198e-02 -0.278790
2019-12-04 02:55:29,778 epoch 17 lr 2.000000e-03
2019-12-04 02:55:30,084 train 000 6.676835e-03 0.781147
2019-12-04 02:55:41,031 train 050 5.607244e-03 0.808964
2019-12-04 02:55:51,980 train 100 5.757357e-03 0.811533
2019-12-04 02:56:02,924 train 150 6.219811e-03 0.795592
2019-12-04 02:56:13,869 train 200 6.146441e-03 0.793080
2019-12-04 02:56:18,789 training loss; R2: 6.068175e-03 0.792484
2019-12-04 02:56:18,919 valid 000 6.014550e-03 0.824406
2019-12-04 02:56:20,118 validation loss; R2: 7.035253e-03 0.769855
2019-12-04 02:56:20,137 epoch 18 lr 2.000000e-03
2019-12-04 02:56:20,439 train 000 4.990983e-03 0.820965
2019-12-04 02:56:31,376 train 050 5.985760e-03 0.795136
2019-12-04 02:56:42,314 train 100 5.944740e-03 0.799530
2019-12-04 02:56:53,253 train 150 6.007922e-03 0.798816
2019-12-04 02:57:04,192 train 200 6.041307e-03 0.794852
2019-12-04 02:57:09,112 training loss; R2: 6.078057e-03 0.795792
2019-12-04 02:57:09,243 valid 000 7.500791e-02 -2.114693
2019-12-04 02:57:10,442 validation loss; R2: 6.906729e-02 -1.457934
2019-12-04 02:57:10,467 epoch 19 lr 2.000000e-03
2019-12-04 02:57:10,767 train 000 5.996827e-03 0.795006
2019-12-04 02:57:21,707 train 050 6.108395e-03 0.793545
2019-12-04 02:57:32,645 train 100 6.069137e-03 0.799212
2019-12-04 02:57:43,583 train 150 5.860674e-03 0.799716
2019-12-04 02:57:54,516 train 200 5.859801e-03 0.801660
2019-12-04 02:57:59,432 training loss; R2: 5.865357e-03 0.803848
2019-12-04 02:57:59,565 valid 000 1.886182e-02 0.006137
2019-12-04 02:58:00,764 validation loss; R2: 3.176656e-02 -0.005768
