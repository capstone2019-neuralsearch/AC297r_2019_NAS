2019-12-04 06:51:01,648 gpu device = 1
2019-12-04 06:51:01,649 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=96, cutout=False, cutout_length=16, data='../data', dataset='graphene', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name=None, gpu=1, grad_clip=5, gz_dtree=False, init_channels=36, layers=8, learning_rate=0.002, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-graphene-EXP-20191204-065101', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-12-04 06:51:04,847 param size = 0.820981MB
2019-12-04 06:51:04,850 epoch 0 lr 2.000000e-03
2019-12-04 06:51:07,204 train 000 8.595321e-01 -36.154597
2019-12-04 06:51:16,251 train 050 2.277150e-01 -8.077982
2019-12-04 06:51:25,221 train 100 1.334308e-01 -4.194280
2019-12-04 06:51:34,197 train 150 1.001396e-01 -2.836329
2019-12-04 06:51:43,171 train 200 8.194160e-02 -2.105102
2019-12-04 06:51:48,178 training loss; R2: 7.636976e-02 -1.871415
2019-12-04 06:51:48,310 valid 000 1.099544e-02 0.372448
2019-12-04 06:51:49,616 validation loss; R2: 1.707138e-02 0.443810
2019-12-04 06:51:49,630 epoch 1 lr 2.000000e-03
2019-12-04 06:51:49,943 train 000 1.539900e-02 0.426160
2019-12-04 06:51:58,919 train 050 2.235309e-02 0.242512
2019-12-04 06:52:07,899 train 100 2.040970e-02 0.314221
2019-12-04 06:52:16,877 train 150 1.966366e-02 0.360268
2019-12-04 06:52:25,856 train 200 1.913219e-02 0.377784
2019-12-04 06:52:29,895 training loss; R2: 1.894740e-02 0.386770
2019-12-04 06:52:30,017 valid 000 8.958942e-03 0.775085
2019-12-04 06:52:31,012 validation loss; R2: 9.790451e-03 0.696490
2019-12-04 06:52:31,027 epoch 2 lr 2.000000e-03
2019-12-04 06:52:31,283 train 000 1.265792e-02 0.555040
2019-12-04 06:52:40,267 train 050 1.471483e-02 0.528898
2019-12-04 06:52:49,253 train 100 1.316361e-02 0.546134
2019-12-04 06:52:58,237 train 150 1.317118e-02 0.549473
2019-12-04 06:53:07,220 train 200 1.283894e-02 0.567300
2019-12-04 06:53:11,258 training loss; R2: 1.288618e-02 0.568130
2019-12-04 06:53:11,383 valid 000 6.158873e-03 0.694296
2019-12-04 06:53:12,379 validation loss; R2: 8.398072e-03 0.729731
2019-12-04 06:53:12,394 epoch 3 lr 2.000000e-03
2019-12-04 06:53:12,652 train 000 1.142001e-02 0.384704
2019-12-04 06:53:21,631 train 050 1.194828e-02 0.603428
2019-12-04 06:53:30,612 train 100 1.163814e-02 0.604999
2019-12-04 06:53:39,594 train 150 1.121267e-02 0.615808
2019-12-04 06:53:48,574 train 200 1.092528e-02 0.631935
2019-12-04 06:53:52,612 training loss; R2: 1.085115e-02 0.637221
2019-12-04 06:53:52,736 valid 000 9.097219e-03 0.826766
2019-12-04 06:53:53,732 validation loss; R2: 6.793114e-03 0.771731
2019-12-04 06:53:53,746 epoch 4 lr 2.000000e-03
2019-12-04 06:53:54,004 train 000 1.379836e-02 0.657495
2019-12-04 06:54:02,991 train 050 8.837692e-03 0.711655
2019-12-04 06:54:11,976 train 100 8.814852e-03 0.694159
2019-12-04 06:54:20,965 train 150 8.923706e-03 0.698050
2019-12-04 06:54:29,949 train 200 9.098952e-03 0.699638
2019-12-04 06:54:33,990 training loss; R2: 9.061469e-03 0.696177
2019-12-04 06:54:34,112 valid 000 2.195316e-02 0.624066
2019-12-04 06:54:35,108 validation loss; R2: 1.092167e-02 0.629199
2019-12-04 06:54:35,123 epoch 5 lr 2.000000e-03
2019-12-04 06:54:35,391 train 000 9.373906e-03 0.774151
2019-12-04 06:54:44,377 train 050 8.758069e-03 0.703201
2019-12-04 06:54:53,361 train 100 8.605047e-03 0.700962
2019-12-04 06:55:02,346 train 150 9.530527e-03 0.670886
2019-12-04 06:55:11,331 train 200 9.493114e-03 0.676502
2019-12-04 06:55:15,372 training loss; R2: 9.381417e-03 0.681822
2019-12-04 06:55:15,498 valid 000 5.615082e-03 0.747531
2019-12-04 06:55:16,494 validation loss; R2: 6.318552e-03 0.795797
2019-12-04 06:55:16,515 epoch 6 lr 2.000000e-03
2019-12-04 06:55:16,777 train 000 6.515324e-03 0.702481
2019-12-04 06:55:25,759 train 050 8.055826e-03 0.734004
2019-12-04 06:55:34,755 train 100 7.846166e-03 0.743973
2019-12-04 06:55:43,753 train 150 7.666439e-03 0.748229
2019-12-04 06:55:52,748 train 200 7.789164e-03 0.741334
2019-12-04 06:55:56,791 training loss; R2: 7.707565e-03 0.742951
2019-12-04 06:55:56,911 valid 000 5.199934e-03 0.889466
2019-12-04 06:55:57,907 validation loss; R2: 4.700059e-03 0.846220
2019-12-04 06:55:57,921 epoch 7 lr 2.000000e-03
2019-12-04 06:55:58,179 train 000 4.549524e-03 0.764885
2019-12-04 06:56:07,164 train 050 6.688141e-03 0.775452
2019-12-04 06:56:16,149 train 100 7.352678e-03 0.763600
2019-12-04 06:56:25,134 train 150 7.413263e-03 0.759904
2019-12-04 06:56:34,122 train 200 7.405268e-03 0.751957
2019-12-04 06:56:38,160 training loss; R2: 7.382933e-03 0.748483
2019-12-04 06:56:38,295 valid 000 4.439456e-03 0.797033
2019-12-04 06:56:39,290 validation loss; R2: 3.958671e-03 0.858038
2019-12-04 06:56:39,305 epoch 8 lr 2.000000e-03
2019-12-04 06:56:39,565 train 000 6.836555e-03 0.806716
2019-12-04 06:56:48,545 train 050 7.141958e-03 0.766111
2019-12-04 06:56:57,525 train 100 7.059143e-03 0.762150
2019-12-04 06:57:06,506 train 150 6.971839e-03 0.760626
2019-12-04 06:57:15,485 train 200 6.867920e-03 0.764058
2019-12-04 06:57:19,521 training loss; R2: 6.844406e-03 0.765364
2019-12-04 06:57:19,643 valid 000 5.140664e-03 0.818922
2019-12-04 06:57:20,639 validation loss; R2: 3.898653e-03 0.870952
2019-12-04 06:57:20,653 epoch 9 lr 2.000000e-03
2019-12-04 06:57:20,915 train 000 5.859696e-03 0.740926
2019-12-04 06:57:29,895 train 050 6.291468e-03 0.779245
2019-12-04 06:57:38,877 train 100 6.706619e-03 0.765993
2019-12-04 06:57:47,860 train 150 6.878845e-03 0.758677
2019-12-04 06:57:56,837 train 200 6.705134e-03 0.768062
2019-12-04 06:58:00,875 training loss; R2: 6.757010e-03 0.764448
2019-12-04 06:58:00,998 valid 000 5.581062e-03 0.742258
2019-12-04 06:58:01,993 validation loss; R2: 3.765668e-03 0.862092
2019-12-04 06:58:02,008 epoch 10 lr 2.000000e-03
2019-12-04 06:58:02,270 train 000 7.644345e-03 0.856150
2019-12-04 06:58:11,256 train 050 6.016742e-03 0.794169
2019-12-04 06:58:20,240 train 100 5.910762e-03 0.798207
2019-12-04 06:58:29,225 train 150 6.410963e-03 0.779502
2019-12-04 06:58:38,211 train 200 6.264965e-03 0.783678
2019-12-04 06:58:42,250 training loss; R2: 6.250875e-03 0.785867
2019-12-04 06:58:42,384 valid 000 3.782147e-03 0.775077
2019-12-04 06:58:43,380 validation loss; R2: 3.492273e-03 0.872148
2019-12-04 06:58:43,395 epoch 11 lr 2.000000e-03
2019-12-04 06:58:43,654 train 000 4.869138e-03 0.869393
2019-12-04 06:58:52,637 train 050 6.505022e-03 0.786927
2019-12-04 06:59:01,663 train 100 6.217053e-03 0.796216
2019-12-04 06:59:10,825 train 150 6.152634e-03 0.798020
2019-12-04 06:59:19,987 train 200 6.088030e-03 0.795968
2019-12-04 06:59:24,106 training loss; R2: 5.992754e-03 0.798250
2019-12-04 06:59:24,229 valid 000 2.320469e-03 0.865298
2019-12-04 06:59:25,226 validation loss; R2: 3.239761e-03 0.882680
2019-12-04 06:59:25,240 epoch 12 lr 2.000000e-03
2019-12-04 06:59:25,504 train 000 3.184176e-03 0.867788
2019-12-04 06:59:34,679 train 050 5.637821e-03 0.812677
2019-12-04 06:59:43,853 train 100 6.027659e-03 0.790425
2019-12-04 06:59:53,023 train 150 5.706039e-03 0.801535
2019-12-04 07:00:02,195 train 200 5.809004e-03 0.800107
2019-12-04 07:00:06,317 training loss; R2: 5.901451e-03 0.799706
2019-12-04 07:00:06,441 valid 000 3.277991e-03 0.840646
2019-12-04 07:00:07,438 validation loss; R2: 4.876838e-03 0.833556
2019-12-04 07:00:07,453 epoch 13 lr 2.000000e-03
2019-12-04 07:00:07,716 train 000 6.263478e-03 0.825645
2019-12-04 07:00:16,879 train 050 6.363135e-03 0.771231
2019-12-04 07:00:26,039 train 100 6.258433e-03 0.778173
2019-12-04 07:00:35,205 train 150 6.091696e-03 0.788542
2019-12-04 07:00:44,366 train 200 5.922866e-03 0.799712
2019-12-04 07:00:48,486 training loss; R2: 5.863251e-03 0.797758
2019-12-04 07:00:48,612 valid 000 2.448492e-03 0.884219
2019-12-04 07:00:49,609 validation loss; R2: 3.920911e-03 0.870848
2019-12-04 07:00:49,624 epoch 14 lr 2.000000e-03
2019-12-04 07:00:49,885 train 000 3.365029e-03 0.824150
2019-12-04 07:00:59,051 train 050 5.830455e-03 0.797942
2019-12-04 07:01:08,218 train 100 5.481359e-03 0.817741
2019-12-04 07:01:17,384 train 150 5.620754e-03 0.811075
2019-12-04 07:01:26,546 train 200 5.497141e-03 0.816069
2019-12-04 07:01:30,669 training loss; R2: 5.481332e-03 0.817439
2019-12-04 07:01:30,793 valid 000 3.783024e-03 0.889446
2019-12-04 07:01:31,790 validation loss; R2: 2.863737e-03 0.897072
2019-12-04 07:01:31,805 epoch 15 lr 2.000000e-03
2019-12-04 07:01:32,068 train 000 4.282957e-03 0.840294
2019-12-04 07:01:41,233 train 050 5.418142e-03 0.798589
2019-12-04 07:01:50,392 train 100 5.366139e-03 0.800706
2019-12-04 07:01:59,553 train 150 5.466168e-03 0.810481
2019-12-04 07:02:08,715 train 200 5.335532e-03 0.815162
2019-12-04 07:02:12,836 training loss; R2: 5.316349e-03 0.816336
2019-12-04 07:02:12,958 valid 000 5.902410e-03 0.741570
2019-12-04 07:02:13,955 validation loss; R2: 6.462125e-03 0.777123
2019-12-04 07:02:13,970 epoch 16 lr 2.000000e-03
2019-12-04 07:02:14,236 train 000 9.582568e-03 0.845406
2019-12-04 07:02:23,398 train 050 6.346846e-03 0.790805
2019-12-04 07:02:32,552 train 100 5.694970e-03 0.805096
2019-12-04 07:02:41,708 train 150 5.381948e-03 0.817901
2019-12-04 07:02:50,862 train 200 5.378879e-03 0.817123
2019-12-04 07:02:54,976 training loss; R2: 5.399777e-03 0.816191
2019-12-04 07:02:55,098 valid 000 5.124700e-03 0.814387
2019-12-04 07:02:56,095 validation loss; R2: 5.919083e-03 0.782861
2019-12-04 07:02:56,117 epoch 17 lr 2.000000e-03
2019-12-04 07:02:56,382 train 000 2.998420e-03 0.848323
2019-12-04 07:03:05,541 train 050 5.770016e-03 0.817257
2019-12-04 07:03:14,702 train 100 5.532153e-03 0.822197
2019-12-04 07:03:23,857 train 150 5.213523e-03 0.828798
2019-12-04 07:03:33,016 train 200 5.220482e-03 0.828213
2019-12-04 07:03:37,136 training loss; R2: 5.142944e-03 0.829068
2019-12-04 07:03:37,257 valid 000 2.506715e-03 0.934625
2019-12-04 07:03:38,254 validation loss; R2: 3.115880e-03 0.895453
2019-12-04 07:03:38,269 epoch 18 lr 2.000000e-03
2019-12-04 07:03:38,534 train 000 5.127548e-03 0.906388
2019-12-04 07:03:47,685 train 050 4.247484e-03 0.826586
2019-12-04 07:03:56,832 train 100 4.875559e-03 0.824181
2019-12-04 07:04:05,979 train 150 5.154736e-03 0.825325
2019-12-04 07:04:15,127 train 200 5.050730e-03 0.825598
2019-12-04 07:04:19,240 training loss; R2: 5.026548e-03 0.826495
2019-12-04 07:04:19,364 valid 000 2.193787e-02 -0.096474
2019-12-04 07:04:20,360 validation loss; R2: 2.624298e-02 0.062551
2019-12-04 07:04:20,375 epoch 19 lr 2.000000e-03
2019-12-04 07:04:20,640 train 000 2.716523e-03 0.868023
2019-12-04 07:04:29,788 train 050 5.362512e-03 0.815371
2019-12-04 07:04:38,910 train 100 4.914373e-03 0.831929
2019-12-04 07:04:47,861 train 150 4.959005e-03 0.833553
2019-12-04 07:04:56,813 train 200 4.992064e-03 0.832742
2019-12-04 07:05:00,834 training loss; R2: 4.970800e-03 0.832634
2019-12-04 07:05:00,958 valid 000 2.440194e-03 0.884407
2019-12-04 07:05:01,952 validation loss; R2: 3.445982e-03 0.884453
