2019-11-12 15:39:22,987 gpu device = 1
2019-11-12 15:39:22,988 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name='galaxy-zoo-cleaned', gpu=1, grad_clip=5, gz_dtree=False, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-galaxy-zoo-EXP-20191112-153922', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-11-12 15:39:34,535 param size = 0.276117MB
2019-11-12 15:39:34,539 epoch 0 lr 1.000000e-03
2019-11-12 15:39:36,696 train 000 5.589578e-01 -165.001349
2019-11-12 15:39:43,408 train 050 4.912625e-02 -12.232856
2019-11-12 15:39:50,239 train 100 3.775921e-02 -6.427096
2019-11-12 15:39:57,013 train 150 3.357592e-02 -4.411211
2019-11-12 15:40:03,771 train 200 3.118834e-02 -3.368654
2019-11-12 15:40:10,460 train 250 2.951772e-02 -2.730480
2019-11-12 15:40:17,078 train 300 2.831215e-02 -2.304737
2019-11-12 15:40:23,635 train 350 2.725052e-02 -1.997081
2019-11-12 15:40:30,201 train 400 2.637463e-02 -1.762940
2019-11-12 15:40:36,907 train 450 2.572385e-02 -1.570716
2019-11-12 15:40:43,477 train 500 2.506572e-02 -1.413463
2019-11-12 15:40:50,014 train 550 2.445624e-02 -1.282067
2019-11-12 15:40:56,525 train 600 2.393058e-02 -1.170954
2019-11-12 15:41:03,150 train 650 2.351452e-02 -1.075150
2019-11-12 15:41:09,857 train 700 2.308829e-02 -0.994693
2019-11-12 15:41:16,383 train 750 2.274025e-02 -0.923624
2019-11-12 15:41:23,058 train 800 2.241285e-02 -0.865692
2019-11-12 15:41:29,765 train 850 2.209962e-02 -0.807568
2019-11-12 15:41:32,413 training loss; R2: 2.201409e-02 -0.792025
2019-11-12 15:41:32,673 valid 000 1.757746e-02 0.125254
2019-11-12 15:41:34,378 valid 050 1.568030e-02 0.114167
2019-11-12 15:41:35,956 validation loss; R2: 1.565693e-02 0.093094
2019-11-12 15:41:35,980 epoch 1 lr 1.000000e-03
2019-11-12 15:41:36,490 train 000 1.490625e-02 0.073070
2019-11-12 15:41:43,149 train 050 1.621126e-02 0.089063
2019-11-12 15:41:49,844 train 100 1.648504e-02 0.096002
2019-11-12 15:41:56,440 train 150 1.643644e-02 0.107112
2019-11-12 15:42:03,203 train 200 1.645294e-02 0.112287
2019-11-12 15:42:09,778 train 250 1.635290e-02 0.121343
2019-11-12 15:42:16,385 train 300 1.625615e-02 0.128228
2019-11-12 15:42:23,157 train 350 1.619650e-02 0.129225
2019-11-12 15:42:29,668 train 400 1.616673e-02 0.132695
2019-11-12 15:42:36,395 train 450 1.612636e-02 0.135280
2019-11-12 15:42:43,117 train 500 1.602713e-02 0.136777
2019-11-12 15:42:49,860 train 550 1.595706e-02 0.133493
2019-11-12 15:42:56,434 train 600 1.586291e-02 0.133872
2019-11-12 15:43:02,954 train 650 1.577889e-02 0.138061
2019-11-12 15:43:09,547 train 700 1.570673e-02 0.137753
2019-11-12 15:43:16,002 train 750 1.564262e-02 0.140307
2019-11-12 15:43:22,454 train 800 1.556133e-02 0.144881
2019-11-12 15:43:28,909 train 850 1.550015e-02 0.147939
2019-11-12 15:43:30,837 training loss; R2: 1.547398e-02 0.149708
2019-11-12 15:43:31,121 valid 000 1.309187e-02 0.266967
2019-11-12 15:43:32,791 valid 050 1.263785e-02 0.278785
2019-11-12 15:43:34,308 validation loss; R2: 1.272887e-02 0.264036
2019-11-12 15:43:34,324 epoch 2 lr 1.000000e-03
2019-11-12 15:43:34,694 train 000 1.434705e-02 0.173319
2019-11-12 15:43:41,138 train 050 1.408713e-02 0.016624
2019-11-12 15:43:47,583 train 100 1.418966e-02 0.094535
2019-11-12 15:43:54,028 train 150 1.426276e-02 0.131998
2019-11-12 15:44:00,468 train 200 1.419585e-02 0.143485
2019-11-12 15:44:06,908 train 250 1.419646e-02 0.145651
2019-11-12 15:44:13,353 train 300 1.413151e-02 0.149776
2019-11-12 15:44:19,794 train 350 1.406609e-02 0.145428
2019-11-12 15:44:26,238 train 400 1.407969e-02 0.152078
2019-11-12 15:44:32,678 train 450 1.403688e-02 0.148983
2019-11-12 15:44:39,115 train 500 1.398025e-02 0.156061
2019-11-12 15:44:45,558 train 550 1.393299e-02 0.163949
2019-11-12 15:44:52,011 train 600 1.391534e-02 0.167418
2019-11-12 15:44:58,452 train 650 1.390100e-02 0.159461
2019-11-12 15:45:04,888 train 700 1.384733e-02 0.163788
2019-11-12 15:45:11,330 train 750 1.380674e-02 0.169604
2019-11-12 15:45:17,767 train 800 1.376422e-02 0.175672
2019-11-12 15:45:24,265 train 850 1.370950e-02 0.179579
2019-11-12 15:45:26,191 training loss; R2: 1.370549e-02 0.180917
2019-11-12 15:45:26,465 valid 000 1.235894e-02 0.390456
2019-11-12 15:45:28,196 valid 050 1.195573e-02 -0.026739
2019-11-12 15:45:29,773 validation loss; R2: 1.187698e-02 0.089574
2019-11-12 15:45:29,789 epoch 3 lr 1.000000e-03
2019-11-12 15:45:30,140 train 000 1.192628e-02 -1.136365
2019-11-12 15:45:36,924 train 050 1.292315e-02 0.220510
2019-11-12 15:45:43,780 train 100 1.290056e-02 0.236996
2019-11-12 15:45:50,390 train 150 1.294099e-02 0.245591
2019-11-12 15:45:56,842 train 200 1.298917e-02 0.216869
2019-11-12 15:46:03,292 train 250 1.288336e-02 0.231857
2019-11-12 15:46:09,746 train 300 1.290759e-02 0.236557
2019-11-12 15:46:16,199 train 350 1.291153e-02 0.232805
2019-11-12 15:46:22,652 train 400 1.289204e-02 0.236352
2019-11-12 15:46:29,098 train 450 1.290136e-02 0.238657
2019-11-12 15:46:35,549 train 500 1.290385e-02 0.243527
2019-11-12 15:46:42,007 train 550 1.287890e-02 0.221298
2019-11-12 15:46:48,465 train 600 1.285899e-02 0.220496
2019-11-12 15:46:54,921 train 650 1.281676e-02 0.222971
2019-11-12 15:47:01,381 train 700 1.278256e-02 0.219872
2019-11-12 15:47:07,890 train 750 1.276859e-02 0.223183
2019-11-12 15:47:14,344 train 800 1.275000e-02 0.227408
2019-11-12 15:47:20,792 train 850 1.272849e-02 0.229646
2019-11-12 15:47:22,723 training loss; R2: 1.272148e-02 0.230648
2019-11-12 15:47:23,013 valid 000 1.202843e-02 0.326024
2019-11-12 15:47:24,711 valid 050 1.282137e-02 0.283243
2019-11-12 15:47:26,276 validation loss; R2: 1.259294e-02 0.288994
2019-11-12 15:47:26,299 epoch 4 lr 1.000000e-03
2019-11-12 15:47:26,670 train 000 1.266733e-02 0.329052
2019-11-12 15:47:33,500 train 050 1.242033e-02 0.270941
2019-11-12 15:47:40,313 train 100 1.229002e-02 0.274194
2019-11-12 15:47:46,846 train 150 1.232974e-02 0.273745
2019-11-12 15:47:53,576 train 200 1.232748e-02 0.276525
2019-11-12 15:48:00,215 train 250 1.225313e-02 0.273622
2019-11-12 15:48:06,754 train 300 1.220700e-02 0.269612
2019-11-12 15:48:13,362 train 350 1.218787e-02 0.274057
2019-11-12 15:48:19,897 train 400 1.220142e-02 0.274179
2019-11-12 15:48:26,478 train 450 1.213521e-02 0.276419
2019-11-12 15:48:33,133 train 500 1.214689e-02 0.277527
2019-11-12 15:48:39,775 train 550 1.213978e-02 0.278395
2019-11-12 15:48:46,501 train 600 1.209272e-02 0.274885
2019-11-12 15:48:53,188 train 650 1.209137e-02 0.268577
2019-11-12 15:48:59,807 train 700 1.208026e-02 0.270042
2019-11-12 15:49:06,458 train 750 1.207464e-02 0.272426
2019-11-12 15:49:13,044 train 800 1.205746e-02 0.274011
2019-11-12 15:49:19,625 train 850 1.206533e-02 0.274736
2019-11-12 15:49:21,577 training loss; R2: 1.205978e-02 0.275376
2019-11-12 15:49:21,879 valid 000 1.850043e-02 0.179724
2019-11-12 15:49:23,570 valid 050 1.889556e-02 0.170880
2019-11-12 15:49:25,102 validation loss; R2: 1.874954e-02 0.175165
2019-11-12 15:49:25,121 epoch 5 lr 1.000000e-03
2019-11-12 15:49:25,533 train 000 1.076690e-02 0.334719
2019-11-12 15:49:32,093 train 050 1.142049e-02 0.264539
2019-11-12 15:49:38,598 train 100 1.149072e-02 0.262184
2019-11-12 15:49:45,093 train 150 1.170443e-02 0.272921
2019-11-12 15:49:51,637 train 200 1.171591e-02 0.275419
2019-11-12 15:49:58,229 train 250 1.180254e-02 0.273662
2019-11-12 15:50:04,750 train 300 1.179136e-02 0.278678
2019-11-12 15:50:11,243 train 350 1.176791e-02 0.274455
2019-11-12 15:50:17,838 train 400 1.171619e-02 0.257370
2019-11-12 15:50:24,336 train 450 1.168313e-02 0.266957
2019-11-12 15:50:30,913 train 500 1.169053e-02 0.272391
2019-11-12 15:50:37,420 train 550 1.166762e-02 0.275383
2019-11-12 15:50:43,904 train 600 1.163580e-02 0.277439
2019-11-12 15:50:50,538 train 650 1.164009e-02 0.279375
2019-11-12 15:50:57,038 train 700 1.164212e-02 0.281325
2019-11-12 15:51:03,533 train 750 1.163019e-02 0.283167
2019-11-12 15:51:10,097 train 800 1.159925e-02 0.270164
2019-11-12 15:51:16,795 train 850 1.158666e-02 0.269578
2019-11-12 15:51:18,735 training loss; R2: 1.158912e-02 0.270214
2019-11-12 15:51:19,023 valid 000 1.097601e-02 0.417420
2019-11-12 15:51:20,753 valid 050 1.117480e-02 0.384673
2019-11-12 15:51:22,315 validation loss; R2: 1.100602e-02 0.345715
2019-11-12 15:51:22,330 epoch 6 lr 1.000000e-03
2019-11-12 15:51:22,714 train 000 1.057389e-02 0.371854
2019-11-12 15:51:29,540 train 050 1.133074e-02 0.214295
2019-11-12 15:51:36,168 train 100 1.129015e-02 0.263485
2019-11-12 15:51:42,889 train 150 1.120885e-02 0.287729
2019-11-12 15:51:49,540 train 200 1.126482e-02 0.285446
2019-11-12 15:51:56,318 train 250 1.131955e-02 0.285607
2019-11-12 15:52:03,150 train 300 1.134040e-02 0.180677
2019-11-12 15:52:09,795 train 350 1.130002e-02 0.199936
2019-11-12 15:52:16,314 train 400 1.128393e-02 0.213194
2019-11-12 15:52:22,875 train 450 1.129735e-02 0.221929
2019-11-12 15:52:29,455 train 500 1.128485e-02 0.228926
2019-11-12 15:52:36,076 train 550 1.127426e-02 0.236574
2019-11-12 15:52:42,671 train 600 1.128323e-02 0.242435
2019-11-12 15:52:49,205 train 650 1.127901e-02 0.250052
2019-11-12 15:52:55,743 train 700 1.125895e-02 0.255630
2019-11-12 15:53:02,433 train 750 1.125416e-02 0.258359
2019-11-12 15:53:09,017 train 800 1.123645e-02 0.263243
2019-11-12 15:53:15,679 train 850 1.124074e-02 0.262425
2019-11-12 15:53:17,648 training loss; R2: 1.124216e-02 0.263671
2019-11-12 15:53:17,933 valid 000 9.909077e-03 0.442297
2019-11-12 15:53:19,648 valid 050 1.015178e-02 0.394573
2019-11-12 15:53:21,209 validation loss; R2: 1.020384e-02 0.383455
2019-11-12 15:53:21,223 epoch 7 lr 1.000000e-03
2019-11-12 15:53:21,607 train 000 1.210856e-02 0.360483
2019-11-12 15:53:28,360 train 050 1.074998e-02 0.317332
2019-11-12 15:53:35,080 train 100 1.086880e-02 0.313775
2019-11-12 15:53:41,803 train 150 1.088140e-02 0.314489
2019-11-12 15:53:48,518 train 200 1.090702e-02 0.322021
2019-11-12 15:53:55,240 train 250 1.098253e-02 0.315814
2019-11-12 15:54:01,957 train 300 1.099504e-02 0.317245
2019-11-12 15:54:08,668 train 350 1.102286e-02 0.316094
2019-11-12 15:54:15,384 train 400 1.102402e-02 0.319585
2019-11-12 15:54:22,094 train 450 1.101630e-02 0.318365
2019-11-12 15:54:28,804 train 500 1.101483e-02 0.316365
2019-11-12 15:54:35,239 train 550 1.103320e-02 0.316342
2019-11-12 15:54:41,670 train 600 1.103955e-02 0.315358
2019-11-12 15:54:48,097 train 650 1.104230e-02 0.314655
2019-11-12 15:54:54,522 train 700 1.103253e-02 0.315677
2019-11-12 15:55:00,951 train 750 1.100751e-02 0.314895
2019-11-12 15:55:07,378 train 800 1.098712e-02 0.314935
2019-11-12 15:55:13,807 train 850 1.099684e-02 0.315498
2019-11-12 15:55:15,731 training loss; R2: 1.099559e-02 0.316144
2019-11-12 15:55:16,031 valid 000 8.609008e-03 0.443770
2019-11-12 15:55:17,711 valid 050 1.059556e-02 0.368080
2019-11-12 15:55:19,244 validation loss; R2: 1.052422e-02 0.380813
2019-11-12 15:55:19,269 epoch 8 lr 1.000000e-03
2019-11-12 15:55:19,676 train 000 9.201186e-03 0.396515
2019-11-12 15:55:26,362 train 050 1.098306e-02 0.323616
2019-11-12 15:55:33,116 train 100 1.081773e-02 0.332573
2019-11-12 15:55:39,728 train 150 1.093121e-02 0.334022
2019-11-12 15:55:46,357 train 200 1.094017e-02 0.336947
2019-11-12 15:55:52,945 train 250 1.092323e-02 0.327025
2019-11-12 15:55:59,584 train 300 1.092777e-02 0.197310
2019-11-12 15:56:06,259 train 350 1.089171e-02 0.215832
2019-11-12 15:56:12,815 train 400 1.085999e-02 0.222340
2019-11-12 15:56:19,366 train 450 1.082534e-02 0.238523
2019-11-12 15:56:25,919 train 500 1.078114e-02 0.250733
2019-11-12 15:56:32,610 train 550 1.081832e-02 0.252564
2019-11-12 15:56:39,235 train 600 1.080947e-02 0.258145
2019-11-12 15:56:46,007 train 650 1.078534e-02 0.227435
2019-11-12 15:56:52,833 train 700 1.076342e-02 0.233466
2019-11-12 15:56:59,451 train 750 1.076075e-02 0.230139
2019-11-12 15:57:06,246 train 800 1.076454e-02 0.230573
2019-11-12 15:57:12,787 train 850 1.075903e-02 0.236814
2019-11-12 15:57:14,729 training loss; R2: 1.076317e-02 0.238643
2019-11-12 15:57:15,005 valid 000 1.017838e-02 0.331138
2019-11-12 15:57:16,782 valid 050 9.641489e-03 0.396845
2019-11-12 15:57:18,327 validation loss; R2: 9.747894e-03 0.395069
2019-11-12 15:57:18,346 epoch 9 lr 1.000000e-03
2019-11-12 15:57:18,768 train 000 1.024728e-02 0.384304
2019-11-12 15:57:25,350 train 050 1.048378e-02 0.309237
2019-11-12 15:57:31,861 train 100 1.048147e-02 0.298803
2019-11-12 15:57:38,616 train 150 1.057590e-02 0.313928
2019-11-12 15:57:45,481 train 200 1.066849e-02 0.317728
2019-11-12 15:57:52,246 train 250 1.063660e-02 0.319011
2019-11-12 15:57:58,922 train 300 1.062584e-02 0.316593
2019-11-12 15:58:05,557 train 350 1.069634e-02 0.300304
2019-11-12 15:58:12,332 train 400 1.068355e-02 0.306654
2019-11-12 15:58:19,023 train 450 1.064643e-02 0.307273
2019-11-12 15:58:25,767 train 500 1.063839e-02 0.313169
2019-11-12 15:58:32,425 train 550 1.060989e-02 0.318065
2019-11-12 15:58:39,241 train 600 1.058878e-02 0.308973
2019-11-12 15:58:45,908 train 650 1.056718e-02 0.313123
2019-11-12 15:58:52,549 train 700 1.057330e-02 0.315615
2019-11-12 15:58:59,187 train 750 1.057202e-02 0.316277
2019-11-12 15:59:05,818 train 800 1.056463e-02 0.308374
2019-11-12 15:59:12,363 train 850 1.055297e-02 0.308909
2019-11-12 15:59:14,313 training loss; R2: 1.054729e-02 0.310328
2019-11-12 15:59:14,598 valid 000 9.577500e-03 0.330900
2019-11-12 15:59:16,337 valid 050 9.073854e-03 0.246980
2019-11-12 15:59:17,837 validation loss; R2: 9.113577e-03 0.314199
2019-11-12 15:59:17,856 epoch 10 lr 1.000000e-03
2019-11-12 15:59:18,274 train 000 1.007707e-02 0.270512
2019-11-12 15:59:25,028 train 050 1.057475e-02 0.096300
2019-11-12 15:59:31,845 train 100 1.060048e-02 0.213188
2019-11-12 15:59:38,669 train 150 1.062330e-02 0.165324
2019-11-12 15:59:45,283 train 200 1.053300e-02 0.212798
2019-11-12 15:59:51,849 train 250 1.057261e-02 0.236158
2019-11-12 15:59:58,419 train 300 1.053648e-02 0.249244
2019-11-12 16:00:05,034 train 350 1.054107e-02 0.263086
2019-11-12 16:00:11,832 train 400 1.052041e-02 0.261261
2019-11-12 16:00:18,634 train 450 1.050178e-02 0.259904
2019-11-12 16:00:25,176 train 500 1.047108e-02 0.259777
2019-11-12 16:00:31,828 train 550 1.046193e-02 0.266231
2019-11-12 16:00:38,636 train 600 1.046553e-02 0.270070
2019-11-12 16:00:45,311 train 650 1.045660e-02 0.275551
2019-11-12 16:00:52,106 train 700 1.044787e-02 0.280481
2019-11-12 16:00:58,800 train 750 1.046441e-02 0.059265
2019-11-12 16:01:05,330 train 800 1.044884e-02 0.078685
2019-11-12 16:01:11,952 train 850 1.045724e-02 0.095656
2019-11-12 16:01:13,935 training loss; R2: 1.044650e-02 0.100646
2019-11-12 16:01:14,223 valid 000 2.572558e-02 0.105605
2019-11-12 16:01:15,891 valid 050 2.214855e-02 0.043157
2019-11-12 16:01:17,460 validation loss; R2: 2.204297e-02 0.022519
2019-11-12 16:01:17,480 epoch 11 lr 1.000000e-03
2019-11-12 16:01:17,856 train 000 1.125507e-02 0.424888
2019-11-12 16:01:24,443 train 050 1.026969e-02 0.350298
2019-11-12 16:01:31,191 train 100 1.027055e-02 -1.628534
2019-11-12 16:01:37,735 train 150 1.030674e-02 -0.973744
2019-11-12 16:01:44,279 train 200 1.030506e-02 -0.641647
2019-11-12 16:01:51,028 train 250 1.030412e-02 -0.444385
2019-11-12 16:01:57,849 train 300 1.031287e-02 -0.314262
2019-11-12 16:02:04,359 train 350 1.033613e-02 -0.215788
2019-11-12 16:02:10,823 train 400 1.033184e-02 -0.143910
2019-11-12 16:02:17,296 train 450 1.035556e-02 -0.088898
2019-11-12 16:02:23,756 train 500 1.034766e-02 -0.046794
2019-11-12 16:02:30,237 train 550 1.034894e-02 -1.866407
2019-11-12 16:02:36,690 train 600 1.037180e-02 -1.683685
2019-11-12 16:02:43,148 train 650 1.035970e-02 -1.527428
2019-11-12 16:02:49,610 train 700 1.036711e-02 -1.395537
2019-11-12 16:02:56,072 train 750 1.036022e-02 -1.278863
2019-11-12 16:03:02,533 train 800 1.036252e-02 -1.178259
2019-11-12 16:03:08,993 train 850 1.035690e-02 -1.088707
2019-11-12 16:03:10,926 training loss; R2: 1.035109e-02 -1.064604
2019-11-12 16:03:11,232 valid 000 8.829637e-03 0.416689
2019-11-12 16:03:12,929 valid 050 1.021184e-02 0.366043
2019-11-12 16:03:14,469 validation loss; R2: 1.025251e-02 0.376484
2019-11-12 16:03:14,483 epoch 12 lr 1.000000e-03
2019-11-12 16:03:14,871 train 000 1.125905e-02 0.343864
2019-11-12 16:03:21,382 train 050 1.018380e-02 0.309762
2019-11-12 16:03:27,860 train 100 1.019363e-02 0.332147
2019-11-12 16:03:34,422 train 150 1.021330e-02 0.327683
2019-11-12 16:03:40,902 train 200 1.019159e-02 0.340244
2019-11-12 16:03:47,519 train 250 1.015696e-02 0.335921
2019-11-12 16:03:54,104 train 300 1.019893e-02 0.335283
2019-11-12 16:04:00,609 train 350 1.018456e-02 0.334890
2019-11-12 16:04:07,213 train 400 1.017815e-02 0.340419
2019-11-12 16:04:13,732 train 450 1.021694e-02 0.338911
2019-11-12 16:04:20,441 train 500 1.023348e-02 0.338618
2019-11-12 16:04:26,935 train 550 1.022326e-02 0.339011
2019-11-12 16:04:33,412 train 600 1.024889e-02 0.334595
2019-11-12 16:04:39,896 train 650 1.023951e-02 0.335007
2019-11-12 16:04:46,367 train 700 1.025065e-02 0.336635
2019-11-12 16:04:52,862 train 750 1.025317e-02 0.336857
2019-11-12 16:04:59,344 train 800 1.023683e-02 0.310056
2019-11-12 16:05:05,822 train 850 1.023424e-02 0.312364
2019-11-12 16:05:07,758 training loss; R2: 1.022786e-02 0.313025
2019-11-12 16:05:08,036 valid 000 6.221683e-02 -4.023121
2019-11-12 16:05:09,776 valid 050 6.186226e-02 -6.030421
2019-11-12 16:05:11,333 validation loss; R2: 6.169825e-02 -6.893896
2019-11-12 16:05:11,360 epoch 13 lr 1.000000e-03
2019-11-12 16:05:11,777 train 000 1.052191e-02 0.162381
2019-11-12 16:05:18,530 train 050 1.054039e-02 0.319561
2019-11-12 16:05:25,097 train 100 1.033550e-02 0.284933
2019-11-12 16:05:31,690 train 150 1.022615e-02 0.309699
2019-11-12 16:05:38,243 train 200 1.011127e-02 0.327293
2019-11-12 16:05:45,038 train 250 1.016305e-02 0.324567
2019-11-12 16:05:51,643 train 300 1.017302e-02 0.318119
2019-11-12 16:05:58,288 train 350 1.017867e-02 0.318847
2019-11-12 16:06:04,977 train 400 1.015952e-02 0.323844
2019-11-12 16:06:11,616 train 450 1.017838e-02 0.324732
2019-11-12 16:06:18,402 train 500 1.018161e-02 0.327801
2019-11-12 16:06:25,221 train 550 1.016599e-02 0.321817
2019-11-12 16:06:32,150 train 600 1.017254e-02 0.320703
2019-11-12 16:06:38,951 train 650 1.017673e-02 0.314231
2019-11-12 16:06:45,416 train 700 1.017627e-02 0.314984
2019-11-12 16:06:51,855 train 750 1.019255e-02 0.316653
2019-11-12 16:06:58,290 train 800 1.017295e-02 0.312472
2019-11-12 16:07:04,736 train 850 1.017274e-02 0.313882
2019-11-12 16:07:06,660 training loss; R2: 1.016598e-02 0.314554
2019-11-12 16:07:06,948 valid 000 9.036531e-02 -6.818943
2019-11-12 16:07:08,618 valid 050 9.261123e-02 -8.931027
2019-11-12 16:07:10,127 validation loss; R2: 9.222525e-02 -9.851490
2019-11-12 16:07:10,142 epoch 14 lr 1.000000e-03
2019-11-12 16:07:10,525 train 000 9.572653e-03 0.387310
2019-11-12 16:07:17,206 train 050 9.948765e-03 0.330772
2019-11-12 16:07:24,035 train 100 1.002007e-02 0.228479
2019-11-12 16:07:30,583 train 150 1.008204e-02 0.266277
2019-11-12 16:07:37,292 train 200 9.997374e-03 0.280009
2019-11-12 16:07:43,811 train 250 9.988267e-03 0.299006
2019-11-12 16:07:50,474 train 300 1.002135e-02 0.300320
2019-11-12 16:07:57,053 train 350 1.002419e-02 0.309183
2019-11-12 16:08:03,811 train 400 1.007122e-02 0.315558
2019-11-12 16:08:10,568 train 450 1.008667e-02 0.315762
2019-11-12 16:08:17,310 train 500 1.008346e-02 0.311044
2019-11-12 16:08:24,142 train 550 1.008561e-02 0.312318
2019-11-12 16:08:30,813 train 600 1.010132e-02 0.315872
2019-11-12 16:08:37,641 train 650 1.008875e-02 0.318430
2019-11-12 16:08:44,269 train 700 1.008288e-02 0.320152
2019-11-12 16:08:50,892 train 750 1.007292e-02 0.322135
2019-11-12 16:08:57,656 train 800 1.007568e-02 0.307123
2019-11-12 16:09:04,359 train 850 1.007992e-02 0.307589
2019-11-12 16:09:06,352 training loss; R2: 1.007886e-02 0.308935
2019-11-12 16:09:06,622 valid 000 3.271510e+00 -260.934768
2019-11-12 16:09:08,294 valid 050 3.242060e+00 -342.614436
2019-11-12 16:09:09,808 validation loss; R2: 3.242800e+00 -363.885889
2019-11-12 16:09:09,829 epoch 15 lr 1.000000e-03
2019-11-12 16:09:10,240 train 000 8.699973e-03 0.354422
2019-11-12 16:09:16,825 train 050 9.913974e-03 0.371841
2019-11-12 16:09:23,375 train 100 9.956968e-03 0.340897
2019-11-12 16:09:29,919 train 150 9.894901e-03 0.342635
2019-11-12 16:09:36,371 train 200 9.959885e-03 0.337856
2019-11-12 16:09:42,824 train 250 9.960622e-03 0.319112
2019-11-12 16:09:49,284 train 300 9.940653e-03 0.327366
2019-11-12 16:09:55,735 train 350 9.916762e-03 0.321656
2019-11-12 16:10:02,193 train 400 9.955611e-03 0.324705
2019-11-12 16:10:08,661 train 450 9.962549e-03 0.331757
2019-11-12 16:10:15,116 train 500 9.961091e-03 0.330051
2019-11-12 16:10:21,580 train 550 9.963617e-03 0.332390
2019-11-12 16:10:28,036 train 600 9.953293e-03 0.334572
2019-11-12 16:10:34,497 train 650 9.959544e-03 0.336866
2019-11-12 16:10:40,962 train 700 9.966390e-03 0.336148
2019-11-12 16:10:47,419 train 750 9.966813e-03 0.338783
2019-11-12 16:10:53,883 train 800 9.955346e-03 0.340186
2019-11-12 16:11:00,341 train 850 9.968649e-03 0.340426
2019-11-12 16:11:02,273 training loss; R2: 9.969137e-03 0.340977
2019-11-12 16:11:02,570 valid 000 1.769464e+02 -17249.431681
2019-11-12 16:11:04,288 valid 050 1.772532e+02 -12285.494926
2019-11-12 16:11:05,813 validation loss; R2: 1.772462e+02 -11723.055966
2019-11-12 16:11:05,828 epoch 16 lr 1.000000e-03
2019-11-12 16:11:06,204 train 000 8.268716e-03 0.450974
2019-11-12 16:11:12,829 train 050 1.004280e-02 0.323056
2019-11-12 16:11:19,488 train 100 1.006205e-02 0.345552
2019-11-12 16:11:26,064 train 150 1.002773e-02 0.338411
2019-11-12 16:11:32,651 train 200 9.996399e-03 0.247998
2019-11-12 16:11:39,208 train 250 9.991411e-03 0.272211
2019-11-12 16:11:45,783 train 300 9.989820e-03 0.284216
2019-11-12 16:11:52,366 train 350 9.989330e-03 0.291402
2019-11-12 16:11:59,088 train 400 9.964993e-03 0.299989
2019-11-12 16:12:05,936 train 450 9.977805e-03 0.310242
2019-11-12 16:12:12,640 train 500 9.969398e-03 0.313901
2019-11-12 16:12:19,285 train 550 9.977111e-03 0.310537
2019-11-12 16:12:25,806 train 600 9.972919e-03 0.315003
2019-11-12 16:12:32,505 train 650 9.984223e-03 0.317743
2019-11-12 16:12:39,262 train 700 9.979329e-03 0.318492
2019-11-12 16:12:45,856 train 750 9.969296e-03 0.315531
2019-11-12 16:12:52,423 train 800 9.963856e-03 0.316341
2019-11-12 16:12:59,039 train 850 1.016934e-02 0.262582
2019-11-12 16:13:00,999 training loss; R2: 1.017660e-02 0.260136
2019-11-12 16:13:01,313 valid 000 8.706734e-01 -421.820096
2019-11-12 16:13:02,982 valid 050 8.533252e-01 -784.078624
2019-11-12 16:13:04,504 validation loss; R2: 8.535297e-01 -764.012293
2019-11-12 16:13:04,524 epoch 17 lr 1.000000e-03
2019-11-12 16:13:04,956 train 000 1.069806e-02 0.341890
2019-11-12 16:13:11,584 train 050 1.054071e-02 0.139135
2019-11-12 16:13:18,107 train 100 1.042754e-02 0.217065
2019-11-12 16:13:24,654 train 150 1.030164e-02 0.220413
2019-11-12 16:13:31,164 train 200 1.029949e-02 0.250786
2019-11-12 16:13:37,831 train 250 1.022949e-02 0.270992
2019-11-12 16:13:44,564 train 300 1.020390e-02 0.275723
2019-11-12 16:13:51,177 train 350 1.019729e-02 0.287283
2019-11-12 16:13:57,784 train 400 1.020822e-02 0.290454
2019-11-12 16:14:04,349 train 450 1.021144e-02 0.297874
2019-11-12 16:14:11,000 train 500 1.016042e-02 0.301296
2019-11-12 16:14:17,549 train 550 1.019632e-02 0.301660
2019-11-12 16:14:24,197 train 600 1.017104e-02 0.304598
2019-11-12 16:14:30,801 train 650 1.015252e-02 0.308623
2019-11-12 16:14:37,466 train 700 1.014534e-02 0.306999
2019-11-12 16:14:44,107 train 750 1.013111e-02 0.301386
2019-11-12 16:14:50,648 train 800 1.013846e-02 0.299381
2019-11-12 16:14:57,325 train 850 1.013755e-02 0.300763
2019-11-12 16:14:59,289 training loss; R2: 1.012927e-02 0.301350
2019-11-12 16:14:59,577 valid 000 1.910490e+01 -779.994413
2019-11-12 16:15:01,271 valid 050 1.913985e+01 -813.204203
2019-11-12 16:15:02,809 validation loss; R2: 1.913895e+01 -843.425883
2019-11-12 16:15:02,832 epoch 18 lr 1.000000e-03
2019-11-12 16:15:03,292 train 000 9.101628e-03 0.374667
2019-11-12 16:15:10,039 train 050 9.912850e-03 0.313382
2019-11-12 16:15:16,859 train 100 9.938798e-03 0.319615
2019-11-12 16:15:23,662 train 150 9.998279e-03 0.308846
2019-11-12 16:15:30,384 train 200 1.004737e-02 0.318157
2019-11-12 16:15:37,118 train 250 9.997642e-03 0.322354
2019-11-12 16:15:43,834 train 300 1.000263e-02 0.329486
2019-11-12 16:15:50,548 train 350 1.002742e-02 0.300105
2019-11-12 16:15:57,257 train 400 1.007626e-02 0.290539
2019-11-12 16:16:03,991 train 450 1.008073e-02 0.293089
2019-11-12 16:16:10,703 train 500 1.008332e-02 0.297604
2019-11-12 16:16:17,411 train 550 1.010183e-02 0.300872
2019-11-12 16:16:24,123 train 600 1.009687e-02 0.303142
2019-11-12 16:16:30,837 train 650 1.009688e-02 0.298928
2019-11-12 16:16:37,549 train 700 1.010378e-02 0.302274
2019-11-12 16:16:44,189 train 750 1.010251e-02 0.305297
2019-11-12 16:16:50,638 train 800 1.008878e-02 0.307594
2019-11-12 16:16:57,076 train 850 1.010454e-02 0.299247
2019-11-12 16:16:59,002 training loss; R2: 1.011121e-02 0.299282
2019-11-12 16:16:59,301 valid 000 1.258858e+01 -571.285730
2019-11-12 16:17:00,972 valid 050 1.257585e+01 -719.090544
2019-11-12 16:17:02,543 validation loss; R2: 1.257048e+01 -704.401564
2019-11-12 16:17:02,563 epoch 19 lr 1.000000e-03
2019-11-12 16:17:02,966 train 000 9.634147e-03 0.461271
2019-11-12 16:17:09,756 train 050 1.060656e-02 0.302719
2019-11-12 16:17:16,537 train 100 1.049994e-02 0.271964
2019-11-12 16:17:23,238 train 150 1.035689e-02 0.289221
2019-11-12 16:17:30,023 train 200 1.030793e-02 0.293259
2019-11-12 16:17:36,737 train 250 1.024316e-02 0.299348
2019-11-12 16:17:43,466 train 300 1.022193e-02 0.305318
2019-11-12 16:17:50,094 train 350 1.017459e-02 0.312946
2019-11-12 16:17:56,872 train 400 1.013671e-02 0.308943
2019-11-12 16:18:03,473 train 450 1.012722e-02 0.308188
2019-11-12 16:18:10,172 train 500 1.011044e-02 0.104381
2019-11-12 16:18:16,864 train 550 1.006339e-02 0.125735
2019-11-12 16:18:23,645 train 600 1.004097e-02 0.143501
2019-11-12 16:18:30,368 train 650 1.004181e-02 0.156440
2019-11-12 16:18:36,988 train 700 1.001957e-02 0.171083
2019-11-12 16:18:43,797 train 750 1.000231e-02 0.182422
2019-11-12 16:18:50,539 train 800 9.997460e-03 0.188266
2019-11-12 16:18:57,326 train 850 9.994614e-03 0.196481
2019-11-12 16:18:59,284 training loss; R2: 9.993619e-03 0.199649
2019-11-12 16:18:59,556 valid 000 8.360059e-01 -61.703963
2019-11-12 16:19:01,271 valid 050 8.352793e-01 -93.935095
2019-11-12 16:19:02,783 validation loss; R2: 8.354680e-01 -134.084557
