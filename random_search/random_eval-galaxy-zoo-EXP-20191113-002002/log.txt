2019-11-13 00:20:02,775 gpu device = 1
2019-11-13 00:20:02,775 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name='galaxy-zoo-cleaned', gpu=1, grad_clip=5, gz_dtree=False, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-galaxy-zoo-EXP-20191113-002002', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-11-13 00:20:14,340 param size = 0.189717MB
2019-11-13 00:20:14,343 epoch 0 lr 1.000000e-03
2019-11-13 00:20:16,404 train 000 1.159308e+00 -499.371628
2019-11-13 00:20:21,591 train 050 8.218263e-02 -22.883053
2019-11-13 00:20:26,853 train 100 5.784944e-02 -12.495874
2019-11-13 00:20:32,056 train 150 4.808828e-02 -8.697997
2019-11-13 00:20:37,144 train 200 4.274030e-02 -6.716080
2019-11-13 00:20:42,224 train 250 3.923135e-02 -5.507763
2019-11-13 00:20:47,210 train 300 3.684177e-02 -4.690259
2019-11-13 00:20:52,172 train 350 3.508504e-02 -4.076171
2019-11-13 00:20:57,147 train 400 3.363708e-02 -3.619887
2019-11-13 00:21:02,108 train 450 3.246339e-02 -3.257073
2019-11-13 00:21:07,085 train 500 3.147522e-02 -2.978970
2019-11-13 00:21:12,038 train 550 3.061867e-02 -2.735710
2019-11-13 00:21:17,002 train 600 2.987722e-02 -2.586980
2019-11-13 00:21:21,958 train 650 2.924652e-02 -2.410707
2019-11-13 00:21:26,922 train 700 2.862431e-02 -2.252008
2019-11-13 00:21:31,894 train 750 2.807734e-02 -2.112407
2019-11-13 00:21:36,871 train 800 2.756743e-02 -1.987205
2019-11-13 00:21:41,826 train 850 2.711032e-02 -1.877420
2019-11-13 00:21:43,951 training loss; R2: 2.697474e-02 -1.844918
2019-11-13 00:21:44,258 valid 000 1.887287e-02 0.100155
2019-11-13 00:21:45,952 valid 050 1.748526e-02 -0.049528
2019-11-13 00:21:47,568 validation loss; R2: 1.775094e-02 0.001137
2019-11-13 00:21:47,577 epoch 1 lr 1.000000e-03
2019-11-13 00:21:48,053 train 000 1.789227e-02 -0.076056
2019-11-13 00:21:52,827 train 050 1.930396e-02 -0.148957
2019-11-13 00:21:57,602 train 100 1.921516e-02 -0.131397
2019-11-13 00:22:02,372 train 150 1.915912e-02 -0.117663
2019-11-13 00:22:07,141 train 200 1.918806e-02 -0.106330
2019-11-13 00:22:11,915 train 250 1.906511e-02 -0.095786
2019-11-13 00:22:16,686 train 300 1.887052e-02 -0.085258
2019-11-13 00:22:21,474 train 350 1.879739e-02 -0.077748
2019-11-13 00:22:26,436 train 400 1.874209e-02 -0.070750
2019-11-13 00:22:31,398 train 450 1.862749e-02 -0.070085
2019-11-13 00:22:36,363 train 500 1.856571e-02 -0.066330
2019-11-13 00:22:41,339 train 550 1.846332e-02 -0.059821
2019-11-13 00:22:46,304 train 600 1.837748e-02 -0.059198
2019-11-13 00:22:51,260 train 650 1.827605e-02 -0.056183
2019-11-13 00:22:56,241 train 700 1.821036e-02 -0.048811
2019-11-13 00:23:01,195 train 750 1.812023e-02 -0.047701
2019-11-13 00:23:06,152 train 800 1.805580e-02 -0.039596
2019-11-13 00:23:11,126 train 850 1.796460e-02 -0.037598
2019-11-13 00:23:12,614 training loss; R2: 1.793722e-02 -0.037774
2019-11-13 00:23:12,930 valid 000 1.681839e-02 0.224319
2019-11-13 00:23:14,652 valid 050 1.527912e-02 0.207807
2019-11-13 00:23:16,219 validation loss; R2: 1.541970e-02 0.203355
2019-11-13 00:23:16,230 epoch 2 lr 1.000000e-03
2019-11-13 00:23:16,597 train 000 1.812953e-02 0.169318
2019-11-13 00:23:21,592 train 050 1.648033e-02 0.075321
2019-11-13 00:23:26,544 train 100 1.646632e-02 0.091544
2019-11-13 00:23:31,506 train 150 1.648103e-02 0.100257
2019-11-13 00:23:36,471 train 200 1.646466e-02 0.085620
2019-11-13 00:23:41,436 train 250 1.638611e-02 0.087745
2019-11-13 00:23:46,400 train 300 1.630848e-02 0.090203
2019-11-13 00:23:51,364 train 350 1.624194e-02 -3.751876
2019-11-13 00:23:56,314 train 400 1.619233e-02 -3.272462
2019-11-13 00:24:01,271 train 450 1.618409e-02 -2.904819
2019-11-13 00:24:06,231 train 500 1.613039e-02 -2.604288
2019-11-13 00:24:11,179 train 550 1.605851e-02 -2.356795
2019-11-13 00:24:16,131 train 600 1.601754e-02 -2.183216
2019-11-13 00:24:21,077 train 650 1.598040e-02 -2.008113
2019-11-13 00:24:26,026 train 700 1.592170e-02 -1.855520
2019-11-13 00:24:30,994 train 750 1.585263e-02 -1.720378
2019-11-13 00:24:35,957 train 800 1.579522e-02 -1.605146
2019-11-13 00:24:40,930 train 850 1.574749e-02 -1.500627
2019-11-13 00:24:42,416 training loss; R2: 1.573109e-02 -1.471966
2019-11-13 00:24:42,740 valid 000 1.444212e-02 0.312105
2019-11-13 00:24:44,495 valid 050 1.351928e-02 0.269536
2019-11-13 00:24:46,070 validation loss; R2: 1.356508e-02 0.268444
2019-11-13 00:24:46,080 epoch 3 lr 1.000000e-03
2019-11-13 00:24:46,424 train 000 1.586182e-02 0.179953
2019-11-13 00:24:51,231 train 050 1.494892e-02 0.190480
2019-11-13 00:24:56,015 train 100 1.505333e-02 0.149114
2019-11-13 00:25:00,788 train 150 1.497581e-02 0.166494
2019-11-13 00:25:05,572 train 200 1.488531e-02 0.161613
2019-11-13 00:25:10,352 train 250 1.490087e-02 0.152183
2019-11-13 00:25:15,128 train 300 1.478634e-02 0.153376
2019-11-13 00:25:19,902 train 350 1.480376e-02 0.155998
2019-11-13 00:25:24,675 train 400 1.474225e-02 0.159893
2019-11-13 00:25:29,453 train 450 1.469013e-02 0.159382
2019-11-13 00:25:34,227 train 500 1.467456e-02 0.141082
2019-11-13 00:25:39,010 train 550 1.463137e-02 0.140325
2019-11-13 00:25:43,785 train 600 1.459827e-02 0.139510
2019-11-13 00:25:48,562 train 650 1.456009e-02 0.143486
2019-11-13 00:25:53,346 train 700 1.454829e-02 0.134282
2019-11-13 00:25:58,126 train 750 1.451532e-02 0.136107
2019-11-13 00:26:02,893 train 800 1.448626e-02 0.125966
2019-11-13 00:26:07,672 train 850 1.445802e-02 0.111791
2019-11-13 00:26:09,113 training loss; R2: 1.443574e-02 0.113691
2019-11-13 00:26:09,398 valid 000 1.617058e-02 0.352567
2019-11-13 00:26:11,097 valid 050 1.286260e-02 0.294113
2019-11-13 00:26:12,639 validation loss; R2: 1.298084e-02 0.283348
2019-11-13 00:26:12,651 epoch 4 lr 1.000000e-03
2019-11-13 00:26:13,004 train 000 1.397039e-02 0.244522
2019-11-13 00:26:17,986 train 050 1.427032e-02 0.170444
2019-11-13 00:26:22,973 train 100 1.410039e-02 0.185436
2019-11-13 00:26:27,944 train 150 1.393551e-02 0.184494
2019-11-13 00:26:32,916 train 200 1.393269e-02 0.185237
2019-11-13 00:26:37,893 train 250 1.388444e-02 0.175407
2019-11-13 00:26:42,866 train 300 1.390705e-02 0.179488
2019-11-13 00:26:47,827 train 350 1.392009e-02 0.183894
2019-11-13 00:26:52,790 train 400 1.390764e-02 0.183128
2019-11-13 00:26:57,767 train 450 1.389332e-02 0.182246
2019-11-13 00:27:02,740 train 500 1.384222e-02 0.173685
2019-11-13 00:27:07,713 train 550 1.379978e-02 0.176913
2019-11-13 00:27:12,678 train 600 1.379447e-02 0.162120
2019-11-13 00:27:17,640 train 650 1.377546e-02 0.165939
2019-11-13 00:27:22,608 train 700 1.372628e-02 0.170641
2019-11-13 00:27:27,575 train 750 1.373183e-02 0.173675
2019-11-13 00:27:32,538 train 800 1.370173e-02 0.174124
2019-11-13 00:27:37,505 train 850 1.369084e-02 0.170326
2019-11-13 00:27:38,988 training loss; R2: 1.368306e-02 0.171120
2019-11-13 00:27:39,309 valid 000 1.067798e-02 0.351121
2019-11-13 00:27:41,009 valid 050 1.194369e-02 0.336244
2019-11-13 00:27:42,551 validation loss; R2: 1.188075e-02 0.296918
2019-11-13 00:27:42,561 epoch 5 lr 1.000000e-03
2019-11-13 00:27:42,909 train 000 1.478200e-02 0.054985
2019-11-13 00:27:47,755 train 050 1.313734e-02 0.214715
2019-11-13 00:27:52,530 train 100 1.327093e-02 0.235354
2019-11-13 00:27:57,515 train 150 1.335150e-02 0.230545
2019-11-13 00:28:02,493 train 200 1.330448e-02 0.217457
2019-11-13 00:28:07,464 train 250 1.328472e-02 0.215487
2019-11-13 00:28:12,440 train 300 1.325494e-02 0.214667
2019-11-13 00:28:17,407 train 350 1.328083e-02 0.211477
2019-11-13 00:28:22,378 train 400 1.321042e-02 0.216164
2019-11-13 00:28:27,348 train 450 1.322409e-02 0.216975
2019-11-13 00:28:32,323 train 500 1.318356e-02 0.211662
2019-11-13 00:28:37,300 train 550 1.313215e-02 0.207729
2019-11-13 00:28:42,296 train 600 1.310338e-02 0.171815
2019-11-13 00:28:47,265 train 650 1.310239e-02 0.176479
2019-11-13 00:28:52,231 train 700 1.306926e-02 0.183987
2019-11-13 00:28:57,193 train 750 1.304811e-02 0.187002
2019-11-13 00:29:02,181 train 800 1.300902e-02 0.191235
2019-11-13 00:29:07,147 train 850 1.298225e-02 0.193145
2019-11-13 00:29:08,631 training loss; R2: 1.297602e-02 0.194757
2019-11-13 00:29:08,953 valid 000 1.037120e-02 0.340108
2019-11-13 00:29:10,669 valid 050 1.155015e-02 0.319505
2019-11-13 00:29:12,223 validation loss; R2: 1.133540e-02 0.311856
2019-11-13 00:29:12,233 epoch 6 lr 1.000000e-03
2019-11-13 00:29:12,556 train 000 1.351109e-02 0.303065
2019-11-13 00:29:17,368 train 050 1.284863e-02 0.148797
2019-11-13 00:29:22,163 train 100 1.290253e-02 0.200735
2019-11-13 00:29:26,962 train 150 1.276666e-02 0.226634
2019-11-13 00:29:31,748 train 200 1.266418e-02 0.236748
2019-11-13 00:29:36,523 train 250 1.261082e-02 0.235680
2019-11-13 00:29:41,301 train 300 1.264431e-02 0.214831
2019-11-13 00:29:46,073 train 350 1.259764e-02 0.221238
2019-11-13 00:29:50,876 train 400 1.260724e-02 0.223654
2019-11-13 00:29:55,649 train 450 1.259078e-02 0.226397
2019-11-13 00:30:00,436 train 500 1.257363e-02 0.232462
2019-11-13 00:30:05,219 train 550 1.254745e-02 0.235402
2019-11-13 00:30:10,019 train 600 1.251448e-02 0.225900
2019-11-13 00:30:14,803 train 650 1.252506e-02 0.226874
2019-11-13 00:30:19,594 train 700 1.252019e-02 0.229754
2019-11-13 00:30:24,387 train 750 1.249610e-02 0.231000
2019-11-13 00:30:29,157 train 800 1.248447e-02 0.232980
2019-11-13 00:30:33,942 train 850 1.247716e-02 0.235134
2019-11-13 00:30:35,421 training loss; R2: 1.246927e-02 0.235861
2019-11-13 00:30:35,734 valid 000 1.001747e-02 0.406419
2019-11-13 00:30:37,433 valid 050 1.123538e-02 0.259136
2019-11-13 00:30:38,968 validation loss; R2: 1.108227e-02 0.301154
2019-11-13 00:30:38,981 epoch 7 lr 1.000000e-03
2019-11-13 00:30:39,358 train 000 1.193685e-02 0.250901
2019-11-13 00:30:44,391 train 050 1.183095e-02 0.242318
2019-11-13 00:30:49,380 train 100 1.186539e-02 0.192981
2019-11-13 00:30:54,370 train 150 1.195678e-02 0.217486
2019-11-13 00:30:59,348 train 200 1.200141e-02 0.231135
2019-11-13 00:31:04,327 train 250 1.208752e-02 0.235743
2019-11-13 00:31:09,298 train 300 1.211131e-02 0.242240
2019-11-13 00:31:14,254 train 350 1.210667e-02 0.237470
2019-11-13 00:31:19,225 train 400 1.209816e-02 0.238841
2019-11-13 00:31:24,184 train 450 1.207122e-02 0.241774
2019-11-13 00:31:29,140 train 500 1.204220e-02 0.245951
2019-11-13 00:31:34,109 train 550 1.203293e-02 0.249533
2019-11-13 00:31:39,072 train 600 1.204973e-02 0.236777
2019-11-13 00:31:44,064 train 650 1.205519e-02 0.239926
2019-11-13 00:31:49,025 train 700 1.204296e-02 0.242228
2019-11-13 00:31:53,999 train 750 1.206455e-02 0.245334
2019-11-13 00:31:58,956 train 800 1.205242e-02 0.244166
2019-11-13 00:32:03,920 train 850 1.201584e-02 0.246012
2019-11-13 00:32:05,412 training loss; R2: 1.201517e-02 0.245873
2019-11-13 00:32:05,715 valid 000 1.210660e-02 0.324432
2019-11-13 00:32:07,452 valid 050 1.142807e-02 0.133557
2019-11-13 00:32:08,989 validation loss; R2: 1.148051e-02 0.232751
2019-11-13 00:32:09,001 epoch 8 lr 1.000000e-03
2019-11-13 00:32:09,380 train 000 1.137550e-02 0.347063
2019-11-13 00:32:14,425 train 050 1.205736e-02 0.159126
2019-11-13 00:32:19,416 train 100 1.199937e-02 0.216424
2019-11-13 00:32:24,387 train 150 1.190682e-02 0.240282
2019-11-13 00:32:29,366 train 200 1.183254e-02 0.245416
2019-11-13 00:32:34,326 train 250 1.185849e-02 0.254860
2019-11-13 00:32:39,298 train 300 1.184150e-02 0.257631
2019-11-13 00:32:44,263 train 350 1.182520e-02 0.262325
2019-11-13 00:32:49,237 train 400 1.181935e-02 0.267081
2019-11-13 00:32:54,203 train 450 1.180172e-02 0.268936
2019-11-13 00:32:59,171 train 500 1.179682e-02 0.266554
2019-11-13 00:33:04,157 train 550 1.179630e-02 0.270792
2019-11-13 00:33:09,128 train 600 1.179593e-02 0.272897
2019-11-13 00:33:14,110 train 650 1.179019e-02 0.270135
2019-11-13 00:33:19,084 train 700 1.177065e-02 0.272634
2019-11-13 00:33:24,050 train 750 1.174595e-02 0.274063
2019-11-13 00:33:29,020 train 800 1.172385e-02 0.269075
2019-11-13 00:33:33,978 train 850 1.170415e-02 0.271599
2019-11-13 00:33:35,465 training loss; R2: 1.170595e-02 0.269244
2019-11-13 00:33:35,792 valid 000 8.839573e-03 0.442602
2019-11-13 00:33:37,489 valid 050 1.019431e-02 0.366712
2019-11-13 00:33:39,045 validation loss; R2: 1.007030e-02 0.362977
2019-11-13 00:33:39,055 epoch 9 lr 1.000000e-03
2019-11-13 00:33:39,402 train 000 1.272345e-02 0.356629
2019-11-13 00:33:44,337 train 050 1.154094e-02 0.310055
2019-11-13 00:33:49,301 train 100 1.154525e-02 0.276345
2019-11-13 00:33:54,270 train 150 1.149813e-02 0.226710
2019-11-13 00:33:59,234 train 200 1.145520e-02 0.208186
2019-11-13 00:34:04,201 train 250 1.146062e-02 0.200446
2019-11-13 00:34:09,163 train 300 1.139020e-02 0.201671
2019-11-13 00:34:14,117 train 350 1.138407e-02 0.217904
2019-11-13 00:34:19,076 train 400 1.139015e-02 0.227880
2019-11-13 00:34:24,046 train 450 1.136839e-02 0.237142
2019-11-13 00:34:29,007 train 500 1.134408e-02 0.239863
2019-11-13 00:34:33,968 train 550 1.135719e-02 0.246080
2019-11-13 00:34:38,926 train 600 1.138049e-02 0.246590
2019-11-13 00:34:43,888 train 650 1.140290e-02 0.251106
2019-11-13 00:34:48,845 train 700 1.142304e-02 0.253894
2019-11-13 00:34:53,815 train 750 1.141178e-02 0.259212
2019-11-13 00:34:58,776 train 800 1.139695e-02 0.260939
2019-11-13 00:35:03,733 train 850 1.137229e-02 0.262485
2019-11-13 00:35:05,222 training loss; R2: 1.136503e-02 0.262717
2019-11-13 00:35:05,538 valid 000 9.891101e-03 0.372824
2019-11-13 00:35:07,244 valid 050 1.124516e-02 0.255631
2019-11-13 00:35:08,792 validation loss; R2: 1.133871e-02 0.288720
2019-11-13 00:35:08,809 epoch 10 lr 1.000000e-03
2019-11-13 00:35:09,156 train 000 9.887966e-03 0.380359
2019-11-13 00:35:13,980 train 050 1.129713e-02 0.305638
2019-11-13 00:35:18,815 train 100 1.125470e-02 0.238225
2019-11-13 00:35:23,798 train 150 1.121911e-02 0.253667
2019-11-13 00:35:28,771 train 200 1.125640e-02 0.270575
2019-11-13 00:35:33,761 train 250 1.123461e-02 0.283338
2019-11-13 00:35:38,728 train 300 1.124453e-02 0.278845
2019-11-13 00:35:43,721 train 350 1.122010e-02 0.286308
2019-11-13 00:35:48,693 train 400 1.120513e-02 0.290778
2019-11-13 00:35:53,660 train 450 1.120102e-02 0.290338
2019-11-13 00:35:58,637 train 500 1.120964e-02 0.293308
2019-11-13 00:36:03,598 train 550 1.117857e-02 0.293363
2019-11-13 00:36:08,582 train 600 1.115779e-02 0.291521
2019-11-13 00:36:13,543 train 650 1.114067e-02 0.289293
2019-11-13 00:36:18,510 train 700 1.113242e-02 0.247413
2019-11-13 00:36:23,471 train 750 1.113214e-02 0.252114
2019-11-13 00:36:28,434 train 800 1.111796e-02 0.254461
2019-11-13 00:36:33,403 train 850 1.113861e-02 0.255606
2019-11-13 00:36:34,886 training loss; R2: 1.114500e-02 0.255291
2019-11-13 00:36:35,198 valid 000 9.914326e-03 0.399416
2019-11-13 00:36:36,899 valid 050 1.053373e-02 0.377442
2019-11-13 00:36:38,429 validation loss; R2: 1.059794e-02 0.367946
2019-11-13 00:36:38,439 epoch 11 lr 1.000000e-03
2019-11-13 00:36:38,786 train 000 1.123313e-02 0.077067
2019-11-13 00:36:43,623 train 050 1.090280e-02 0.214736
2019-11-13 00:36:48,396 train 100 1.112580e-02 0.261184
2019-11-13 00:36:53,187 train 150 1.103332e-02 0.253749
2019-11-13 00:36:57,982 train 200 1.111386e-02 0.270246
2019-11-13 00:37:02,766 train 250 1.104967e-02 0.279394
2019-11-13 00:37:07,542 train 300 1.104642e-02 0.287268
2019-11-13 00:37:12,318 train 350 1.105130e-02 0.142479
2019-11-13 00:37:17,091 train 400 1.102956e-02 0.165024
2019-11-13 00:37:21,862 train 450 1.102341e-02 0.183097
2019-11-13 00:37:26,643 train 500 1.101735e-02 0.195395
2019-11-13 00:37:31,427 train 550 1.099281e-02 0.207400
2019-11-13 00:37:36,202 train 600 1.100680e-02 0.217113
2019-11-13 00:37:40,971 train 650 1.101692e-02 0.224299
2019-11-13 00:37:45,765 train 700 1.101919e-02 0.227209
2019-11-13 00:37:50,547 train 750 1.101354e-02 0.233284
2019-11-13 00:37:55,320 train 800 1.099486e-02 0.237799
2019-11-13 00:38:00,117 train 850 1.099376e-02 0.241394
2019-11-13 00:38:01,544 training loss; R2: 1.098925e-02 0.242755
2019-11-13 00:38:01,833 valid 000 8.744799e-03 0.427610
2019-11-13 00:38:03,559 valid 050 9.695069e-03 0.311207
2019-11-13 00:38:05,102 validation loss; R2: 9.751894e-03 0.352936
2019-11-13 00:38:05,112 epoch 12 lr 1.000000e-03
2019-11-13 00:38:05,471 train 000 1.071802e-02 0.402237
2019-11-13 00:38:10,456 train 050 1.066868e-02 0.339620
2019-11-13 00:38:15,444 train 100 1.062995e-02 0.338267
2019-11-13 00:38:20,420 train 150 1.076982e-02 0.329887
2019-11-13 00:38:25,383 train 200 1.081140e-02 0.317159
2019-11-13 00:38:30,352 train 250 1.080315e-02 0.323784
2019-11-13 00:38:35,324 train 300 1.083740e-02 0.320584
2019-11-13 00:38:40,323 train 350 1.084245e-02 0.311610
2019-11-13 00:38:45,293 train 400 1.086280e-02 0.306856
2019-11-13 00:38:50,258 train 450 1.088130e-02 0.305962
2019-11-13 00:38:55,229 train 500 1.086148e-02 0.308218
2019-11-13 00:39:00,213 train 550 1.085902e-02 0.310053
2019-11-13 00:39:05,181 train 600 1.086738e-02 0.312031
2019-11-13 00:39:10,148 train 650 1.084082e-02 0.311317
2019-11-13 00:39:15,117 train 700 1.084498e-02 0.310696
2019-11-13 00:39:20,092 train 750 1.085935e-02 0.309972
2019-11-13 00:39:25,060 train 800 1.084233e-02 0.309130
2019-11-13 00:39:30,021 train 850 1.083467e-02 0.309282
2019-11-13 00:39:31,510 training loss; R2: 1.082931e-02 0.308577
2019-11-13 00:39:31,839 valid 000 9.600917e-03 0.394120
2019-11-13 00:39:33,539 valid 050 9.950871e-03 0.412474
2019-11-13 00:39:35,077 validation loss; R2: 1.013121e-02 0.408692
2019-11-13 00:39:35,089 epoch 13 lr 1.000000e-03
2019-11-13 00:39:35,437 train 000 1.013086e-02 -3.391582
2019-11-13 00:39:40,526 train 050 1.074876e-02 0.271349
2019-11-13 00:39:45,614 train 100 1.061255e-02 0.307213
2019-11-13 00:39:50,681 train 150 1.054098e-02 0.298421
2019-11-13 00:39:55,739 train 200 1.055983e-02 0.306215
2019-11-13 00:40:00,801 train 250 1.057091e-02 0.310846
2019-11-13 00:40:05,864 train 300 1.061049e-02 0.302558
2019-11-13 00:40:10,920 train 350 1.059518e-02 0.286245
2019-11-13 00:40:15,971 train 400 1.056948e-02 0.291812
2019-11-13 00:40:21,001 train 450 1.058716e-02 0.297019
2019-11-13 00:40:26,031 train 500 1.057999e-02 0.299844
2019-11-13 00:40:31,017 train 550 1.056088e-02 0.300374
2019-11-13 00:40:36,012 train 600 1.057534e-02 0.303010
2019-11-13 00:40:41,013 train 650 1.057299e-02 0.306868
2019-11-13 00:40:46,053 train 700 1.058784e-02 0.308207
2019-11-13 00:40:51,070 train 750 1.058285e-02 0.307885
2019-11-13 00:40:56,064 train 800 1.058081e-02 0.305297
2019-11-13 00:41:01,056 train 850 1.059238e-02 0.308283
2019-11-13 00:41:02,544 training loss; R2: 1.059758e-02 0.308978
2019-11-13 00:41:02,867 valid 000 1.080313e-02 0.452275
2019-11-13 00:41:04,573 valid 050 9.667087e-03 0.407603
2019-11-13 00:41:06,126 validation loss; R2: 9.755975e-03 0.410261
2019-11-13 00:41:06,135 epoch 14 lr 1.000000e-03
2019-11-13 00:41:06,477 train 000 1.067523e-02 0.232577
2019-11-13 00:41:11,421 train 050 1.088519e-02 0.255856
2019-11-13 00:41:16,476 train 100 1.076365e-02 0.301652
2019-11-13 00:41:21,543 train 150 1.064419e-02 0.305930
2019-11-13 00:41:26,611 train 200 1.063999e-02 0.308279
2019-11-13 00:41:31,659 train 250 1.059233e-02 0.317857
2019-11-13 00:41:36,713 train 300 1.055568e-02 0.322367
2019-11-13 00:41:41,755 train 350 1.053781e-02 0.318164
2019-11-13 00:41:46,793 train 400 1.050278e-02 0.324778
2019-11-13 00:41:51,837 train 450 1.049922e-02 0.321773
2019-11-13 00:41:56,874 train 500 1.051561e-02 0.325864
2019-11-13 00:42:01,904 train 550 1.048108e-02 0.328360
2019-11-13 00:42:06,892 train 600 1.049852e-02 0.329791
2019-11-13 00:42:11,877 train 650 1.051239e-02 0.330873
2019-11-13 00:42:16,866 train 700 1.052119e-02 0.326216
2019-11-13 00:42:21,856 train 750 1.053691e-02 0.324096
2019-11-13 00:42:26,836 train 800 1.052973e-02 0.325755
2019-11-13 00:42:31,839 train 850 1.051735e-02 0.325433
2019-11-13 00:42:33,334 training loss; R2: 1.050734e-02 0.324885
2019-11-13 00:42:33,639 valid 000 1.854786e-02 -1.444042
2019-11-13 00:42:35,369 valid 050 1.750221e-02 0.141220
2019-11-13 00:42:36,911 validation loss; R2: 1.737103e-02 0.154780
2019-11-13 00:42:36,923 epoch 15 lr 1.000000e-03
2019-11-13 00:42:37,309 train 000 9.513761e-03 0.078031
2019-11-13 00:42:42,171 train 050 1.060083e-02 0.318566
2019-11-13 00:42:47,216 train 100 1.060948e-02 0.297274
2019-11-13 00:42:52,269 train 150 1.046199e-02 0.311130
2019-11-13 00:42:57,334 train 200 1.033862e-02 0.319899
2019-11-13 00:43:02,388 train 250 1.037276e-02 0.318643
2019-11-13 00:43:07,441 train 300 1.036770e-02 0.322723
2019-11-13 00:43:12,496 train 350 1.039174e-02 0.322030
2019-11-13 00:43:17,551 train 400 1.038080e-02 0.322579
2019-11-13 00:43:22,611 train 450 1.038326e-02 0.324894
2019-11-13 00:43:27,682 train 500 1.036698e-02 0.321661
2019-11-13 00:43:32,738 train 550 1.035379e-02 -0.970590
2019-11-13 00:43:37,792 train 600 1.037712e-02 -0.861415
2019-11-13 00:43:42,849 train 650 1.036140e-02 -0.794748
2019-11-13 00:43:47,914 train 700 1.036095e-02 -0.713683
2019-11-13 00:43:52,974 train 750 1.036939e-02 -0.644242
2019-11-13 00:43:58,029 train 800 1.038671e-02 -0.585401
2019-11-13 00:44:03,111 train 850 1.039193e-02 -0.530977
2019-11-13 00:44:04,624 training loss; R2: 1.038559e-02 -0.515922
2019-11-13 00:44:04,937 valid 000 1.540902e-02 0.209208
2019-11-13 00:44:06,645 valid 050 1.507201e-02 0.263841
2019-11-13 00:44:08,190 validation loss; R2: 1.508389e-02 0.269481
2019-11-13 00:44:08,202 epoch 16 lr 1.000000e-03
2019-11-13 00:44:08,589 train 000 9.350248e-03 0.353971
2019-11-13 00:44:13,637 train 050 1.034656e-02 0.325628
2019-11-13 00:44:18,677 train 100 1.041489e-02 0.342525
2019-11-13 00:44:23,721 train 150 1.035454e-02 0.124894
2019-11-13 00:44:28,763 train 200 1.038888e-02 0.172227
2019-11-13 00:44:33,911 train 250 1.035476e-02 0.199348
2019-11-13 00:44:38,987 train 300 1.030912e-02 0.217702
2019-11-13 00:44:44,060 train 350 1.035580e-02 0.234571
2019-11-13 00:44:49,120 train 400 1.032299e-02 0.244158
2019-11-13 00:44:54,162 train 450 1.032581e-02 0.256492
2019-11-13 00:44:59,228 train 500 1.036111e-02 0.263146
2019-11-13 00:45:04,292 train 550 1.036441e-02 0.259875
2019-11-13 00:45:09,334 train 600 1.033974e-02 0.267589
2019-11-13 00:45:14,398 train 650 1.034864e-02 0.229342
2019-11-13 00:45:19,454 train 700 1.033669e-02 0.235378
2019-11-13 00:45:24,497 train 750 1.033121e-02 0.242428
2019-11-13 00:45:29,538 train 800 1.032380e-02 0.247557
2019-11-13 00:45:34,601 train 850 1.030611e-02 0.253329
2019-11-13 00:45:36,110 training loss; R2: 1.030848e-02 0.253675
2019-11-13 00:45:36,418 valid 000 2.215171e-02 0.039210
2019-11-13 00:45:38,123 valid 050 1.968388e-02 0.127854
2019-11-13 00:45:39,698 validation loss; R2: 1.969451e-02 0.122447
2019-11-13 00:45:39,716 epoch 17 lr 1.000000e-03
2019-11-13 00:45:40,143 train 000 8.985370e-03 0.390889
2019-11-13 00:45:45,094 train 050 1.041551e-02 0.352745
2019-11-13 00:45:49,932 train 100 1.038187e-02 -2.398657
2019-11-13 00:45:54,782 train 150 1.028722e-02 -1.498237
2019-11-13 00:45:59,844 train 200 1.025603e-02 -1.041853
2019-11-13 00:46:04,937 train 250 1.026931e-02 -0.770032
2019-11-13 00:46:10,016 train 300 1.024167e-02 -0.587269
2019-11-13 00:46:15,084 train 350 1.024727e-02 -0.491203
2019-11-13 00:46:20,130 train 400 1.022383e-02 -0.392125
2019-11-13 00:46:25,219 train 450 1.020327e-02 -0.330605
2019-11-13 00:46:30,150 train 500 1.020470e-02 -0.263396
2019-11-13 00:46:35,017 train 550 1.020846e-02 -0.207679
2019-11-13 00:46:39,860 train 600 1.018454e-02 -0.162957
2019-11-13 00:46:44,738 train 650 1.019086e-02 -0.121791
2019-11-13 00:46:49,761 train 700 1.019624e-02 -0.088247
2019-11-13 00:46:54,809 train 750 1.018313e-02 -0.059434
2019-11-13 00:46:59,867 train 800 1.018564e-02 -0.033689
2019-11-13 00:47:04,902 train 850 1.017789e-02 -0.010869
2019-11-13 00:47:06,407 training loss; R2: 1.017550e-02 -0.003910
2019-11-13 00:47:06,714 valid 000 1.966948e-02 0.156916
2019-11-13 00:47:08,434 valid 050 2.116062e-02 -0.041738
2019-11-13 00:47:09,970 validation loss; R2: 2.131961e-02 -0.066723
2019-11-13 00:47:09,982 epoch 18 lr 1.000000e-03
2019-11-13 00:47:10,354 train 000 8.997661e-03 0.350077
2019-11-13 00:47:15,437 train 050 1.012338e-02 0.370199
2019-11-13 00:47:20,510 train 100 1.012358e-02 0.301364
2019-11-13 00:47:25,594 train 150 1.014426e-02 0.168599
2019-11-13 00:47:30,676 train 200 1.019243e-02 0.207575
2019-11-13 00:47:35,756 train 250 1.014906e-02 0.235236
2019-11-13 00:47:40,829 train 300 1.013034e-02 0.251337
2019-11-13 00:47:45,918 train 350 1.011040e-02 0.270950
2019-11-13 00:47:50,978 train 400 1.012020e-02 0.278534
2019-11-13 00:47:56,023 train 450 1.009745e-02 0.244997
2019-11-13 00:48:01,086 train 500 1.008717e-02 0.254766
2019-11-13 00:48:06,131 train 550 1.010389e-02 0.265474
2019-11-13 00:48:11,112 train 600 1.010224e-02 0.272978
2019-11-13 00:48:16,157 train 650 1.011118e-02 0.277428
2019-11-13 00:48:21,227 train 700 1.012584e-02 0.280333
2019-11-13 00:48:26,276 train 750 1.012442e-02 0.283566
2019-11-13 00:48:31,320 train 800 1.011434e-02 0.288045
2019-11-13 00:48:36,369 train 850 1.010699e-02 0.291591
2019-11-13 00:48:37,875 training loss; R2: 1.011120e-02 0.291673
2019-11-13 00:48:38,195 valid 000 2.262568e-02 -0.047569
2019-11-13 00:48:39,883 valid 050 2.333989e-02 -0.044182
2019-11-13 00:48:41,442 validation loss; R2: 2.354232e-02 -0.058594
2019-11-13 00:48:41,460 epoch 19 lr 1.000000e-03
2019-11-13 00:48:41,794 train 000 1.129714e-02 0.337199
2019-11-13 00:48:46,620 train 050 9.832546e-03 0.363634
2019-11-13 00:48:51,444 train 100 9.961093e-03 0.366619
2019-11-13 00:48:56,266 train 150 9.985957e-03 0.208314
2019-11-13 00:49:01,104 train 200 1.008421e-02 0.205598
2019-11-13 00:49:06,161 train 250 1.004216e-02 0.233367
2019-11-13 00:49:11,242 train 300 1.003223e-02 0.248009
2019-11-13 00:49:16,348 train 350 1.001944e-02 0.253061
2019-11-13 00:49:21,479 train 400 1.002946e-02 0.262563
2019-11-13 00:49:26,541 train 450 1.001400e-02 0.274937
2019-11-13 00:49:31,521 train 500 1.001887e-02 0.273838
2019-11-13 00:49:36,537 train 550 1.002030e-02 0.272740
2019-11-13 00:49:41,606 train 600 1.002347e-02 0.281109
2019-11-13 00:49:46,702 train 650 1.003477e-02 0.285007
2019-11-13 00:49:51,773 train 700 1.002616e-02 0.289304
2019-11-13 00:49:56,816 train 750 1.001326e-02 0.293512
2019-11-13 00:50:01,856 train 800 1.002779e-02 0.297038
2019-11-13 00:50:06,911 train 850 1.003056e-02 0.167025
2019-11-13 00:50:08,414 training loss; R2: 1.002858e-02 0.170484
2019-11-13 00:50:08,739 valid 000 1.164591e-02 0.391339
2019-11-13 00:50:10,441 valid 050 9.501605e-03 0.421528
2019-11-13 00:50:11,977 validation loss; R2: 9.584329e-03 0.420556
