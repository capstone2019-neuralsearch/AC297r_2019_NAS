2019-11-13 19:28:19,662 gpu device = 1
2019-11-13 19:28:19,662 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name='galaxy-zoo-cleaned', gpu=1, grad_clip=5, gz_dtree=False, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-galaxy-zoo-EXP-20191113-192819', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-11-13 19:28:31,254 param size = 0.294613MB
2019-11-13 19:28:31,258 epoch 0 lr 1.000000e-03
2019-11-13 19:28:33,509 train 000 4.802341e-01 -240.676394
2019-11-13 19:28:40,645 train 050 5.299281e-02 -10.365290
2019-11-13 19:28:47,645 train 100 4.044429e-02 -5.604101
2019-11-13 19:28:54,861 train 150 3.571475e-02 -3.876545
2019-11-13 19:29:02,190 train 200 3.301951e-02 -2.982387
2019-11-13 19:29:09,322 train 250 3.134968e-02 -2.432557
2019-11-13 19:29:16,616 train 300 3.006371e-02 -2.068979
2019-11-13 19:29:23,799 train 350 2.911639e-02 -1.809756
2019-11-13 19:29:30,989 train 400 2.841315e-02 -1.595540
2019-11-13 19:29:38,114 train 450 2.783023e-02 -1.431574
2019-11-13 19:29:45,071 train 500 2.731731e-02 -1.308798
2019-11-13 19:29:52,010 train 550 2.688322e-02 -1.201608
2019-11-13 19:29:58,936 train 600 2.647877e-02 -1.275408
2019-11-13 19:30:05,876 train 650 2.606197e-02 -1.179257
2019-11-13 19:30:12,842 train 700 2.568877e-02 -1.097397
2019-11-13 19:30:19,766 train 750 2.538514e-02 -1.027291
2019-11-13 19:30:26,699 train 800 2.508836e-02 -0.968436
2019-11-13 19:30:33,676 train 850 2.480149e-02 -0.911712
2019-11-13 19:30:36,582 training loss; R2: 2.470248e-02 -0.895307
2019-11-13 19:30:36,953 valid 000 2.354453e-02 0.093955
2019-11-13 19:30:38,685 valid 050 2.325453e-02 -0.076084
2019-11-13 19:30:40,344 validation loss; R2: 2.308564e-02 -0.086986
2019-11-13 19:30:40,363 epoch 1 lr 1.000000e-03
2019-11-13 19:30:41,030 train 000 1.790071e-02 0.066360
2019-11-13 19:30:47,982 train 050 1.939279e-02 0.032810
2019-11-13 19:30:54,924 train 100 1.972593e-02 0.010310
2019-11-13 19:31:01,879 train 150 1.966154e-02 0.019242
2019-11-13 19:31:08,823 train 200 1.965583e-02 0.017557
2019-11-13 19:31:15,768 train 250 1.961929e-02 0.023637
2019-11-13 19:31:22,719 train 300 1.947855e-02 0.032183
2019-11-13 19:31:29,666 train 350 1.938571e-02 0.032194
2019-11-13 19:31:36,615 train 400 1.930019e-02 0.032205
2019-11-13 19:31:43,554 train 450 1.921869e-02 0.034150
2019-11-13 19:31:50,491 train 500 1.909990e-02 0.039097
2019-11-13 19:31:57,441 train 550 1.903172e-02 0.039596
2019-11-13 19:32:04,382 train 600 1.895856e-02 0.044430
2019-11-13 19:32:11,336 train 650 1.886977e-02 0.048964
2019-11-13 19:32:18,283 train 700 1.882473e-02 0.052619
2019-11-13 19:32:25,223 train 750 1.874608e-02 0.055726
2019-11-13 19:32:32,169 train 800 1.867264e-02 0.057920
2019-11-13 19:32:39,116 train 850 1.860488e-02 0.058104
2019-11-13 19:32:41,194 training loss; R2: 1.857831e-02 0.059508
2019-11-13 19:32:41,536 valid 000 1.635154e-02 0.251143
2019-11-13 19:32:43,260 valid 050 1.613950e-02 0.193353
2019-11-13 19:32:44,850 validation loss; R2: 1.597052e-02 0.190994
2019-11-13 19:32:44,876 epoch 2 lr 1.000000e-03
2019-11-13 19:32:45,335 train 000 1.512678e-02 0.256981
2019-11-13 19:32:52,312 train 050 1.689068e-02 0.103959
2019-11-13 19:32:59,272 train 100 1.700385e-02 0.105800
2019-11-13 19:33:06,277 train 150 1.690120e-02 0.108950
2019-11-13 19:33:13,213 train 200 1.698220e-02 0.108884
2019-11-13 19:33:20,155 train 250 1.693191e-02 0.114618
2019-11-13 19:33:27,086 train 300 1.684586e-02 0.119456
2019-11-13 19:33:34,110 train 350 1.682191e-02 0.119053
2019-11-13 19:33:41,046 train 400 1.676671e-02 0.123103
2019-11-13 19:33:47,983 train 450 1.665860e-02 0.126577
2019-11-13 19:33:54,916 train 500 1.662573e-02 0.131318
2019-11-13 19:34:01,855 train 550 1.662656e-02 0.127872
2019-11-13 19:34:08,792 train 600 1.657071e-02 0.131150
2019-11-13 19:34:15,730 train 650 1.649904e-02 0.131838
2019-11-13 19:34:22,664 train 700 1.647047e-02 0.134850
2019-11-13 19:34:29,605 train 750 1.642484e-02 0.134620
2019-11-13 19:34:36,545 train 800 1.637404e-02 0.135009
2019-11-13 19:34:43,476 train 850 1.634166e-02 0.131568
2019-11-13 19:34:45,553 training loss; R2: 1.632140e-02 0.132516
2019-11-13 19:34:45,900 valid 000 1.557324e-02 0.285400
2019-11-13 19:34:47,649 valid 050 1.451432e-02 0.200506
2019-11-13 19:34:49,253 validation loss; R2: 1.443220e-02 0.190469
2019-11-13 19:34:49,272 epoch 3 lr 1.000000e-03
2019-11-13 19:34:49,711 train 000 1.537992e-02 0.019021
2019-11-13 19:34:56,733 train 050 1.594244e-02 0.160486
2019-11-13 19:35:03,715 train 100 1.566028e-02 0.180193
2019-11-13 19:35:10,674 train 150 1.554510e-02 0.151054
2019-11-13 19:35:17,640 train 200 1.545129e-02 0.147773
2019-11-13 19:35:24,592 train 250 1.533635e-02 0.157543
2019-11-13 19:35:31,545 train 300 1.532795e-02 0.149538
2019-11-13 19:35:38,496 train 350 1.520127e-02 0.157428
2019-11-13 19:35:45,477 train 400 1.519355e-02 0.161549
2019-11-13 19:35:52,419 train 450 1.519102e-02 0.165898
2019-11-13 19:35:59,367 train 500 1.515906e-02 0.169834
2019-11-13 19:36:06,325 train 550 1.511768e-02 0.169913
2019-11-13 19:36:13,271 train 600 1.504575e-02 0.168628
2019-11-13 19:36:20,217 train 650 1.498342e-02 0.171589
2019-11-13 19:36:27,164 train 700 1.496998e-02 0.172772
2019-11-13 19:36:34,120 train 750 1.493382e-02 0.106596
2019-11-13 19:36:41,065 train 800 1.490662e-02 0.114194
2019-11-13 19:36:48,005 train 850 1.489773e-02 0.118326
2019-11-13 19:36:50,079 training loss; R2: 1.490208e-02 0.120063
2019-11-13 19:36:50,441 valid 000 1.422680e-02 0.354680
2019-11-13 19:36:52,211 valid 050 1.391451e-02 0.192058
2019-11-13 19:36:53,800 validation loss; R2: 1.382386e-02 0.159981
2019-11-13 19:36:53,818 epoch 4 lr 1.000000e-03
2019-11-13 19:36:54,265 train 000 1.577896e-02 0.079084
2019-11-13 19:37:01,291 train 050 1.441692e-02 0.183985
2019-11-13 19:37:08,314 train 100 1.459204e-02 0.198755
2019-11-13 19:37:15,320 train 150 1.437510e-02 0.205993
2019-11-13 19:37:22,252 train 200 1.426440e-02 0.176580
2019-11-13 19:37:29,185 train 250 1.422946e-02 0.182140
2019-11-13 19:37:36,133 train 300 1.414147e-02 0.185619
2019-11-13 19:37:43,064 train 350 1.413398e-02 0.194715
2019-11-13 19:37:50,000 train 400 1.415507e-02 0.194498
2019-11-13 19:37:56,933 train 450 1.416053e-02 0.199275
2019-11-13 19:38:03,861 train 500 1.417027e-02 0.196181
2019-11-13 19:38:10,786 train 550 1.412818e-02 0.199063
2019-11-13 19:38:17,713 train 600 1.409126e-02 0.197915
2019-11-13 19:38:24,643 train 650 1.408972e-02 0.185470
2019-11-13 19:38:31,577 train 700 1.405774e-02 0.189457
2019-11-13 19:38:38,504 train 750 1.402678e-02 0.192278
2019-11-13 19:38:45,433 train 800 1.400985e-02 0.192684
2019-11-13 19:38:52,352 train 850 1.399641e-02 0.195597
2019-11-13 19:38:54,425 training loss; R2: 1.399534e-02 0.195978
2019-11-13 19:38:54,765 valid 000 9.623996e-03 0.464462
2019-11-13 19:38:56,538 valid 050 1.179790e-02 0.337903
2019-11-13 19:38:58,119 validation loss; R2: 1.170103e-02 0.342334
2019-11-13 19:38:58,139 epoch 5 lr 1.000000e-03
2019-11-13 19:38:58,551 train 000 1.047365e-02 0.247565
2019-11-13 19:39:05,526 train 050 1.322254e-02 0.232523
2019-11-13 19:39:12,472 train 100 1.354267e-02 0.236080
2019-11-13 19:39:19,405 train 150 1.368372e-02 0.231880
2019-11-13 19:39:26,340 train 200 1.360864e-02 0.234805
2019-11-13 19:39:33,290 train 250 1.354328e-02 0.231415
2019-11-13 19:39:40,219 train 300 1.353414e-02 0.229703
2019-11-13 19:39:47,155 train 350 1.355523e-02 0.228063
2019-11-13 19:39:54,081 train 400 1.357275e-02 0.226357
2019-11-13 19:40:01,018 train 450 1.356583e-02 0.227112
2019-11-13 19:40:07,955 train 500 1.355885e-02 0.228923
2019-11-13 19:40:14,889 train 550 1.352210e-02 0.232864
2019-11-13 19:40:21,815 train 600 1.349216e-02 0.234556
2019-11-13 19:40:28,759 train 650 1.348187e-02 0.226054
2019-11-13 19:40:35,696 train 700 1.349277e-02 0.228341
2019-11-13 19:40:42,630 train 750 1.345840e-02 0.232046
2019-11-13 19:40:49,565 train 800 1.343225e-02 0.232828
2019-11-13 19:40:56,495 train 850 1.341817e-02 0.235430
2019-11-13 19:40:58,576 training loss; R2: 1.341025e-02 0.234761
2019-11-13 19:40:58,930 valid 000 1.033387e-02 0.321810
2019-11-13 19:41:00,684 valid 050 1.212716e-02 0.301315
2019-11-13 19:41:02,280 validation loss; R2: 1.208965e-02 0.304782
2019-11-13 19:41:02,299 epoch 6 lr 1.000000e-03
2019-11-13 19:41:02,748 train 000 1.217731e-02 0.321138
2019-11-13 19:41:09,721 train 050 1.276807e-02 0.168954
2019-11-13 19:41:16,652 train 100 1.283205e-02 0.207494
2019-11-13 19:41:23,579 train 150 1.296086e-02 0.216774
2019-11-13 19:41:30,509 train 200 1.302461e-02 0.223953
2019-11-13 19:41:37,438 train 250 1.302466e-02 0.219133
2019-11-13 19:41:44,358 train 300 1.303464e-02 0.228208
2019-11-13 19:41:51,288 train 350 1.304064e-02 0.233006
2019-11-13 19:41:58,291 train 400 1.305732e-02 0.236180
2019-11-13 19:42:05,217 train 450 1.302942e-02 0.234483
2019-11-13 19:42:12,147 train 500 1.302826e-02 0.227711
2019-11-13 19:42:19,074 train 550 1.300300e-02 0.229957
2019-11-13 19:42:26,006 train 600 1.303300e-02 0.228991
2019-11-13 19:42:32,974 train 650 1.300356e-02 0.231532
2019-11-13 19:42:39,947 train 700 1.297985e-02 0.235886
2019-11-13 19:42:46,874 train 750 1.299724e-02 0.234673
2019-11-13 19:42:53,806 train 800 1.296492e-02 0.236008
2019-11-13 19:43:00,744 train 850 1.296336e-02 0.237574
2019-11-13 19:43:02,815 training loss; R2: 1.295957e-02 0.235708
2019-11-13 19:43:03,158 valid 000 1.130041e-02 0.349460
2019-11-13 19:43:04,915 valid 050 1.079889e-02 0.374044
2019-11-13 19:43:06,519 validation loss; R2: 1.076356e-02 0.374172
2019-11-13 19:43:06,537 epoch 7 lr 1.000000e-03
2019-11-13 19:43:06,988 train 000 1.295858e-02 0.342945
2019-11-13 19:43:14,134 train 050 1.302523e-02 0.240225
2019-11-13 19:43:21,098 train 100 1.291093e-02 0.188408
2019-11-13 19:43:28,038 train 150 1.291009e-02 0.214462
2019-11-13 19:43:34,997 train 200 1.279954e-02 0.226275
2019-11-13 19:43:41,940 train 250 1.272776e-02 0.236089
2019-11-13 19:43:48,873 train 300 1.270563e-02 0.242038
2019-11-13 19:43:55,809 train 350 1.267795e-02 0.244007
2019-11-13 19:44:02,744 train 400 1.267439e-02 0.250300
2019-11-13 19:44:09,672 train 450 1.267835e-02 0.249002
2019-11-13 19:44:16,602 train 500 1.266884e-02 0.254834
2019-11-13 19:44:23,537 train 550 1.267427e-02 0.255299
2019-11-13 19:44:30,479 train 600 1.264293e-02 0.255188
2019-11-13 19:44:37,417 train 650 1.265612e-02 0.255536
2019-11-13 19:44:44,349 train 700 1.264355e-02 0.253639
2019-11-13 19:44:51,345 train 750 1.263144e-02 0.254682
2019-11-13 19:44:58,277 train 800 1.260167e-02 0.257421
2019-11-13 19:45:05,209 train 850 1.258373e-02 0.257277
2019-11-13 19:45:07,284 training loss; R2: 1.258449e-02 0.254105
2019-11-13 19:45:07,649 valid 000 8.173031e-01 -60.556235
2019-11-13 19:45:09,412 valid 050 8.260552e-01 -70.196226
2019-11-13 19:45:11,003 validation loss; R2: 8.252702e-01 -65.922179
2019-11-13 19:45:11,023 epoch 8 lr 1.000000e-03
2019-11-13 19:45:11,464 train 000 1.094111e-02 0.414024
2019-11-13 19:45:18,493 train 050 1.200625e-02 0.279779
2019-11-13 19:45:25,442 train 100 1.207172e-02 0.284844
2019-11-13 19:45:32,362 train 150 1.217695e-02 0.274823
2019-11-13 19:45:39,303 train 200 1.224104e-02 0.278661
2019-11-13 19:45:46,229 train 250 1.222512e-02 0.276226
2019-11-13 19:45:53,152 train 300 1.224956e-02 0.265915
2019-11-13 19:46:00,078 train 350 1.226002e-02 0.266583
2019-11-13 19:46:07,008 train 400 1.225926e-02 0.267915
2019-11-13 19:46:13,929 train 450 1.224155e-02 0.266775
2019-11-13 19:46:20,851 train 500 1.224327e-02 0.271030
2019-11-13 19:46:27,770 train 550 1.223341e-02 0.274317
2019-11-13 19:46:34,692 train 600 1.225311e-02 0.274418
2019-11-13 19:46:41,628 train 650 1.226245e-02 0.272079
2019-11-13 19:46:48,634 train 700 1.228871e-02 0.273405
2019-11-13 19:46:55,554 train 750 1.227479e-02 0.273393
2019-11-13 19:47:02,475 train 800 1.227450e-02 0.271516
2019-11-13 19:47:09,446 train 850 1.227211e-02 0.272049
2019-11-13 19:47:11,521 training loss; R2: 1.227392e-02 0.272639
2019-11-13 19:47:11,861 valid 000 1.100707e+00 -55.049645
2019-11-13 19:47:13,628 valid 050 1.092131e+00 -142.786532
2019-11-13 19:47:15,223 validation loss; R2: 1.091825e+00 -143.178779
2019-11-13 19:47:15,243 epoch 9 lr 1.000000e-03
2019-11-13 19:47:15,658 train 000 1.389947e-02 0.293935
2019-11-13 19:47:22,712 train 050 1.228470e-02 0.266617
2019-11-13 19:47:29,661 train 100 1.230661e-02 0.278992
2019-11-13 19:47:36,608 train 150 1.227960e-02 0.288302
2019-11-13 19:47:43,536 train 200 1.223014e-02 0.281911
2019-11-13 19:47:50,469 train 250 1.210952e-02 0.280669
2019-11-13 19:47:57,393 train 300 1.213315e-02 0.281948
2019-11-13 19:48:04,322 train 350 1.212785e-02 0.280067
2019-11-13 19:48:11,253 train 400 1.213663e-02 0.278914
2019-11-13 19:48:18,178 train 450 1.214152e-02 0.282453
2019-11-13 19:48:25,102 train 500 1.213718e-02 0.284046
2019-11-13 19:48:32,021 train 550 1.212900e-02 0.280948
2019-11-13 19:48:38,949 train 600 1.209479e-02 0.283523
2019-11-13 19:48:45,864 train 650 1.211863e-02 0.282917
2019-11-13 19:48:52,781 train 700 1.212419e-02 0.281880
2019-11-13 19:48:59,695 train 750 1.211889e-02 0.278239
2019-11-13 19:49:06,626 train 800 1.212164e-02 0.276351
2019-11-13 19:49:13,544 train 850 1.210773e-02 0.278603
2019-11-13 19:49:15,613 training loss; R2: 1.210409e-02 0.277804
2019-11-13 19:49:15,955 valid 000 4.321168e+00 -387.160106
2019-11-13 19:49:17,748 valid 050 4.322798e+00 -563.436523
2019-11-13 19:49:19,347 validation loss; R2: 4.324000e+00 -522.995090
2019-11-13 19:49:19,371 epoch 10 lr 1.000000e-03
2019-11-13 19:49:19,850 train 000 1.169179e-02 0.399552
2019-11-13 19:49:26,859 train 050 1.199478e-02 0.297217
2019-11-13 19:49:33,795 train 100 1.188107e-02 0.302197
2019-11-13 19:49:40,737 train 150 1.187426e-02 0.293339
2019-11-13 19:49:47,670 train 200 1.190881e-02 0.267159
2019-11-13 19:49:54,591 train 250 1.191049e-02 0.264541
2019-11-13 19:50:01,513 train 300 1.192213e-02 0.261448
2019-11-13 19:50:08,434 train 350 1.185851e-02 0.218606
2019-11-13 19:50:15,361 train 400 1.188557e-02 0.230109
2019-11-13 19:50:22,294 train 450 1.184016e-02 0.236862
2019-11-13 19:50:29,215 train 500 1.186133e-02 0.234889
2019-11-13 19:50:36,137 train 550 1.187280e-02 0.234533
2019-11-13 19:50:43,062 train 600 1.186323e-02 0.239407
2019-11-13 19:50:49,992 train 650 1.186832e-02 0.242504
2019-11-13 19:50:56,909 train 700 1.186682e-02 0.247217
2019-11-13 19:51:03,832 train 750 1.187834e-02 0.249103
2019-11-13 19:51:10,757 train 800 1.187583e-02 0.211670
2019-11-13 19:51:17,836 train 850 1.186412e-02 0.214894
2019-11-13 19:51:19,934 training loss; R2: 1.185373e-02 0.216304
2019-11-13 19:51:20,284 valid 000 7.589180e+00 -555.928665
2019-11-13 19:51:22,036 valid 050 7.567722e+00 -498.449634
2019-11-13 19:51:23,630 validation loss; R2: 7.566587e+00 -489.566158
2019-11-13 19:51:23,649 epoch 11 lr 1.000000e-03
2019-11-13 19:51:24,085 train 000 1.196040e-02 0.380486
2019-11-13 19:51:31,051 train 050 1.165264e-02 0.290557
2019-11-13 19:51:37,983 train 100 1.170048e-02 0.303696
2019-11-13 19:51:44,949 train 150 1.170271e-02 0.246805
2019-11-13 19:51:51,929 train 200 1.173356e-02 0.260023
2019-11-13 19:51:58,843 train 250 1.173033e-02 0.269219
2019-11-13 19:52:05,746 train 300 1.177323e-02 0.217337
2019-11-13 19:52:12,666 train 350 1.169418e-02 0.228804
2019-11-13 19:52:19,574 train 400 1.171106e-02 0.241641
2019-11-13 19:52:26,474 train 450 1.171301e-02 0.245125
2019-11-13 19:52:33,404 train 500 1.171639e-02 0.250988
2019-11-13 19:52:40,329 train 550 1.172943e-02 0.253835
2019-11-13 19:52:47,248 train 600 1.174772e-02 0.258827
2019-11-13 19:52:54,167 train 650 1.172320e-02 0.261371
2019-11-13 19:53:01,082 train 700 1.170731e-02 0.263259
2019-11-13 19:53:08,004 train 750 1.169231e-02 0.263608
2019-11-13 19:53:14,925 train 800 1.170286e-02 0.264842
2019-11-13 19:53:21,832 train 850 1.169705e-02 0.265082
2019-11-13 19:53:23,905 training loss; R2: 1.168581e-02 0.266177
2019-11-13 19:53:24,257 valid 000 4.702345e+00 -4024.746484
2019-11-13 19:53:26,048 valid 050 4.711464e+00 -1412.686933
2019-11-13 19:53:27,640 validation loss; R2: 4.712044e+00 -1123.240915
2019-11-13 19:53:27,659 epoch 12 lr 1.000000e-03
2019-11-13 19:53:28,110 train 000 1.571693e-02 0.259161
2019-11-13 19:53:35,190 train 050 1.144454e-02 0.297234
2019-11-13 19:53:42,231 train 100 1.153838e-02 0.241970
2019-11-13 19:53:49,161 train 150 1.156636e-02 0.233663
2019-11-13 19:53:56,102 train 200 1.160710e-02 0.238794
2019-11-13 19:54:03,045 train 250 1.154685e-02 0.234792
2019-11-13 19:54:10,027 train 300 1.154368e-02 0.242754
2019-11-13 19:54:16,990 train 350 1.149762e-02 0.255723
2019-11-13 19:54:23,927 train 400 1.152079e-02 0.260998
2019-11-13 19:54:30,860 train 450 1.152503e-02 0.261612
2019-11-13 19:54:37,829 train 500 1.153336e-02 0.267984
2019-11-13 19:54:44,782 train 550 1.155298e-02 0.272308
2019-11-13 19:54:51,732 train 600 1.155725e-02 0.271359
2019-11-13 19:54:58,723 train 650 1.155117e-02 0.273834
2019-11-13 19:55:05,626 train 700 1.156236e-02 0.274244
2019-11-13 19:55:12,571 train 750 1.156283e-02 0.274492
2019-11-13 19:55:19,511 train 800 1.156634e-02 0.276464
2019-11-13 19:55:26,454 train 850 1.157397e-02 0.274455
2019-11-13 19:55:28,539 training loss; R2: 1.158124e-02 0.275076
2019-11-13 19:55:28,900 valid 000 3.916668e+01 -1208.210479
2019-11-13 19:55:30,667 valid 050 3.920080e+01 -1888.208958
2019-11-13 19:55:32,245 validation loss; R2: 3.919436e+01 -1624.382160
2019-11-13 19:55:32,265 epoch 13 lr 1.000000e-03
2019-11-13 19:55:32,700 train 000 1.127295e-02 0.418167
2019-11-13 19:55:39,721 train 050 1.186432e-02 0.322634
2019-11-13 19:55:46,693 train 100 1.175992e-02 0.317728
2019-11-13 19:55:53,628 train 150 1.168702e-02 0.300389
2019-11-13 19:56:00,560 train 200 1.166789e-02 0.307116
2019-11-13 19:56:07,520 train 250 1.157811e-02 0.305038
2019-11-13 19:56:14,479 train 300 1.156052e-02 0.306023
2019-11-13 19:56:21,430 train 350 1.155962e-02 0.309491
2019-11-13 19:56:28,379 train 400 1.154384e-02 0.308802
2019-11-13 19:56:35,373 train 450 1.149836e-02 0.311111
2019-11-13 19:56:42,359 train 500 1.145419e-02 0.314252
2019-11-13 19:56:49,323 train 550 1.141123e-02 0.316589
2019-11-13 19:56:56,258 train 600 1.141147e-02 0.316578
2019-11-13 19:57:03,183 train 650 1.142441e-02 0.307701
2019-11-13 19:57:10,134 train 700 1.142808e-02 0.307933
2019-11-13 19:57:17,121 train 750 1.142304e-02 0.307720
2019-11-13 19:57:24,073 train 800 1.141790e-02 0.307748
2019-11-13 19:57:31,024 train 850 1.141303e-02 0.305983
2019-11-13 19:57:33,122 training loss; R2: 1.141569e-02 0.306170
2019-11-13 19:57:33,461 valid 000 4.935475e-01 -33.174059
2019-11-13 19:57:35,230 valid 050 4.866478e-01 -42.509105
2019-11-13 19:57:36,851 validation loss; R2: 4.884829e-01 -44.129751
2019-11-13 19:57:36,874 epoch 14 lr 1.000000e-03
2019-11-13 19:57:37,349 train 000 1.095594e-02 0.377030
2019-11-13 19:57:44,380 train 050 1.141096e-02 0.329873
2019-11-13 19:57:51,375 train 100 1.131920e-02 0.333330
2019-11-13 19:57:58,362 train 150 1.133692e-02 0.328634
2019-11-13 19:58:05,320 train 200 1.136319e-02 0.329751
2019-11-13 19:58:12,277 train 250 1.137233e-02 0.324828
2019-11-13 19:58:19,241 train 300 1.137955e-02 0.285769
2019-11-13 19:58:26,174 train 350 1.140849e-02 -0.527390
2019-11-13 19:58:33,106 train 400 1.146165e-02 -0.424313
2019-11-13 19:58:40,068 train 450 1.148570e-02 -0.345622
2019-11-13 19:58:47,029 train 500 1.146446e-02 -0.287778
2019-11-13 19:58:53,991 train 550 1.144322e-02 -0.232829
2019-11-13 19:59:00,924 train 600 1.143195e-02 -0.188145
2019-11-13 19:59:07,880 train 650 1.144526e-02 -0.152588
2019-11-13 19:59:14,878 train 700 1.143164e-02 -0.120203
2019-11-13 19:59:21,800 train 750 1.141785e-02 -0.092116
2019-11-13 19:59:28,765 train 800 1.140280e-02 -0.069157
2019-11-13 19:59:35,713 train 850 1.143907e-02 -0.047870
2019-11-13 19:59:37,782 training loss; R2: 1.145488e-02 -0.041893
2019-11-13 19:59:38,139 valid 000 2.336485e+00 -422.187297
2019-11-13 19:59:39,886 valid 050 2.362268e+00 -332.107140
2019-11-13 19:59:41,470 validation loss; R2: 2.362448e+00 -365.003915
2019-11-13 19:59:41,489 epoch 15 lr 1.000000e-03
2019-11-13 19:59:41,915 train 000 1.209341e-02 0.315291
2019-11-13 19:59:48,947 train 050 1.152704e-02 0.329918
2019-11-13 19:59:55,941 train 100 1.152467e-02 0.299012
2019-11-13 20:00:02,930 train 150 1.129560e-02 0.309572
2019-11-13 20:00:09,919 train 200 1.131174e-02 0.307838
2019-11-13 20:00:16,884 train 250 1.134131e-02 0.304393
2019-11-13 20:00:23,859 train 300 1.132382e-02 0.305094
2019-11-13 20:00:30,829 train 350 1.135662e-02 0.305448
2019-11-13 20:00:37,802 train 400 1.132016e-02 0.308025
2019-11-13 20:00:44,750 train 450 1.134645e-02 0.298889
2019-11-13 20:00:51,734 train 500 1.135006e-02 0.300230
2019-11-13 20:00:58,697 train 550 1.136040e-02 0.299121
2019-11-13 20:01:05,677 train 600 1.136520e-02 0.288137
2019-11-13 20:01:12,647 train 650 1.135457e-02 0.283697
2019-11-13 20:01:19,600 train 700 1.135249e-02 0.284867
2019-11-13 20:01:26,570 train 750 1.132184e-02 0.285437
2019-11-13 20:01:33,541 train 800 1.132138e-02 0.282119
2019-11-13 20:01:40,525 train 850 1.131260e-02 0.285283
2019-11-13 20:01:42,616 training loss; R2: 1.130839e-02 0.286314
2019-11-13 20:01:42,969 valid 000 7.197407e-01 -42.746702
2019-11-13 20:01:44,712 valid 050 6.984622e-01 -63.417500
2019-11-13 20:01:46,288 validation loss; R2: 6.950803e-01 -62.142639
2019-11-13 20:01:46,310 epoch 16 lr 1.000000e-03
2019-11-13 20:01:46,758 train 000 1.067244e-02 0.421965
2019-11-13 20:01:53,806 train 050 1.076484e-02 0.322594
2019-11-13 20:02:00,776 train 100 1.107479e-02 0.316390
2019-11-13 20:02:07,762 train 150 1.103727e-02 0.315416
2019-11-13 20:02:14,754 train 200 1.112980e-02 0.309868
2019-11-13 20:02:21,689 train 250 1.116869e-02 0.315444
2019-11-13 20:02:28,655 train 300 1.120123e-02 0.315521
2019-11-13 20:02:35,621 train 350 1.122543e-02 0.318059
2019-11-13 20:02:42,564 train 400 1.127587e-02 0.316337
2019-11-13 20:02:49,469 train 450 1.128455e-02 0.309623
2019-11-13 20:02:56,376 train 500 1.129922e-02 0.310230
2019-11-13 20:03:03,289 train 550 1.131196e-02 0.305376
2019-11-13 20:03:10,211 train 600 1.134586e-02 0.301407
2019-11-13 20:03:17,108 train 650 1.134402e-02 0.303030
2019-11-13 20:03:24,010 train 700 1.137823e-02 0.290661
2019-11-13 20:03:30,913 train 750 1.138361e-02 0.292214
2019-11-13 20:03:37,816 train 800 1.138078e-02 0.288528
2019-11-13 20:03:44,723 train 850 1.139833e-02 0.290814
2019-11-13 20:03:46,791 training loss; R2: 1.140964e-02 0.288460
2019-11-13 20:03:47,144 valid 000 2.804708e+01 -1459.583037
2019-11-13 20:03:48,881 valid 050 2.809474e+01 -1359.972862
2019-11-13 20:03:50,471 validation loss; R2: 2.808939e+01 -1408.399282
2019-11-13 20:03:50,490 epoch 17 lr 1.000000e-03
2019-11-13 20:03:50,890 train 000 1.154256e-02 0.320860
2019-11-13 20:03:57,812 train 050 1.127564e-02 0.335644
2019-11-13 20:04:04,713 train 100 1.140632e-02 0.303525
2019-11-13 20:04:11,612 train 150 1.134253e-02 0.312818
2019-11-13 20:04:18,534 train 200 1.131510e-02 0.312525
2019-11-13 20:04:25,437 train 250 1.136242e-02 0.211595
2019-11-13 20:04:32,375 train 300 1.142168e-02 0.225686
2019-11-13 20:04:39,276 train 350 1.142227e-02 0.232895
2019-11-13 20:04:46,174 train 400 1.141548e-02 0.244328
2019-11-13 20:04:53,088 train 450 1.142123e-02 0.252548
2019-11-13 20:05:00,001 train 500 1.140073e-02 0.259396
2019-11-13 20:05:06,897 train 550 1.137223e-02 0.261887
2019-11-13 20:05:13,800 train 600 1.138483e-02 0.261087
2019-11-13 20:05:20,723 train 650 1.137132e-02 0.222912
2019-11-13 20:05:27,632 train 700 1.134163e-02 0.228818
2019-11-13 20:05:34,527 train 750 1.135143e-02 0.232900
2019-11-13 20:05:41,419 train 800 1.136073e-02 0.239514
2019-11-13 20:05:48,313 train 850 1.135851e-02 0.243668
2019-11-13 20:05:50,376 training loss; R2: 1.135398e-02 0.245405
2019-11-13 20:05:50,721 valid 000 3.509416e-01 -9.025346
2019-11-13 20:05:52,480 valid 050 3.468082e-01 -12.175239
2019-11-13 20:05:54,073 validation loss; R2: 3.462637e-01 -12.404904
2019-11-13 20:05:54,100 epoch 18 lr 1.000000e-03
2019-11-13 20:05:54,528 train 000 1.063985e-02 0.334504
2019-11-13 20:06:01,563 train 050 1.114284e-02 0.328657
2019-11-13 20:06:08,584 train 100 1.126494e-02 0.328932
2019-11-13 20:06:15,502 train 150 1.136586e-02 0.327146
2019-11-13 20:06:22,425 train 200 1.132329e-02 0.321361
2019-11-13 20:06:29,336 train 250 1.123979e-02 0.326675
2019-11-13 20:06:36,255 train 300 1.125029e-02 0.318400
2019-11-13 20:06:43,172 train 350 1.121897e-02 0.297907
2019-11-13 20:06:50,075 train 400 1.123560e-02 0.302161
2019-11-13 20:06:56,982 train 450 1.121220e-02 0.296361
2019-11-13 20:07:03,887 train 500 1.121418e-02 0.295144
2019-11-13 20:07:10,795 train 550 1.121219e-02 0.298506
2019-11-13 20:07:17,689 train 600 1.117076e-02 0.298834
2019-11-13 20:07:24,586 train 650 1.116578e-02 0.296733
2019-11-13 20:07:31,488 train 700 1.114325e-02 0.265351
2019-11-13 20:07:38,400 train 750 1.111764e-02 0.271531
2019-11-13 20:07:45,301 train 800 1.111727e-02 0.264672
2019-11-13 20:07:52,214 train 850 1.110495e-02 0.254610
2019-11-13 20:07:54,279 training loss; R2: 1.110115e-02 0.255630
2019-11-13 20:07:54,642 valid 000 9.649870e-02 -7.143887
2019-11-13 20:07:56,414 valid 050 1.062225e-01 -7.842671
2019-11-13 20:07:58,008 validation loss; R2: 1.061776e-01 -7.825496
2019-11-13 20:07:58,033 epoch 19 lr 1.000000e-03
2019-11-13 20:07:58,526 train 000 1.229503e-02 0.413248
2019-11-13 20:08:05,644 train 050 1.099431e-02 0.313335
2019-11-13 20:08:12,578 train 100 1.108571e-02 0.304294
2019-11-13 20:08:19,497 train 150 1.101228e-02 0.309626
2019-11-13 20:08:26,415 train 200 1.099495e-02 0.302063
2019-11-13 20:08:33,327 train 250 1.101516e-02 0.298823
2019-11-13 20:08:40,224 train 300 1.101910e-02 0.304650
2019-11-13 20:08:47,135 train 350 1.103315e-02 0.301803
2019-11-13 20:08:54,030 train 400 1.102919e-02 0.306244
2019-11-13 20:09:00,937 train 450 1.103123e-02 0.306771
2019-11-13 20:09:07,844 train 500 1.100178e-02 0.251233
2019-11-13 20:09:14,739 train 550 1.101805e-02 0.259180
2019-11-13 20:09:21,633 train 600 1.101733e-02 -0.346489
2019-11-13 20:09:28,521 train 650 1.103372e-02 -0.294360
2019-11-13 20:09:35,424 train 700 1.104151e-02 -0.249332
2019-11-13 20:09:42,324 train 750 1.106402e-02 -0.211229
2019-11-13 20:09:49,227 train 800 1.105813e-02 -0.177496
2019-11-13 20:09:56,122 train 850 1.103555e-02 -0.146207
2019-11-13 20:09:58,184 training loss; R2: 1.103645e-02 -0.137761
2019-11-13 20:09:58,552 valid 000 2.221411e+00 -318.186794
2019-11-13 20:10:00,316 valid 050 2.225030e+00 -298.031156
2019-11-13 20:10:01,915 validation loss; R2: 2.224266e+00 -290.493282
