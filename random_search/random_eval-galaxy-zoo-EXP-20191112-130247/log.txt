2019-11-12 13:02:47,613 gpu device = 1
2019-11-12 13:02:47,614 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name='galaxy-zoo-cleaned', gpu=1, grad_clip=5, gz_dtree=False, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-galaxy-zoo-EXP-20191112-130247', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-11-12 13:02:59,200 param size = 0.241845MB
2019-11-12 13:02:59,204 epoch 0 lr 1.000000e-03
2019-11-12 13:03:01,453 train 000 1.490243e+00 -690.050445
2019-11-12 13:03:07,945 train 050 8.565705e-02 -24.362343
2019-11-12 13:03:14,300 train 100 5.955349e-02 -13.521682
2019-11-12 13:03:20,649 train 150 4.948867e-02 -9.552665
2019-11-12 13:03:26,965 train 200 4.390502e-02 -7.400745
2019-11-12 13:03:33,012 train 250 4.052451e-02 -6.090417
2019-11-12 13:03:39,061 train 300 3.815135e-02 -5.150722
2019-11-12 13:03:45,116 train 350 3.633763e-02 -4.480577
2019-11-12 13:03:51,166 train 400 3.490607e-02 -3.965891
2019-11-12 13:03:57,216 train 450 3.378411e-02 -3.569359
2019-11-12 13:04:03,272 train 500 3.288270e-02 -3.240893
2019-11-12 13:04:09,320 train 550 3.207960e-02 -2.970254
2019-11-12 13:04:15,355 train 600 3.135639e-02 -2.739767
2019-11-12 13:04:21,403 train 650 3.076816e-02 -2.543265
2019-11-12 13:04:27,443 train 700 3.018331e-02 -2.397442
2019-11-12 13:04:33,493 train 750 2.971050e-02 -2.254227
2019-11-12 13:04:39,539 train 800 2.928969e-02 -2.184664
2019-11-12 13:04:45,598 train 850 2.885775e-02 -2.064246
2019-11-12 13:04:48,130 training loss; R2: 2.871792e-02 -2.033070
2019-11-12 13:04:48,409 valid 000 2.033382e-02 -0.021014
2019-11-12 13:04:50,125 valid 050 2.080400e-02 -0.196884
2019-11-12 13:04:51,769 validation loss; R2: 2.061042e-02 -0.290882
2019-11-12 13:04:51,785 epoch 1 lr 1.000000e-03
2019-11-12 13:04:52,288 train 000 1.791761e-02 -0.282030
2019-11-12 13:04:58,358 train 050 2.125740e-02 -0.189965
2019-11-12 13:05:04,424 train 100 2.119765e-02 -0.150120
2019-11-12 13:05:10,503 train 150 2.125679e-02 -0.122074
2019-11-12 13:05:16,559 train 200 2.115795e-02 -0.106206
2019-11-12 13:05:22,607 train 250 2.104220e-02 -0.089506
2019-11-12 13:05:28,663 train 300 2.102370e-02 -0.085068
2019-11-12 13:05:34,722 train 350 2.093979e-02 -0.080059
2019-11-12 13:05:40,776 train 400 2.082472e-02 -0.070656
2019-11-12 13:05:46,843 train 450 2.070843e-02 -0.165134
2019-11-12 13:05:52,898 train 500 2.061542e-02 -0.159856
2019-11-12 13:05:58,970 train 550 2.049915e-02 -0.150193
2019-11-12 13:06:05,042 train 600 2.049250e-02 -0.166547
2019-11-12 13:06:11,116 train 650 2.042084e-02 -0.157810
2019-11-12 13:06:17,193 train 700 2.037234e-02 -0.144723
2019-11-12 13:06:23,263 train 750 2.031232e-02 -0.137283
2019-11-12 13:06:29,343 train 800 2.027543e-02 -0.129216
2019-11-12 13:06:35,420 train 850 2.022269e-02 -0.119011
2019-11-12 13:06:37,241 training loss; R2: 2.019633e-02 -0.117154
2019-11-12 13:06:37,505 valid 000 1.916426e-02 0.140071
2019-11-12 13:06:39,238 valid 050 1.804487e-02 0.167598
2019-11-12 13:06:40,805 validation loss; R2: 1.809391e-02 0.161672
2019-11-12 13:06:40,824 epoch 2 lr 1.000000e-03
2019-11-12 13:06:41,165 train 000 1.753003e-02 0.130766
2019-11-12 13:06:47,452 train 050 1.864743e-02 0.067930
2019-11-12 13:06:53,518 train 100 1.859339e-02 0.032548
2019-11-12 13:06:59,578 train 150 1.859577e-02 0.048011
2019-11-12 13:07:05,636 train 200 1.864270e-02 0.039148
2019-11-12 13:07:11,764 train 250 1.861380e-02 0.011204
2019-11-12 13:07:17,818 train 300 1.857191e-02 0.024005
2019-11-12 13:07:23,881 train 350 1.853059e-02 0.010098
2019-11-12 13:07:29,935 train 400 1.851407e-02 0.019136
2019-11-12 13:07:36,002 train 450 1.850669e-02 0.012174
2019-11-12 13:07:42,057 train 500 1.847681e-02 0.014960
2019-11-12 13:07:48,125 train 550 1.844380e-02 0.023276
2019-11-12 13:07:54,200 train 600 1.840520e-02 0.028160
2019-11-12 13:08:00,281 train 650 1.836953e-02 0.034079
2019-11-12 13:08:06,360 train 700 1.831209e-02 0.036295
2019-11-12 13:08:12,431 train 750 1.829692e-02 0.039402
2019-11-12 13:08:18,505 train 800 1.829589e-02 0.035265
2019-11-12 13:08:24,579 train 850 1.825873e-02 0.038444
2019-11-12 13:08:26,393 training loss; R2: 1.825562e-02 0.038878
2019-11-12 13:08:26,659 valid 000 1.811871e-02 0.025002
2019-11-12 13:08:28,390 valid 050 1.893181e-02 0.081379
2019-11-12 13:08:29,954 validation loss; R2: 1.898074e-02 0.096680
2019-11-12 13:08:29,968 epoch 3 lr 1.000000e-03
2019-11-12 13:08:30,318 train 000 1.470700e-02 0.198176
2019-11-12 13:08:36,571 train 050 1.772789e-02 0.101880
2019-11-12 13:08:42,642 train 100 1.774443e-02 0.114514
2019-11-12 13:08:48,708 train 150 1.760239e-02 0.086717
2019-11-12 13:08:54,782 train 200 1.757795e-02 0.084480
2019-11-12 13:09:00,845 train 250 1.755026e-02 0.083919
2019-11-12 13:09:06,913 train 300 1.756017e-02 0.094812
2019-11-12 13:09:12,978 train 350 1.747590e-02 0.091932
2019-11-12 13:09:19,189 train 400 1.740681e-02 0.086584
2019-11-12 13:09:25,509 train 450 1.733133e-02 0.088617
2019-11-12 13:09:31,715 train 500 1.728786e-02 0.095098
2019-11-12 13:09:37,775 train 550 1.726676e-02 -0.878971
2019-11-12 13:09:43,828 train 600 1.724309e-02 -0.802631
2019-11-12 13:09:49,872 train 650 1.721759e-02 -0.817562
2019-11-12 13:09:56,193 train 700 1.719679e-02 -0.751048
2019-11-12 13:10:02,394 train 750 1.715544e-02 -0.691772
2019-11-12 13:10:08,444 train 800 1.713560e-02 -0.642399
2019-11-12 13:10:14,502 train 850 1.711185e-02 -0.595009
2019-11-12 13:10:16,313 training loss; R2: 1.709707e-02 -0.582387
2019-11-12 13:10:16,584 valid 000 1.835626e-02 0.175945
2019-11-12 13:10:18,298 valid 050 1.769191e-02 0.172649
2019-11-12 13:10:19,858 validation loss; R2: 1.779730e-02 0.177198
2019-11-12 13:10:19,878 epoch 4 lr 1.000000e-03
2019-11-12 13:10:20,234 train 000 2.066184e-02 0.152803
2019-11-12 13:10:26,568 train 050 1.651910e-02 0.162587
2019-11-12 13:10:32,927 train 100 1.665829e-02 0.124834
2019-11-12 13:10:39,244 train 150 1.664280e-02 0.128642
2019-11-12 13:10:45,466 train 200 1.666141e-02 0.133322
2019-11-12 13:10:51,526 train 250 1.649171e-02 0.143948
2019-11-12 13:10:57,874 train 300 1.645443e-02 0.145637
2019-11-12 13:11:04,081 train 350 1.641917e-02 0.148137
2019-11-12 13:11:10,131 train 400 1.643597e-02 0.144354
2019-11-12 13:11:16,192 train 450 1.640797e-02 0.146877
2019-11-12 13:11:22,234 train 500 1.642534e-02 0.149733
2019-11-12 13:11:28,282 train 550 1.641914e-02 0.143887
2019-11-12 13:11:34,341 train 600 1.640364e-02 0.129920
2019-11-12 13:11:40,402 train 650 1.637071e-02 0.132885
2019-11-12 13:11:46,454 train 700 1.634533e-02 0.133353
2019-11-12 13:11:52,504 train 750 1.630990e-02 0.135172
2019-11-12 13:11:58,543 train 800 1.624661e-02 0.136463
2019-11-12 13:12:04,597 train 850 1.620967e-02 0.140285
2019-11-12 13:12:06,408 training loss; R2: 1.620288e-02 0.140162
2019-11-12 13:12:06,676 valid 000 1.320974e-02 0.305355
2019-11-12 13:12:08,404 valid 050 1.431022e-02 0.176269
2019-11-12 13:12:09,963 validation loss; R2: 1.440185e-02 0.197195
2019-11-12 13:12:09,976 epoch 5 lr 1.000000e-03
2019-11-12 13:12:10,333 train 000 1.690928e-02 0.022966
2019-11-12 13:12:16,698 train 050 1.611941e-02 0.142804
2019-11-12 13:12:23,002 train 100 1.581062e-02 0.154539
2019-11-12 13:12:29,273 train 150 1.567083e-02 0.168776
2019-11-12 13:12:35,689 train 200 1.567276e-02 0.167324
2019-11-12 13:12:41,858 train 250 1.561154e-02 0.164686
2019-11-12 13:12:47,963 train 300 1.561353e-02 0.163760
2019-11-12 13:12:54,055 train 350 1.563174e-02 0.169476
2019-11-12 13:13:00,314 train 400 1.560114e-02 0.170445
2019-11-12 13:13:06,676 train 450 1.558926e-02 0.171200
2019-11-12 13:13:13,034 train 500 1.556614e-02 0.169667
2019-11-12 13:13:19,302 train 550 1.554977e-02 0.172613
2019-11-12 13:13:25,371 train 600 1.556021e-02 0.175121
2019-11-12 13:13:31,434 train 650 1.552783e-02 0.176548
2019-11-12 13:13:37,493 train 700 1.549373e-02 0.177904
2019-11-12 13:13:43,565 train 750 1.546727e-02 0.176498
2019-11-12 13:13:49,620 train 800 1.541910e-02 0.176483
2019-11-12 13:13:55,686 train 850 1.540473e-02 0.178239
2019-11-12 13:13:57,500 training loss; R2: 1.538108e-02 0.178838
2019-11-12 13:13:57,760 valid 000 1.259523e-02 0.383490
2019-11-12 13:13:59,491 valid 050 1.418803e-02 0.215928
2019-11-12 13:14:01,056 validation loss; R2: 1.439061e-02 0.210374
2019-11-12 13:14:01,075 epoch 6 lr 1.000000e-03
2019-11-12 13:14:01,402 train 000 1.381711e-02 0.268901
2019-11-12 13:14:07,668 train 050 1.485556e-02 0.208010
2019-11-12 13:14:13,784 train 100 1.492899e-02 0.164777
2019-11-12 13:14:19,946 train 150 1.477958e-02 0.171666
2019-11-12 13:14:26,000 train 200 1.479839e-02 0.179912
2019-11-12 13:14:32,059 train 250 1.485093e-02 0.184714
2019-11-12 13:14:38,112 train 300 1.484619e-02 0.181997
2019-11-12 13:14:44,170 train 350 1.483275e-02 0.179863
2019-11-12 13:14:50,229 train 400 1.482500e-02 0.175110
2019-11-12 13:14:56,285 train 450 1.476812e-02 0.175196
2019-11-12 13:15:02,340 train 500 1.473374e-02 0.175818
2019-11-12 13:15:08,392 train 550 1.472528e-02 0.180229
2019-11-12 13:15:14,440 train 600 1.469017e-02 0.173575
2019-11-12 13:15:20,510 train 650 1.466734e-02 0.173798
2019-11-12 13:15:26,567 train 700 1.463996e-02 0.176124
2019-11-12 13:15:32,621 train 750 1.461512e-02 0.180876
2019-11-12 13:15:38,677 train 800 1.460750e-02 0.178942
2019-11-12 13:15:44,731 train 850 1.458190e-02 0.179732
2019-11-12 13:15:46,542 training loss; R2: 1.457499e-02 0.180660
2019-11-12 13:15:46,807 valid 000 1.002044e-02 0.373506
2019-11-12 13:15:48,542 valid 050 1.281085e-02 0.341380
2019-11-12 13:15:50,116 validation loss; R2: 1.278212e-02 0.322362
2019-11-12 13:15:50,130 epoch 7 lr 1.000000e-03
2019-11-12 13:15:50,492 train 000 1.721852e-02 0.139487
2019-11-12 13:15:56,837 train 050 1.444397e-02 0.150894
2019-11-12 13:16:03,215 train 100 1.430795e-02 0.164476
2019-11-12 13:16:09,554 train 150 1.445670e-02 0.170709
2019-11-12 13:16:15,768 train 200 1.441527e-02 0.160327
2019-11-12 13:16:22,120 train 250 1.435427e-02 0.175138
2019-11-12 13:16:28,461 train 300 1.429983e-02 0.179420
2019-11-12 13:16:34,555 train 350 1.423652e-02 0.186959
2019-11-12 13:16:40,595 train 400 1.416731e-02 0.192920
2019-11-12 13:16:46,645 train 450 1.409289e-02 0.200560
2019-11-12 13:16:52,701 train 500 1.410645e-02 0.204493
2019-11-12 13:16:58,752 train 550 1.406942e-02 0.202605
2019-11-12 13:17:04,807 train 600 1.405923e-02 0.106275
2019-11-12 13:17:10,866 train 650 1.406361e-02 0.112199
2019-11-12 13:17:16,906 train 700 1.407041e-02 0.118017
2019-11-12 13:17:22,951 train 750 1.406520e-02 0.124278
2019-11-12 13:17:29,008 train 800 1.405742e-02 0.129624
2019-11-12 13:17:35,062 train 850 1.404921e-02 0.136199
2019-11-12 13:17:36,872 training loss; R2: 1.402889e-02 0.137385
2019-11-12 13:17:37,146 valid 000 1.074016e-02 0.386031
2019-11-12 13:17:38,869 valid 050 1.272006e-02 0.332389
2019-11-12 13:17:40,442 validation loss; R2: 1.260891e-02 0.335065
2019-11-12 13:17:40,455 epoch 8 lr 1.000000e-03
2019-11-12 13:17:40,818 train 000 1.255505e-02 0.293284
2019-11-12 13:17:47,149 train 050 1.401021e-02 0.232632
2019-11-12 13:17:53,415 train 100 1.361535e-02 0.229265
2019-11-12 13:17:59,501 train 150 1.348621e-02 0.234573
2019-11-12 13:18:05,565 train 200 1.358656e-02 0.240534
2019-11-12 13:18:11,633 train 250 1.356427e-02 0.235563
2019-11-12 13:18:17,700 train 300 1.356210e-02 0.218725
2019-11-12 13:18:23,990 train 350 1.361099e-02 0.213479
2019-11-12 13:18:30,335 train 400 1.364994e-02 0.209001
2019-11-12 13:18:36,685 train 450 1.363774e-02 0.210665
2019-11-12 13:18:43,023 train 500 1.359252e-02 0.214594
2019-11-12 13:18:49,313 train 550 1.360284e-02 0.216655
2019-11-12 13:18:55,625 train 600 1.357229e-02 0.220635
2019-11-12 13:19:01,710 train 650 1.354867e-02 0.213565
2019-11-12 13:19:07,772 train 700 1.354551e-02 0.215343
2019-11-12 13:19:13,828 train 750 1.355088e-02 0.217617
2019-11-12 13:19:19,878 train 800 1.352630e-02 0.220042
2019-11-12 13:19:26,069 train 850 1.354573e-02 0.222616
2019-11-12 13:19:27,904 training loss; R2: 1.354655e-02 0.222979
2019-11-12 13:19:28,176 valid 000 1.179800e-02 0.427168
2019-11-12 13:19:29,908 valid 050 1.217353e-02 0.349830
2019-11-12 13:19:31,479 validation loss; R2: 1.216220e-02 0.343182
2019-11-12 13:19:31,498 epoch 9 lr 1.000000e-03
2019-11-12 13:19:31,850 train 000 1.369176e-02 0.332964
2019-11-12 13:19:38,007 train 050 1.377037e-02 0.260344
2019-11-12 13:19:44,094 train 100 1.356722e-02 0.181079
2019-11-12 13:19:50,209 train 150 1.347379e-02 0.205037
2019-11-12 13:19:56,310 train 200 1.342761e-02 0.211970
2019-11-12 13:20:02,345 train 250 1.341888e-02 0.211559
2019-11-12 13:20:08,383 train 300 1.336957e-02 0.208106
2019-11-12 13:20:14,427 train 350 1.338790e-02 0.210253
2019-11-12 13:20:20,466 train 400 1.336454e-02 0.210435
2019-11-12 13:20:26,507 train 450 1.335059e-02 0.211140
2019-11-12 13:20:32,549 train 500 1.332023e-02 0.216858
2019-11-12 13:20:38,589 train 550 1.327964e-02 0.221567
2019-11-12 13:20:44,627 train 600 1.326751e-02 0.220568
2019-11-12 13:20:50,666 train 650 1.323703e-02 0.223890
2019-11-12 13:20:56,708 train 700 1.322878e-02 0.206006
2019-11-12 13:21:02,752 train 750 1.323818e-02 0.208256
2019-11-12 13:21:08,792 train 800 1.318845e-02 0.211461
2019-11-12 13:21:14,833 train 850 1.318753e-02 0.215007
2019-11-12 13:21:16,641 training loss; R2: 1.318550e-02 0.216146
2019-11-12 13:21:16,912 valid 000 9.897024e-03 0.339706
2019-11-12 13:21:18,664 valid 050 1.191862e-02 -0.003149
2019-11-12 13:21:20,228 validation loss; R2: 1.194173e-02 0.141533
2019-11-12 13:21:20,248 epoch 10 lr 1.000000e-03
2019-11-12 13:21:20,597 train 000 1.374364e-02 0.299317
2019-11-12 13:21:26,928 train 050 1.292982e-02 0.267068
2019-11-12 13:21:33,052 train 100 1.297656e-02 0.272573
2019-11-12 13:21:39,224 train 150 1.299841e-02 0.224965
2019-11-12 13:21:45,386 train 200 1.295147e-02 0.237970
2019-11-12 13:21:51,436 train 250 1.297009e-02 0.245271
2019-11-12 13:21:57,485 train 300 1.294258e-02 0.240433
2019-11-12 13:22:03,547 train 350 1.294828e-02 0.242426
2019-11-12 13:22:09,603 train 400 1.292299e-02 0.244929
2019-11-12 13:22:15,913 train 450 1.290114e-02 0.245802
2019-11-12 13:22:22,233 train 500 1.289773e-02 0.245823
2019-11-12 13:22:28,404 train 550 1.286999e-02 0.248120
2019-11-12 13:22:34,456 train 600 1.287399e-02 0.250947
2019-11-12 13:22:40,516 train 650 1.285518e-02 0.245483
2019-11-12 13:22:46,553 train 700 1.284907e-02 0.243687
2019-11-12 13:22:52,592 train 750 1.284023e-02 0.246332
2019-11-12 13:22:58,633 train 800 1.282436e-02 0.249423
2019-11-12 13:23:04,677 train 850 1.282016e-02 0.251407
2019-11-12 13:23:06,493 training loss; R2: 1.281425e-02 0.252346
2019-11-12 13:23:06,764 valid 000 1.134264e-02 0.368500
2019-11-12 13:23:08,498 valid 050 1.149226e-02 0.367211
2019-11-12 13:23:10,055 validation loss; R2: 1.146718e-02 0.368642
2019-11-12 13:23:10,072 epoch 11 lr 1.000000e-03
2019-11-12 13:23:10,461 train 000 1.225715e-02 0.340676
2019-11-12 13:23:16,847 train 050 1.255094e-02 0.260596
2019-11-12 13:23:23,117 train 100 1.271112e-02 0.262885
2019-11-12 13:23:29,177 train 150 1.274590e-02 0.229446
2019-11-12 13:23:35,227 train 200 1.265578e-02 0.242184
2019-11-12 13:23:41,429 train 250 1.260401e-02 0.249712
2019-11-12 13:23:47,751 train 300 1.263752e-02 0.254304
2019-11-12 13:23:54,076 train 350 1.263175e-02 0.254695
2019-11-12 13:24:00,399 train 400 1.259468e-02 0.258915
2019-11-12 13:24:06,721 train 450 1.258097e-02 0.256534
2019-11-12 13:24:12,867 train 500 1.260068e-02 0.252827
2019-11-12 13:24:18,915 train 550 1.258469e-02 0.250690
2019-11-12 13:24:24,949 train 600 1.257432e-02 0.249798
2019-11-12 13:24:30,992 train 650 1.257759e-02 0.245248
2019-11-12 13:24:37,028 train 700 1.256602e-02 0.245241
2019-11-12 13:24:43,099 train 750 1.254878e-02 0.244362
2019-11-12 13:24:49,145 train 800 1.256445e-02 0.246577
2019-11-12 13:24:55,195 train 850 1.256373e-02 0.247095
2019-11-12 13:24:57,001 training loss; R2: 1.256385e-02 0.247451
2019-11-12 13:24:57,279 valid 000 5.725846e-01 -55.850858
2019-11-12 13:24:59,031 valid 050 5.581248e-01 -45.927525
2019-11-12 13:25:00,608 validation loss; R2: 5.581285e-01 -45.143012
2019-11-12 13:25:00,622 epoch 12 lr 1.000000e-03
2019-11-12 13:25:00,985 train 000 1.211606e-02 0.392054
2019-11-12 13:25:07,257 train 050 1.221419e-02 0.168579
2019-11-12 13:25:13,605 train 100 1.220716e-02 0.151845
2019-11-12 13:25:19,812 train 150 1.227383e-02 0.200175
2019-11-12 13:25:25,883 train 200 1.230518e-02 0.224046
2019-11-12 13:25:31,945 train 250 1.229012e-02 0.239523
2019-11-12 13:25:38,002 train 300 1.228124e-02 0.245025
2019-11-12 13:25:44,075 train 350 1.229558e-02 0.248519
2019-11-12 13:25:50,156 train 400 1.229303e-02 0.249376
2019-11-12 13:25:56,242 train 450 1.228103e-02 0.246768
2019-11-12 13:26:02,325 train 500 1.226877e-02 0.246428
2019-11-12 13:26:08,419 train 550 1.225981e-02 0.250610
2019-11-12 13:26:14,501 train 600 1.227405e-02 0.249887
2019-11-12 13:26:20,588 train 650 1.226936e-02 0.251600
2019-11-12 13:26:26,673 train 700 1.228477e-02 0.254330
2019-11-12 13:26:32,759 train 750 1.227229e-02 0.250345
2019-11-12 13:26:38,846 train 800 1.224698e-02 0.254464
2019-11-12 13:26:44,930 train 850 1.224585e-02 0.256201
2019-11-12 13:26:46,748 training loss; R2: 1.224690e-02 0.255305
2019-11-12 13:26:47,013 valid 000 4.550817e+01 -1699.761450
2019-11-12 13:26:48,703 valid 050 4.563856e+01 -1980.703663
2019-11-12 13:26:50,243 validation loss; R2: 4.563547e+01 -2056.504393
2019-11-12 13:26:50,261 epoch 13 lr 1.000000e-03
2019-11-12 13:26:50,576 train 000 1.657273e-02 0.345467
2019-11-12 13:26:56,858 train 050 1.224265e-02 0.271948
2019-11-12 13:27:03,226 train 100 1.237398e-02 0.267733
2019-11-12 13:27:09,686 train 150 1.236359e-02 0.261900
2019-11-12 13:27:16,203 train 200 1.232286e-02 0.260010
2019-11-12 13:27:22,672 train 250 1.232726e-02 0.262923
2019-11-12 13:27:29,145 train 300 1.234951e-02 0.267008
2019-11-12 13:27:35,598 train 350 1.230925e-02 0.271423
2019-11-12 13:27:42,039 train 400 1.230630e-02 0.232280
2019-11-12 13:27:48,490 train 450 1.225334e-02 0.239813
2019-11-12 13:27:54,925 train 500 1.225513e-02 0.241982
2019-11-12 13:28:01,371 train 550 1.222125e-02 0.224940
2019-11-12 13:28:07,834 train 600 1.222771e-02 0.222030
2019-11-12 13:28:14,231 train 650 1.220552e-02 0.229281
2019-11-12 13:28:20,670 train 700 1.219718e-02 0.234409
2019-11-12 13:28:27,096 train 750 1.218355e-02 0.235463
2019-11-12 13:28:33,513 train 800 1.216848e-02 0.235124
2019-11-12 13:28:39,913 train 850 1.216097e-02 0.237580
2019-11-12 13:28:41,826 training loss; R2: 1.215467e-02 0.238460
2019-11-12 13:28:42,100 valid 000 8.946669e+00 -1623.440384
2019-11-12 13:28:43,802 valid 050 8.920831e+00 -1114.853819
2019-11-12 13:28:45,318 validation loss; R2: 8.916208e+00 -1187.400339
2019-11-12 13:28:45,340 epoch 14 lr 1.000000e-03
2019-11-12 13:28:45,730 train 000 1.358248e-02 0.323643
2019-11-12 13:28:52,143 train 050 1.226566e-02 0.260915
2019-11-12 13:28:58,322 train 100 1.187352e-02 0.213070
2019-11-12 13:29:04,587 train 150 1.193354e-02 0.233494
2019-11-12 13:29:10,891 train 200 1.190445e-02 0.247871
2019-11-12 13:29:17,115 train 250 1.193208e-02 0.256629
2019-11-12 13:29:23,513 train 300 1.196886e-02 0.262790
2019-11-12 13:29:29,876 train 350 1.191158e-02 0.270610
2019-11-12 13:29:36,133 train 400 1.192281e-02 0.274693
2019-11-12 13:29:42,335 train 450 1.193965e-02 0.270540
2019-11-12 13:29:48,673 train 500 1.197663e-02 0.271240
2019-11-12 13:29:54,770 train 550 1.195153e-02 0.273473
2019-11-12 13:30:01,086 train 600 1.195070e-02 0.274859
2019-11-12 13:30:07,139 train 650 1.194990e-02 0.276454
2019-11-12 13:30:13,189 train 700 1.193197e-02 0.277616
2019-11-12 13:30:19,229 train 750 1.193678e-02 0.278120
2019-11-12 13:30:25,295 train 800 1.191983e-02 0.275942
2019-11-12 13:30:31,342 train 850 1.193991e-02 0.276411
2019-11-12 13:30:33,152 training loss; R2: 1.193550e-02 0.277054
2019-11-12 13:30:33,429 valid 000 9.045646e+01 -4894.597768
2019-11-12 13:30:35,104 valid 050 9.037783e+01 -5311.005608
2019-11-12 13:30:36,660 validation loss; R2: 9.037268e+01 -5172.897873
2019-11-12 13:30:36,680 epoch 15 lr 1.000000e-03
2019-11-12 13:30:37,011 train 000 1.030590e-02 0.345764
2019-11-12 13:30:43,049 train 050 1.203123e-02 0.290887
2019-11-12 13:30:49,093 train 100 1.203909e-02 0.287849
2019-11-12 13:30:55,139 train 150 1.196155e-02 0.291482
2019-11-12 13:31:01,174 train 200 1.197977e-02 0.290706
2019-11-12 13:31:07,221 train 250 1.191370e-02 0.292379
2019-11-12 13:31:13,271 train 300 1.190821e-02 0.289653
2019-11-12 13:31:19,306 train 350 1.187692e-02 0.283817
2019-11-12 13:31:25,341 train 400 1.187270e-02 0.286396
2019-11-12 13:31:31,378 train 450 1.186110e-02 0.287193
2019-11-12 13:31:37,434 train 500 1.186640e-02 0.290298
2019-11-12 13:31:43,479 train 550 1.185455e-02 0.289994
2019-11-12 13:31:49,519 train 600 1.183013e-02 0.289769
2019-11-12 13:31:55,573 train 650 1.183092e-02 0.287473
2019-11-12 13:32:01,619 train 700 1.182982e-02 0.288450
2019-11-12 13:32:07,667 train 750 1.180732e-02 0.281833
2019-11-12 13:32:13,710 train 800 1.179418e-02 0.283067
2019-11-12 13:32:19,761 train 850 1.180976e-02 0.282270
2019-11-12 13:32:21,583 training loss; R2: 1.180553e-02 0.282834
2019-11-12 13:32:21,857 valid 000 1.359111e+02 -11803.255124
2019-11-12 13:32:23,595 valid 050 1.357865e+02 -9493.430412
2019-11-12 13:32:25,172 validation loss; R2: 1.357841e+02 -9909.099381
2019-11-12 13:32:25,189 epoch 16 lr 1.000000e-03
2019-11-12 13:32:25,550 train 000 1.043977e-02 0.331495
2019-11-12 13:32:31,934 train 050 1.163930e-02 0.325115
2019-11-12 13:32:37,989 train 100 1.163771e-02 0.312860
2019-11-12 13:32:44,048 train 150 1.167946e-02 0.292518
2019-11-12 13:32:50,096 train 200 1.173668e-02 0.291502
2019-11-12 13:32:56,273 train 250 1.171637e-02 0.280930
2019-11-12 13:33:02,324 train 300 1.173535e-02 0.285981
2019-11-12 13:33:08,382 train 350 1.172361e-02 0.278942
2019-11-12 13:33:14,438 train 400 1.172041e-02 0.279236
2019-11-12 13:33:20,496 train 450 1.171634e-02 0.284409
2019-11-12 13:33:26,554 train 500 1.169919e-02 0.283036
2019-11-12 13:33:32,605 train 550 1.167703e-02 0.286238
2019-11-12 13:33:38,667 train 600 1.169605e-02 0.276680
2019-11-12 13:33:44,965 train 650 1.169262e-02 0.278527
2019-11-12 13:33:51,294 train 700 1.167009e-02 0.282606
2019-11-12 13:33:57,390 train 750 1.168789e-02 0.281403
2019-11-12 13:34:03,441 train 800 1.170479e-02 0.280829
2019-11-12 13:34:09,494 train 850 1.170116e-02 0.283172
2019-11-12 13:34:11,308 training loss; R2: 1.170323e-02 0.276684
2019-11-12 13:34:11,586 valid 000 2.932741e+01 -1943.516759
2019-11-12 13:34:13,326 valid 050 2.934734e+01 -1954.052756
2019-11-12 13:34:14,880 validation loss; R2: 2.933713e+01 -1942.318486
2019-11-12 13:34:14,893 epoch 17 lr 1.000000e-03
2019-11-12 13:34:15,230 train 000 1.004275e-02 0.350950
2019-11-12 13:34:21,616 train 050 1.158803e-02 0.306656
2019-11-12 13:34:27,767 train 100 1.177302e-02 0.300725
2019-11-12 13:34:33,926 train 150 1.179590e-02 0.303401
2019-11-12 13:34:40,100 train 200 1.179544e-02 0.287868
2019-11-12 13:34:46,312 train 250 1.178148e-02 0.289023
2019-11-12 13:34:52,540 train 300 1.180659e-02 0.286874
2019-11-12 13:34:58,720 train 350 1.181867e-02 0.290528
2019-11-12 13:35:04,957 train 400 1.180292e-02 0.263635
2019-11-12 13:35:11,249 train 450 1.176588e-02 0.265644
2019-11-12 13:35:17,560 train 500 1.175076e-02 0.268110
2019-11-12 13:35:23,828 train 550 1.173385e-02 0.270009
2019-11-12 13:35:30,308 train 600 1.170194e-02 0.272189
2019-11-12 13:35:36,678 train 650 1.166493e-02 0.273925
2019-11-12 13:35:42,932 train 700 1.163821e-02 0.275032
2019-11-12 13:35:49,194 train 750 1.163442e-02 0.278035
2019-11-12 13:35:55,445 train 800 1.164664e-02 0.279843
2019-11-12 13:36:01,692 train 850 1.164536e-02 0.277845
2019-11-12 13:36:03,565 training loss; R2: 1.163943e-02 0.278475
2019-11-12 13:36:03,834 valid 000 9.704710e+01 -6275.761467
2019-11-12 13:36:05,523 valid 050 9.688014e+01 -6121.296036
2019-11-12 13:36:07,057 validation loss; R2: 9.688486e+01 -5861.807034
2019-11-12 13:36:07,071 epoch 18 lr 1.000000e-03
2019-11-12 13:36:07,430 train 000 1.146550e-02 -1.606646
2019-11-12 13:36:13,864 train 050 1.135630e-02 0.292816
2019-11-12 13:36:20,365 train 100 1.137868e-02 0.302384
2019-11-12 13:36:26,870 train 150 1.144350e-02 0.297975
2019-11-12 13:36:33,291 train 200 1.142112e-02 0.301232
2019-11-12 13:36:39,560 train 250 1.141282e-02 0.305002
2019-11-12 13:36:45,885 train 300 1.143346e-02 0.302218
2019-11-12 13:36:52,201 train 350 1.145271e-02 0.297832
2019-11-12 13:36:58,535 train 400 1.148700e-02 0.297173
2019-11-12 13:37:04,969 train 450 1.147161e-02 0.296126
2019-11-12 13:37:11,383 train 500 1.148617e-02 0.299369
2019-11-12 13:37:17,847 train 550 1.147788e-02 0.297572
2019-11-12 13:37:24,297 train 600 1.151005e-02 0.299462
2019-11-12 13:37:30,760 train 650 1.151343e-02 0.301543
2019-11-12 13:37:37,214 train 700 1.151762e-02 0.299596
2019-11-12 13:37:43,619 train 750 1.152251e-02 0.299188
2019-11-12 13:37:50,052 train 800 1.152479e-02 0.300754
2019-11-12 13:37:56,389 train 850 1.150822e-02 0.300335
2019-11-12 13:37:58,259 training loss; R2: 1.150824e-02 0.300619
2019-11-12 13:37:58,545 valid 000 1.196576e+00 -122.165550
2019-11-12 13:38:00,252 valid 050 1.183678e+00 -399.620410
2019-11-12 13:38:01,777 validation loss; R2: 1.184590e+00 -319.872716
2019-11-12 13:38:01,794 epoch 19 lr 1.000000e-03
2019-11-12 13:38:02,184 train 000 1.270733e-02 0.363651
2019-11-12 13:38:08,599 train 050 1.152690e-02 0.306522
2019-11-12 13:38:14,904 train 100 1.178559e-02 0.313312
2019-11-12 13:38:21,213 train 150 1.177169e-02 0.315311
2019-11-12 13:38:27,544 train 200 1.156611e-02 0.319722
2019-11-12 13:38:33,591 train 250 1.156159e-02 0.319196
2019-11-12 13:38:39,647 train 300 1.152272e-02 0.318287
2019-11-12 13:38:45,696 train 350 1.148397e-02 0.315133
2019-11-12 13:38:51,948 train 400 1.152032e-02 0.313615
2019-11-12 13:38:58,037 train 450 1.150796e-02 0.315450
2019-11-12 13:39:04,082 train 500 1.151524e-02 0.312269
2019-11-12 13:39:10,145 train 550 1.149724e-02 0.309715
2019-11-12 13:39:16,193 train 600 1.148724e-02 0.304572
2019-11-12 13:39:22,247 train 650 1.147092e-02 0.304895
2019-11-12 13:39:28,292 train 700 1.148013e-02 0.296894
2019-11-12 13:39:34,350 train 750 1.145808e-02 0.296976
2019-11-12 13:39:40,399 train 800 1.145925e-02 0.294767
2019-11-12 13:39:46,453 train 850 1.145521e-02 0.296169
2019-11-12 13:39:48,260 training loss; R2: 1.146094e-02 0.296401
2019-11-12 13:39:48,539 valid 000 4.281294e+00 -286.330920
2019-11-12 13:39:50,222 valid 050 4.272497e+00 -364.441819
2019-11-12 13:39:51,752 validation loss; R2: 4.274142e+00 -380.047463
