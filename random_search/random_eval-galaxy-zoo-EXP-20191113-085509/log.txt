2019-11-13 08:55:09,484 gpu device = 1
2019-11-13 08:55:09,484 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name='galaxy-zoo-cleaned', gpu=1, grad_clip=5, gz_dtree=False, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-galaxy-zoo-EXP-20191113-085509', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-11-13 08:55:20,996 param size = 0.318677MB
2019-11-13 08:55:21,000 epoch 0 lr 1.000000e-03
2019-11-13 08:55:23,269 train 000 8.224378e-01 -1923.885808
2019-11-13 08:55:31,099 train 050 6.467855e-02 -48.099438
2019-11-13 08:55:38,800 train 100 4.744177e-02 -24.687919
2019-11-13 08:55:46,515 train 150 4.030816e-02 -16.681545
2019-11-13 08:55:54,324 train 200 3.665667e-02 -12.632172
2019-11-13 08:56:01,972 train 250 3.431679e-02 -10.182086
2019-11-13 08:56:09,706 train 300 3.260287e-02 -8.527636
2019-11-13 08:56:17,351 train 350 3.145862e-02 -7.348635
2019-11-13 08:56:24,986 train 400 3.046683e-02 -6.446680
2019-11-13 08:56:32,618 train 450 2.971143e-02 -5.747571
2019-11-13 08:56:40,263 train 500 2.901091e-02 -5.192424
2019-11-13 08:56:47,915 train 550 2.842721e-02 -4.726894
2019-11-13 08:56:55,562 train 600 2.788029e-02 -4.341163
2019-11-13 08:57:03,207 train 650 2.735372e-02 -4.015260
2019-11-13 08:57:10,857 train 700 2.693255e-02 -3.735055
2019-11-13 08:57:18,504 train 750 2.654058e-02 -3.493933
2019-11-13 08:57:26,150 train 800 2.616302e-02 -3.287146
2019-11-13 08:57:33,796 train 850 2.582494e-02 -3.094650
2019-11-13 08:57:36,940 training loss; R2: 2.571950e-02 -3.042304
2019-11-13 08:57:37,253 valid 000 1.806257e-02 0.166862
2019-11-13 08:57:38,952 valid 050 1.813684e-02 0.098389
2019-11-13 08:57:40,593 validation loss; R2: 1.790221e-02 0.120482
2019-11-13 08:57:40,616 epoch 1 lr 1.000000e-03
2019-11-13 08:57:41,249 train 000 1.877573e-02 0.094999
2019-11-13 08:57:48,913 train 050 1.961726e-02 0.009302
2019-11-13 08:57:56,559 train 100 1.942312e-02 0.006412
2019-11-13 08:58:04,210 train 150 1.943260e-02 0.017484
2019-11-13 08:58:11,866 train 200 1.925582e-02 0.027480
2019-11-13 08:58:19,533 train 250 1.914606e-02 0.033652
2019-11-13 08:58:27,192 train 300 1.906935e-02 0.036177
2019-11-13 08:58:34,851 train 350 1.901411e-02 0.024760
2019-11-13 08:58:42,512 train 400 1.892352e-02 0.033327
2019-11-13 08:58:50,165 train 450 1.885452e-02 0.026494
2019-11-13 08:58:57,812 train 500 1.876097e-02 0.026961
2019-11-13 08:59:05,460 train 550 1.869551e-02 -0.019650
2019-11-13 08:59:13,109 train 600 1.860809e-02 -0.013206
2019-11-13 08:59:20,757 train 650 1.853410e-02 -0.002816
2019-11-13 08:59:28,406 train 700 1.846747e-02 0.005083
2019-11-13 08:59:36,054 train 750 1.838008e-02 0.009486
2019-11-13 08:59:43,745 train 800 1.828076e-02 0.014970
2019-11-13 08:59:51,397 train 850 1.819131e-02 0.020932
2019-11-13 08:59:53,683 training loss; R2: 1.815484e-02 0.023359
2019-11-13 08:59:53,982 valid 000 1.393747e-02 0.255490
2019-11-13 08:59:55,681 valid 050 1.714198e-02 0.076903
2019-11-13 08:59:57,233 validation loss; R2: 1.708681e-02 0.087868
2019-11-13 08:59:57,252 epoch 2 lr 1.000000e-03
2019-11-13 08:59:57,690 train 000 1.730818e-02 0.243616
2019-11-13 09:00:05,472 train 050 1.707462e-02 0.095740
2019-11-13 09:00:13,142 train 100 1.694528e-02 0.046450
2019-11-13 09:00:20,805 train 150 1.669417e-02 0.071069
2019-11-13 09:00:28,466 train 200 1.660905e-02 0.080686
2019-11-13 09:00:36,118 train 250 1.654054e-02 0.090899
2019-11-13 09:00:43,772 train 300 1.648717e-02 0.103007
2019-11-13 09:00:51,467 train 350 1.640304e-02 0.107309
2019-11-13 09:00:59,140 train 400 1.636880e-02 0.113118
2019-11-13 09:01:06,809 train 450 1.635798e-02 0.112643
2019-11-13 09:01:14,466 train 500 1.630577e-02 0.119277
2019-11-13 09:01:22,119 train 550 1.623796e-02 0.125269
2019-11-13 09:01:29,775 train 600 1.619254e-02 0.124441
2019-11-13 09:01:37,431 train 650 1.612489e-02 0.128575
2019-11-13 09:01:45,082 train 700 1.605961e-02 0.131476
2019-11-13 09:01:52,730 train 750 1.599595e-02 0.132129
2019-11-13 09:02:00,381 train 800 1.593824e-02 0.132527
2019-11-13 09:02:08,035 train 850 1.588541e-02 0.133348
2019-11-13 09:02:10,324 training loss; R2: 1.586838e-02 0.133692
2019-11-13 09:02:10,632 valid 000 1.161130e-02 0.313814
2019-11-13 09:02:12,372 valid 050 1.308623e-02 0.237182
2019-11-13 09:02:13,935 validation loss; R2: 1.307959e-02 0.257837
2019-11-13 09:02:13,959 epoch 3 lr 1.000000e-03
2019-11-13 09:02:14,372 train 000 1.604380e-02 0.221367
2019-11-13 09:02:22,104 train 050 1.461489e-02 0.191766
2019-11-13 09:02:29,897 train 100 1.488472e-02 0.174604
2019-11-13 09:02:37,550 train 150 1.486601e-02 0.175382
2019-11-13 09:02:45,203 train 200 1.486499e-02 0.167006
2019-11-13 09:02:52,857 train 250 1.478534e-02 0.171929
2019-11-13 09:03:00,510 train 300 1.463164e-02 0.174758
2019-11-13 09:03:08,167 train 350 1.464974e-02 0.177122
2019-11-13 09:03:15,818 train 400 1.458194e-02 0.182384
2019-11-13 09:03:23,599 train 450 1.455929e-02 0.189488
2019-11-13 09:03:31,318 train 500 1.450523e-02 0.193293
2019-11-13 09:03:38,990 train 550 1.447231e-02 0.193189
2019-11-13 09:03:46,655 train 600 1.440484e-02 0.196408
2019-11-13 09:03:54,303 train 650 1.437096e-02 0.198341
2019-11-13 09:04:01,955 train 700 1.433916e-02 0.199422
2019-11-13 09:04:09,604 train 750 1.431535e-02 0.197918
2019-11-13 09:04:17,260 train 800 1.426918e-02 0.188726
2019-11-13 09:04:24,912 train 850 1.423905e-02 0.187404
2019-11-13 09:04:27,243 training loss; R2: 1.423235e-02 0.187497
2019-11-13 09:04:27,560 valid 000 1.199088e-02 0.293465
2019-11-13 09:04:29,286 valid 050 1.252723e-02 0.293107
2019-11-13 09:04:30,857 validation loss; R2: 1.265578e-02 0.294109
2019-11-13 09:04:30,876 epoch 4 lr 1.000000e-03
2019-11-13 09:04:31,289 train 000 1.617629e-02 0.288861
2019-11-13 09:04:39,089 train 050 1.381238e-02 0.209351
2019-11-13 09:04:46,836 train 100 1.375462e-02 0.198806
2019-11-13 09:04:54,494 train 150 1.374265e-02 0.185377
2019-11-13 09:05:02,163 train 200 1.374738e-02 0.180102
2019-11-13 09:05:09,813 train 250 1.367121e-02 0.193034
2019-11-13 09:05:17,475 train 300 1.360570e-02 0.187455
2019-11-13 09:05:25,130 train 350 1.359060e-02 0.192964
2019-11-13 09:05:32,793 train 400 1.352223e-02 0.197613
2019-11-13 09:05:40,707 train 450 1.350094e-02 0.204371
2019-11-13 09:05:48,560 train 500 1.348736e-02 0.207962
2019-11-13 09:05:56,250 train 550 1.347536e-02 0.209066
2019-11-13 09:06:03,953 train 600 1.346542e-02 0.208799
2019-11-13 09:06:11,615 train 650 1.344869e-02 0.212842
2019-11-13 09:06:19,320 train 700 1.341789e-02 0.212769
2019-11-13 09:06:26,967 train 750 1.338884e-02 0.215500
2019-11-13 09:06:34,628 train 800 1.336785e-02 0.216965
2019-11-13 09:06:42,286 train 850 1.333032e-02 0.218755
2019-11-13 09:06:44,573 training loss; R2: 1.331959e-02 0.218797
2019-11-13 09:06:44,883 valid 000 2.199133e-02 0.007725
2019-11-13 09:06:46,603 valid 050 1.971285e-02 0.014147
2019-11-13 09:06:48,172 validation loss; R2: 1.966486e-02 0.003391
2019-11-13 09:06:48,193 epoch 5 lr 1.000000e-03
2019-11-13 09:06:48,599 train 000 1.229651e-02 -0.075392
2019-11-13 09:06:56,383 train 050 1.328682e-02 0.235172
2019-11-13 09:07:04,117 train 100 1.295922e-02 0.254040
2019-11-13 09:07:11,846 train 150 1.287392e-02 0.261849
2019-11-13 09:07:19,635 train 200 1.291414e-02 0.254639
2019-11-13 09:07:27,373 train 250 1.285693e-02 0.250973
2019-11-13 09:07:35,064 train 300 1.283133e-02 0.249825
2019-11-13 09:07:42,733 train 350 1.282296e-02 0.247494
2019-11-13 09:07:50,377 train 400 1.285361e-02 0.248387
2019-11-13 09:07:58,023 train 450 1.282730e-02 0.250306
2019-11-13 09:08:05,682 train 500 1.280536e-02 0.246470
2019-11-13 09:08:13,355 train 550 1.275782e-02 0.248725
2019-11-13 09:08:21,031 train 600 1.275437e-02 0.248620
2019-11-13 09:08:28,675 train 650 1.272246e-02 0.250368
2019-11-13 09:08:36,311 train 700 1.271538e-02 0.251136
2019-11-13 09:08:43,946 train 750 1.272467e-02 0.250604
2019-11-13 09:08:51,603 train 800 1.272890e-02 0.250314
2019-11-13 09:08:59,249 train 850 1.272363e-02 0.249257
2019-11-13 09:09:01,540 training loss; R2: 1.272090e-02 0.250308
2019-11-13 09:09:01,849 valid 000 4.634042e-02 -1.222535
2019-11-13 09:09:03,575 valid 050 4.210380e-02 -2.927019
2019-11-13 09:09:05,121 validation loss; R2: 4.193420e-02 -2.243326
2019-11-13 09:09:05,140 epoch 6 lr 1.000000e-03
2019-11-13 09:09:05,553 train 000 1.324801e-02 0.275631
2019-11-13 09:09:13,230 train 050 1.243384e-02 0.209673
2019-11-13 09:09:20,936 train 100 1.232269e-02 0.214640
2019-11-13 09:09:28,640 train 150 1.238052e-02 0.205214
2019-11-13 09:09:36,419 train 200 1.234518e-02 0.221708
2019-11-13 09:09:44,127 train 250 1.236752e-02 0.233598
2019-11-13 09:09:51,818 train 300 1.244185e-02 0.232719
2019-11-13 09:09:59,545 train 350 1.239786e-02 0.236412
2019-11-13 09:10:07,221 train 400 1.237505e-02 0.242874
2019-11-13 09:10:14,891 train 450 1.233525e-02 0.244377
2019-11-13 09:10:22,568 train 500 1.232820e-02 0.241194
2019-11-13 09:10:30,244 train 550 1.230480e-02 0.238238
2019-11-13 09:10:38,086 train 600 1.233080e-02 0.240518
2019-11-13 09:10:45,851 train 650 1.234300e-02 0.243435
2019-11-13 09:10:53,527 train 700 1.233343e-02 0.246538
2019-11-13 09:11:01,233 train 750 1.229104e-02 0.245711
2019-11-13 09:11:08,959 train 800 1.228261e-02 0.246840
2019-11-13 09:11:16,687 train 850 1.226216e-02 0.249727
2019-11-13 09:11:19,052 training loss; R2: 1.225841e-02 0.248945
2019-11-13 09:11:19,364 valid 000 8.383354e-01 -87.878712
2019-11-13 09:11:21,110 valid 050 8.560893e-01 -78.687817
2019-11-13 09:11:22,657 validation loss; R2: 8.573585e-01 -80.397778
2019-11-13 09:11:22,676 epoch 7 lr 1.000000e-03
2019-11-13 09:11:23,070 train 000 1.284317e-02 0.386674
2019-11-13 09:11:30,832 train 050 1.210420e-02 0.280923
2019-11-13 09:11:38,522 train 100 1.211438e-02 0.286972
2019-11-13 09:11:46,224 train 150 1.220286e-02 0.281836
2019-11-13 09:11:53,916 train 200 1.208803e-02 0.246312
2019-11-13 09:12:01,648 train 250 1.210593e-02 0.255825
2019-11-13 09:12:09,321 train 300 1.210963e-02 0.261209
2019-11-13 09:12:17,059 train 350 1.212541e-02 0.267096
2019-11-13 09:12:24,824 train 400 1.210012e-02 0.270300
2019-11-13 09:12:32,517 train 450 1.206076e-02 0.265429
2019-11-13 09:12:40,193 train 500 1.205906e-02 0.264767
2019-11-13 09:12:47,872 train 550 1.204964e-02 0.262474
2019-11-13 09:12:55,606 train 600 1.202279e-02 0.209324
2019-11-13 09:13:03,357 train 650 1.200850e-02 0.212094
2019-11-13 09:13:11,113 train 700 1.198936e-02 0.215550
2019-11-13 09:13:18,910 train 750 1.197360e-02 0.219181
2019-11-13 09:13:26,644 train 800 1.194831e-02 0.220091
2019-11-13 09:13:34,368 train 850 1.194416e-02 0.224486
2019-11-13 09:13:36,659 training loss; R2: 1.193212e-02 0.224998
2019-11-13 09:13:36,974 valid 000 7.497838e-01 -162.291796
2019-11-13 09:13:38,689 valid 050 7.262075e-01 -104.172501
2019-11-13 09:13:40,257 validation loss; R2: 7.225952e-01 -102.365429
2019-11-13 09:13:40,291 epoch 8 lr 1.000000e-03
2019-11-13 09:13:40,696 train 000 1.167341e-02 0.371803
2019-11-13 09:13:48,457 train 050 1.178514e-02 0.291414
2019-11-13 09:13:56,086 train 100 1.162740e-02 0.299855
2019-11-13 09:14:03,721 train 150 1.174100e-02 0.288753
2019-11-13 09:14:11,360 train 200 1.181109e-02 0.282220
2019-11-13 09:14:19,075 train 250 1.180342e-02 0.274433
2019-11-13 09:14:26,699 train 300 1.173441e-02 0.268333
2019-11-13 09:14:34,342 train 350 1.170245e-02 0.273757
2019-11-13 09:14:41,971 train 400 1.167179e-02 0.249068
2019-11-13 09:14:49,587 train 450 1.165292e-02 0.243649
2019-11-13 09:14:57,192 train 500 1.163399e-02 0.244653
2019-11-13 09:15:04,818 train 550 1.164991e-02 0.249568
2019-11-13 09:15:12,441 train 600 1.165831e-02 -0.047954
2019-11-13 09:15:20,056 train 650 1.165402e-02 -0.022282
2019-11-13 09:15:27,664 train 700 1.164052e-02 0.001804
2019-11-13 09:15:35,301 train 750 1.164871e-02 0.020073
2019-11-13 09:15:42,911 train 800 1.162660e-02 0.038031
2019-11-13 09:15:50,527 train 850 1.162963e-02 0.052311
2019-11-13 09:15:52,809 training loss; R2: 1.162154e-02 0.057271
2019-11-13 09:15:53,110 valid 000 2.448969e+01 -1412.385378
2019-11-13 09:15:54,786 valid 050 2.440124e+01 -4744.197833
2019-11-13 09:15:56,320 validation loss; R2: 2.440072e+01 -3792.136022
2019-11-13 09:15:56,340 epoch 9 lr 1.000000e-03
2019-11-13 09:15:56,755 train 000 1.025083e-02 0.310521
2019-11-13 09:16:04,706 train 050 1.192782e-02 0.288120
2019-11-13 09:16:12,618 train 100 1.171630e-02 -1.949499
2019-11-13 09:16:20,507 train 150 1.169842e-02 -1.201281
2019-11-13 09:16:28,586 train 200 1.169888e-02 -0.826952
2019-11-13 09:16:36,372 train 250 1.165032e-02 -0.607527
2019-11-13 09:16:44,448 train 300 1.156649e-02 -0.462749
2019-11-13 09:16:52,521 train 350 1.152150e-02 -0.351116
2019-11-13 09:17:00,488 train 400 1.149515e-02 -0.267239
2019-11-13 09:17:08,476 train 450 1.147533e-02 -0.221223
2019-11-13 09:17:16,331 train 500 1.145912e-02 -0.222077
2019-11-13 09:17:24,111 train 550 1.142326e-02 -0.172836
2019-11-13 09:17:31,870 train 600 1.144073e-02 -0.131140
2019-11-13 09:17:39,825 train 650 1.143290e-02 -0.096892
2019-11-13 09:17:47,757 train 700 1.143151e-02 -0.069478
2019-11-13 09:17:55,555 train 750 1.142562e-02 -0.046953
2019-11-13 09:18:03,581 train 800 1.142663e-02 -0.025856
2019-11-13 09:18:11,721 train 850 1.141982e-02 -0.008841
2019-11-13 09:18:14,164 training loss; R2: 1.141055e-02 -0.002959
2019-11-13 09:18:14,489 valid 000 5.263474e+00 -1894.169759
2019-11-13 09:18:16,198 valid 050 5.277776e+00 -1932.869741
2019-11-13 09:18:17,720 validation loss; R2: 5.279642e+00 -1509.889510
2019-11-13 09:18:17,738 epoch 10 lr 1.000000e-03
2019-11-13 09:18:18,138 train 000 1.002905e-02 0.386737
2019-11-13 09:18:25,997 train 050 1.128196e-02 0.313151
2019-11-13 09:18:33,971 train 100 1.129800e-02 0.292398
2019-11-13 09:18:41,826 train 150 1.128547e-02 0.297756
2019-11-13 09:18:49,645 train 200 1.123020e-02 0.303863
2019-11-13 09:18:57,438 train 250 1.123272e-02 0.292346
2019-11-13 09:19:05,367 train 300 1.119742e-02 0.294647
2019-11-13 09:19:13,383 train 350 1.123870e-02 0.295430
2019-11-13 09:19:21,350 train 400 1.124102e-02 0.298976
2019-11-13 09:19:29,100 train 450 1.126559e-02 0.298685
2019-11-13 09:19:36,768 train 500 1.126760e-02 0.299638
2019-11-13 09:19:44,436 train 550 1.126296e-02 0.288145
2019-11-13 09:19:52,226 train 600 1.126922e-02 0.283557
2019-11-13 09:19:59,936 train 650 1.125938e-02 0.285789
2019-11-13 09:20:07,595 train 700 1.125867e-02 0.286119
2019-11-13 09:20:15,278 train 750 1.126279e-02 0.288187
2019-11-13 09:20:22,931 train 800 1.125285e-02 0.285970
2019-11-13 09:20:30,602 train 850 1.125291e-02 0.287099
2019-11-13 09:20:32,887 training loss; R2: 1.123741e-02 0.288033
2019-11-13 09:20:33,204 valid 000 1.369445e-02 0.223652
2019-11-13 09:20:34,901 valid 050 1.369910e-02 -0.191983
2019-11-13 09:20:36,421 validation loss; R2: 1.361257e-02 -0.026629
2019-11-13 09:20:36,440 epoch 11 lr 1.000000e-03
2019-11-13 09:20:36,844 train 000 9.965892e-03 0.402758
2019-11-13 09:20:44,501 train 050 1.108259e-02 0.330643
2019-11-13 09:20:52,168 train 100 1.118429e-02 0.315594
2019-11-13 09:20:59,856 train 150 1.107080e-02 0.319606
2019-11-13 09:21:07,661 train 200 1.111448e-02 0.319154
2019-11-13 09:21:15,339 train 250 1.114570e-02 0.319889
2019-11-13 09:21:23,055 train 300 1.111353e-02 0.320855
2019-11-13 09:21:30,735 train 350 1.111793e-02 0.314762
2019-11-13 09:21:38,434 train 400 1.111716e-02 0.314880
2019-11-13 09:21:46,160 train 450 1.106222e-02 0.316627
2019-11-13 09:21:53,865 train 500 1.104144e-02 0.313492
2019-11-13 09:22:01,542 train 550 1.102894e-02 0.303096
2019-11-13 09:22:09,286 train 600 1.103348e-02 0.302498
2019-11-13 09:22:16,975 train 650 1.103963e-02 0.290365
2019-11-13 09:22:24,734 train 700 1.104139e-02 0.292380
2019-11-13 09:22:32,526 train 750 1.105499e-02 0.293398
2019-11-13 09:22:40,344 train 800 1.103282e-02 0.296236
2019-11-13 09:22:48,039 train 850 1.104521e-02 0.298308
2019-11-13 09:22:50,335 training loss; R2: 1.103986e-02 0.296937
2019-11-13 09:22:50,650 valid 000 6.519170e+00 -323.820153
2019-11-13 09:22:52,394 valid 050 6.401163e+00 -362.881034
2019-11-13 09:22:53,932 validation loss; R2: 6.405670e+00 -364.548539
2019-11-13 09:22:53,951 epoch 12 lr 1.000000e-03
2019-11-13 09:22:54,386 train 000 1.051487e-02 0.344485
2019-11-13 09:23:02,339 train 050 1.080395e-02 0.316105
2019-11-13 09:23:10,257 train 100 1.087722e-02 0.314923
2019-11-13 09:23:18,039 train 150 1.094950e-02 0.315894
2019-11-13 09:23:25,900 train 200 1.092155e-02 0.317995
2019-11-13 09:23:33,646 train 250 1.098300e-02 0.306567
2019-11-13 09:23:41,394 train 300 1.102170e-02 0.304874
2019-11-13 09:23:49,182 train 350 1.099630e-02 0.301212
2019-11-13 09:23:57,017 train 400 1.099430e-02 0.301780
2019-11-13 09:24:04,839 train 450 1.097390e-02 0.298521
2019-11-13 09:24:12,647 train 500 1.096445e-02 0.303414
2019-11-13 09:24:20,485 train 550 1.096552e-02 0.302671
2019-11-13 09:24:28,220 train 600 1.099197e-02 0.296572
2019-11-13 09:24:36,004 train 650 1.098774e-02 0.295509
2019-11-13 09:24:43,740 train 700 1.098247e-02 0.295186
2019-11-13 09:24:51,604 train 750 1.096192e-02 0.296958
2019-11-13 09:24:59,407 train 800 1.095866e-02 0.299620
2019-11-13 09:25:07,149 train 850 1.093396e-02 0.302423
2019-11-13 09:25:09,503 training loss; R2: 1.093576e-02 0.302982
2019-11-13 09:25:09,824 valid 000 5.529059e+01 -5687.682788
2019-11-13 09:25:11,530 valid 050 5.527140e+01 -5651.379920
2019-11-13 09:25:13,088 validation loss; R2: 5.526528e+01 -5862.158815
2019-11-13 09:25:13,108 epoch 13 lr 1.000000e-03
2019-11-13 09:25:13,526 train 000 9.958465e-03 0.451796
2019-11-13 09:25:21,357 train 050 1.106090e-02 0.202612
2019-11-13 09:25:29,071 train 100 1.087480e-02 0.254165
2019-11-13 09:25:36,837 train 150 1.099716e-02 0.253718
2019-11-13 09:25:44,565 train 200 1.093608e-02 0.267024
2019-11-13 09:25:52,364 train 250 1.095891e-02 0.273698
2019-11-13 09:26:00,114 train 300 1.095775e-02 0.276903
2019-11-13 09:26:07,886 train 350 1.098471e-02 0.285755
2019-11-13 09:26:15,612 train 400 1.099013e-02 0.282869
2019-11-13 09:26:23,340 train 450 1.098083e-02 0.284975
2019-11-13 09:26:31,053 train 500 1.099827e-02 0.289215
2019-11-13 09:26:38,759 train 550 1.100711e-02 0.286926
2019-11-13 09:26:46,467 train 600 1.101668e-02 0.290720
2019-11-13 09:26:54,306 train 650 1.103331e-02 0.292360
2019-11-13 09:27:02,038 train 700 1.102822e-02 0.291561
2019-11-13 09:27:09,808 train 750 1.100868e-02 0.289405
2019-11-13 09:27:17,632 train 800 1.099000e-02 0.292515
2019-11-13 09:27:25,614 train 850 1.096319e-02 0.295329
2019-11-13 09:27:27,995 training loss; R2: 1.095083e-02 0.295226
2019-11-13 09:27:28,295 valid 000 1.619851e+00 -233.346774
2019-11-13 09:27:29,976 valid 050 1.625350e+00 -265.102518
2019-11-13 09:27:31,505 validation loss; R2: 1.624893e+00 -263.103262
2019-11-13 09:27:31,524 epoch 14 lr 1.000000e-03
2019-11-13 09:27:31,991 train 000 1.164088e-02 0.320337
2019-11-13 09:27:39,965 train 050 1.073726e-02 0.311372
2019-11-13 09:27:47,820 train 100 1.063468e-02 0.309494
2019-11-13 09:27:55,569 train 150 1.067684e-02 0.316904
2019-11-13 09:28:03,404 train 200 1.077804e-02 0.309079
2019-11-13 09:28:11,410 train 250 1.077666e-02 0.315809
2019-11-13 09:28:19,447 train 300 1.079974e-02 0.308996
2019-11-13 09:28:27,177 train 350 1.079395e-02 0.304641
2019-11-13 09:28:34,937 train 400 1.074919e-02 0.305353
2019-11-13 09:28:42,682 train 450 1.073583e-02 0.306773
2019-11-13 09:28:50,540 train 500 1.072180e-02 0.307766
2019-11-13 09:28:58,634 train 550 1.073180e-02 0.304537
2019-11-13 09:29:06,704 train 600 1.073873e-02 0.303849
2019-11-13 09:29:14,445 train 650 1.073541e-02 0.304027
2019-11-13 09:29:22,112 train 700 1.073397e-02 0.304002
2019-11-13 09:29:29,787 train 750 1.071330e-02 0.305601
2019-11-13 09:29:37,453 train 800 1.070421e-02 0.307667
2019-11-13 09:29:45,097 train 850 1.070834e-02 0.306624
2019-11-13 09:29:47,383 training loss; R2: 1.071485e-02 0.306247
2019-11-13 09:29:47,710 valid 000 4.877897e-01 -70.786224
2019-11-13 09:29:49,416 valid 050 4.875488e-01 -108.931331
2019-11-13 09:29:50,963 validation loss; R2: 4.881547e-01 -114.235450
2019-11-13 09:29:50,983 epoch 15 lr 1.000000e-03
2019-11-13 09:29:51,426 train 000 1.047889e-02 0.437031
2019-11-13 09:29:59,295 train 050 1.046685e-02 0.315280
2019-11-13 09:30:07,017 train 100 1.047926e-02 0.319310
2019-11-13 09:30:14,732 train 150 1.053512e-02 0.296146
2019-11-13 09:30:22,485 train 200 1.051011e-02 0.305320
2019-11-13 09:30:30,233 train 250 1.056863e-02 0.307508
2019-11-13 09:30:38,121 train 300 1.059295e-02 0.302792
2019-11-13 09:30:45,924 train 350 1.059202e-02 0.286464
2019-11-13 09:30:53,655 train 400 1.062091e-02 0.292195
2019-11-13 09:31:01,403 train 450 1.060671e-02 0.291682
2019-11-13 09:31:09,112 train 500 1.062161e-02 0.294488
2019-11-13 09:31:16,873 train 550 1.063520e-02 0.292248
2019-11-13 09:31:24,590 train 600 1.066378e-02 0.293595
2019-11-13 09:31:32,311 train 650 1.064341e-02 0.298164
2019-11-13 09:31:39,978 train 700 1.060035e-02 0.296130
2019-11-13 09:31:47,859 train 750 1.061795e-02 0.288801
2019-11-13 09:31:55,762 train 800 1.063067e-02 0.292042
2019-11-13 09:32:03,442 train 850 1.063634e-02 0.295171
2019-11-13 09:32:05,734 training loss; R2: 1.062975e-02 0.285251
2019-11-13 09:32:06,057 valid 000 4.314674e-01 -46.545097
2019-11-13 09:32:07,880 valid 050 4.282412e-01 -30.072617
2019-11-13 09:32:09,455 validation loss; R2: 4.294252e-01 -33.743910
2019-11-13 09:32:09,479 epoch 16 lr 1.000000e-03
2019-11-13 09:32:09,916 train 000 1.108951e-02 0.404032
2019-11-13 09:32:17,693 train 050 1.061843e-02 0.195066
2019-11-13 09:32:25,546 train 100 1.063463e-02 0.236474
2019-11-13 09:32:33,365 train 150 1.064710e-02 0.263119
2019-11-13 09:32:41,141 train 200 1.060813e-02 0.275817
2019-11-13 09:32:48,893 train 250 1.057858e-02 0.278181
2019-11-13 09:32:56,605 train 300 1.060068e-02 0.281650
2019-11-13 09:33:04,357 train 350 1.063546e-02 0.284082
2019-11-13 09:33:12,073 train 400 1.057392e-02 0.286353
2019-11-13 09:33:19,894 train 450 1.058631e-02 0.290410
2019-11-13 09:33:27,651 train 500 1.056685e-02 0.295297
2019-11-13 09:33:35,404 train 550 1.053706e-02 0.298775
2019-11-13 09:33:43,157 train 600 1.054026e-02 0.303177
2019-11-13 09:33:50,868 train 650 1.056321e-02 0.306492
2019-11-13 09:33:58,616 train 700 1.054623e-02 0.308407
2019-11-13 09:34:06,408 train 750 1.053796e-02 0.310294
2019-11-13 09:34:14,176 train 800 1.053290e-02 0.311214
2019-11-13 09:34:21,946 train 850 1.052374e-02 0.311937
2019-11-13 09:34:24,255 training loss; R2: 1.051993e-02 0.312876
2019-11-13 09:34:24,553 valid 000 1.086725e+00 -53.358173
2019-11-13 09:34:26,315 valid 050 1.052287e+00 -69.795377
2019-11-13 09:34:27,873 validation loss; R2: 1.050776e+00 -70.649662
2019-11-13 09:34:27,893 epoch 17 lr 1.000000e-03
2019-11-13 09:34:28,295 train 000 1.229425e-02 0.388556
2019-11-13 09:34:36,051 train 050 1.045723e-02 0.213814
2019-11-13 09:34:43,825 train 100 1.040115e-02 0.264324
2019-11-13 09:34:51,580 train 150 1.044687e-02 0.279130
2019-11-13 09:34:59,333 train 200 1.038522e-02 0.296077
2019-11-13 09:35:07,054 train 250 1.042738e-02 0.303745
2019-11-13 09:35:14,772 train 300 1.044892e-02 0.309030
2019-11-13 09:35:22,498 train 350 1.042306e-02 0.314149
2019-11-13 09:35:30,207 train 400 1.040688e-02 0.318106
2019-11-13 09:35:38,000 train 450 1.042243e-02 0.310492
2019-11-13 09:35:45,730 train 500 1.046437e-02 0.311470
2019-11-13 09:35:53,507 train 550 1.048861e-02 0.311416
2019-11-13 09:36:01,226 train 600 1.048111e-02 0.305367
2019-11-13 09:36:09,047 train 650 1.047140e-02 0.306348
2019-11-13 09:36:17,015 train 700 1.045524e-02 0.306341
2019-11-13 09:36:24,780 train 750 1.044878e-02 0.307207
2019-11-13 09:36:32,611 train 800 1.045486e-02 0.307751
2019-11-13 09:36:40,617 train 850 1.045244e-02 0.304006
2019-11-13 09:36:43,010 training loss; R2: 1.045086e-02 0.304894
2019-11-13 09:36:43,319 valid 000 1.123258e+00 -69.091045
2019-11-13 09:36:44,998 valid 050 1.146792e+00 -71.654674
2019-11-13 09:36:46,517 validation loss; R2: 1.145313e+00 -74.846183
2019-11-13 09:36:46,539 epoch 18 lr 1.000000e-03
2019-11-13 09:36:46,945 train 000 1.076691e-02 0.432585
2019-11-13 09:36:54,664 train 050 1.021827e-02 0.355346
2019-11-13 09:37:02,390 train 100 1.028315e-02 0.328004
2019-11-13 09:37:10,157 train 150 1.038007e-02 0.316846
2019-11-13 09:37:17,861 train 200 1.042594e-02 0.328529
2019-11-13 09:37:25,597 train 250 1.041770e-02 0.320138
2019-11-13 09:37:33,360 train 300 1.039516e-02 0.324732
2019-11-13 09:37:41,056 train 350 1.035482e-02 0.328963
2019-11-13 09:37:48,735 train 400 1.031763e-02 0.329903
2019-11-13 09:37:56,446 train 450 1.029642e-02 0.334109
2019-11-13 09:38:04,118 train 500 1.030564e-02 0.329929
2019-11-13 09:38:11,793 train 550 1.030787e-02 0.327235
2019-11-13 09:38:19,479 train 600 1.031003e-02 0.320510
2019-11-13 09:38:27,149 train 650 1.033329e-02 0.322057
2019-11-13 09:38:34,818 train 700 1.033395e-02 0.321147
2019-11-13 09:38:42,496 train 750 1.034123e-02 0.319188
2019-11-13 09:38:50,164 train 800 1.034222e-02 0.318790
2019-11-13 09:38:57,833 train 850 1.033679e-02 0.321007
2019-11-13 09:39:00,122 training loss; R2: 1.034839e-02 0.321165
2019-11-13 09:39:00,431 valid 000 2.564057e+00 -130.891322
2019-11-13 09:39:02,132 valid 050 2.596856e+00 -121.063964
2019-11-13 09:39:03,698 validation loss; R2: 2.597772e+00 -117.306260
2019-11-13 09:39:03,719 epoch 19 lr 1.000000e-03
2019-11-13 09:39:04,150 train 000 1.110591e-02 0.421173
2019-11-13 09:39:11,864 train 050 1.048511e-02 0.295251
2019-11-13 09:39:19,593 train 100 1.040182e-02 0.318036
2019-11-13 09:39:27,340 train 150 1.036000e-02 0.321098
2019-11-13 09:39:35,063 train 200 1.035208e-02 0.327712
2019-11-13 09:39:42,775 train 250 1.034972e-02 0.339192
2019-11-13 09:39:50,489 train 300 1.035892e-02 0.340681
2019-11-13 09:39:58,202 train 350 1.034013e-02 0.341287
2019-11-13 09:40:05,980 train 400 1.033846e-02 0.333157
2019-11-13 09:40:13,743 train 450 1.031743e-02 0.336025
2019-11-13 09:40:21,503 train 500 1.030193e-02 0.333533
2019-11-13 09:40:29,241 train 550 1.029817e-02 0.335481
2019-11-13 09:40:37,069 train 600 1.031151e-02 0.322374
2019-11-13 09:40:44,825 train 650 1.030991e-02 0.323142
2019-11-13 09:40:52,675 train 700 1.031845e-02 0.323988
2019-11-13 09:41:00,315 train 750 1.032873e-02 0.326146
2019-11-13 09:41:08,215 train 800 1.033724e-02 0.324492
2019-11-13 09:41:16,214 train 850 1.033470e-02 0.325480
2019-11-13 09:41:18,599 training loss; R2: 1.033103e-02 0.325844
2019-11-13 09:41:18,912 valid 000 2.664569e+00 -432.960413
2019-11-13 09:41:20,574 valid 050 2.654988e+00 -300.246819
2019-11-13 09:41:22,114 validation loss; R2: 2.652516e+00 -299.577456
