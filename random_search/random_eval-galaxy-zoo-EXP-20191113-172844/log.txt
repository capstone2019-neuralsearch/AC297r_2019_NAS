2019-11-13 17:28:44,548 gpu device = 1
2019-11-13 17:28:44,548 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name='galaxy-zoo-cleaned', gpu=1, grad_clip=5, gz_dtree=False, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-galaxy-zoo-EXP-20191113-172844', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-11-13 17:28:56,178 param size = 0.316565MB
2019-11-13 17:28:56,182 epoch 0 lr 1.000000e-03
2019-11-13 17:28:58,498 train 000 3.192535e-01 -504.582767
2019-11-13 17:29:06,879 train 050 4.805082e-02 -14.917877
2019-11-13 17:29:15,087 train 100 3.791271e-02 -8.081775
2019-11-13 17:29:23,281 train 150 3.392781e-02 -5.592869
2019-11-13 17:29:31,464 train 200 3.168820e-02 -4.316716
2019-11-13 17:29:39,668 train 250 3.022909e-02 -3.514943
2019-11-13 17:29:47,596 train 300 2.923424e-02 -2.949988
2019-11-13 17:29:55,505 train 350 2.838148e-02 -2.547477
2019-11-13 17:30:03,403 train 400 2.767655e-02 -2.241926
2019-11-13 17:30:11,295 train 450 2.714019e-02 -2.003610
2019-11-13 17:30:19,232 train 500 2.663393e-02 -1.811399
2019-11-13 17:30:27,148 train 550 2.614866e-02 -1.655145
2019-11-13 17:30:35,210 train 600 2.576458e-02 -1.520879
2019-11-13 17:30:43,472 train 650 2.544112e-02 -1.406773
2019-11-13 17:30:51,617 train 700 2.512434e-02 -1.329821
2019-11-13 17:30:59,872 train 750 2.484083e-02 -1.243085
2019-11-13 17:31:08,101 train 800 2.459247e-02 -1.178353
2019-11-13 17:31:16,301 train 850 2.434828e-02 -1.109290
2019-11-13 17:31:19,614 training loss; R2: 2.427654e-02 -1.088842
2019-11-13 17:31:19,916 valid 000 2.577671e-02 -0.073627
2019-11-13 17:31:21,638 valid 050 2.493751e-02 -0.216994
2019-11-13 17:31:23,262 validation loss; R2: 2.493532e-02 -0.275899
2019-11-13 17:31:23,279 epoch 1 lr 1.000000e-03
2019-11-13 17:31:23,880 train 000 1.892860e-02 0.014767
2019-11-13 17:31:31,803 train 050 2.059885e-02 -0.205404
2019-11-13 17:31:39,803 train 100 2.047439e-02 -0.100607
2019-11-13 17:31:47,738 train 150 2.030892e-02 -0.052495
2019-11-13 17:31:55,629 train 200 1.994155e-02 -0.025659
2019-11-13 17:32:03,510 train 250 1.982480e-02 -0.014389
2019-11-13 17:32:11,401 train 300 1.974741e-02 -0.002708
2019-11-13 17:32:19,287 train 350 1.966398e-02 0.006079
2019-11-13 17:32:27,185 train 400 1.963390e-02 0.013306
2019-11-13 17:32:35,075 train 450 1.953886e-02 0.006425
2019-11-13 17:32:42,977 train 500 1.946180e-02 0.012715
2019-11-13 17:32:50,864 train 550 1.938697e-02 0.021624
2019-11-13 17:32:58,752 train 600 1.930380e-02 0.026396
2019-11-13 17:33:06,633 train 650 1.924904e-02 0.031660
2019-11-13 17:33:14,511 train 700 1.918734e-02 0.038122
2019-11-13 17:33:22,390 train 750 1.915919e-02 0.043996
2019-11-13 17:33:30,291 train 800 1.910472e-02 0.048840
2019-11-13 17:33:38,178 train 850 1.907092e-02 0.052081
2019-11-13 17:33:40,538 training loss; R2: 1.905084e-02 0.052961
2019-11-13 17:33:40,910 valid 000 2.015470e-02 0.101885
2019-11-13 17:33:42,619 valid 050 2.074762e-02 0.078269
2019-11-13 17:33:44,222 validation loss; R2: 2.077989e-02 0.068861
2019-11-13 17:33:44,240 epoch 2 lr 1.000000e-03
2019-11-13 17:33:44,698 train 000 1.830257e-02 0.198961
2019-11-13 17:33:52,591 train 050 1.780325e-02 0.110676
2019-11-13 17:34:00,487 train 100 1.781512e-02 0.111060
2019-11-13 17:34:08,372 train 150 1.779381e-02 0.112767
2019-11-13 17:34:16,308 train 200 1.773590e-02 0.118520
2019-11-13 17:34:24,192 train 250 1.764750e-02 0.123396
2019-11-13 17:34:32,087 train 300 1.764538e-02 0.124397
2019-11-13 17:34:39,967 train 350 1.754079e-02 0.127713
2019-11-13 17:34:47,873 train 400 1.755076e-02 0.130064
2019-11-13 17:34:55,774 train 450 1.742462e-02 -0.044702
2019-11-13 17:35:03,680 train 500 1.736797e-02 -0.025842
2019-11-13 17:35:11,558 train 550 1.730206e-02 -0.015991
2019-11-13 17:35:19,443 train 600 1.720873e-02 -0.000258
2019-11-13 17:35:27,327 train 650 1.714922e-02 0.011220
2019-11-13 17:35:35,231 train 700 1.711479e-02 0.022065
2019-11-13 17:35:43,104 train 750 1.704503e-02 0.021983
2019-11-13 17:35:51,003 train 800 1.698995e-02 0.030269
2019-11-13 17:35:58,897 train 850 1.693096e-02 0.037122
2019-11-13 17:36:01,257 training loss; R2: 1.690386e-02 0.039635
2019-11-13 17:36:01,591 valid 000 1.723973e-02 0.231837
2019-11-13 17:36:03,333 valid 050 1.589822e-02 -0.218434
2019-11-13 17:36:04,919 validation loss; R2: 1.591640e-02 -0.111462
2019-11-13 17:36:04,939 epoch 3 lr 1.000000e-03
2019-11-13 17:36:05,407 train 000 1.576708e-02 0.247423
2019-11-13 17:36:13,329 train 050 1.588206e-02 0.198082
2019-11-13 17:36:21,244 train 100 1.598846e-02 0.182613
2019-11-13 17:36:29,147 train 150 1.590042e-02 0.175457
2019-11-13 17:36:37,058 train 200 1.584205e-02 0.172748
2019-11-13 17:36:44,961 train 250 1.580039e-02 0.167446
2019-11-13 17:36:52,867 train 300 1.579648e-02 0.175853
2019-11-13 17:37:00,763 train 350 1.575856e-02 0.175571
2019-11-13 17:37:08,666 train 400 1.572151e-02 0.172116
2019-11-13 17:37:16,570 train 450 1.569832e-02 0.166726
2019-11-13 17:37:24,462 train 500 1.568086e-02 0.170003
2019-11-13 17:37:32,352 train 550 1.562961e-02 0.172828
2019-11-13 17:37:40,252 train 600 1.562472e-02 0.170036
2019-11-13 17:37:48,150 train 650 1.559640e-02 0.171116
2019-11-13 17:37:56,045 train 700 1.553747e-02 0.173790
2019-11-13 17:38:03,934 train 750 1.547687e-02 0.176719
2019-11-13 17:38:11,832 train 800 1.542850e-02 0.178711
2019-11-13 17:38:19,727 train 850 1.537296e-02 0.176071
2019-11-13 17:38:22,090 training loss; R2: 1.536955e-02 0.175033
2019-11-13 17:38:22,434 valid 000 1.374268e-02 0.103110
2019-11-13 17:38:24,184 valid 050 1.361063e-02 0.200262
2019-11-13 17:38:25,777 validation loss; R2: 1.369961e-02 0.178528
2019-11-13 17:38:25,795 epoch 4 lr 1.000000e-03
2019-11-13 17:38:26,239 train 000 1.620335e-02 0.290960
2019-11-13 17:38:34,246 train 050 1.494503e-02 0.201501
2019-11-13 17:38:42,157 train 100 1.478580e-02 0.203727
2019-11-13 17:38:50,044 train 150 1.475800e-02 0.209389
2019-11-13 17:38:57,965 train 200 1.479469e-02 0.187264
2019-11-13 17:39:05,840 train 250 1.475864e-02 0.191199
2019-11-13 17:39:13,734 train 300 1.462931e-02 0.196465
2019-11-13 17:39:21,633 train 350 1.461314e-02 0.098257
2019-11-13 17:39:29,524 train 400 1.453309e-02 0.116222
2019-11-13 17:39:37,400 train 450 1.455437e-02 0.121024
2019-11-13 17:39:45,291 train 500 1.450129e-02 0.130714
2019-11-13 17:39:53,174 train 550 1.446379e-02 0.139501
2019-11-13 17:40:01,053 train 600 1.444502e-02 0.147564
2019-11-13 17:40:08,931 train 650 1.442715e-02 0.155033
2019-11-13 17:40:16,812 train 700 1.443213e-02 0.158196
2019-11-13 17:40:24,682 train 750 1.440721e-02 0.163061
2019-11-13 17:40:32,565 train 800 1.436725e-02 0.168338
2019-11-13 17:40:40,447 train 850 1.433716e-02 0.157514
2019-11-13 17:40:42,824 training loss; R2: 1.433080e-02 0.158588
2019-11-13 17:40:43,192 valid 000 1.295936e-02 0.330287
2019-11-13 17:40:44,965 valid 050 1.312854e-02 -0.119000
2019-11-13 17:40:46,594 validation loss; R2: 1.327015e-02 0.006331
2019-11-13 17:40:46,612 epoch 5 lr 1.000000e-03
2019-11-13 17:40:47,025 train 000 1.205961e-02 0.163969
2019-11-13 17:40:54,929 train 050 1.404498e-02 0.253349
2019-11-13 17:41:02,801 train 100 1.393805e-02 0.247235
2019-11-13 17:41:10,673 train 150 1.392438e-02 0.191677
2019-11-13 17:41:18,553 train 200 1.387099e-02 0.201882
2019-11-13 17:41:26,424 train 250 1.379063e-02 0.210736
2019-11-13 17:41:34,300 train 300 1.377151e-02 0.215117
2019-11-13 17:41:42,187 train 350 1.373876e-02 0.214406
2019-11-13 17:41:50,069 train 400 1.374304e-02 0.215919
2019-11-13 17:41:57,940 train 450 1.371128e-02 0.220101
2019-11-13 17:42:05,815 train 500 1.373598e-02 0.222524
2019-11-13 17:42:13,684 train 550 1.367229e-02 0.225415
2019-11-13 17:42:21,562 train 600 1.367748e-02 0.226490
2019-11-13 17:42:29,433 train 650 1.369362e-02 0.224696
2019-11-13 17:42:37,312 train 700 1.370441e-02 0.225887
2019-11-13 17:42:45,185 train 750 1.367858e-02 0.225836
2019-11-13 17:42:53,057 train 800 1.366527e-02 0.227076
2019-11-13 17:43:00,919 train 850 1.362796e-02 0.228297
2019-11-13 17:43:03,275 training loss; R2: 1.362551e-02 0.228754
2019-11-13 17:43:03,629 valid 000 8.117190e+00 -4527.101876
2019-11-13 17:43:05,376 valid 050 8.103271e+00 -1368.131974
2019-11-13 17:43:06,964 validation loss; R2: 8.104998e+00 -1425.330532
2019-11-13 17:43:06,982 epoch 6 lr 1.000000e-03
2019-11-13 17:43:07,428 train 000 1.542051e-02 0.294415
2019-11-13 17:43:15,352 train 050 1.345183e-02 0.255298
2019-11-13 17:43:23,245 train 100 1.337077e-02 0.248891
2019-11-13 17:43:31,122 train 150 1.335580e-02 0.248617
2019-11-13 17:43:39,013 train 200 1.333974e-02 0.250643
2019-11-13 17:43:46,890 train 250 1.335610e-02 0.238170
2019-11-13 17:43:54,777 train 300 1.335161e-02 0.240715
2019-11-13 17:44:02,695 train 350 1.334407e-02 0.240052
2019-11-13 17:44:10,599 train 400 1.333452e-02 0.242722
2019-11-13 17:44:18,497 train 450 1.330958e-02 0.242833
2019-11-13 17:44:26,400 train 500 1.327108e-02 0.243149
2019-11-13 17:44:34,299 train 550 1.328782e-02 0.244976
2019-11-13 17:44:42,212 train 600 1.324794e-02 0.233696
2019-11-13 17:44:50,085 train 650 1.322854e-02 0.194963
2019-11-13 17:44:57,969 train 700 1.320867e-02 0.197842
2019-11-13 17:45:05,855 train 750 1.319204e-02 0.202702
2019-11-13 17:45:13,741 train 800 1.318550e-02 0.204072
2019-11-13 17:45:21,622 train 850 1.317870e-02 0.200744
2019-11-13 17:45:23,980 training loss; R2: 1.317296e-02 0.202070
2019-11-13 17:45:24,315 valid 000 8.725987e-01 -60.958017
2019-11-13 17:45:26,080 valid 050 8.720260e-01 -165.857788
2019-11-13 17:45:27,653 validation loss; R2: 8.705360e-01 -171.772786
2019-11-13 17:45:27,673 epoch 7 lr 1.000000e-03
2019-11-13 17:45:28,148 train 000 1.225109e-02 0.335146
2019-11-13 17:45:36,064 train 050 1.307791e-02 0.269803
2019-11-13 17:45:43,948 train 100 1.307584e-02 0.255404
2019-11-13 17:45:51,824 train 150 1.303306e-02 0.260337
2019-11-13 17:45:59,706 train 200 1.308069e-02 0.256864
2019-11-13 17:46:07,629 train 250 1.308716e-02 0.250141
2019-11-13 17:46:15,511 train 300 1.304463e-02 0.253082
2019-11-13 17:46:23,390 train 350 1.297691e-02 0.253865
2019-11-13 17:46:31,269 train 400 1.300789e-02 0.255261
2019-11-13 17:46:39,162 train 450 1.294346e-02 0.258121
2019-11-13 17:46:47,086 train 500 1.292000e-02 0.254398
2019-11-13 17:46:54,974 train 550 1.290380e-02 0.252567
2019-11-13 17:47:02,847 train 600 1.288756e-02 0.253953
2019-11-13 17:47:10,725 train 650 1.285769e-02 0.253465
2019-11-13 17:47:18,602 train 700 1.283758e-02 0.253360
2019-11-13 17:47:26,477 train 750 1.281344e-02 0.253554
2019-11-13 17:47:34,355 train 800 1.278844e-02 -1.313565
2019-11-13 17:47:42,228 train 850 1.279740e-02 -1.220075
2019-11-13 17:47:44,586 training loss; R2: 1.279897e-02 -1.193640
2019-11-13 17:47:44,928 valid 000 5.994200e+00 -496.552305
2019-11-13 17:47:46,668 valid 050 6.023054e+00 -554.768909
2019-11-13 17:47:48,267 validation loss; R2: 6.020486e+00 -613.639092
2019-11-13 17:47:48,293 epoch 8 lr 1.000000e-03
2019-11-13 17:47:48,763 train 000 1.251580e-02 0.274221
2019-11-13 17:47:56,704 train 050 1.284389e-02 0.257150
2019-11-13 17:48:04,677 train 100 1.261290e-02 0.255333
2019-11-13 17:48:12,583 train 150 1.264923e-02 0.260200
2019-11-13 17:48:20,486 train 200 1.258653e-02 0.261699
2019-11-13 17:48:28,413 train 250 1.254456e-02 0.263878
2019-11-13 17:48:36,308 train 300 1.254238e-02 0.269955
2019-11-13 17:48:44,191 train 350 1.254940e-02 0.273609
2019-11-13 17:48:52,081 train 400 1.253948e-02 0.270408
2019-11-13 17:48:59,965 train 450 1.253514e-02 0.272040
2019-11-13 17:49:07,866 train 500 1.252186e-02 0.271533
2019-11-13 17:49:15,770 train 550 1.251099e-02 0.272853
2019-11-13 17:49:23,684 train 600 1.250958e-02 0.273534
2019-11-13 17:49:31,566 train 650 1.251703e-02 0.275296
2019-11-13 17:49:39,457 train 700 1.251214e-02 0.277620
2019-11-13 17:49:47,350 train 750 1.250872e-02 0.276736
2019-11-13 17:49:55,258 train 800 1.249386e-02 0.266409
2019-11-13 17:50:03,168 train 850 1.250062e-02 0.267510
2019-11-13 17:50:05,533 training loss; R2: 1.249845e-02 0.268145
2019-11-13 17:50:05,881 valid 000 4.432901e+01 -4043.079473
2019-11-13 17:50:07,636 valid 050 4.431919e+01 -4018.118969
2019-11-13 17:50:09,221 validation loss; R2: 4.432379e+01 -4318.489536
2019-11-13 17:50:09,238 epoch 9 lr 1.000000e-03
2019-11-13 17:50:09,681 train 000 1.261338e-02 0.404984
2019-11-13 17:50:17,601 train 050 1.226367e-02 0.289411
2019-11-13 17:50:25,559 train 100 1.225179e-02 0.295007
2019-11-13 17:50:33,468 train 150 1.233038e-02 0.192916
2019-11-13 17:50:41,391 train 200 1.239834e-02 0.217459
2019-11-13 17:50:49,291 train 250 1.242985e-02 0.222517
2019-11-13 17:50:57,205 train 300 1.238479e-02 0.234073
2019-11-13 17:51:05,115 train 350 1.238471e-02 0.238936
2019-11-13 17:51:13,037 train 400 1.238226e-02 0.244227
2019-11-13 17:51:20,966 train 450 1.231409e-02 0.251478
2019-11-13 17:51:28,927 train 500 1.229543e-02 0.256948
2019-11-13 17:51:36,910 train 550 1.227571e-02 0.263112
2019-11-13 17:51:44,873 train 600 1.226079e-02 0.265179
2019-11-13 17:51:52,836 train 650 1.226358e-02 0.261541
2019-11-13 17:52:00,799 train 700 1.223259e-02 0.261173
2019-11-13 17:52:08,755 train 750 1.220764e-02 0.264866
2019-11-13 17:52:16,729 train 800 1.217252e-02 0.268536
2019-11-13 17:52:24,688 train 850 1.214482e-02 0.269073
2019-11-13 17:52:27,077 training loss; R2: 1.215006e-02 0.270125
2019-11-13 17:52:27,424 valid 000 1.508384e+01 -1066.242611
2019-11-13 17:52:29,192 valid 050 1.510226e+01 -1349.292742
2019-11-13 17:52:30,807 validation loss; R2: 1.510876e+01 -1967.421109
2019-11-13 17:52:30,825 epoch 10 lr 1.000000e-03
2019-11-13 17:52:31,270 train 000 1.152314e-02 0.394572
2019-11-13 17:52:39,233 train 050 1.202171e-02 0.263158
2019-11-13 17:52:47,139 train 100 1.214798e-02 0.279128
2019-11-13 17:52:55,040 train 150 1.224672e-02 0.273282
2019-11-13 17:53:02,978 train 200 1.228019e-02 0.275067
2019-11-13 17:53:10,910 train 250 1.225392e-02 0.271461
2019-11-13 17:53:18,830 train 300 1.217670e-02 0.275544
2019-11-13 17:53:26,791 train 350 1.216277e-02 0.272545
2019-11-13 17:53:34,756 train 400 1.210626e-02 0.279653
2019-11-13 17:53:42,711 train 450 1.207913e-02 0.281078
2019-11-13 17:53:50,680 train 500 1.203536e-02 0.282486
2019-11-13 17:53:58,636 train 550 1.202236e-02 0.282654
2019-11-13 17:54:06,595 train 600 1.201900e-02 0.283423
2019-11-13 17:54:14,473 train 650 1.203986e-02 0.284864
2019-11-13 17:54:22,377 train 700 1.204860e-02 0.284327
2019-11-13 17:54:30,276 train 750 1.207209e-02 0.283543
2019-11-13 17:54:38,233 train 800 1.208977e-02 0.281209
2019-11-13 17:54:46,136 train 850 1.208744e-02 0.279747
2019-11-13 17:54:48,510 training loss; R2: 1.208819e-02 0.279868
2019-11-13 17:54:48,846 valid 000 1.568388e+02 -18197.750352
2019-11-13 17:54:50,643 valid 050 1.570688e+02 -10363.810041
2019-11-13 17:54:52,233 validation loss; R2: 1.570342e+02 -10256.073678
2019-11-13 17:54:52,260 epoch 11 lr 1.000000e-03
2019-11-13 17:54:52,712 train 000 1.170088e-02 0.350084
2019-11-13 17:55:00,720 train 050 1.165631e-02 0.320677
2019-11-13 17:55:08,619 train 100 1.177843e-02 0.309269
2019-11-13 17:55:16,506 train 150 1.192158e-02 0.291496
2019-11-13 17:55:24,485 train 200 1.200341e-02 0.279512
2019-11-13 17:55:32,462 train 250 1.200460e-02 0.285027
2019-11-13 17:55:40,425 train 300 1.194820e-02 0.294136
2019-11-13 17:55:48,384 train 350 1.186272e-02 0.301327
2019-11-13 17:55:56,362 train 400 1.182318e-02 0.281826
2019-11-13 17:56:04,312 train 450 1.182238e-02 0.277334
2019-11-13 17:56:12,265 train 500 1.180722e-02 0.280350
2019-11-13 17:56:20,183 train 550 1.182159e-02 0.280108
2019-11-13 17:56:28,118 train 600 1.185683e-02 0.281589
2019-11-13 17:56:36,069 train 650 1.182992e-02 0.283013
2019-11-13 17:56:43,984 train 700 1.182841e-02 0.284623
2019-11-13 17:56:51,901 train 750 1.184905e-02 0.282058
2019-11-13 17:56:59,860 train 800 1.188060e-02 0.283688
2019-11-13 17:57:07,819 train 850 1.186784e-02 0.281499
2019-11-13 17:57:10,203 training loss; R2: 1.186464e-02 0.282621
2019-11-13 17:57:10,576 valid 000 1.079891e+01 -1746.867785
2019-11-13 17:57:12,329 valid 050 1.081132e+01 -2608.360965
2019-11-13 17:57:13,901 validation loss; R2: 1.081181e+01 -2980.723436
2019-11-13 17:57:13,919 epoch 12 lr 1.000000e-03
2019-11-13 17:57:14,369 train 000 1.258467e-02 0.303150
2019-11-13 17:57:22,382 train 050 1.120713e-02 0.344626
2019-11-13 17:57:30,330 train 100 1.146542e-02 0.314658
2019-11-13 17:57:38,279 train 150 1.159860e-02 0.292912
2019-11-13 17:57:46,242 train 200 1.156960e-02 0.297691
2019-11-13 17:57:54,203 train 250 1.156866e-02 0.299694
2019-11-13 17:58:02,121 train 300 1.157735e-02 0.303203
2019-11-13 17:58:10,002 train 350 1.168154e-02 0.284439
2019-11-13 17:58:17,884 train 400 1.176909e-02 0.283699
2019-11-13 17:58:25,776 train 450 1.182104e-02 0.283662
2019-11-13 17:58:33,813 train 500 1.187226e-02 0.282351
2019-11-13 17:58:41,768 train 550 1.188348e-02 0.276402
2019-11-13 17:58:49,735 train 600 1.186404e-02 0.282484
2019-11-13 17:58:57,671 train 650 1.181591e-02 0.285793
2019-11-13 17:59:05,565 train 700 1.184379e-02 0.284708
2019-11-13 17:59:13,485 train 750 1.187443e-02 0.281551
2019-11-13 17:59:21,409 train 800 1.188086e-02 0.282919
2019-11-13 17:59:29,311 train 850 1.188195e-02 0.284780
2019-11-13 17:59:31,674 training loss; R2: 1.189146e-02 0.285292
2019-11-13 17:59:32,036 valid 000 4.662891e+01 -3931.861088
2019-11-13 17:59:33,796 valid 050 4.670743e+01 -3546.807190
2019-11-13 17:59:35,368 validation loss; R2: 4.670061e+01 -4320.250910
2019-11-13 17:59:35,387 epoch 13 lr 1.000000e-03
2019-11-13 17:59:35,860 train 000 1.173327e-02 0.160266
2019-11-13 17:59:43,798 train 050 1.221242e-02 0.277452
2019-11-13 17:59:51,732 train 100 1.213900e-02 0.278092
2019-11-13 17:59:59,711 train 150 1.216285e-02 0.263925
2019-11-13 18:00:07,675 train 200 1.205473e-02 0.267461
2019-11-13 18:00:15,622 train 250 1.203678e-02 0.275371
2019-11-13 18:00:23,538 train 300 1.208226e-02 0.281206
2019-11-13 18:00:31,492 train 350 1.207068e-02 0.285885
2019-11-13 18:00:39,412 train 400 1.206234e-02 0.257130
2019-11-13 18:00:47,327 train 450 1.206214e-02 -0.074991
2019-11-13 18:00:55,288 train 500 1.207305e-02 -0.039683
2019-11-13 18:01:03,178 train 550 1.207191e-02 -0.006621
2019-11-13 18:01:11,097 train 600 1.205056e-02 0.009882
2019-11-13 18:01:19,056 train 650 1.200744e-02 0.034919
2019-11-13 18:01:27,014 train 700 1.198672e-02 0.051143
2019-11-13 18:01:34,985 train 750 1.192927e-02 0.067550
2019-11-13 18:01:42,957 train 800 1.189392e-02 0.083455
2019-11-13 18:01:50,904 train 850 1.185453e-02 0.098504
2019-11-13 18:01:53,289 training loss; R2: 1.184945e-02 0.102155
2019-11-13 18:01:53,641 valid 000 1.342768e+01 -2035.512266
2019-11-13 18:01:55,400 valid 050 1.344832e+01 -7973.916158
2019-11-13 18:01:56,986 validation loss; R2: 1.345004e+01 -5565.797011
2019-11-13 18:01:57,010 epoch 14 lr 1.000000e-03
2019-11-13 18:01:57,475 train 000 1.053456e-02 -0.073329
2019-11-13 18:02:05,475 train 050 1.131190e-02 0.294685
2019-11-13 18:02:13,411 train 100 1.126113e-02 0.309029
2019-11-13 18:02:21,333 train 150 1.147839e-02 0.304662
2019-11-13 18:02:29,241 train 200 1.159985e-02 0.295153
2019-11-13 18:02:37,137 train 250 1.165241e-02 0.284562
2019-11-13 18:02:45,051 train 300 1.170460e-02 0.289184
2019-11-13 18:02:52,944 train 350 1.175417e-02 0.290601
2019-11-13 18:03:00,856 train 400 1.176692e-02 0.284016
2019-11-13 18:03:08,776 train 450 1.176964e-02 0.217669
2019-11-13 18:03:16,693 train 500 1.178262e-02 0.219667
2019-11-13 18:03:24,623 train 550 1.175610e-02 0.229002
2019-11-13 18:03:32,582 train 600 1.175233e-02 0.232234
2019-11-13 18:03:40,599 train 650 1.171128e-02 0.241114
2019-11-13 18:03:48,571 train 700 1.169416e-02 0.248644
2019-11-13 18:03:56,528 train 750 1.164810e-02 0.253431
2019-11-13 18:04:04,499 train 800 1.161509e-02 0.258795
2019-11-13 18:04:12,449 train 850 1.158109e-02 0.264866
2019-11-13 18:04:14,816 training loss; R2: 1.157778e-02 0.265504
2019-11-13 18:04:15,190 valid 000 1.007230e+01 -822.617108
2019-11-13 18:04:16,945 valid 050 1.007074e+01 -865.759394
2019-11-13 18:04:18,550 validation loss; R2: 1.007089e+01 -974.079966
2019-11-13 18:04:18,568 epoch 15 lr 1.000000e-03
2019-11-13 18:04:19,016 train 000 1.088359e-02 0.361537
2019-11-13 18:04:26,995 train 050 1.168691e-02 -0.548810
2019-11-13 18:04:34,941 train 100 1.162142e-02 -0.123687
2019-11-13 18:04:42,862 train 150 1.152623e-02 0.025666
2019-11-13 18:04:50,755 train 200 1.170115e-02 0.074922
2019-11-13 18:04:58,669 train 250 1.169717e-02 0.117750
2019-11-13 18:05:06,544 train 300 1.177697e-02 0.143747
2019-11-13 18:05:14,411 train 350 1.183244e-02 0.156683
2019-11-13 18:05:22,313 train 400 1.190428e-02 0.171021
2019-11-13 18:05:30,240 train 450 1.184731e-02 0.184927
2019-11-13 18:05:38,179 train 500 1.183755e-02 0.197281
2019-11-13 18:05:46,098 train 550 1.184473e-02 0.197915
2019-11-13 18:05:54,029 train 600 1.181611e-02 0.208314
2019-11-13 18:06:01,910 train 650 1.184440e-02 0.195621
2019-11-13 18:06:09,815 train 700 1.185969e-02 0.201687
2019-11-13 18:06:17,734 train 750 1.185652e-02 0.208563
2019-11-13 18:06:25,627 train 800 1.186516e-02 0.211270
2019-11-13 18:06:33,518 train 850 1.186062e-02 0.214966
2019-11-13 18:06:35,893 training loss; R2: 1.185813e-02 0.216744
2019-11-13 18:06:36,250 valid 000 2.657859e+01 -5266.917343
2019-11-13 18:06:38,009 valid 050 2.660892e+01 -12246.565557
2019-11-13 18:06:39,592 validation loss; R2: 2.661146e+01 -8287.888155
2019-11-13 18:06:39,611 epoch 16 lr 1.000000e-03
2019-11-13 18:06:40,045 train 000 9.606370e-03 0.366062
2019-11-13 18:06:48,033 train 050 1.152669e-02 0.312357
2019-11-13 18:06:55,973 train 100 1.146372e-02 0.318563
2019-11-13 18:07:03,929 train 150 1.135747e-02 0.311057
2019-11-13 18:07:11,881 train 200 1.131684e-02 0.310959
2019-11-13 18:07:19,756 train 250 1.135003e-02 0.310739
2019-11-13 18:07:27,642 train 300 1.146060e-02 0.312692
2019-11-13 18:07:35,520 train 350 1.152862e-02 0.306833
2019-11-13 18:07:43,404 train 400 1.162925e-02 0.303991
2019-11-13 18:07:51,357 train 450 1.166069e-02 0.303637
2019-11-13 18:07:59,278 train 500 1.168435e-02 0.304626
2019-11-13 18:08:07,178 train 550 1.170187e-02 0.303091
2019-11-13 18:08:15,098 train 600 1.169885e-02 0.304819
2019-11-13 18:08:22,959 train 650 1.171716e-02 0.303881
2019-11-13 18:08:30,824 train 700 1.172777e-02 0.299895
2019-11-13 18:08:38,709 train 750 1.170827e-02 0.298721
2019-11-13 18:08:46,566 train 800 1.170568e-02 0.298910
2019-11-13 18:08:54,430 train 850 1.173864e-02 0.297980
2019-11-13 18:08:56,781 training loss; R2: 1.173737e-02 0.296730
2019-11-13 18:08:57,112 valid 000 6.695921e-01 -441.160330
2019-11-13 18:08:58,850 valid 050 6.613326e-01 -60.001776
2019-11-13 18:09:00,452 validation loss; R2: 6.620960e-01 -62.262992
2019-11-13 18:09:00,471 epoch 17 lr 1.000000e-03
2019-11-13 18:09:00,917 train 000 1.244875e-02 0.354902
2019-11-13 18:09:08,819 train 050 1.202238e-02 0.276354
2019-11-13 18:09:16,802 train 100 1.189253e-02 0.282687
2019-11-13 18:09:24,739 train 150 1.194389e-02 0.272900
2019-11-13 18:09:32,623 train 200 1.202368e-02 0.277920
2019-11-13 18:09:40,541 train 250 1.193704e-02 0.274868
2019-11-13 18:09:48,408 train 300 1.189858e-02 0.232439
2019-11-13 18:09:56,270 train 350 1.182349e-02 0.235007
2019-11-13 18:10:04,132 train 400 1.178313e-02 0.242150
2019-11-13 18:10:12,000 train 450 1.178205e-02 0.245441
2019-11-13 18:10:19,869 train 500 1.176443e-02 0.252099
2019-11-13 18:10:27,749 train 550 1.178978e-02 0.258121
2019-11-13 18:10:35,630 train 600 1.177587e-02 0.254752
2019-11-13 18:10:43,490 train 650 1.176217e-02 0.259440
2019-11-13 18:10:51,349 train 700 1.173933e-02 0.259743
2019-11-13 18:10:59,201 train 750 1.173285e-02 0.265276
2019-11-13 18:11:07,065 train 800 1.171402e-02 0.266863
2019-11-13 18:11:14,913 train 850 1.172947e-02 0.267344
2019-11-13 18:11:17,269 training loss; R2: 1.172635e-02 0.267810
2019-11-13 18:11:17,622 valid 000 6.226422e-02 -12.858832
2019-11-13 18:11:19,399 valid 050 6.096457e-02 -3.380909
2019-11-13 18:11:20,964 validation loss; R2: 6.041078e-02 -4.936791
2019-11-13 18:11:20,982 epoch 18 lr 1.000000e-03
2019-11-13 18:11:21,448 train 000 1.031380e-02 0.394715
2019-11-13 18:11:29,350 train 050 1.147270e-02 0.305532
2019-11-13 18:11:37,233 train 100 1.149400e-02 0.313233
2019-11-13 18:11:45,115 train 150 1.154475e-02 0.305016
2019-11-13 18:11:53,011 train 200 1.152258e-02 0.308182
2019-11-13 18:12:00,889 train 250 1.153730e-02 0.303891
2019-11-13 18:12:08,770 train 300 1.156529e-02 0.260873
2019-11-13 18:12:16,651 train 350 1.154824e-02 0.225832
2019-11-13 18:12:24,536 train 400 1.154690e-02 0.237136
2019-11-13 18:12:32,406 train 450 1.150090e-02 0.246456
2019-11-13 18:12:40,280 train 500 1.151264e-02 0.251888
2019-11-13 18:12:48,138 train 550 1.149412e-02 0.256187
2019-11-13 18:12:56,008 train 600 1.147305e-02 0.254232
2019-11-13 18:13:03,870 train 650 1.145214e-02 0.255982
2019-11-13 18:13:11,762 train 700 1.148647e-02 0.260062
2019-11-13 18:13:19,630 train 750 1.149073e-02 0.262910
2019-11-13 18:13:27,527 train 800 1.150759e-02 0.266237
2019-11-13 18:13:35,402 train 850 1.151243e-02 0.266020
2019-11-13 18:13:37,760 training loss; R2: 1.151998e-02 0.266778
2019-11-13 18:13:38,122 valid 000 3.719971e+00 -190.476639
2019-11-13 18:13:39,890 valid 050 3.695487e+00 -231.049100
2019-11-13 18:13:41,481 validation loss; R2: 3.694710e+00 -225.960121
2019-11-13 18:13:41,501 epoch 19 lr 1.000000e-03
2019-11-13 18:13:41,933 train 000 9.433231e-03 0.421563
2019-11-13 18:13:49,834 train 050 1.169133e-02 0.294263
2019-11-13 18:13:57,695 train 100 1.152879e-02 0.305458
2019-11-13 18:14:05,555 train 150 1.142157e-02 0.304941
2019-11-13 18:14:13,413 train 200 1.144103e-02 0.302952
2019-11-13 18:14:21,260 train 250 1.143728e-02 0.308360
2019-11-13 18:14:29,124 train 300 1.139391e-02 0.311627
2019-11-13 18:14:36,975 train 350 1.136366e-02 0.313292
2019-11-13 18:14:44,838 train 400 1.135593e-02 0.294814
2019-11-13 18:14:52,691 train 450 1.135935e-02 0.296763
2019-11-13 18:15:00,554 train 500 1.135606e-02 0.296869
2019-11-13 18:15:08,403 train 550 1.135408e-02 0.299450
2019-11-13 18:15:16,262 train 600 1.134713e-02 0.299987
2019-11-13 18:15:24,112 train 650 1.136342e-02 0.294816
2019-11-13 18:15:31,966 train 700 1.137139e-02 0.285154
2019-11-13 18:15:39,834 train 750 1.138003e-02 0.286461
2019-11-13 18:15:47,689 train 800 1.136953e-02 0.287897
2019-11-13 18:15:55,531 train 850 1.135295e-02 0.290207
2019-11-13 18:15:57,882 training loss; R2: 1.135797e-02 0.290610
2019-11-13 18:15:58,238 valid 000 1.545922e-01 -11.141989
2019-11-13 18:15:59,999 valid 050 1.531134e-01 -14.480631
2019-11-13 18:16:01,614 validation loss; R2: 1.530392e-01 -16.189336
