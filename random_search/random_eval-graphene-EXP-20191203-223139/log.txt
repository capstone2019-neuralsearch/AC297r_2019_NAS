2019-12-03 22:31:39,746 gpu device = 1
2019-12-03 22:31:39,746 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=96, cutout=False, cutout_length=16, data='../data', dataset='graphene', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name=None, gpu=1, grad_clip=5, gz_dtree=False, init_channels=36, layers=8, learning_rate=0.002, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-graphene-EXP-20191203-223139', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-12-03 22:31:42,950 param size = 1.097677MB
2019-12-03 22:31:42,953 epoch 0 lr 2.000000e-03
2019-12-03 22:31:45,608 train 000 2.132684e+00 -79.327280
2019-12-03 22:31:57,587 train 050 1.984642e-01 -8.191590
2019-12-03 22:32:09,488 train 100 1.159771e-01 -4.145694
2019-12-03 22:32:21,388 train 150 8.767225e-02 -2.725931
2019-12-03 22:32:33,288 train 200 7.180914e-02 -1.987105
2019-12-03 22:32:39,722 training loss; R2: 6.647537e-02 -1.760739
2019-12-03 22:32:39,870 valid 000 6.738523e-03 0.583160
2019-12-03 22:32:41,425 validation loss; R2: 1.459099e-02 0.548530
2019-12-03 22:32:41,447 epoch 1 lr 2.000000e-03
2019-12-03 22:32:41,836 train 000 1.636607e-02 0.209054
2019-12-03 22:32:53,733 train 050 2.176314e-02 0.323076
2019-12-03 22:33:05,638 train 100 2.008070e-02 0.349174
2019-12-03 22:33:17,545 train 150 1.909464e-02 0.378146
2019-12-03 22:33:29,454 train 200 1.890206e-02 0.391685
2019-12-03 22:33:34,811 training loss; R2: 1.867317e-02 0.399022
2019-12-03 22:33:34,949 valid 000 4.698822e-03 0.636725
2019-12-03 22:33:36,192 validation loss; R2: 8.510346e-03 0.723273
2019-12-03 22:33:36,216 epoch 2 lr 2.000000e-03
2019-12-03 22:33:36,542 train 000 1.536385e-02 0.593900
2019-12-03 22:33:48,463 train 050 1.530542e-02 0.483488
2019-12-03 22:34:00,380 train 100 1.395495e-02 0.529009
2019-12-03 22:34:12,298 train 150 1.355710e-02 0.538354
2019-12-03 22:34:24,216 train 200 1.365068e-02 0.547028
2019-12-03 22:34:29,577 training loss; R2: 1.364149e-02 0.536939
2019-12-03 22:34:29,722 valid 000 8.487280e-03 0.806736
2019-12-03 22:34:30,964 validation loss; R2: 7.965120e-03 0.723728
2019-12-03 22:34:30,985 epoch 3 lr 2.000000e-03
2019-12-03 22:34:31,301 train 000 1.775892e-02 0.662237
2019-12-03 22:34:42,912 train 050 1.093246e-02 0.626824
2019-12-03 22:34:54,522 train 100 1.062315e-02 0.637389
2019-12-03 22:35:06,145 train 150 1.091540e-02 0.631132
2019-12-03 22:35:17,771 train 200 1.089347e-02 0.632225
2019-12-03 22:35:22,997 training loss; R2: 1.096141e-02 0.635000
2019-12-03 22:35:23,129 valid 000 3.571904e-03 0.883303
2019-12-03 22:35:24,373 validation loss; R2: 6.040637e-03 0.809341
2019-12-03 22:35:24,396 epoch 4 lr 2.000000e-03
2019-12-03 22:35:24,721 train 000 7.005452e-03 0.798786
2019-12-03 22:35:36,346 train 050 1.055619e-02 0.650367
2019-12-03 22:35:47,971 train 100 1.032332e-02 0.664219
2019-12-03 22:35:59,591 train 150 1.022401e-02 0.678363
2019-12-03 22:36:11,209 train 200 9.878881e-03 0.672734
2019-12-03 22:36:16,442 training loss; R2: 9.803918e-03 0.673434
2019-12-03 22:36:16,589 valid 000 3.321936e-03 0.870213
2019-12-03 22:36:17,831 validation loss; R2: 5.274300e-03 0.832146
2019-12-03 22:36:17,853 epoch 5 lr 2.000000e-03
2019-12-03 22:36:18,171 train 000 8.170950e-03 0.749059
2019-12-03 22:36:29,784 train 050 1.100242e-02 0.652044
2019-12-03 22:36:41,403 train 100 9.563718e-03 0.687937
2019-12-03 22:36:53,023 train 150 9.430419e-03 0.686015
2019-12-03 22:37:04,638 train 200 8.967350e-03 0.697538
2019-12-03 22:37:09,864 training loss; R2: 9.105411e-03 0.698964
2019-12-03 22:37:10,002 valid 000 3.250395e-03 0.840430
2019-12-03 22:37:11,243 validation loss; R2: 5.783465e-03 0.810039
2019-12-03 22:37:11,272 epoch 6 lr 2.000000e-03
2019-12-03 22:37:11,597 train 000 7.170246e-03 0.752246
2019-12-03 22:37:23,220 train 050 9.292588e-03 0.701235
2019-12-03 22:37:34,838 train 100 8.617057e-03 0.716332
2019-12-03 22:37:46,459 train 150 8.532284e-03 0.718124
2019-12-03 22:37:58,084 train 200 8.136825e-03 0.724491
2019-12-03 22:38:03,314 training loss; R2: 8.199028e-03 0.725245
2019-12-03 22:38:03,458 valid 000 4.138414e-03 0.735747
2019-12-03 22:38:04,700 validation loss; R2: 5.467688e-03 0.809277
2019-12-03 22:38:04,722 epoch 7 lr 2.000000e-03
2019-12-03 22:38:05,044 train 000 9.964746e-03 0.715528
2019-12-03 22:38:16,663 train 050 8.034538e-03 0.712600
2019-12-03 22:38:28,281 train 100 7.775336e-03 0.717071
2019-12-03 22:38:39,901 train 150 7.877716e-03 0.720354
2019-12-03 22:38:51,519 train 200 8.202986e-03 0.726637
2019-12-03 22:38:56,748 training loss; R2: 8.193437e-03 0.724768
2019-12-03 22:38:56,879 valid 000 1.595102e-02 0.682658
2019-12-03 22:38:58,121 validation loss; R2: 8.059226e-03 0.744776
2019-12-03 22:38:58,143 epoch 8 lr 2.000000e-03
2019-12-03 22:38:58,460 train 000 4.988133e-03 0.794613
2019-12-03 22:39:10,081 train 050 6.553260e-03 0.773635
2019-12-03 22:39:21,699 train 100 6.998541e-03 0.757458
2019-12-03 22:39:33,315 train 150 6.943488e-03 0.757761
2019-12-03 22:39:45,081 train 200 6.834310e-03 0.763719
2019-12-03 22:39:50,439 training loss; R2: 7.061280e-03 0.760173
2019-12-03 22:39:50,570 valid 000 6.538434e-03 0.797725
2019-12-03 22:39:51,812 validation loss; R2: 6.356756e-03 0.784533
2019-12-03 22:39:51,837 epoch 9 lr 2.000000e-03
2019-12-03 22:39:52,162 train 000 6.906596e-03 0.815470
2019-12-03 22:40:04,073 train 050 6.955210e-03 0.776559
2019-12-03 22:40:15,983 train 100 7.123114e-03 0.759413
2019-12-03 22:40:27,889 train 150 7.457985e-03 0.751556
2019-12-03 22:40:39,793 train 200 7.465740e-03 0.752269
2019-12-03 22:40:45,149 training loss; R2: 7.320022e-03 0.755177
2019-12-03 22:40:45,280 valid 000 3.396979e-02 -0.122712
2019-12-03 22:40:46,522 validation loss; R2: 3.143225e-02 -0.132046
2019-12-03 22:40:46,545 epoch 10 lr 2.000000e-03
2019-12-03 22:40:46,869 train 000 5.044924e-03 0.788126
2019-12-03 22:40:58,758 train 050 6.175454e-03 0.785753
2019-12-03 22:41:10,644 train 100 6.607937e-03 0.773754
2019-12-03 22:41:22,529 train 150 6.770681e-03 0.763059
2019-12-03 22:41:34,414 train 200 7.116503e-03 0.756190
2019-12-03 22:41:39,762 training loss; R2: 7.076522e-03 0.757289
2019-12-03 22:41:39,892 valid 000 6.636946e-03 0.793050
2019-12-03 22:41:41,134 validation loss; R2: 6.281826e-03 0.798238
2019-12-03 22:41:41,156 epoch 11 lr 2.000000e-03
2019-12-03 22:41:41,484 train 000 5.104663e-03 0.790389
2019-12-03 22:41:53,377 train 050 6.381486e-03 0.784307
2019-12-03 22:42:05,268 train 100 6.275828e-03 0.789120
2019-12-03 22:42:17,162 train 150 6.149424e-03 0.788294
2019-12-03 22:42:29,049 train 200 6.327851e-03 0.784682
2019-12-03 22:42:34,398 training loss; R2: 6.448687e-03 0.782342
2019-12-03 22:42:34,535 valid 000 7.410156e-03 0.872178
2019-12-03 22:42:35,776 validation loss; R2: 4.483494e-03 0.851272
2019-12-03 22:42:35,798 epoch 12 lr 2.000000e-03
2019-12-03 22:42:36,116 train 000 5.569675e-03 0.876301
2019-12-03 22:42:48,003 train 050 6.164087e-03 0.790830
2019-12-03 22:42:59,884 train 100 6.044574e-03 0.793805
2019-12-03 22:43:11,772 train 150 5.963609e-03 0.796953
2019-12-03 22:43:23,499 train 200 5.926573e-03 0.798368
2019-12-03 22:43:28,715 training loss; R2: 5.959776e-03 0.799611
2019-12-03 22:43:28,862 valid 000 9.693352e-02 -2.197266
2019-12-03 22:43:30,102 validation loss; R2: 9.892917e-02 -2.563159
2019-12-03 22:43:30,124 epoch 13 lr 2.000000e-03
2019-12-03 22:43:30,440 train 000 3.755501e-03 0.787778
2019-12-03 22:43:42,047 train 050 5.629452e-03 0.785572
2019-12-03 22:43:53,649 train 100 5.741941e-03 0.782069
2019-12-03 22:44:05,250 train 150 5.940402e-03 0.787413
2019-12-03 22:44:16,855 train 200 6.214926e-03 0.791759
2019-12-03 22:44:22,071 training loss; R2: 6.318251e-03 0.788532
2019-12-03 22:44:22,202 valid 000 5.114076e-01 -18.864550
2019-12-03 22:44:23,442 validation loss; R2: 4.821752e-01 -17.436053
2019-12-03 22:44:23,464 epoch 14 lr 2.000000e-03
2019-12-03 22:44:23,781 train 000 3.987390e-03 0.800864
2019-12-03 22:44:35,384 train 050 7.081330e-03 0.742772
2019-12-03 22:44:46,979 train 100 7.124663e-03 0.762191
2019-12-03 22:44:58,577 train 150 6.709808e-03 0.773221
2019-12-03 22:45:10,171 train 200 6.384015e-03 0.782128
2019-12-03 22:45:15,385 training loss; R2: 6.271048e-03 0.787811
2019-12-03 22:45:15,517 valid 000 3.048563e+00 -69.171495
2019-12-03 22:45:16,756 validation loss; R2: 3.072627e+00 -115.068747
2019-12-03 22:45:16,779 epoch 15 lr 2.000000e-03
2019-12-03 22:45:17,095 train 000 6.666478e-03 0.780171
2019-12-03 22:45:28,682 train 050 6.208994e-03 0.800484
2019-12-03 22:45:40,269 train 100 6.334565e-03 0.788009
2019-12-03 22:45:51,854 train 150 6.388403e-03 0.777249
2019-12-03 22:46:03,437 train 200 6.466901e-03 0.782588
2019-12-03 22:46:08,644 training loss; R2: 6.350980e-03 0.784564
2019-12-03 22:46:08,775 valid 000 1.217608e-01 -3.493098
2019-12-03 22:46:10,014 validation loss; R2: 1.287562e-01 -3.952243
2019-12-03 22:46:10,035 epoch 16 lr 2.000000e-03
2019-12-03 22:46:10,352 train 000 6.712427e-03 0.827826
2019-12-03 22:46:21,940 train 050 5.983864e-03 0.805703
2019-12-03 22:46:33,525 train 100 5.701987e-03 0.807454
2019-12-03 22:46:45,116 train 150 6.113749e-03 0.800837
2019-12-03 22:46:56,698 train 200 5.948655e-03 0.803984
2019-12-03 22:47:01,910 training loss; R2: 5.980006e-03 0.801664
2019-12-03 22:47:02,043 valid 000 3.700982e-01 -15.145674
2019-12-03 22:47:03,282 validation loss; R2: 3.631510e-01 -13.201748
2019-12-03 22:47:03,305 epoch 17 lr 2.000000e-03
2019-12-03 22:47:03,622 train 000 1.564133e-02 0.831579
2019-12-03 22:47:15,199 train 050 6.399947e-03 0.789339
2019-12-03 22:47:26,774 train 100 5.650276e-03 0.806700
2019-12-03 22:47:38,353 train 150 5.479532e-03 0.813512
2019-12-03 22:47:49,930 train 200 5.299079e-03 0.818667
2019-12-03 22:47:55,138 training loss; R2: 5.318621e-03 0.818329
2019-12-03 22:47:55,277 valid 000 2.071500e-01 -4.934530
2019-12-03 22:47:56,518 validation loss; R2: 2.188949e-01 -6.590869
2019-12-03 22:47:56,540 epoch 18 lr 2.000000e-03
2019-12-03 22:47:56,855 train 000 6.384864e-03 0.724491
2019-12-03 22:48:08,435 train 050 5.920494e-03 0.798894
2019-12-03 22:48:20,005 train 100 5.745153e-03 0.806107
2019-12-03 22:48:31,581 train 150 5.645558e-03 0.808425
2019-12-03 22:48:43,160 train 200 5.489154e-03 0.812125
2019-12-03 22:48:48,371 training loss; R2: 5.494293e-03 0.814690
2019-12-03 22:48:48,515 valid 000 1.496327e-01 -1.918889
2019-12-03 22:48:49,755 validation loss; R2: 1.318643e-01 -3.743598
2019-12-03 22:48:49,776 epoch 19 lr 2.000000e-03
2019-12-03 22:48:50,090 train 000 3.982540e-03 0.840115
2019-12-03 22:49:01,669 train 050 5.346587e-03 0.811066
2019-12-03 22:49:13,244 train 100 5.981023e-03 0.797536
2019-12-03 22:49:24,819 train 150 5.832099e-03 0.802616
2019-12-03 22:49:36,397 train 200 5.694187e-03 0.804530
2019-12-03 22:49:41,603 training loss; R2: 5.686133e-03 0.806156
2019-12-03 22:49:41,739 valid 000 1.144419e+00 -39.596931
2019-12-03 22:49:42,978 validation loss; R2: 1.174576e+00 -41.703049
