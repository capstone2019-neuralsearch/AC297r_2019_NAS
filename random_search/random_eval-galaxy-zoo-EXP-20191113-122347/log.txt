2019-11-13 12:23:48,228 gpu device = 1
2019-11-13 12:23:48,228 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name='galaxy-zoo-cleaned', gpu=1, grad_clip=5, gz_dtree=False, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-galaxy-zoo-EXP-20191113-122347', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-11-13 12:23:59,809 param size = 0.268149MB
2019-11-13 12:23:59,813 epoch 0 lr 1.000000e-03
2019-11-13 12:24:01,987 train 000 2.583005e-01 -354.315379
2019-11-13 12:24:08,624 train 050 4.436901e-02 -10.169316
2019-11-13 12:24:15,340 train 100 3.570223e-02 -5.444989
2019-11-13 12:24:21,876 train 150 3.236083e-02 -3.773676
2019-11-13 12:24:28,408 train 200 3.044536e-02 -2.873944
2019-11-13 12:24:34,877 train 250 2.920586e-02 -2.339511
2019-11-13 12:24:41,456 train 300 2.830174e-02 -1.969516
2019-11-13 12:24:48,005 train 350 2.749797e-02 -1.707581
2019-11-13 12:24:54,535 train 400 2.682595e-02 -1.511299
2019-11-13 12:25:00,978 train 450 2.627443e-02 -1.347785
2019-11-13 12:25:07,456 train 500 2.585763e-02 -1.216195
2019-11-13 12:25:13,835 train 550 2.544369e-02 -1.105875
2019-11-13 12:25:20,173 train 600 2.509971e-02 -1.017076
2019-11-13 12:25:26,507 train 650 2.480312e-02 -0.937459
2019-11-13 12:25:32,836 train 700 2.452525e-02 -0.870546
2019-11-13 12:25:39,205 train 750 2.428545e-02 -0.809261
2019-11-13 12:25:45,566 train 800 2.404885e-02 -0.756141
2019-11-13 12:25:51,836 train 850 2.382224e-02 -0.735366
2019-11-13 12:25:54,478 training loss; R2: 2.373797e-02 -0.721679
2019-11-13 12:25:54,795 valid 000 1.898033e-02 0.160985
2019-11-13 12:25:56,498 valid 050 2.015123e-02 0.099511
2019-11-13 12:25:58,157 validation loss; R2: 1.980519e-02 0.094352
2019-11-13 12:25:58,177 epoch 1 lr 1.000000e-03
2019-11-13 12:25:58,740 train 000 2.339675e-02 -0.002857
2019-11-13 12:26:05,354 train 050 2.061733e-02 0.067591
2019-11-13 12:26:11,682 train 100 2.035288e-02 0.063356
2019-11-13 12:26:17,979 train 150 2.010669e-02 0.057499
2019-11-13 12:26:24,288 train 200 1.999634e-02 0.060742
2019-11-13 12:26:30,592 train 250 1.995332e-02 0.069352
2019-11-13 12:26:36,888 train 300 1.990893e-02 0.068241
2019-11-13 12:26:43,189 train 350 1.979848e-02 0.073422
2019-11-13 12:26:49,489 train 400 1.972689e-02 0.077756
2019-11-13 12:26:55,785 train 450 1.964980e-02 0.080009
2019-11-13 12:27:02,114 train 500 1.957210e-02 0.081355
2019-11-13 12:27:08,413 train 550 1.948589e-02 0.083902
2019-11-13 12:27:14,714 train 600 1.943479e-02 0.087898
2019-11-13 12:27:21,003 train 650 1.934401e-02 0.090510
2019-11-13 12:27:27,283 train 700 1.931698e-02 0.090715
2019-11-13 12:27:33,569 train 750 1.924325e-02 0.092570
2019-11-13 12:27:39,845 train 800 1.920355e-02 0.094279
2019-11-13 12:27:46,116 train 850 1.912571e-02 0.095530
2019-11-13 12:27:47,990 training loss; R2: 1.910190e-02 0.095695
2019-11-13 12:27:48,307 valid 000 1.816428e-02 0.242914
2019-11-13 12:27:49,991 valid 050 1.675867e-02 0.197068
2019-11-13 12:27:51,501 validation loss; R2: 1.671269e-02 0.200105
2019-11-13 12:27:51,517 epoch 2 lr 1.000000e-03
2019-11-13 12:27:51,910 train 000 1.770010e-02 0.169434
2019-11-13 12:27:58,642 train 050 1.801993e-02 0.121902
2019-11-13 12:28:04,968 train 100 1.780997e-02 0.119182
2019-11-13 12:28:11,264 train 150 1.755198e-02 0.136260
2019-11-13 12:28:17,569 train 200 1.762662e-02 0.135435
2019-11-13 12:28:23,857 train 250 1.757526e-02 0.138932
2019-11-13 12:28:30,145 train 300 1.744390e-02 0.140887
2019-11-13 12:28:36,439 train 350 1.730895e-02 0.148572
2019-11-13 12:28:42,726 train 400 1.726340e-02 0.151993
2019-11-13 12:28:49,014 train 450 1.717289e-02 0.144137
2019-11-13 12:28:55,308 train 500 1.707775e-02 0.147083
2019-11-13 12:29:01,598 train 550 1.694411e-02 0.146623
2019-11-13 12:29:07,880 train 600 1.685199e-02 0.149275
2019-11-13 12:29:14,177 train 650 1.675192e-02 0.151531
2019-11-13 12:29:20,460 train 700 1.668546e-02 0.152062
2019-11-13 12:29:26,746 train 750 1.663655e-02 0.154864
2019-11-13 12:29:33,040 train 800 1.658854e-02 0.156342
2019-11-13 12:29:39,336 train 850 1.652855e-02 0.156589
2019-11-13 12:29:41,213 training loss; R2: 1.651274e-02 0.157172
2019-11-13 12:29:41,518 valid 000 1.811637e-02 0.152808
2019-11-13 12:29:43,240 valid 050 1.517940e-02 0.220495
2019-11-13 12:29:44,829 validation loss; R2: 1.528201e-02 0.218795
2019-11-13 12:29:44,846 epoch 3 lr 1.000000e-03
2019-11-13 12:29:45,252 train 000 1.744097e-02 0.188326
2019-11-13 12:29:51,750 train 050 1.554262e-02 0.181620
2019-11-13 12:29:58,210 train 100 1.532016e-02 0.149811
2019-11-13 12:30:04,844 train 150 1.538507e-02 0.144872
2019-11-13 12:30:11,284 train 200 1.530434e-02 0.157005
2019-11-13 12:30:17,550 train 250 1.529869e-02 0.158202
2019-11-13 12:30:23,815 train 300 1.523143e-02 0.163562
2019-11-13 12:30:30,085 train 350 1.523384e-02 0.171351
2019-11-13 12:30:36,345 train 400 1.513486e-02 0.173149
2019-11-13 12:30:42,608 train 450 1.511363e-02 0.173521
2019-11-13 12:30:48,869 train 500 1.508224e-02 0.178705
2019-11-13 12:30:55,129 train 550 1.505755e-02 0.177978
2019-11-13 12:31:01,409 train 600 1.500681e-02 0.182709
2019-11-13 12:31:07,673 train 650 1.495851e-02 0.186329
2019-11-13 12:31:13,940 train 700 1.492271e-02 0.189321
2019-11-13 12:31:20,203 train 750 1.488498e-02 0.183252
2019-11-13 12:31:26,465 train 800 1.485476e-02 0.183793
2019-11-13 12:31:32,739 train 850 1.483872e-02 0.178654
2019-11-13 12:31:34,613 training loss; R2: 1.481743e-02 0.179353
2019-11-13 12:31:34,926 valid 000 1.236427e-02 0.376153
2019-11-13 12:31:36,663 valid 050 1.231993e-02 0.292946
2019-11-13 12:31:38,235 validation loss; R2: 1.252244e-02 0.263433
2019-11-13 12:31:38,251 epoch 4 lr 1.000000e-03
2019-11-13 12:31:38,647 train 000 1.331811e-02 0.203484
2019-11-13 12:31:45,285 train 050 1.425089e-02 0.243124
2019-11-13 12:31:51,611 train 100 1.415078e-02 0.239946
2019-11-13 12:31:57,905 train 150 1.420473e-02 0.236135
2019-11-13 12:32:04,192 train 200 1.417151e-02 0.188196
2019-11-13 12:32:10,513 train 250 1.416727e-02 0.167376
2019-11-13 12:32:16,809 train 300 1.415291e-02 0.182592
2019-11-13 12:32:23,091 train 350 1.409021e-02 0.190897
2019-11-13 12:32:29,384 train 400 1.406179e-02 0.193273
2019-11-13 12:32:35,679 train 450 1.398733e-02 0.199741
2019-11-13 12:32:41,965 train 500 1.393298e-02 0.206483
2019-11-13 12:32:48,245 train 550 1.388758e-02 0.212194
2019-11-13 12:32:54,531 train 600 1.386331e-02 0.215414
2019-11-13 12:33:00,820 train 650 1.384213e-02 0.216071
2019-11-13 12:33:07,101 train 700 1.381739e-02 0.217958
2019-11-13 12:33:13,380 train 750 1.378980e-02 0.220212
2019-11-13 12:33:19,671 train 800 1.375624e-02 0.223435
2019-11-13 12:33:25,961 train 850 1.373778e-02 0.225191
2019-11-13 12:33:27,849 training loss; R2: 1.373284e-02 0.226182
2019-11-13 12:33:28,192 valid 000 1.291509e-02 -0.478999
2019-11-13 12:33:29,903 valid 050 1.205565e-02 0.299219
2019-11-13 12:33:31,457 validation loss; R2: 1.197568e-02 0.305306
2019-11-13 12:33:31,484 epoch 5 lr 1.000000e-03
2019-11-13 12:33:31,905 train 000 1.227158e-02 0.293357
2019-11-13 12:33:38,418 train 050 1.313531e-02 0.273595
2019-11-13 12:33:44,695 train 100 1.327501e-02 0.270396
2019-11-13 12:33:50,980 train 150 1.319475e-02 0.271581
2019-11-13 12:33:57,256 train 200 1.323117e-02 0.267291
2019-11-13 12:34:03,544 train 250 1.324120e-02 0.268131
2019-11-13 12:34:09,816 train 300 1.324033e-02 0.266725
2019-11-13 12:34:16,094 train 350 1.316816e-02 0.264626
2019-11-13 12:34:22,380 train 400 1.314198e-02 0.266278
2019-11-13 12:34:28,666 train 450 1.314558e-02 0.267010
2019-11-13 12:34:34,946 train 500 1.314309e-02 0.270342
2019-11-13 12:34:41,208 train 550 1.312944e-02 0.272178
2019-11-13 12:34:47,467 train 600 1.314768e-02 0.273960
2019-11-13 12:34:53,728 train 650 1.313087e-02 0.268245
2019-11-13 12:35:00,218 train 700 1.309708e-02 0.267548
2019-11-13 12:35:06,848 train 750 1.308741e-02 0.267068
2019-11-13 12:35:13,306 train 800 1.308023e-02 0.255046
2019-11-13 12:35:19,581 train 850 1.307168e-02 0.256021
2019-11-13 12:35:21,452 training loss; R2: 1.306532e-02 0.255236
2019-11-13 12:35:21,762 valid 000 1.249830e-02 0.367631
2019-11-13 12:35:23,489 valid 050 1.109523e-02 0.353029
2019-11-13 12:35:25,052 validation loss; R2: 1.108039e-02 0.347824
2019-11-13 12:35:25,069 epoch 6 lr 1.000000e-03
2019-11-13 12:35:25,477 train 000 1.409041e-02 -0.164792
2019-11-13 12:35:32,133 train 050 1.288930e-02 0.278465
2019-11-13 12:35:38,464 train 100 1.284852e-02 0.266625
2019-11-13 12:35:44,971 train 150 1.289576e-02 0.255693
2019-11-13 12:35:51,250 train 200 1.287516e-02 0.264473
2019-11-13 12:35:57,540 train 250 1.282474e-02 0.269763
2019-11-13 12:36:03,810 train 300 1.283342e-02 0.268791
2019-11-13 12:36:10,260 train 350 1.286712e-02 0.270489
2019-11-13 12:36:16,904 train 400 1.286025e-02 0.217789
2019-11-13 12:36:23,544 train 450 1.287267e-02 0.225246
2019-11-13 12:36:30,076 train 500 1.285098e-02 0.228080
2019-11-13 12:36:36,438 train 550 1.279865e-02 0.233410
2019-11-13 12:36:42,710 train 600 1.278496e-02 0.238402
2019-11-13 12:36:49,241 train 650 1.277651e-02 0.240276
2019-11-13 12:36:55,508 train 700 1.276196e-02 0.241285
2019-11-13 12:37:01,788 train 750 1.276697e-02 0.242543
2019-11-13 12:37:08,060 train 800 1.273115e-02 0.246397
2019-11-13 12:37:14,337 train 850 1.271523e-02 0.248788
2019-11-13 12:37:16,211 training loss; R2: 1.271425e-02 0.249422
2019-11-13 12:37:16,521 valid 000 5.485297e-02 -1.796962
2019-11-13 12:37:18,232 valid 050 5.967316e-02 -1.910975
2019-11-13 12:37:19,802 validation loss; R2: 6.050013e-02 -1.964769
2019-11-13 12:37:19,825 epoch 7 lr 1.000000e-03
2019-11-13 12:37:20,223 train 000 1.310382e-02 0.343925
2019-11-13 12:37:26,568 train 050 1.231710e-02 0.251123
2019-11-13 12:37:32,870 train 100 1.260044e-02 0.263455
2019-11-13 12:37:39,159 train 150 1.247428e-02 0.263247
2019-11-13 12:37:45,437 train 200 1.235227e-02 0.265158
2019-11-13 12:37:51,720 train 250 1.236303e-02 0.265948
2019-11-13 12:37:57,994 train 300 1.237732e-02 0.267450
2019-11-13 12:38:04,274 train 350 1.237368e-02 0.267665
2019-11-13 12:38:10,550 train 400 1.239549e-02 0.265046
2019-11-13 12:38:16,823 train 450 1.239850e-02 0.269889
2019-11-13 12:38:23,093 train 500 1.240179e-02 0.272425
2019-11-13 12:38:29,362 train 550 1.241636e-02 0.272726
2019-11-13 12:38:35,632 train 600 1.241233e-02 0.271446
2019-11-13 12:38:41,960 train 650 1.239785e-02 0.271531
2019-11-13 12:38:48,601 train 700 1.238764e-02 0.267803
2019-11-13 12:38:55,195 train 750 1.238563e-02 0.268837
2019-11-13 12:39:01,648 train 800 1.238245e-02 0.270495
2019-11-13 12:39:08,044 train 850 1.239071e-02 0.271258
2019-11-13 12:39:09,917 training loss; R2: 1.237975e-02 0.272455
2019-11-13 12:39:10,227 valid 000 1.225664e-01 -10.358221
2019-11-13 12:39:11,950 valid 050 1.241418e-01 -12.337603
2019-11-13 12:39:13,498 validation loss; R2: 1.247229e-01 -12.024474
2019-11-13 12:39:13,519 epoch 8 lr 1.000000e-03
2019-11-13 12:39:13,955 train 000 1.259114e-02 0.352102
2019-11-13 12:39:20,521 train 050 1.227735e-02 0.287939
2019-11-13 12:39:26,868 train 100 1.244370e-02 0.288718
2019-11-13 12:39:33,166 train 150 1.234747e-02 0.268210
2019-11-13 12:39:39,443 train 200 1.234031e-02 0.278309
2019-11-13 12:39:45,710 train 250 1.234143e-02 0.279821
2019-11-13 12:39:51,989 train 300 1.230206e-02 0.283110
2019-11-13 12:39:58,261 train 350 1.227690e-02 0.288341
2019-11-13 12:40:04,546 train 400 1.224928e-02 0.290229
2019-11-13 12:40:10,828 train 450 1.221797e-02 0.276865
2019-11-13 12:40:17,481 train 500 1.220840e-02 0.278456
2019-11-13 12:40:23,826 train 550 1.220026e-02 0.279672
2019-11-13 12:40:30,121 train 600 1.217097e-02 0.278346
2019-11-13 12:40:36,407 train 650 1.217095e-02 0.281484
2019-11-13 12:40:42,927 train 700 1.217561e-02 0.282441
2019-11-13 12:40:49,573 train 750 1.218012e-02 0.283411
2019-11-13 12:40:56,232 train 800 1.216801e-02 0.284826
2019-11-13 12:41:02,867 train 850 1.214576e-02 0.285820
2019-11-13 12:41:04,856 training loss; R2: 1.214414e-02 0.285990
2019-11-13 12:41:05,167 valid 000 3.827072e-02 -0.302270
2019-11-13 12:41:06,897 valid 050 4.167449e-02 -1.102538
2019-11-13 12:41:08,452 validation loss; R2: 4.163134e-02 -1.187062
2019-11-13 12:41:08,477 epoch 9 lr 1.000000e-03
2019-11-13 12:41:08,855 train 000 1.395799e-02 0.377848
2019-11-13 12:41:15,465 train 050 1.168149e-02 0.303722
2019-11-13 12:41:21,742 train 100 1.200374e-02 0.300728
2019-11-13 12:41:28,031 train 150 1.198627e-02 0.304940
2019-11-13 12:41:34,317 train 200 1.197671e-02 0.309059
2019-11-13 12:41:40,630 train 250 1.199686e-02 0.304300
2019-11-13 12:41:46,929 train 300 1.196955e-02 0.303613
2019-11-13 12:41:53,204 train 350 1.199517e-02 0.297871
2019-11-13 12:41:59,480 train 400 1.199454e-02 0.295466
2019-11-13 12:42:05,766 train 450 1.196898e-02 0.293298
2019-11-13 12:42:12,062 train 500 1.197276e-02 0.293671
2019-11-13 12:42:18,341 train 550 1.195775e-02 0.293719
2019-11-13 12:42:24,617 train 600 1.196183e-02 0.291503
2019-11-13 12:42:30,895 train 650 1.196151e-02 0.290466
2019-11-13 12:42:37,195 train 700 1.193815e-02 0.291204
2019-11-13 12:42:43,507 train 750 1.193335e-02 0.293700
2019-11-13 12:42:49,819 train 800 1.192535e-02 0.293060
2019-11-13 12:42:56,133 train 850 1.193398e-02 0.293249
2019-11-13 12:42:58,029 training loss; R2: 1.193303e-02 0.293262
2019-11-13 12:42:58,344 valid 000 1.390422e-01 -6.455685
2019-11-13 12:43:00,054 valid 050 1.319899e-01 -6.076211
2019-11-13 12:43:01,580 validation loss; R2: 1.314750e-01 -6.323650
2019-11-13 12:43:01,603 epoch 10 lr 1.000000e-03
2019-11-13 12:43:02,004 train 000 1.090996e-02 0.390474
2019-11-13 12:43:08,302 train 050 1.165138e-02 0.277848
2019-11-13 12:43:14,589 train 100 1.177156e-02 0.247476
2019-11-13 12:43:20,890 train 150 1.178933e-02 0.193192
2019-11-13 12:43:27,230 train 200 1.173024e-02 0.227546
2019-11-13 12:43:33,565 train 250 1.171988e-02 0.238473
2019-11-13 12:43:39,870 train 300 1.172462e-02 0.250567
2019-11-13 12:43:46,233 train 350 1.174353e-02 0.242730
2019-11-13 12:43:52,596 train 400 1.176648e-02 0.250152
2019-11-13 12:43:58,901 train 450 1.176875e-02 0.256069
2019-11-13 12:44:05,211 train 500 1.176036e-02 0.260647
2019-11-13 12:44:11,532 train 550 1.176258e-02 0.266727
2019-11-13 12:44:17,825 train 600 1.174796e-02 0.267426
2019-11-13 12:44:24,146 train 650 1.177494e-02 0.269592
2019-11-13 12:44:30,478 train 700 1.174912e-02 0.272644
2019-11-13 12:44:36,783 train 750 1.174130e-02 0.276762
2019-11-13 12:44:43,066 train 800 1.173998e-02 0.279639
2019-11-13 12:44:49,377 train 850 1.174894e-02 0.280577
2019-11-13 12:44:51,272 training loss; R2: 1.175747e-02 0.280670
2019-11-13 12:44:51,581 valid 000 7.804817e-02 -2.560319
2019-11-13 12:44:53,327 valid 050 7.943048e-02 -4.761569
2019-11-13 12:44:54,880 validation loss; R2: 7.937800e-02 -4.103619
2019-11-13 12:44:54,903 epoch 11 lr 1.000000e-03
2019-11-13 12:44:55,288 train 000 1.510775e-02 0.333880
2019-11-13 12:45:01,803 train 050 1.181490e-02 0.300654
2019-11-13 12:45:08,347 train 100 1.182908e-02 0.293057
2019-11-13 12:45:14,901 train 150 1.183654e-02 0.288912
2019-11-13 12:45:21,242 train 200 1.180690e-02 -0.691099
2019-11-13 12:45:27,553 train 250 1.178700e-02 -0.493825
2019-11-13 12:45:33,887 train 300 1.177472e-02 -0.357712
2019-11-13 12:45:40,213 train 350 1.177560e-02 -0.264761
2019-11-13 12:45:46,542 train 400 1.177033e-02 -0.191929
2019-11-13 12:45:52,869 train 450 1.177171e-02 -0.134612
2019-11-13 12:45:59,178 train 500 1.178428e-02 -0.093817
2019-11-13 12:46:05,512 train 550 1.176225e-02 -0.055948
2019-11-13 12:46:11,851 train 600 1.174862e-02 -0.026555
2019-11-13 12:46:18,181 train 650 1.173089e-02 -0.002090
2019-11-13 12:46:24,541 train 700 1.171443e-02 0.020415
2019-11-13 12:46:30,871 train 750 1.170542e-02 0.039639
2019-11-13 12:46:37,193 train 800 1.170965e-02 0.053385
2019-11-13 12:46:43,506 train 850 1.170854e-02 0.068739
2019-11-13 12:46:45,403 training loss; R2: 1.171352e-02 0.072930
2019-11-13 12:46:45,710 valid 000 3.690969e-02 -2.600073
2019-11-13 12:46:47,447 valid 050 4.037004e-02 -1.821876
2019-11-13 12:46:49,012 validation loss; R2: 4.009362e-02 -1.707857
2019-11-13 12:46:49,029 epoch 12 lr 1.000000e-03
2019-11-13 12:46:49,421 train 000 1.114304e-02 0.321218
2019-11-13 12:46:55,918 train 050 1.169455e-02 -0.536367
2019-11-13 12:47:02,444 train 100 1.151380e-02 -0.115092
2019-11-13 12:47:08,760 train 150 1.150100e-02 0.019205
2019-11-13 12:47:15,091 train 200 1.151192e-02 0.087212
2019-11-13 12:47:21,377 train 250 1.155695e-02 0.133078
2019-11-13 12:47:27,705 train 300 1.154961e-02 0.161384
2019-11-13 12:47:34,019 train 350 1.156704e-02 0.174304
2019-11-13 12:47:40,307 train 400 1.158093e-02 0.191689
2019-11-13 12:47:46,605 train 450 1.158134e-02 0.204509
2019-11-13 12:47:52,914 train 500 1.160278e-02 0.214356
2019-11-13 12:47:59,210 train 550 1.163566e-02 0.219671
2019-11-13 12:48:05,550 train 600 1.164940e-02 0.228876
2019-11-13 12:48:11,855 train 650 1.162958e-02 0.235682
2019-11-13 12:48:18,179 train 700 1.161267e-02 0.241254
2019-11-13 12:48:24,498 train 750 1.162956e-02 0.245610
2019-11-13 12:48:30,784 train 800 1.161987e-02 0.246517
2019-11-13 12:48:37,105 train 850 1.163463e-02 0.249795
2019-11-13 12:48:38,992 training loss; R2: 1.163775e-02 0.250964
2019-11-13 12:48:39,309 valid 000 1.328190e-01 -6.369372
2019-11-13 12:48:41,025 valid 050 1.351368e-01 -12.886154
2019-11-13 12:48:42,600 validation loss; R2: 1.353011e-01 -11.992175
2019-11-13 12:48:42,628 epoch 13 lr 1.000000e-03
2019-11-13 12:48:43,057 train 000 1.219142e-02 0.372571
2019-11-13 12:48:49,842 train 050 1.164937e-02 0.304594
2019-11-13 12:48:56,524 train 100 1.149142e-02 0.301083
2019-11-13 12:49:02,902 train 150 1.157286e-02 0.308381
2019-11-13 12:49:09,187 train 200 1.165613e-02 0.295542
2019-11-13 12:49:15,483 train 250 1.165164e-02 0.295056
2019-11-13 12:49:21,756 train 300 1.157656e-02 0.296829
2019-11-13 12:49:28,054 train 350 1.151595e-02 0.292060
2019-11-13 12:49:34,360 train 400 1.156030e-02 0.286218
2019-11-13 12:49:40,663 train 450 1.155024e-02 0.287275
2019-11-13 12:49:46,973 train 500 1.156446e-02 0.290723
2019-11-13 12:49:53,265 train 550 1.156314e-02 0.292071
2019-11-13 12:49:59,544 train 600 1.158544e-02 0.291151
2019-11-13 12:50:05,825 train 650 1.160383e-02 0.291070
2019-11-13 12:50:12,106 train 700 1.161250e-02 0.292125
2019-11-13 12:50:18,380 train 750 1.160614e-02 0.293224
2019-11-13 12:50:24,659 train 800 1.159560e-02 0.292130
2019-11-13 12:50:30,931 train 850 1.158141e-02 0.289350
2019-11-13 12:50:32,814 training loss; R2: 1.157965e-02 0.289993
2019-11-13 12:50:33,124 valid 000 3.835542e+01 -3088.451602
2019-11-13 12:50:34,827 valid 050 3.831481e+01 -2627.649112
2019-11-13 12:50:36,381 validation loss; R2: 3.830302e+01 -2657.943842
2019-11-13 12:50:36,405 epoch 14 lr 1.000000e-03
2019-11-13 12:50:36,782 train 000 1.170201e-02 0.370636
2019-11-13 12:50:43,421 train 050 1.117243e-02 0.333436
2019-11-13 12:50:50,145 train 100 1.149133e-02 0.309701
2019-11-13 12:50:56,740 train 150 1.142153e-02 0.305310
2019-11-13 12:51:03,241 train 200 1.142414e-02 0.311968
2019-11-13 12:51:09,596 train 250 1.144082e-02 0.315996
2019-11-13 12:51:15,898 train 300 1.149436e-02 0.310637
2019-11-13 12:51:22,231 train 350 1.147433e-02 0.315056
2019-11-13 12:51:28,544 train 400 1.149089e-02 0.299959
2019-11-13 12:51:34,844 train 450 1.145354e-02 0.293235
2019-11-13 12:51:41,123 train 500 1.145484e-02 0.295312
2019-11-13 12:51:47,411 train 550 1.145782e-02 0.295220
2019-11-13 12:51:53,696 train 600 1.144312e-02 0.295282
2019-11-13 12:51:59,975 train 650 1.146175e-02 0.296699
2019-11-13 12:52:06,276 train 700 1.145153e-02 0.296753
2019-11-13 12:52:12,573 train 750 1.145605e-02 0.297270
2019-11-13 12:52:18,864 train 800 1.145284e-02 0.298001
2019-11-13 12:52:25,163 train 850 1.145330e-02 0.300088
2019-11-13 12:52:27,041 training loss; R2: 1.144437e-02 0.300863
2019-11-13 12:52:27,363 valid 000 1.010545e+02 -7225.884479
2019-11-13 12:52:29,080 valid 050 1.008746e+02 -8475.918335
2019-11-13 12:52:30,636 validation loss; R2: 1.008754e+02 -10809.156423
2019-11-13 12:52:30,659 epoch 15 lr 1.000000e-03
2019-11-13 12:52:31,041 train 000 1.085475e-02 0.363935
2019-11-13 12:52:37,369 train 050 1.119481e-02 0.322288
2019-11-13 12:52:43,674 train 100 1.132870e-02 0.288626
2019-11-13 12:52:49,962 train 150 1.135130e-02 0.291389
2019-11-13 12:52:56,247 train 200 1.133476e-02 0.294956
2019-11-13 12:53:02,545 train 250 1.132202e-02 0.290282
2019-11-13 12:53:08,826 train 300 1.132282e-02 0.292189
2019-11-13 12:53:15,100 train 350 1.137473e-02 0.293860
2019-11-13 12:53:21,379 train 400 1.136157e-02 0.293264
2019-11-13 12:53:27,675 train 450 1.133851e-02 0.288555
2019-11-13 12:53:33,967 train 500 1.136910e-02 0.290893
2019-11-13 12:53:40,432 train 550 1.137584e-02 0.294743
2019-11-13 12:53:46,725 train 600 1.138846e-02 0.295545
2019-11-13 12:53:53,017 train 650 1.137570e-02 0.296526
2019-11-13 12:53:59,301 train 700 1.137308e-02 0.297293
2019-11-13 12:54:05,594 train 750 1.137811e-02 0.296310
2019-11-13 12:54:11,893 train 800 1.137775e-02 0.294406
2019-11-13 12:54:18,203 train 850 1.134496e-02 0.297051
2019-11-13 12:54:20,078 training loss; R2: 1.133413e-02 0.298079
2019-11-13 12:54:20,385 valid 000 4.952045e+01 -4318.271854
2019-11-13 12:54:22,157 valid 050 4.939655e+01 -4099.614470
2019-11-13 12:54:23,709 validation loss; R2: 4.938750e+01 -4252.941751
2019-11-13 12:54:23,735 epoch 16 lr 1.000000e-03
2019-11-13 12:54:24,110 train 000 9.161437e-03 0.441278
2019-11-13 12:54:30,454 train 050 1.129592e-02 0.274333
2019-11-13 12:54:36,784 train 100 1.133030e-02 0.265795
2019-11-13 12:54:43,263 train 150 1.122019e-02 0.284015
2019-11-13 12:54:49,578 train 200 1.123375e-02 0.292358
2019-11-13 12:54:55,885 train 250 1.127792e-02 0.303230
2019-11-13 12:55:02,259 train 300 1.127179e-02 0.292625
2019-11-13 12:55:08,919 train 350 1.124945e-02 0.276205
2019-11-13 12:55:15,284 train 400 1.125093e-02 0.280785
2019-11-13 12:55:21,612 train 450 1.123796e-02 0.286373
2019-11-13 12:55:27,929 train 500 1.124576e-02 0.289147
2019-11-13 12:55:34,275 train 550 1.125295e-02 0.292880
2019-11-13 12:55:40,616 train 600 1.122666e-02 0.296549
2019-11-13 12:55:46,940 train 650 1.122154e-02 0.295858
2019-11-13 12:55:53,292 train 700 1.120939e-02 0.298305
2019-11-13 12:55:59,630 train 750 1.120790e-02 0.300683
2019-11-13 12:56:05,991 train 800 1.120083e-02 0.300110
2019-11-13 12:56:12,332 train 850 1.120519e-02 0.302124
2019-11-13 12:56:14,228 training loss; R2: 1.121152e-02 0.303241
2019-11-13 12:56:14,532 valid 000 2.306678e+02 -31607.076680
2019-11-13 12:56:16,275 valid 050 2.307187e+02 -40174.061938
2019-11-13 12:56:17,842 validation loss; R2: 2.307093e+02 -42270.686105
2019-11-13 12:56:17,864 epoch 17 lr 1.000000e-03
2019-11-13 12:56:18,290 train 000 1.222895e-02 0.211954
2019-11-13 12:56:24,752 train 050 1.099696e-02 0.290228
2019-11-13 12:56:31,119 train 100 1.110212e-02 0.322603
2019-11-13 12:56:37,469 train 150 1.107212e-02 0.323650
2019-11-13 12:56:43,819 train 200 1.107647e-02 0.313529
2019-11-13 12:56:50,178 train 250 1.109743e-02 0.316362
2019-11-13 12:56:56,535 train 300 1.106629e-02 0.323454
2019-11-13 12:57:02,899 train 350 1.105087e-02 0.320306
2019-11-13 12:57:09,262 train 400 1.105872e-02 0.305283
2019-11-13 12:57:15,618 train 450 1.104896e-02 0.306800
2019-11-13 12:57:21,987 train 500 1.105358e-02 0.305096
2019-11-13 12:57:28,558 train 550 1.103104e-02 0.309880
2019-11-13 12:57:34,925 train 600 1.102591e-02 0.311483
2019-11-13 12:57:41,302 train 650 1.102215e-02 0.311736
2019-11-13 12:57:47,669 train 700 1.101462e-02 0.301945
2019-11-13 12:57:54,035 train 750 1.098778e-02 0.303914
2019-11-13 12:58:00,388 train 800 1.099921e-02 0.303987
2019-11-13 12:58:06,755 train 850 1.099352e-02 0.304180
2019-11-13 12:58:08,662 training loss; R2: 1.098846e-02 0.305028
2019-11-13 12:58:08,972 valid 000 1.882112e+02 -27103.979832
2019-11-13 12:58:10,699 valid 050 1.881756e+02 -38647.526072
2019-11-13 12:58:12,227 validation loss; R2: 1.881906e+02 -32738.703480
2019-11-13 12:58:12,249 epoch 18 lr 1.000000e-03
2019-11-13 12:58:12,644 train 000 1.122351e-02 0.341224
2019-11-13 12:58:19,477 train 050 1.118735e-02 0.313359
2019-11-13 12:58:25,958 train 100 1.095614e-02 0.345254
2019-11-13 12:58:32,546 train 150 1.092721e-02 0.338554
2019-11-13 12:58:38,907 train 200 1.093549e-02 0.329318
2019-11-13 12:58:45,273 train 250 1.087291e-02 0.334288
2019-11-13 12:58:51,630 train 300 1.085289e-02 0.336727
2019-11-13 12:58:57,978 train 350 1.081682e-02 0.339667
2019-11-13 12:59:04,323 train 400 1.083171e-02 0.339505
2019-11-13 12:59:10,667 train 450 1.087398e-02 0.340115
2019-11-13 12:59:17,013 train 500 1.087306e-02 0.334952
2019-11-13 12:59:23,355 train 550 1.088328e-02 0.334461
2019-11-13 12:59:29,694 train 600 1.086921e-02 0.336147
2019-11-13 12:59:36,036 train 650 1.087144e-02 0.338092
2019-11-13 12:59:42,527 train 700 1.086965e-02 0.336806
2019-11-13 12:59:48,859 train 750 1.083764e-02 0.335291
2019-11-13 12:59:55,200 train 800 1.084668e-02 0.331691
2019-11-13 13:00:01,535 train 850 1.085202e-02 0.187227
2019-11-13 13:00:03,431 training loss; R2: 1.086873e-02 0.188785
2019-11-13 13:00:03,738 valid 000 3.642813e+02 -41463.895582
2019-11-13 13:00:05,459 valid 050 3.646423e+02 -67077.182931
2019-11-13 13:00:07,015 validation loss; R2: 3.646552e+02 -55355.433702
2019-11-13 13:00:07,034 epoch 19 lr 1.000000e-03
2019-11-13 13:00:07,437 train 000 1.161331e-02 0.259238
2019-11-13 13:00:14,143 train 050 1.114746e-02 0.261290
2019-11-13 13:00:20,614 train 100 1.114202e-02 0.273860
2019-11-13 13:00:27,026 train 150 1.100775e-02 0.294228
2019-11-13 13:00:33,435 train 200 1.099077e-02 0.269362
2019-11-13 13:00:39,797 train 250 1.104786e-02 0.242382
2019-11-13 13:00:46,179 train 300 1.101144e-02 0.260600
2019-11-13 13:00:52,735 train 350 1.100109e-02 0.265393
2019-11-13 13:00:59,082 train 400 1.100743e-02 -2.011858
2019-11-13 13:01:05,450 train 450 1.097915e-02 -1.750466
2019-11-13 13:01:11,800 train 500 1.097108e-02 -1.541654
2019-11-13 13:01:18,146 train 550 1.094701e-02 -1.369821
2019-11-13 13:01:24,493 train 600 1.094044e-02 -1.224737
2019-11-13 13:01:30,848 train 650 1.092644e-02 -1.104614
2019-11-13 13:01:37,206 train 700 1.092976e-02 -1.002417
2019-11-13 13:01:43,554 train 750 1.093035e-02 -0.914386
2019-11-13 13:01:49,907 train 800 1.094637e-02 -0.838097
2019-11-13 13:01:56,257 train 850 1.094943e-02 -0.774217
2019-11-13 13:01:58,157 training loss; R2: 1.095560e-02 -0.755934
2019-11-13 13:01:58,478 valid 000 4.359869e+01 -15380.358135
2019-11-13 13:02:00,203 valid 050 4.374975e+01 -23884.940501
2019-11-13 13:02:01,743 validation loss; R2: 4.374127e+01 -24449.636894
