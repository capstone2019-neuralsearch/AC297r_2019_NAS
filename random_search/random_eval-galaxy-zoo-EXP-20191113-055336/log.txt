2019-11-13 05:53:36,690 gpu device = 1
2019-11-13 05:53:36,690 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name='galaxy-zoo-cleaned', gpu=1, grad_clip=5, gz_dtree=False, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-galaxy-zoo-EXP-20191113-055336', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-11-13 05:53:48,144 param size = 0.297205MB
2019-11-13 05:53:48,148 epoch 0 lr 1.000000e-03
2019-11-13 05:53:50,339 train 000 3.177373e-01 -173.965895
2019-11-13 05:53:57,884 train 050 4.403743e-02 -7.407951
2019-11-13 05:54:05,445 train 100 3.510683e-02 -4.006520
2019-11-13 05:54:12,987 train 150 3.166333e-02 -2.794947
2019-11-13 05:54:20,519 train 200 2.965425e-02 -2.146446
2019-11-13 05:54:28,056 train 250 2.839879e-02 -1.745470
2019-11-13 05:54:35,517 train 300 2.745151e-02 -1.466731
2019-11-13 05:54:43,077 train 350 2.656049e-02 -1.269848
2019-11-13 05:54:50,628 train 400 2.592391e-02 -1.119112
2019-11-13 05:54:58,171 train 450 2.534381e-02 -0.993729
2019-11-13 05:55:05,719 train 500 2.485343e-02 -0.906058
2019-11-13 05:55:13,282 train 550 2.444332e-02 -0.822850
2019-11-13 05:55:20,719 train 600 2.408025e-02 -0.750618
2019-11-13 05:55:28,243 train 650 2.368589e-02 -0.693225
2019-11-13 05:55:35,639 train 700 2.333780e-02 -0.643312
2019-11-13 05:55:43,000 train 750 2.304439e-02 -0.598851
2019-11-13 05:55:50,470 train 800 2.274773e-02 -0.562599
2019-11-13 05:55:57,979 train 850 2.248884e-02 -0.523921
2019-11-13 05:56:00,956 training loss; R2: 2.240009e-02 -0.513261
2019-11-13 05:56:01,243 valid 000 1.696594e-02 0.148364
2019-11-13 05:56:02,888 valid 050 1.826758e-02 0.121680
2019-11-13 05:56:04,479 validation loss; R2: 1.822218e-02 0.107278
2019-11-13 05:56:04,496 epoch 1 lr 1.000000e-03
2019-11-13 05:56:05,041 train 000 1.927803e-02 -0.110814
2019-11-13 05:56:12,496 train 050 1.786510e-02 0.094756
2019-11-13 05:56:19,850 train 100 1.761667e-02 0.085129
2019-11-13 05:56:27,219 train 150 1.752305e-02 0.075588
2019-11-13 05:56:34,644 train 200 1.746183e-02 0.078856
2019-11-13 05:56:42,097 train 250 1.735367e-02 0.092959
2019-11-13 05:56:49,473 train 300 1.725662e-02 0.090628
2019-11-13 05:56:56,790 train 350 1.719666e-02 0.092372
2019-11-13 05:57:04,063 train 400 1.711123e-02 0.094454
2019-11-13 05:57:11,339 train 450 1.696380e-02 0.101527
2019-11-13 05:57:18,825 train 500 1.685805e-02 0.108216
2019-11-13 05:57:26,162 train 550 1.674550e-02 0.114604
2019-11-13 05:57:33,386 train 600 1.665055e-02 0.033067
2019-11-13 05:57:40,609 train 650 1.658829e-02 0.044085
2019-11-13 05:57:47,824 train 700 1.647879e-02 0.055129
2019-11-13 05:57:55,039 train 750 1.640959e-02 0.063144
2019-11-13 05:58:02,246 train 800 1.631949e-02 0.070650
2019-11-13 05:58:09,452 train 850 1.624175e-02 0.077503
2019-11-13 05:58:11,650 training loss; R2: 1.621262e-02 0.078404
2019-11-13 05:58:11,957 valid 000 1.395409e-02 -0.240195
2019-11-13 05:58:13,652 valid 050 1.455552e-02 0.117813
2019-11-13 05:58:15,220 validation loss; R2: 1.464160e-02 0.148724
2019-11-13 05:58:15,237 epoch 2 lr 1.000000e-03
2019-11-13 05:58:15,621 train 000 1.311187e-02 0.348545
2019-11-13 05:58:22,824 train 050 1.460581e-02 0.189181
2019-11-13 05:58:30,032 train 100 1.459681e-02 0.171634
2019-11-13 05:58:37,241 train 150 1.456618e-02 0.177474
2019-11-13 05:58:44,458 train 200 1.447795e-02 0.180521
2019-11-13 05:58:51,663 train 250 1.445739e-02 0.181745
2019-11-13 05:58:58,886 train 300 1.437918e-02 0.179592
2019-11-13 05:59:06,105 train 350 1.436810e-02 0.184159
2019-11-13 05:59:13,321 train 400 1.428988e-02 0.185487
2019-11-13 05:59:20,537 train 450 1.425019e-02 0.162244
2019-11-13 05:59:27,757 train 500 1.419176e-02 0.162081
2019-11-13 05:59:34,977 train 550 1.413214e-02 0.166818
2019-11-13 05:59:42,196 train 600 1.410739e-02 0.173210
2019-11-13 05:59:49,411 train 650 1.407024e-02 0.176930
2019-11-13 05:59:56,632 train 700 1.403520e-02 0.177210
2019-11-13 06:00:03,842 train 750 1.401312e-02 0.180615
2019-11-13 06:00:11,056 train 800 1.399265e-02 0.182954
2019-11-13 06:00:18,270 train 850 1.395295e-02 0.167382
2019-11-13 06:00:20,427 training loss; R2: 1.395488e-02 0.169147
2019-11-13 06:00:20,729 valid 000 8.785380e-03 0.332685
2019-11-13 06:00:22,457 valid 050 1.258459e-02 0.215166
2019-11-13 06:00:23,998 validation loss; R2: 1.257982e-02 0.208421
2019-11-13 06:00:24,025 epoch 3 lr 1.000000e-03
2019-11-13 06:00:24,417 train 000 1.654440e-02 -0.697242
2019-11-13 06:00:31,777 train 050 1.359385e-02 0.042345
2019-11-13 06:00:39,060 train 100 1.343541e-02 0.135964
2019-11-13 06:00:46,284 train 150 1.333766e-02 0.163067
2019-11-13 06:00:53,503 train 200 1.340525e-02 0.179747
2019-11-13 06:01:00,720 train 250 1.349167e-02 0.187866
2019-11-13 06:01:07,941 train 300 1.343470e-02 0.201979
2019-11-13 06:01:15,154 train 350 1.336451e-02 0.208002
2019-11-13 06:01:22,365 train 400 1.327509e-02 0.213743
2019-11-13 06:01:29,577 train 450 1.326152e-02 0.215087
2019-11-13 06:01:36,789 train 500 1.319948e-02 0.210949
2019-11-13 06:01:44,003 train 550 1.317524e-02 0.214180
2019-11-13 06:01:51,212 train 600 1.316656e-02 0.214074
2019-11-13 06:01:58,467 train 650 1.318020e-02 0.041879
2019-11-13 06:02:05,675 train 700 1.313456e-02 0.056440
2019-11-13 06:02:12,889 train 750 1.310397e-02 0.070900
2019-11-13 06:02:20,098 train 800 1.309648e-02 0.079695
2019-11-13 06:02:27,311 train 850 1.306418e-02 0.089821
2019-11-13 06:02:29,467 training loss; R2: 1.306022e-02 0.092756
2019-11-13 06:02:29,770 valid 000 1.251289e-02 0.313777
2019-11-13 06:02:31,490 valid 050 1.333644e-02 0.123948
2019-11-13 06:02:33,027 validation loss; R2: 1.335435e-02 0.126384
2019-11-13 06:02:33,044 epoch 4 lr 1.000000e-03
2019-11-13 06:02:33,440 train 000 1.266185e-02 0.314214
2019-11-13 06:02:40,851 train 050 1.237765e-02 0.275065
2019-11-13 06:02:48,163 train 100 1.242247e-02 0.260967
2019-11-13 06:02:55,444 train 150 1.246521e-02 0.236520
2019-11-13 06:03:02,779 train 200 1.253581e-02 0.222754
2019-11-13 06:03:10,037 train 250 1.253195e-02 0.231698
2019-11-13 06:03:17,312 train 300 1.246523e-02 0.241226
2019-11-13 06:03:24,511 train 350 1.244817e-02 0.202101
2019-11-13 06:03:31,718 train 400 1.246096e-02 0.207106
2019-11-13 06:03:38,927 train 450 1.246749e-02 0.204534
2019-11-13 06:03:46,125 train 500 1.244255e-02 0.198642
2019-11-13 06:03:53,330 train 550 1.245132e-02 0.205719
2019-11-13 06:04:00,538 train 600 1.244528e-02 0.212363
2019-11-13 06:04:07,745 train 650 1.244656e-02 0.214317
2019-11-13 06:04:14,984 train 700 1.242344e-02 0.219452
2019-11-13 06:04:22,187 train 750 1.239735e-02 0.223163
2019-11-13 06:04:29,406 train 800 1.236910e-02 0.223897
2019-11-13 06:04:36,615 train 850 1.236574e-02 0.223032
2019-11-13 06:04:38,767 training loss; R2: 1.236035e-02 0.224191
2019-11-13 06:04:39,068 valid 000 9.792754e-03 0.422773
2019-11-13 06:04:40,787 valid 050 1.118471e-02 0.322979
2019-11-13 06:04:42,333 validation loss; R2: 1.112241e-02 0.339973
2019-11-13 06:04:42,350 epoch 5 lr 1.000000e-03
2019-11-13 06:04:42,767 train 000 1.161618e-02 0.277333
2019-11-13 06:04:50,086 train 050 1.220705e-02 0.177650
2019-11-13 06:04:57,377 train 100 1.186197e-02 0.243157
2019-11-13 06:05:04,737 train 150 1.208142e-02 0.252520
2019-11-13 06:05:12,052 train 200 1.212451e-02 0.247563
2019-11-13 06:05:19,286 train 250 1.209435e-02 0.257615
2019-11-13 06:05:26,524 train 300 1.204914e-02 0.248795
2019-11-13 06:05:33,762 train 350 1.201899e-02 0.256802
2019-11-13 06:05:41,108 train 400 1.201082e-02 0.264268
2019-11-13 06:05:48,390 train 450 1.198131e-02 0.266076
2019-11-13 06:05:55,634 train 500 1.198589e-02 0.244240
2019-11-13 06:06:03,063 train 550 1.199650e-02 0.248017
2019-11-13 06:06:10,300 train 600 1.198199e-02 0.251350
2019-11-13 06:06:17,531 train 650 1.199460e-02 0.255571
2019-11-13 06:06:24,933 train 700 1.198022e-02 0.239004
2019-11-13 06:06:32,173 train 750 1.195499e-02 0.243615
2019-11-13 06:06:39,400 train 800 1.193673e-02 0.242519
2019-11-13 06:06:46,629 train 850 1.194081e-02 0.245388
2019-11-13 06:06:48,784 training loss; R2: 1.194256e-02 0.247057
2019-11-13 06:06:49,093 valid 000 1.227386e-02 0.387558
2019-11-13 06:06:50,827 valid 050 1.097739e-02 0.274043
2019-11-13 06:06:52,378 validation loss; R2: 1.097968e-02 0.264364
2019-11-13 06:06:52,395 epoch 6 lr 1.000000e-03
2019-11-13 06:06:52,793 train 000 1.182793e-02 0.320517
2019-11-13 06:07:00,144 train 050 1.171484e-02 0.292053
2019-11-13 06:07:07,523 train 100 1.165892e-02 0.299834
2019-11-13 06:07:14,845 train 150 1.174937e-02 0.280109
2019-11-13 06:07:22,191 train 200 1.179698e-02 0.285934
2019-11-13 06:07:29,610 train 250 1.180007e-02 0.161682
2019-11-13 06:07:36,993 train 300 1.177360e-02 0.157061
2019-11-13 06:07:44,361 train 350 1.174866e-02 0.170786
2019-11-13 06:07:51,827 train 400 1.170557e-02 0.189323
2019-11-13 06:07:59,287 train 450 1.171299e-02 0.200619
2019-11-13 06:08:06,550 train 500 1.170009e-02 0.210193
2019-11-13 06:08:14,042 train 550 1.172138e-02 0.218386
2019-11-13 06:08:21,526 train 600 1.173723e-02 0.222647
2019-11-13 06:08:28,991 train 650 1.172121e-02 0.227868
2019-11-13 06:08:36,453 train 700 1.170313e-02 -1.766873
2019-11-13 06:08:43,913 train 750 1.169593e-02 -1.632894
2019-11-13 06:08:51,376 train 800 1.167591e-02 -1.512063
2019-11-13 06:08:58,831 train 850 1.165162e-02 -1.403123
2019-11-13 06:09:01,061 training loss; R2: 1.164758e-02 -1.374571
2019-11-13 06:09:01,371 valid 000 1.674624e-02 0.035249
2019-11-13 06:09:03,035 valid 050 2.062941e-02 -0.105020
2019-11-13 06:09:04,559 validation loss; R2: 2.059697e-02 -0.096613
2019-11-13 06:09:04,576 epoch 7 lr 1.000000e-03
2019-11-13 06:09:04,968 train 000 1.258679e-02 0.244426
2019-11-13 06:09:12,531 train 050 1.165373e-02 0.290696
2019-11-13 06:09:20,010 train 100 1.151178e-02 0.297863
2019-11-13 06:09:27,499 train 150 1.140015e-02 0.298999
2019-11-13 06:09:34,813 train 200 1.139296e-02 0.302543
2019-11-13 06:09:42,260 train 250 1.135615e-02 0.301864
2019-11-13 06:09:49,740 train 300 1.131703e-02 0.302664
2019-11-13 06:09:57,131 train 350 1.128505e-02 0.299911
2019-11-13 06:10:04,404 train 400 1.131748e-02 0.297368
2019-11-13 06:10:11,808 train 450 1.135112e-02 0.297009
2019-11-13 06:10:19,195 train 500 1.131904e-02 0.294306
2019-11-13 06:10:26,740 train 550 1.131250e-02 0.297411
2019-11-13 06:10:34,136 train 600 1.133597e-02 0.295357
2019-11-13 06:10:41,708 train 650 1.132253e-02 0.294615
2019-11-13 06:10:49,264 train 700 1.132836e-02 0.295500
2019-11-13 06:10:56,595 train 750 1.131249e-02 0.296460
2019-11-13 06:11:03,892 train 800 1.131090e-02 0.297739
2019-11-13 06:11:11,266 train 850 1.132128e-02 0.298852
2019-11-13 06:11:13,459 training loss; R2: 1.132305e-02 0.298411
2019-11-13 06:11:13,737 valid 000 3.268977e-02 -0.599637
2019-11-13 06:11:15,412 valid 050 3.426900e-02 -1.033393
2019-11-13 06:11:16,934 validation loss; R2: 3.444435e-02 -0.951743
2019-11-13 06:11:16,951 epoch 8 lr 1.000000e-03
2019-11-13 06:11:17,352 train 000 1.050153e-02 0.253110
2019-11-13 06:11:24,732 train 050 1.116488e-02 0.201693
2019-11-13 06:11:32,091 train 100 1.111950e-02 0.256185
2019-11-13 06:11:39,503 train 150 1.111907e-02 0.277018
2019-11-13 06:11:46,692 train 200 1.113680e-02 0.277211
2019-11-13 06:11:53,877 train 250 1.119865e-02 0.273291
2019-11-13 06:12:01,061 train 300 1.121339e-02 0.271173
2019-11-13 06:12:08,237 train 350 1.120235e-02 0.276065
2019-11-13 06:12:15,420 train 400 1.116300e-02 0.279824
2019-11-13 06:12:22,603 train 450 1.119970e-02 0.265794
2019-11-13 06:12:29,845 train 500 1.118365e-02 0.272132
2019-11-13 06:12:37,020 train 550 1.117072e-02 0.272766
2019-11-13 06:12:44,200 train 600 1.116851e-02 0.274198
2019-11-13 06:12:51,369 train 650 1.115250e-02 0.269533
2019-11-13 06:12:58,540 train 700 1.115685e-02 0.275415
2019-11-13 06:13:05,724 train 750 1.114958e-02 0.267633
2019-11-13 06:13:12,901 train 800 1.112071e-02 0.271226
2019-11-13 06:13:20,075 train 850 1.109394e-02 0.276041
2019-11-13 06:13:22,220 training loss; R2: 1.109510e-02 0.275647
2019-11-13 06:13:22,528 valid 000 1.354528e+01 -399.775548
2019-11-13 06:13:24,193 valid 050 1.356562e+01 -397.191647
2019-11-13 06:13:25,736 validation loss; R2: 1.356335e+01 -399.314368
2019-11-13 06:13:25,762 epoch 9 lr 1.000000e-03
2019-11-13 06:13:26,165 train 000 8.218003e-03 0.438357
2019-11-13 06:13:33,348 train 050 1.089541e-02 0.303376
2019-11-13 06:13:40,525 train 100 1.091675e-02 0.173488
2019-11-13 06:13:47,716 train 150 1.092069e-02 0.207984
2019-11-13 06:13:54,907 train 200 1.086169e-02 0.240789
2019-11-13 06:14:02,095 train 250 1.089463e-02 0.236584
2019-11-13 06:14:09,287 train 300 1.089398e-02 0.250793
2019-11-13 06:14:16,493 train 350 1.091087e-02 0.263763
2019-11-13 06:14:23,688 train 400 1.091480e-02 0.269355
2019-11-13 06:14:30,879 train 450 1.092470e-02 0.274962
2019-11-13 06:14:38,072 train 500 1.094918e-02 0.264375
2019-11-13 06:14:45,265 train 550 1.094100e-02 0.270413
2019-11-13 06:14:52,469 train 600 1.094166e-02 0.274866
2019-11-13 06:14:59,661 train 650 1.092613e-02 0.278451
2019-11-13 06:15:06,855 train 700 1.088996e-02 0.252488
2019-11-13 06:15:14,053 train 750 1.088650e-02 0.259206
2019-11-13 06:15:21,250 train 800 1.089306e-02 0.259830
2019-11-13 06:15:28,456 train 850 1.089159e-02 0.264353
2019-11-13 06:15:30,613 training loss; R2: 1.088592e-02 0.265746
2019-11-13 06:15:30,923 valid 000 2.703525e+00 -370.131609
2019-11-13 06:15:32,659 valid 050 2.702347e+00 -534.233087
2019-11-13 06:15:34,214 validation loss; R2: 2.703978e+00 -583.355825
2019-11-13 06:15:34,236 epoch 10 lr 1.000000e-03
2019-11-13 06:15:34,643 train 000 1.016817e-02 0.359567
2019-11-13 06:15:41,924 train 050 1.099302e-02 0.275262
2019-11-13 06:15:49,196 train 100 1.095958e-02 0.305600
2019-11-13 06:15:56,449 train 150 1.085134e-02 0.317416
2019-11-13 06:16:03,711 train 200 1.078493e-02 0.323206
2019-11-13 06:16:11,112 train 250 1.076327e-02 0.322510
2019-11-13 06:16:18,472 train 300 1.077872e-02 0.314273
2019-11-13 06:16:25,822 train 350 1.076490e-02 0.318743
2019-11-13 06:16:33,108 train 400 1.074684e-02 0.317997
2019-11-13 06:16:40,376 train 450 1.075381e-02 0.316053
2019-11-13 06:16:47,634 train 500 1.077745e-02 0.315688
2019-11-13 06:16:54,899 train 550 1.075848e-02 0.317558
2019-11-13 06:17:02,367 train 600 1.077016e-02 0.316982
2019-11-13 06:17:09,860 train 650 1.074146e-02 0.320466
2019-11-13 06:17:17,174 train 700 1.074860e-02 0.322453
2019-11-13 06:17:24,484 train 750 1.074430e-02 0.320431
2019-11-13 06:17:31,739 train 800 1.073511e-02 0.321122
2019-11-13 06:17:38,989 train 850 1.073840e-02 0.321492
2019-11-13 06:17:41,153 training loss; R2: 1.073526e-02 0.321191
2019-11-13 06:17:41,455 valid 000 2.016426e+02 -11857.542072
2019-11-13 06:17:43,190 valid 050 2.017121e+02 -11383.646064
2019-11-13 06:17:44,750 validation loss; R2: 2.017190e+02 -11974.026669
2019-11-13 06:17:44,772 epoch 11 lr 1.000000e-03
2019-11-13 06:17:45,198 train 000 1.107632e-02 0.427273
2019-11-13 06:17:52,585 train 050 1.049065e-02 0.360940
2019-11-13 06:17:59,998 train 100 1.062207e-02 0.346779
2019-11-13 06:18:07,357 train 150 1.068073e-02 0.330108
2019-11-13 06:18:14,652 train 200 1.064569e-02 0.323331
2019-11-13 06:18:22,048 train 250 1.058404e-02 0.327258
2019-11-13 06:18:29,365 train 300 1.061860e-02 0.328482
2019-11-13 06:18:36,819 train 350 1.062402e-02 0.329241
2019-11-13 06:18:44,172 train 400 1.057945e-02 0.332552
2019-11-13 06:18:51,536 train 450 1.055709e-02 0.331622
2019-11-13 06:18:58,809 train 500 1.056153e-02 0.325452
2019-11-13 06:19:06,091 train 550 1.056103e-02 0.327771
2019-11-13 06:19:13,407 train 600 1.056750e-02 0.327074
2019-11-13 06:19:20,707 train 650 1.053739e-02 0.327935
2019-11-13 06:19:27,993 train 700 1.051085e-02 0.329454
2019-11-13 06:19:35,350 train 750 1.053176e-02 0.328507
2019-11-13 06:19:42,615 train 800 1.052581e-02 0.330633
2019-11-13 06:19:49,953 train 850 1.050706e-02 0.324709
2019-11-13 06:19:52,143 training loss; R2: 1.049946e-02 0.324794
2019-11-13 06:19:52,450 valid 000 1.357687e+02 -5805.889584
2019-11-13 06:19:54,187 valid 050 1.357726e+02 -8227.399505
2019-11-13 06:19:55,735 validation loss; R2: 1.357564e+02 -9353.750544
2019-11-13 06:19:55,761 epoch 12 lr 1.000000e-03
2019-11-13 06:19:56,171 train 000 1.144281e-02 0.414689
2019-11-13 06:20:03,519 train 050 9.842318e-03 0.377673
2019-11-13 06:20:10,829 train 100 1.005539e-02 0.379440
2019-11-13 06:20:18,169 train 150 1.041956e-02 0.217313
2019-11-13 06:20:25,517 train 200 1.046253e-02 0.252112
2019-11-13 06:20:32,894 train 250 1.041699e-02 0.270392
2019-11-13 06:20:40,293 train 300 1.041588e-02 0.278266
2019-11-13 06:20:47,615 train 350 1.043766e-02 0.289921
2019-11-13 06:20:54,977 train 400 1.042628e-02 0.297095
2019-11-13 06:21:02,322 train 450 1.042418e-02 0.285998
2019-11-13 06:21:09,705 train 500 1.045106e-02 0.294018
2019-11-13 06:21:17,141 train 550 1.045646e-02 0.295157
2019-11-13 06:21:24,527 train 600 1.042213e-02 0.301127
2019-11-13 06:21:31,903 train 650 1.039394e-02 0.298079
2019-11-13 06:21:39,340 train 700 1.039182e-02 0.299788
2019-11-13 06:21:46,742 train 750 1.040825e-02 0.302169
2019-11-13 06:21:54,157 train 800 1.041351e-02 0.301934
2019-11-13 06:22:01,532 train 850 1.040445e-02 0.304809
2019-11-13 06:22:03,728 training loss; R2: 1.040181e-02 0.305956
2019-11-13 06:22:04,063 valid 000 1.399954e+02 -5341.219702
2019-11-13 06:22:05,762 valid 050 1.400140e+02 -5739.183841
2019-11-13 06:22:07,302 validation loss; R2: 1.400292e+02 -5774.681686
2019-11-13 06:22:07,318 epoch 13 lr 1.000000e-03
2019-11-13 06:22:07,710 train 000 1.022048e-02 0.457577
2019-11-13 06:22:15,110 train 050 1.031163e-02 0.322388
2019-11-13 06:22:22,511 train 100 1.019742e-02 0.343743
2019-11-13 06:22:29,859 train 150 1.037416e-02 0.349667
2019-11-13 06:22:37,243 train 200 1.038795e-02 0.342964
2019-11-13 06:22:44,690 train 250 1.042625e-02 0.345969
2019-11-13 06:22:52,064 train 300 1.047628e-02 0.341355
2019-11-13 06:22:59,440 train 350 1.046727e-02 0.335482
2019-11-13 06:23:06,824 train 400 1.047041e-02 0.249336
2019-11-13 06:23:14,193 train 450 1.046048e-02 0.260097
2019-11-13 06:23:21,581 train 500 1.045441e-02 0.265770
2019-11-13 06:23:28,976 train 550 1.046592e-02 0.271886
2019-11-13 06:23:36,314 train 600 1.044829e-02 0.276036
2019-11-13 06:23:43,696 train 650 1.044061e-02 0.282453
2019-11-13 06:23:51,051 train 700 1.043528e-02 0.286932
2019-11-13 06:23:58,446 train 750 1.044118e-02 0.290806
2019-11-13 06:24:05,839 train 800 1.043972e-02 0.290364
2019-11-13 06:24:13,320 train 850 1.042358e-02 0.294691
2019-11-13 06:24:15,556 training loss; R2: 1.042596e-02 0.294287
2019-11-13 06:24:15,885 valid 000 1.245932e+02 -30457.238226
2019-11-13 06:24:17,534 valid 050 1.246550e+02 -21338.132063
2019-11-13 06:24:19,092 validation loss; R2: 1.246675e+02 -20736.927480
2019-11-13 06:24:19,110 epoch 14 lr 1.000000e-03
2019-11-13 06:24:19,606 train 000 1.066617e-02 0.432832
2019-11-13 06:24:26,976 train 050 1.078359e-02 0.244643
2019-11-13 06:24:34,369 train 100 1.069389e-02 0.289330
2019-11-13 06:24:41,761 train 150 1.066878e-02 0.290438
2019-11-13 06:24:49,072 train 200 1.054159e-02 0.302353
2019-11-13 06:24:56,399 train 250 1.053227e-02 0.305485
2019-11-13 06:25:03,717 train 300 1.048501e-02 0.314361
2019-11-13 06:25:11,146 train 350 1.040883e-02 0.318334
2019-11-13 06:25:18,459 train 400 1.035752e-02 0.318010
2019-11-13 06:25:25,763 train 450 1.030330e-02 0.318866
2019-11-13 06:25:33,098 train 500 1.027997e-02 0.326037
2019-11-13 06:25:40,454 train 550 1.022642e-02 0.332476
2019-11-13 06:25:47,739 train 600 1.020174e-02 0.333953
2019-11-13 06:25:55,117 train 650 1.019749e-02 0.332452
2019-11-13 06:26:02,507 train 700 1.017361e-02 0.333849
2019-11-13 06:26:09,994 train 750 1.019990e-02 0.333580
2019-11-13 06:26:17,320 train 800 1.020229e-02 0.332633
2019-11-13 06:26:24,645 train 850 1.020706e-02 0.325798
2019-11-13 06:26:26,868 training loss; R2: 1.021233e-02 0.326105
2019-11-13 06:26:27,180 valid 000 7.380938e+01 -4062.345919
2019-11-13 06:26:28,913 valid 050 7.375863e+01 -6961.049076
2019-11-13 06:26:30,475 validation loss; R2: 7.375744e+01 -5500.866431
2019-11-13 06:26:30,498 epoch 15 lr 1.000000e-03
2019-11-13 06:26:30,960 train 000 1.042210e-02 0.298313
2019-11-13 06:26:38,280 train 050 1.083313e-02 0.305971
2019-11-13 06:26:45,539 train 100 1.076907e-02 0.310483
2019-11-13 06:26:52,852 train 150 1.073660e-02 0.304653
2019-11-13 06:27:00,130 train 200 1.068700e-02 0.307643
2019-11-13 06:27:07,441 train 250 1.065321e-02 0.314839
2019-11-13 06:27:14,822 train 300 1.062157e-02 0.319954
2019-11-13 06:27:22,141 train 350 1.049862e-02 0.321313
2019-11-13 06:27:29,576 train 400 1.040950e-02 0.325673
2019-11-13 06:27:36,871 train 450 1.041738e-02 0.318172
2019-11-13 06:27:44,160 train 500 1.042230e-02 0.320006
2019-11-13 06:27:51,463 train 550 1.041593e-02 0.321063
2019-11-13 06:27:58,743 train 600 1.043698e-02 0.320781
2019-11-13 06:28:06,015 train 650 1.043344e-02 0.321669
2019-11-13 06:28:13,318 train 700 1.042055e-02 0.324253
2019-11-13 06:28:20,611 train 750 1.044284e-02 0.325893
2019-11-13 06:28:27,912 train 800 1.043610e-02 0.323554
2019-11-13 06:28:35,237 train 850 1.042285e-02 0.318851
2019-11-13 06:28:37,442 training loss; R2: 1.042202e-02 0.319018
2019-11-13 06:28:37,733 valid 000 7.648144e+01 -5596.819684
2019-11-13 06:28:39,445 valid 050 7.669669e+01 -6863.075302
2019-11-13 06:28:41,022 validation loss; R2: 7.669307e+01 -6720.848043
2019-11-13 06:28:41,051 epoch 16 lr 1.000000e-03
2019-11-13 06:28:41,494 train 000 9.007082e-03 0.398077
2019-11-13 06:28:48,864 train 050 1.067428e-02 0.304128
2019-11-13 06:28:56,153 train 100 1.053325e-02 0.308514
2019-11-13 06:29:03,489 train 150 1.050678e-02 0.303360
2019-11-13 06:29:10,782 train 200 1.061686e-02 0.300192
2019-11-13 06:29:18,043 train 250 1.060172e-02 0.294434
2019-11-13 06:29:25,280 train 300 1.063516e-02 0.301799
2019-11-13 06:29:32,651 train 350 1.063755e-02 0.307228
2019-11-13 06:29:39,984 train 400 1.061268e-02 0.309544
2019-11-13 06:29:47,353 train 450 1.059341e-02 0.308762
2019-11-13 06:29:54,611 train 500 1.058637e-02 0.314815
2019-11-13 06:30:01,959 train 550 1.056166e-02 0.315013
2019-11-13 06:30:09,248 train 600 1.054512e-02 0.291865
2019-11-13 06:30:16,645 train 650 1.052626e-02 0.296299
2019-11-13 06:30:24,064 train 700 1.050648e-02 0.300707
2019-11-13 06:30:31,429 train 750 1.050482e-02 0.304077
2019-11-13 06:30:38,837 train 800 1.050377e-02 0.306640
2019-11-13 06:30:46,319 train 850 1.051683e-02 0.305526
2019-11-13 06:30:48,487 training loss; R2: 1.051078e-02 0.307195
2019-11-13 06:30:48,805 valid 000 3.079564e+02 -9538.582122
2019-11-13 06:30:50,534 valid 050 3.082959e+02 -13355.423787
2019-11-13 06:30:52,093 validation loss; R2: 3.083099e+02 -13642.359407
2019-11-13 06:30:52,110 epoch 17 lr 1.000000e-03
2019-11-13 06:30:52,499 train 000 1.116423e-02 0.440872
2019-11-13 06:30:59,899 train 050 1.034475e-02 0.306758
2019-11-13 06:31:07,246 train 100 1.021607e-02 0.324605
2019-11-13 06:31:14,568 train 150 1.036404e-02 0.331540
2019-11-13 06:31:21,844 train 200 1.042490e-02 0.330544
2019-11-13 06:31:29,196 train 250 1.040043e-02 0.330345
2019-11-13 06:31:36,636 train 300 1.032763e-02 0.335919
2019-11-13 06:31:43,884 train 350 1.030853e-02 0.339772
2019-11-13 06:31:51,123 train 400 1.034668e-02 0.313851
2019-11-13 06:31:58,493 train 450 1.034199e-02 0.306708
2019-11-13 06:32:05,745 train 500 1.031574e-02 0.303827
2019-11-13 06:32:12,983 train 550 1.030072e-02 0.309081
2019-11-13 06:32:20,206 train 600 1.031336e-02 0.309605
2019-11-13 06:32:27,493 train 650 1.030606e-02 0.313167
2019-11-13 06:32:34,760 train 700 1.032220e-02 0.315589
2019-11-13 06:32:42,097 train 750 1.029590e-02 0.299484
2019-11-13 06:32:49,351 train 800 1.031216e-02 0.299538
2019-11-13 06:32:56,568 train 850 1.032121e-02 0.301399
2019-11-13 06:32:58,730 training loss; R2: 1.032385e-02 0.301552
2019-11-13 06:32:59,029 valid 000 4.180643e-01 -39.571498
2019-11-13 06:33:00,773 valid 050 4.260825e-01 -121.281796
2019-11-13 06:33:02,331 validation loss; R2: 4.266163e-01 -81.105601
2019-11-13 06:33:02,353 epoch 18 lr 1.000000e-03
2019-11-13 06:33:02,784 train 000 9.676576e-03 0.342473
2019-11-13 06:33:10,166 train 050 1.020257e-02 0.324132
2019-11-13 06:33:17,421 train 100 1.025251e-02 0.320676
2019-11-13 06:33:24,669 train 150 1.034443e-02 0.317695
2019-11-13 06:33:32,008 train 200 1.036124e-02 0.310257
2019-11-13 06:33:39,305 train 250 1.032807e-02 0.319976
2019-11-13 06:33:46,613 train 300 1.032881e-02 0.305730
2019-11-13 06:33:53,925 train 350 1.033237e-02 0.312368
2019-11-13 06:34:01,239 train 400 1.034434e-02 0.308475
2019-11-13 06:34:08,549 train 450 1.032814e-02 0.312460
2019-11-13 06:34:16,037 train 500 1.032380e-02 0.302501
2019-11-13 06:34:23,414 train 550 1.030282e-02 0.306462
2019-11-13 06:34:30,767 train 600 1.028353e-02 0.308766
2019-11-13 06:34:38,103 train 650 1.025958e-02 0.314133
2019-11-13 06:34:45,442 train 700 1.024993e-02 0.317607
2019-11-13 06:34:52,804 train 750 1.026046e-02 0.315487
2019-11-13 06:35:00,173 train 800 1.025845e-02 0.317956
2019-11-13 06:35:07,503 train 850 1.026031e-02 0.320575
2019-11-13 06:35:09,710 training loss; R2: 1.026774e-02 0.321109
2019-11-13 06:35:10,019 valid 000 5.286409e-01 -38.825727
2019-11-13 06:35:11,742 valid 050 5.391306e-01 -41.475643
2019-11-13 06:35:13,295 validation loss; R2: 5.395367e-01 -78.533692
2019-11-13 06:35:13,317 epoch 19 lr 1.000000e-03
2019-11-13 06:35:13,765 train 000 1.037720e-02 0.444534
2019-11-13 06:35:21,177 train 050 9.978773e-03 0.313544
2019-11-13 06:35:28,538 train 100 1.011932e-02 0.308482
2019-11-13 06:35:35,846 train 150 1.012680e-02 0.217078
2019-11-13 06:35:43,235 train 200 1.005614e-02 0.250404
2019-11-13 06:35:50,590 train 250 1.009953e-02 0.265323
2019-11-13 06:35:57,902 train 300 1.006080e-02 0.279424
2019-11-13 06:36:05,269 train 350 1.003541e-02 0.290011
2019-11-13 06:36:12,642 train 400 1.005616e-02 0.280729
2019-11-13 06:36:20,000 train 450 1.006352e-02 0.287320
2019-11-13 06:36:27,463 train 500 1.005924e-02 0.295321
2019-11-13 06:36:34,825 train 550 1.007847e-02 0.299104
2019-11-13 06:36:42,334 train 600 1.008711e-02 0.301647
2019-11-13 06:36:49,880 train 650 1.014323e-02 0.297516
2019-11-13 06:36:57,396 train 700 1.019232e-02 0.284046
2019-11-13 06:37:04,926 train 750 1.019810e-02 0.284213
2019-11-13 06:37:12,449 train 800 1.018868e-02 0.209721
2019-11-13 06:37:19,994 train 850 1.020796e-02 0.217983
2019-11-13 06:37:22,234 training loss; R2: 1.021595e-02 0.220206
2019-11-13 06:37:22,547 valid 000 3.533545e+00 -303.975007
2019-11-13 06:37:24,216 valid 050 3.543213e+00 -295.612707
2019-11-13 06:37:25,724 validation loss; R2: 3.544680e+00 -307.153518
