2019-12-04 03:50:52,418 gpu device = 1
2019-12-04 03:50:52,418 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=96, cutout=False, cutout_length=16, data='../data', dataset='graphene', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name=None, gpu=1, grad_clip=5, gz_dtree=False, init_channels=36, layers=8, learning_rate=0.002, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-graphene-EXP-20191204-035052', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-12-04 03:50:55,649 param size = 1.367533MB
2019-12-04 03:50:55,652 epoch 0 lr 2.000000e-03
2019-12-04 03:50:58,449 train 000 4.212718e-01 -13.653480
2019-12-04 03:51:13,862 train 050 1.976710e-01 -4.856384
2019-12-04 03:51:28,984 train 100 1.162762e-01 -2.503380
2019-12-04 03:51:44,112 train 150 8.930943e-02 -1.745703
2019-12-04 03:51:59,232 train 200 7.645385e-02 -1.389967
2019-12-04 03:52:07,256 training loss; R2: 7.262209e-02 -1.275437
2019-12-04 03:52:07,412 valid 000 2.446509e-02 0.258653
2019-12-04 03:52:09,305 validation loss; R2: 2.507888e-02 0.218874
2019-12-04 03:52:09,331 epoch 1 lr 2.000000e-03
2019-12-04 03:52:09,938 train 000 3.298159e-02 0.036979
2019-12-04 03:52:25,079 train 050 2.489198e-02 0.190373
2019-12-04 03:52:40,203 train 100 2.330229e-02 0.238097
2019-12-04 03:52:55,324 train 150 2.200788e-02 0.284082
2019-12-04 03:53:10,446 train 200 2.091796e-02 0.316454
2019-12-04 03:53:17,252 training loss; R2: 2.052765e-02 0.329796
2019-12-04 03:53:17,397 valid 000 1.208083e-02 0.432325
2019-12-04 03:53:18,950 validation loss; R2: 1.837240e-02 0.401972
2019-12-04 03:53:18,977 epoch 2 lr 2.000000e-03
2019-12-04 03:53:19,366 train 000 1.387340e-02 0.609702
2019-12-04 03:53:34,287 train 050 1.505845e-02 0.479674
2019-12-04 03:53:49,025 train 100 1.505863e-02 0.485533
2019-12-04 03:54:03,760 train 150 1.610756e-02 0.479565
2019-12-04 03:54:18,503 train 200 1.520875e-02 0.504666
2019-12-04 03:54:25,131 training loss; R2: 1.470929e-02 0.515632
2019-12-04 03:54:25,275 valid 000 6.496118e-03 0.651109
2019-12-04 03:54:26,827 validation loss; R2: 1.001633e-02 0.686305
2019-12-04 03:54:26,853 epoch 3 lr 2.000000e-03
2019-12-04 03:54:27,234 train 000 1.601690e-02 0.618801
2019-12-04 03:54:41,983 train 050 1.261483e-02 0.626318
2019-12-04 03:54:56,736 train 100 1.226896e-02 0.618670
2019-12-04 03:55:11,486 train 150 1.204867e-02 0.615584
2019-12-04 03:55:26,234 train 200 1.199335e-02 0.616418
2019-12-04 03:55:32,874 training loss; R2: 1.181529e-02 0.620295
2019-12-04 03:55:33,018 valid 000 4.632973e-03 0.633529
2019-12-04 03:55:34,570 validation loss; R2: 7.352281e-03 0.756634
2019-12-04 03:55:34,596 epoch 4 lr 2.000000e-03
2019-12-04 03:55:34,978 train 000 7.388023e-03 0.588608
2019-12-04 03:55:49,723 train 050 9.491189e-03 0.662298
2019-12-04 03:56:04,471 train 100 9.836138e-03 0.666779
2019-12-04 03:56:19,223 train 150 9.797922e-03 0.666632
2019-12-04 03:56:33,974 train 200 9.853973e-03 0.672152
2019-12-04 03:56:40,609 training loss; R2: 9.795830e-03 0.674936
2019-12-04 03:56:40,751 valid 000 4.869468e-03 0.826215
2019-12-04 03:56:42,303 validation loss; R2: 6.749977e-03 0.787837
2019-12-04 03:56:42,337 epoch 5 lr 2.000000e-03
2019-12-04 03:56:42,718 train 000 6.302260e-03 0.793050
2019-12-04 03:56:57,461 train 050 1.032672e-02 0.685892
2019-12-04 03:57:12,212 train 100 1.026837e-02 0.674646
2019-12-04 03:57:26,960 train 150 9.894560e-03 0.683195
2019-12-04 03:57:41,714 train 200 9.844542e-03 0.680216
2019-12-04 03:57:48,350 training loss; R2: 9.736249e-03 0.682473
2019-12-04 03:57:48,496 valid 000 4.004121e-03 0.855100
2019-12-04 03:57:50,048 validation loss; R2: 6.688416e-03 0.782825
2019-12-04 03:57:50,076 epoch 6 lr 2.000000e-03
2019-12-04 03:57:50,459 train 000 1.292816e-02 0.805373
2019-12-04 03:58:05,207 train 050 9.090039e-03 0.707760
2019-12-04 03:58:19,953 train 100 9.165056e-03 0.710753
2019-12-04 03:58:34,702 train 150 8.599085e-03 0.718973
2019-12-04 03:58:49,453 train 200 8.463580e-03 0.722668
2019-12-04 03:58:56,088 training loss; R2: 8.498710e-03 0.723352
2019-12-04 03:58:56,232 valid 000 2.034262e-02 0.111115
2019-12-04 03:58:57,784 validation loss; R2: 2.632187e-02 0.164345
2019-12-04 03:58:57,812 epoch 7 lr 2.000000e-03
2019-12-04 03:58:58,192 train 000 5.230745e-03 0.772444
2019-12-04 03:59:12,937 train 050 8.535865e-03 0.716682
2019-12-04 03:59:27,678 train 100 8.141290e-03 0.721015
2019-12-04 03:59:42,408 train 150 7.862135e-03 0.731181
2019-12-04 03:59:57,147 train 200 7.754928e-03 0.734764
2019-12-04 04:00:03,777 training loss; R2: 7.745825e-03 0.737654
2019-12-04 04:00:03,919 valid 000 1.047861e-02 0.824534
2019-12-04 04:00:05,472 validation loss; R2: 5.080416e-03 0.832337
2019-12-04 04:00:05,499 epoch 8 lr 2.000000e-03
2019-12-04 04:00:05,879 train 000 7.644393e-03 0.573351
2019-12-04 04:00:20,614 train 050 8.378394e-03 0.727766
2019-12-04 04:00:35,350 train 100 7.892785e-03 0.734625
2019-12-04 04:00:50,091 train 150 7.865305e-03 0.735286
2019-12-04 04:01:04,826 train 200 7.971368e-03 0.733703
2019-12-04 04:01:11,452 training loss; R2: 8.001300e-03 0.731490
2019-12-04 04:01:11,597 valid 000 5.176002e-03 0.769887
2019-12-04 04:01:13,149 validation loss; R2: 6.773333e-03 0.784303
2019-12-04 04:01:13,176 epoch 9 lr 2.000000e-03
2019-12-04 04:01:13,555 train 000 7.799029e-03 0.757151
2019-12-04 04:01:28,296 train 050 8.241195e-03 0.728791
2019-12-04 04:01:43,040 train 100 8.448501e-03 0.721672
2019-12-04 04:01:57,784 train 150 8.201821e-03 0.723943
2019-12-04 04:02:12,526 train 200 8.030823e-03 0.732479
2019-12-04 04:02:19,157 training loss; R2: 7.861948e-03 0.735314
2019-12-04 04:02:19,298 valid 000 7.345634e-03 0.855871
2019-12-04 04:02:20,850 validation loss; R2: 5.377422e-03 0.820043
2019-12-04 04:02:20,884 epoch 10 lr 2.000000e-03
2019-12-04 04:02:21,263 train 000 6.389561e-03 0.697819
2019-12-04 04:02:36,004 train 050 7.319249e-03 0.757598
2019-12-04 04:02:50,745 train 100 7.389433e-03 0.753331
2019-12-04 04:03:05,487 train 150 7.302590e-03 0.749813
2019-12-04 04:03:20,236 train 200 7.176817e-03 0.754775
2019-12-04 04:03:26,866 training loss; R2: 7.261075e-03 0.756992
2019-12-04 04:03:27,010 valid 000 4.219220e-03 0.849162
2019-12-04 04:03:28,561 validation loss; R2: 4.390325e-03 0.852181
2019-12-04 04:03:28,589 epoch 11 lr 2.000000e-03
2019-12-04 04:03:28,968 train 000 1.019363e-02 0.597041
2019-12-04 04:03:43,707 train 050 7.197431e-03 0.767144
2019-12-04 04:03:58,449 train 100 6.827932e-03 0.771022
2019-12-04 04:04:13,189 train 150 6.835618e-03 0.768002
2019-12-04 04:04:27,930 train 200 6.686635e-03 0.772353
2019-12-04 04:04:34,561 training loss; R2: 6.825644e-03 0.768404
2019-12-04 04:04:34,712 valid 000 3.145641e-03 0.873343
2019-12-04 04:04:36,264 validation loss; R2: 3.753269e-03 0.875363
2019-12-04 04:04:36,290 epoch 12 lr 2.000000e-03
2019-12-04 04:04:36,666 train 000 6.268719e-03 0.478178
2019-12-04 04:04:51,394 train 050 7.917234e-03 0.736509
2019-12-04 04:05:06,123 train 100 7.306847e-03 0.759230
2019-12-04 04:05:20,848 train 150 7.374316e-03 0.761565
2019-12-04 04:05:35,577 train 200 7.214036e-03 0.767578
2019-12-04 04:05:42,206 training loss; R2: 7.191410e-03 0.765810
2019-12-04 04:05:42,350 valid 000 3.368404e-03 0.894698
2019-12-04 04:05:43,902 validation loss; R2: 4.064192e-03 0.867830
2019-12-04 04:05:43,928 epoch 13 lr 2.000000e-03
2019-12-04 04:05:44,308 train 000 3.664701e-03 0.860731
2019-12-04 04:05:59,036 train 050 5.808713e-03 0.794385
2019-12-04 04:06:13,773 train 100 5.846463e-03 0.796770
2019-12-04 04:06:28,503 train 150 6.025916e-03 0.801813
2019-12-04 04:06:43,227 train 200 6.096017e-03 0.796359
2019-12-04 04:06:49,847 training loss; R2: 6.161730e-03 0.797367
2019-12-04 04:06:49,991 valid 000 1.391495e-02 0.663956
2019-12-04 04:06:51,542 validation loss; R2: 1.157553e-02 0.586154
2019-12-04 04:06:51,570 epoch 14 lr 2.000000e-03
2019-12-04 04:06:51,950 train 000 4.840086e-03 0.828149
2019-12-04 04:07:06,676 train 050 7.568234e-03 0.761750
2019-12-04 04:07:21,393 train 100 7.152176e-03 0.773460
2019-12-04 04:07:36,110 train 150 6.851163e-03 0.773859
2019-12-04 04:07:50,827 train 200 6.713282e-03 0.774392
2019-12-04 04:07:57,448 training loss; R2: 6.715980e-03 0.772709
2019-12-04 04:07:57,592 valid 000 3.261706e-02 0.137039
2019-12-04 04:07:59,142 validation loss; R2: 2.813382e-02 0.086198
2019-12-04 04:07:59,169 epoch 15 lr 2.000000e-03
2019-12-04 04:07:59,548 train 000 7.856394e-03 0.743380
2019-12-04 04:08:14,259 train 050 6.195340e-03 0.783341
2019-12-04 04:08:28,967 train 100 6.187927e-03 0.786410
2019-12-04 04:08:43,677 train 150 6.573714e-03 0.783807
2019-12-04 04:08:58,385 train 200 6.505467e-03 0.782407
2019-12-04 04:09:05,002 training loss; R2: 6.514405e-03 0.782021
2019-12-04 04:09:05,146 valid 000 1.009759e+00 -30.096007
2019-12-04 04:09:06,697 validation loss; R2: 1.047259e+00 -38.568532
2019-12-04 04:09:06,725 epoch 16 lr 2.000000e-03
2019-12-04 04:09:07,104 train 000 3.061364e-03 0.893106
2019-12-04 04:09:21,825 train 050 6.667493e-03 0.781432
2019-12-04 04:09:36,545 train 100 6.178830e-03 0.792058
2019-12-04 04:09:51,262 train 150 6.090270e-03 0.788041
2019-12-04 04:10:05,985 train 200 6.078577e-03 0.793855
2019-12-04 04:10:12,602 training loss; R2: 6.157066e-03 0.792157
2019-12-04 04:10:12,748 valid 000 8.937788e-01 -16.089589
2019-12-04 04:10:14,299 validation loss; R2: 9.041302e-01 -32.766234
2019-12-04 04:10:14,333 epoch 17 lr 2.000000e-03
2019-12-04 04:10:14,714 train 000 7.481057e-03 0.762265
2019-12-04 04:10:29,428 train 050 5.944217e-03 0.796402
2019-12-04 04:10:44,138 train 100 6.316936e-03 0.795136
2019-12-04 04:10:58,843 train 150 6.179117e-03 0.798800
2019-12-04 04:11:13,555 train 200 5.909698e-03 0.801496
2019-12-04 04:11:20,169 training loss; R2: 5.989393e-03 0.799128
2019-12-04 04:11:20,311 valid 000 1.051021e+00 -27.860050
2019-12-04 04:11:21,862 validation loss; R2: 1.108032e+00 -40.461469
2019-12-04 04:11:21,895 epoch 18 lr 2.000000e-03
2019-12-04 04:11:22,274 train 000 5.350433e-03 0.886816
2019-12-04 04:11:36,982 train 050 6.844385e-03 0.779447
2019-12-04 04:11:51,684 train 100 6.548829e-03 0.776120
2019-12-04 04:12:06,388 train 150 6.533178e-03 0.779382
2019-12-04 04:12:21,082 train 200 6.507343e-03 0.782119
2019-12-04 04:12:27,691 training loss; R2: 6.472904e-03 0.785196
2019-12-04 04:12:27,836 valid 000 2.722821e+00 -40.787157
2019-12-04 04:12:29,385 validation loss; R2: 2.741720e+00 -98.731860
2019-12-04 04:12:29,413 epoch 19 lr 2.000000e-03
2019-12-04 04:12:29,794 train 000 1.454738e-02 0.577691
2019-12-04 04:12:44,486 train 050 6.342305e-03 0.795349
2019-12-04 04:12:59,175 train 100 5.974599e-03 0.800668
2019-12-04 04:13:13,861 train 150 6.446907e-03 0.784373
2019-12-04 04:13:28,548 train 200 6.393158e-03 0.788469
2019-12-04 04:13:35,156 training loss; R2: 6.453669e-03 0.787294
2019-12-04 04:13:35,301 valid 000 1.072421e+00 -32.009073
2019-12-04 04:13:36,850 validation loss; R2: 1.050881e+00 -38.665932
