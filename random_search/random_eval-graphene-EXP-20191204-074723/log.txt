2019-12-04 07:47:23,289 gpu device = 1
2019-12-04 07:47:23,289 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=96, cutout=False, cutout_length=16, data='../data', dataset='graphene', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name=None, gpu=1, grad_clip=5, gz_dtree=False, init_channels=36, layers=8, learning_rate=0.002, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-graphene-EXP-20191204-074723', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-12-04 07:47:26,485 param size = 1.166869MB
2019-12-04 07:47:26,487 epoch 0 lr 2.000000e-03
2019-12-04 07:47:29,125 train 000 7.162836e-01 -21.626562
2019-12-04 07:47:41,558 train 050 1.409836e-01 -3.812486
2019-12-04 07:47:53,884 train 100 8.931275e-02 -1.995421
2019-12-04 07:48:06,216 train 150 6.974242e-02 -1.319193
2019-12-04 07:48:18,545 train 200 5.907564e-02 -0.972333
2019-12-04 07:48:25,246 training loss; R2: 5.572898e-02 -0.865543
2019-12-04 07:48:25,386 valid 000 1.791658e-02 0.404042
2019-12-04 07:48:26,976 validation loss; R2: 1.650882e-02 0.487930
2019-12-04 07:48:26,998 epoch 1 lr 2.000000e-03
2019-12-04 07:48:27,402 train 000 3.035557e-02 0.310858
2019-12-04 07:48:39,731 train 050 2.307200e-02 0.222546
2019-12-04 07:48:52,063 train 100 2.310051e-02 0.236903
2019-12-04 07:49:04,397 train 150 2.193885e-02 0.275504
2019-12-04 07:49:16,728 train 200 2.106062e-02 0.308228
2019-12-04 07:49:22,272 training loss; R2: 2.072249e-02 0.322665
2019-12-04 07:49:22,410 valid 000 1.070363e-02 0.769995
2019-12-04 07:49:23,723 validation loss; R2: 1.433480e-02 0.510775
2019-12-04 07:49:23,746 epoch 2 lr 2.000000e-03
2019-12-04 07:49:24,076 train 000 1.764371e-02 0.492477
2019-12-04 07:49:36,405 train 050 1.922027e-02 0.359885
2019-12-04 07:49:48,733 train 100 1.767099e-02 0.442863
2019-12-04 07:50:01,060 train 150 1.632419e-02 0.479992
2019-12-04 07:50:13,401 train 200 1.525987e-02 0.498928
2019-12-04 07:50:18,946 training loss; R2: 1.486126e-02 0.511869
2019-12-04 07:50:19,089 valid 000 8.884863e-03 0.807478
2019-12-04 07:50:20,403 validation loss; R2: 8.433696e-03 0.718184
2019-12-04 07:50:20,425 epoch 3 lr 2.000000e-03
2019-12-04 07:50:20,755 train 000 9.111251e-03 0.598011
2019-12-04 07:50:33,089 train 050 1.171562e-02 0.634095
2019-12-04 07:50:45,415 train 100 1.205667e-02 0.614066
2019-12-04 07:50:57,741 train 150 1.206806e-02 0.607684
2019-12-04 07:51:10,063 train 200 1.175278e-02 0.611909
2019-12-04 07:51:15,609 training loss; R2: 1.194235e-02 0.600645
2019-12-04 07:51:15,747 valid 000 3.966729e-03 0.853631
2019-12-04 07:51:17,060 validation loss; R2: 6.021465e-03 0.795910
2019-12-04 07:51:17,089 epoch 4 lr 2.000000e-03
2019-12-04 07:51:17,420 train 000 1.264345e-02 0.522645
2019-12-04 07:51:29,746 train 050 1.426355e-02 0.551285
2019-12-04 07:51:42,071 train 100 1.201295e-02 0.607951
2019-12-04 07:51:54,402 train 150 1.124109e-02 0.627357
2019-12-04 07:52:06,729 train 200 1.122561e-02 0.626142
2019-12-04 07:52:12,274 training loss; R2: 1.140272e-02 0.622691
2019-12-04 07:52:12,410 valid 000 6.403809e-03 0.863667
2019-12-04 07:52:13,724 validation loss; R2: 6.400504e-03 0.791919
2019-12-04 07:52:13,748 epoch 5 lr 2.000000e-03
2019-12-04 07:52:14,080 train 000 1.265064e-02 0.645138
2019-12-04 07:52:26,411 train 050 1.188362e-02 0.633523
2019-12-04 07:52:38,738 train 100 1.178734e-02 0.618257
2019-12-04 07:52:51,066 train 150 1.089186e-02 0.632373
2019-12-04 07:53:03,395 train 200 1.032354e-02 0.648946
2019-12-04 07:53:08,940 training loss; R2: 1.032680e-02 0.653601
2019-12-04 07:53:09,073 valid 000 5.523928e-03 0.838027
2019-12-04 07:53:10,387 validation loss; R2: 5.390985e-03 0.813156
2019-12-04 07:53:10,410 epoch 6 lr 2.000000e-03
2019-12-04 07:53:10,755 train 000 1.128146e-02 0.815306
2019-12-04 07:53:23,083 train 050 9.293156e-03 0.704362
2019-12-04 07:53:35,410 train 100 8.845380e-03 0.693352
2019-12-04 07:53:47,739 train 150 8.686203e-03 0.705196
2019-12-04 07:54:00,065 train 200 8.720609e-03 0.706406
2019-12-04 07:54:05,607 training loss; R2: 8.823770e-03 0.705436
2019-12-04 07:54:05,743 valid 000 8.255366e-03 0.521317
2019-12-04 07:54:07,057 validation loss; R2: 6.714250e-03 0.764995
2019-12-04 07:54:07,082 epoch 7 lr 2.000000e-03
2019-12-04 07:54:07,416 train 000 5.831950e-03 0.714525
2019-12-04 07:54:19,741 train 050 7.233596e-03 0.748879
2019-12-04 07:54:32,064 train 100 7.851367e-03 0.745530
2019-12-04 07:54:44,396 train 150 8.068619e-03 0.731536
2019-12-04 07:54:56,723 train 200 8.027306e-03 0.733209
2019-12-04 07:55:02,265 training loss; R2: 8.021808e-03 0.734528
2019-12-04 07:55:02,403 valid 000 3.379098e-03 0.840643
2019-12-04 07:55:03,716 validation loss; R2: 4.256336e-03 0.855266
2019-12-04 07:55:03,740 epoch 8 lr 2.000000e-03
2019-12-04 07:55:04,074 train 000 5.364243e-03 0.847334
2019-12-04 07:55:16,396 train 050 7.557691e-03 0.741231
2019-12-04 07:55:28,718 train 100 7.438416e-03 0.742807
2019-12-04 07:55:41,037 train 150 7.606552e-03 0.747063
2019-12-04 07:55:53,357 train 200 7.541854e-03 0.750157
2019-12-04 07:55:58,895 training loss; R2: 7.474783e-03 0.751397
2019-12-04 07:55:59,030 valid 000 2.352988e-03 0.930542
2019-12-04 07:56:00,343 validation loss; R2: 3.655584e-03 0.875462
2019-12-04 07:56:00,365 epoch 9 lr 2.000000e-03
2019-12-04 07:56:00,698 train 000 5.567989e-03 0.892244
2019-12-04 07:56:13,016 train 050 6.559190e-03 0.784030
2019-12-04 07:56:25,340 train 100 7.063424e-03 0.753227
2019-12-04 07:56:37,663 train 150 7.337783e-03 0.741325
2019-12-04 07:56:49,979 train 200 7.181783e-03 0.745350
2019-12-04 07:56:55,519 training loss; R2: 7.317885e-03 0.746679
2019-12-04 07:56:55,655 valid 000 4.845377e-03 0.838503
2019-12-04 07:56:56,968 validation loss; R2: 6.654710e-03 0.783196
2019-12-04 07:56:56,991 epoch 10 lr 2.000000e-03
2019-12-04 07:56:57,324 train 000 5.583910e-03 0.828504
2019-12-04 07:57:09,647 train 050 7.269256e-03 0.746442
2019-12-04 07:57:21,967 train 100 7.134404e-03 0.760527
2019-12-04 07:57:34,287 train 150 6.951929e-03 0.763293
2019-12-04 07:57:46,617 train 200 7.248125e-03 0.759660
2019-12-04 07:57:52,161 training loss; R2: 7.281608e-03 0.758721
2019-12-04 07:57:52,297 valid 000 4.794959e-03 0.872494
2019-12-04 07:57:53,610 validation loss; R2: 5.988067e-03 0.786306
2019-12-04 07:57:53,634 epoch 11 lr 2.000000e-03
2019-12-04 07:57:53,966 train 000 6.057266e-03 0.839257
2019-12-04 07:58:06,281 train 050 8.144119e-03 0.749102
2019-12-04 07:58:18,599 train 100 7.469832e-03 0.754123
2019-12-04 07:58:30,912 train 150 7.346132e-03 0.751553
2019-12-04 07:58:43,230 train 200 7.186171e-03 0.755590
2019-12-04 07:58:48,767 training loss; R2: 7.130648e-03 0.759753
2019-12-04 07:58:48,908 valid 000 3.596106e-03 0.852461
2019-12-04 07:58:50,222 validation loss; R2: 3.563744e-03 0.878540
2019-12-04 07:58:50,246 epoch 12 lr 2.000000e-03
2019-12-04 07:58:50,580 train 000 4.873321e-03 0.800563
2019-12-04 07:59:02,897 train 050 8.114313e-03 0.725322
2019-12-04 07:59:15,209 train 100 7.057781e-03 0.757401
2019-12-04 07:59:27,522 train 150 6.776861e-03 0.769747
2019-12-04 07:59:39,837 train 200 6.643624e-03 0.773945
2019-12-04 07:59:45,376 training loss; R2: 6.570987e-03 0.776986
2019-12-04 07:59:45,513 valid 000 2.384185e-03 0.921662
2019-12-04 07:59:46,826 validation loss; R2: 3.667530e-03 0.879862
2019-12-04 07:59:46,850 epoch 13 lr 2.000000e-03
2019-12-04 07:59:47,184 train 000 6.182797e-03 0.836003
2019-12-04 07:59:59,501 train 050 6.736798e-03 0.784351
2019-12-04 08:00:11,810 train 100 6.324992e-03 0.781126
2019-12-04 08:00:24,121 train 150 6.405795e-03 0.784247
2019-12-04 08:00:36,432 train 200 6.515591e-03 0.780662
2019-12-04 08:00:41,967 training loss; R2: 6.540835e-03 0.782503
2019-12-04 08:00:42,104 valid 000 3.746893e-03 0.865454
2019-12-04 08:00:43,417 validation loss; R2: 4.614148e-03 0.850252
2019-12-04 08:00:43,439 epoch 14 lr 2.000000e-03
2019-12-04 08:00:43,770 train 000 5.596103e-03 0.866240
2019-12-04 08:00:56,078 train 050 6.007297e-03 0.805375
2019-12-04 08:01:08,383 train 100 5.848984e-03 0.806840
2019-12-04 08:01:20,688 train 150 5.740858e-03 0.803407
2019-12-04 08:01:32,996 train 200 5.935626e-03 0.797126
2019-12-04 08:01:38,532 training loss; R2: 6.025464e-03 0.795521
2019-12-04 08:01:38,669 valid 000 2.652577e-03 0.937628
2019-12-04 08:01:39,982 validation loss; R2: 3.649282e-03 0.877319
2019-12-04 08:01:40,005 epoch 15 lr 2.000000e-03
2019-12-04 08:01:40,336 train 000 9.794790e-03 0.837014
2019-12-04 08:01:52,647 train 050 6.659502e-03 0.775363
2019-12-04 08:02:04,964 train 100 6.607623e-03 0.768007
2019-12-04 08:02:17,430 train 150 6.302051e-03 0.782525
2019-12-04 08:02:30,055 train 200 6.273164e-03 0.788202
2019-12-04 08:02:35,731 training loss; R2: 6.204705e-03 0.789904
2019-12-04 08:02:35,866 valid 000 2.397207e-03 0.901038
2019-12-04 08:02:37,181 validation loss; R2: 3.641987e-03 0.879025
2019-12-04 08:02:37,204 epoch 16 lr 2.000000e-03
2019-12-04 08:02:37,544 train 000 3.205908e-03 0.787598
2019-12-04 08:02:49,962 train 050 6.359006e-03 0.790859
2019-12-04 08:03:02,269 train 100 6.067881e-03 0.794176
2019-12-04 08:03:14,584 train 150 6.413569e-03 0.792088
2019-12-04 08:03:26,898 train 200 6.295593e-03 0.789201
2019-12-04 08:03:32,427 training loss; R2: 6.402925e-03 0.785831
2019-12-04 08:03:32,580 valid 000 2.594175e-03 0.838312
2019-12-04 08:03:33,892 validation loss; R2: 3.505915e-03 0.883425
2019-12-04 08:03:33,916 epoch 17 lr 2.000000e-03
2019-12-04 08:03:34,249 train 000 5.869146e-03 0.610650
2019-12-04 08:03:46,550 train 050 6.135177e-03 0.797701
2019-12-04 08:03:58,847 train 100 5.888015e-03 0.798608
2019-12-04 08:04:11,148 train 150 6.508217e-03 0.789408
2019-12-04 08:04:23,442 train 200 6.377696e-03 0.786765
2019-12-04 08:04:28,975 training loss; R2: 6.295591e-03 0.788088
2019-12-04 08:04:29,122 valid 000 5.112740e-03 0.883647
2019-12-04 08:04:30,434 validation loss; R2: 4.141977e-03 0.866117
2019-12-04 08:04:30,457 epoch 18 lr 2.000000e-03
2019-12-04 08:04:30,788 train 000 4.342189e-03 0.765307
2019-12-04 08:04:43,083 train 050 5.575589e-03 0.800579
2019-12-04 08:04:55,374 train 100 6.431985e-03 0.794788
2019-12-04 08:05:07,667 train 150 6.436969e-03 0.791233
2019-12-04 08:05:19,955 train 200 6.156938e-03 0.795750
2019-12-04 08:05:25,482 training loss; R2: 6.165749e-03 0.796378
2019-12-04 08:05:25,621 valid 000 2.182956e-03 0.928042
2019-12-04 08:05:26,934 validation loss; R2: 2.961261e-03 0.897146
2019-12-04 08:05:26,958 epoch 19 lr 2.000000e-03
2019-12-04 08:05:27,292 train 000 7.002092e-03 0.834247
2019-12-04 08:05:39,580 train 050 7.015605e-03 0.785421
2019-12-04 08:05:51,860 train 100 6.280403e-03 0.792438
2019-12-04 08:06:04,149 train 150 6.130787e-03 0.798276
2019-12-04 08:06:16,432 train 200 6.140431e-03 0.794694
2019-12-04 08:06:21,954 training loss; R2: 5.991549e-03 0.797722
2019-12-04 08:06:22,091 valid 000 2.702224e-03 0.876510
2019-12-04 08:06:23,403 validation loss; R2: 2.578296e-03 0.900446
