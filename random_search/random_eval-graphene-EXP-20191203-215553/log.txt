2019-12-03 21:55:53,876 gpu device = 1
2019-12-03 21:55:53,876 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=96, cutout=False, cutout_length=16, data='../data', dataset='graphene', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name=None, gpu=1, grad_clip=5, gz_dtree=False, init_channels=36, layers=8, learning_rate=0.002, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-graphene-EXP-20191203-215553', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-12-03 21:55:57,100 param size = 1.393525MB
2019-12-03 21:55:57,104 epoch 0 lr 2.000000e-03
2019-12-03 21:55:59,965 train 000 3.179140e-01 -8.442444
2019-12-03 21:56:15,490 train 050 2.183457e-01 -10.695400
2019-12-03 21:56:30,758 train 100 1.275286e-01 -5.427098
2019-12-03 21:56:46,047 train 150 9.537589e-02 -3.620970
2019-12-03 21:57:01,331 train 200 7.897389e-02 -2.723197
2019-12-03 21:57:09,361 training loss; R2: 7.354052e-02 -2.427337
2019-12-03 21:57:09,515 valid 000 9.273659e-03 0.446993
2019-12-03 21:57:11,393 validation loss; R2: 1.580689e-02 0.511785
2019-12-03 21:57:11,420 epoch 1 lr 2.000000e-03
2019-12-03 21:57:12,041 train 000 2.226584e-02 0.357474
2019-12-03 21:57:27,368 train 050 1.984584e-02 0.329846
2019-12-03 21:57:42,676 train 100 2.072078e-02 0.359712
2019-12-03 21:57:57,988 train 150 1.960594e-02 0.382130
2019-12-03 21:58:13,305 train 200 1.849388e-02 0.409629
2019-12-03 21:58:20,197 training loss; R2: 1.839340e-02 0.412763
2019-12-03 21:58:20,351 valid 000 8.811343e-03 0.732500
2019-12-03 21:58:21,893 validation loss; R2: 1.117486e-02 0.629956
2019-12-03 21:58:21,922 epoch 2 lr 2.000000e-03
2019-12-03 21:58:22,313 train 000 1.497681e-02 0.429615
2019-12-03 21:58:37,611 train 050 1.443827e-02 0.532262
2019-12-03 21:58:52,914 train 100 1.426201e-02 0.547811
2019-12-03 21:59:08,214 train 150 1.400741e-02 0.556654
2019-12-03 21:59:23,513 train 200 1.381281e-02 0.562537
2019-12-03 21:59:30,397 training loss; R2: 1.366719e-02 0.557196
2019-12-03 21:59:30,538 valid 000 3.586591e-03 0.750806
2019-12-03 21:59:32,081 validation loss; R2: 7.357983e-03 0.764296
2019-12-03 21:59:32,110 epoch 3 lr 2.000000e-03
2019-12-03 21:59:32,497 train 000 7.124168e-03 0.589739
2019-12-03 21:59:47,798 train 050 1.187428e-02 0.617055
2019-12-03 22:00:03,095 train 100 1.105923e-02 0.626917
2019-12-03 22:00:18,397 train 150 1.160853e-02 0.627995
2019-12-03 22:00:33,694 train 200 1.118536e-02 0.633157
2019-12-03 22:00:40,579 training loss; R2: 1.096504e-02 0.640884
2019-12-03 22:00:40,727 valid 000 8.811056e-03 0.811322
2019-12-03 22:00:42,270 validation loss; R2: 7.020715e-03 0.771607
2019-12-03 22:00:42,299 epoch 4 lr 2.000000e-03
2019-12-03 22:00:42,698 train 000 1.207557e-02 0.720802
2019-12-03 22:00:58,021 train 050 9.648225e-03 0.682421
2019-12-03 22:01:13,343 train 100 9.373293e-03 0.697910
2019-12-03 22:01:28,657 train 150 9.500665e-03 0.679098
2019-12-03 22:01:43,981 train 200 8.984587e-03 0.694296
2019-12-03 22:01:50,706 training loss; R2: 9.020457e-03 0.693259
2019-12-03 22:01:50,850 valid 000 5.679788e-03 0.740519
2019-12-03 22:01:52,398 validation loss; R2: 6.678374e-03 0.778908
2019-12-03 22:01:52,425 epoch 5 lr 2.000000e-03
2019-12-03 22:01:52,816 train 000 1.511328e-02 0.725513
2019-12-03 22:02:07,816 train 050 8.133679e-03 0.732018
2019-12-03 22:02:22,719 train 100 8.313713e-03 0.721624
2019-12-03 22:02:37,615 train 150 8.035843e-03 0.728490
2019-12-03 22:02:52,508 train 200 7.986670e-03 0.726961
2019-12-03 22:02:59,207 training loss; R2: 8.010173e-03 0.728224
2019-12-03 22:02:59,350 valid 000 3.504271e-03 0.862190
2019-12-03 22:03:00,892 validation loss; R2: 4.127120e-03 0.860580
2019-12-03 22:03:00,926 epoch 6 lr 2.000000e-03
2019-12-03 22:03:01,307 train 000 4.958397e-03 0.859001
2019-12-03 22:03:16,217 train 050 7.539088e-03 0.735872
2019-12-03 22:03:31,123 train 100 7.658128e-03 0.737385
2019-12-03 22:03:46,033 train 150 7.571667e-03 0.743819
2019-12-03 22:04:00,933 train 200 7.700043e-03 0.741835
2019-12-03 22:04:07,636 training loss; R2: 7.687861e-03 0.742485
2019-12-03 22:04:07,778 valid 000 1.764305e-02 0.617653
2019-12-03 22:04:09,319 validation loss; R2: 1.180447e-02 0.622463
2019-12-03 22:04:09,347 epoch 7 lr 2.000000e-03
2019-12-03 22:04:09,729 train 000 2.878789e-03 0.809818
2019-12-03 22:04:24,635 train 050 6.957900e-03 0.748719
2019-12-03 22:04:39,544 train 100 7.303263e-03 0.749912
2019-12-03 22:04:54,451 train 150 7.318212e-03 0.748002
2019-12-03 22:05:09,357 train 200 7.195677e-03 0.749944
2019-12-03 22:05:16,062 training loss; R2: 7.107037e-03 0.754445
2019-12-03 22:05:16,205 valid 000 7.428047e-03 0.843865
2019-12-03 22:05:17,747 validation loss; R2: 4.566226e-03 0.844903
2019-12-03 22:05:17,776 epoch 8 lr 2.000000e-03
2019-12-03 22:05:18,158 train 000 1.760251e-02 0.695837
2019-12-03 22:05:33,076 train 050 7.515898e-03 0.767440
2019-12-03 22:05:47,994 train 100 7.138778e-03 0.763873
2019-12-03 22:06:02,896 train 150 6.925033e-03 0.763889
2019-12-03 22:06:17,785 train 200 7.030309e-03 0.762067
2019-12-03 22:06:24,483 training loss; R2: 7.043460e-03 0.764614
2019-12-03 22:06:24,624 valid 000 8.831662e-03 0.885352
2019-12-03 22:06:26,166 validation loss; R2: 4.433035e-03 0.845561
2019-12-03 22:06:26,199 epoch 9 lr 2.000000e-03
2019-12-03 22:06:26,581 train 000 5.704233e-03 0.811508
2019-12-03 22:06:41,492 train 050 6.208140e-03 0.792513
2019-12-03 22:06:56,401 train 100 6.651443e-03 0.774892
2019-12-03 22:07:11,313 train 150 6.534736e-03 0.778823
2019-12-03 22:07:26,226 train 200 6.454579e-03 0.780905
2019-12-03 22:07:32,935 training loss; R2: 6.537743e-03 0.778147
2019-12-03 22:07:33,075 valid 000 4.719368e-03 0.846979
2019-12-03 22:07:34,617 validation loss; R2: 4.336131e-03 0.846056
2019-12-03 22:07:34,646 epoch 10 lr 2.000000e-03
2019-12-03 22:07:35,030 train 000 7.371654e-03 0.812955
2019-12-03 22:07:49,930 train 050 6.635820e-03 0.787342
2019-12-03 22:08:04,830 train 100 6.241854e-03 0.792019
2019-12-03 22:08:19,724 train 150 6.246624e-03 0.790739
2019-12-03 22:08:34,623 train 200 6.336350e-03 0.789697
2019-12-03 22:08:41,328 training loss; R2: 6.272600e-03 0.791675
2019-12-03 22:08:41,471 valid 000 3.990074e-03 0.832228
2019-12-03 22:08:43,013 validation loss; R2: 4.293250e-03 0.838832
2019-12-03 22:08:43,042 epoch 11 lr 2.000000e-03
2019-12-03 22:08:43,424 train 000 8.455657e-03 0.635689
2019-12-03 22:08:58,313 train 050 5.716127e-03 0.797001
2019-12-03 22:09:13,202 train 100 5.569147e-03 0.801662
2019-12-03 22:09:28,087 train 150 5.815215e-03 0.804891
2019-12-03 22:09:42,973 train 200 6.282219e-03 0.794761
2019-12-03 22:09:49,668 training loss; R2: 6.187740e-03 0.795268
2019-12-03 22:09:49,809 valid 000 6.730348e-03 0.759484
2019-12-03 22:09:51,351 validation loss; R2: 4.995551e-03 0.836345
2019-12-03 22:09:51,380 epoch 12 lr 2.000000e-03
2019-12-03 22:09:51,761 train 000 5.268211e-03 0.864696
2019-12-03 22:10:06,641 train 050 6.596885e-03 0.783405
2019-12-03 22:10:21,514 train 100 6.314290e-03 0.787741
2019-12-03 22:10:36,394 train 150 6.430621e-03 0.780085
2019-12-03 22:10:51,273 train 200 6.220410e-03 0.789423
2019-12-03 22:10:57,965 training loss; R2: 6.329951e-03 0.789679
2019-12-03 22:10:58,109 valid 000 3.847822e-03 0.790558
2019-12-03 22:10:59,653 validation loss; R2: 7.045813e-03 0.774210
2019-12-03 22:10:59,681 epoch 13 lr 2.000000e-03
2019-12-03 22:11:00,063 train 000 6.558725e-03 0.791117
2019-12-03 22:11:14,958 train 050 6.505664e-03 0.782089
2019-12-03 22:11:29,848 train 100 6.103074e-03 0.794626
2019-12-03 22:11:44,742 train 150 5.963470e-03 0.796803
2019-12-03 22:11:59,635 train 200 5.907319e-03 0.799178
2019-12-03 22:12:06,334 training loss; R2: 5.867420e-03 0.799804
2019-12-03 22:12:06,476 valid 000 3.220236e-02 0.064211
2019-12-03 22:12:08,018 validation loss; R2: 2.955050e-02 -0.069976
2019-12-03 22:12:08,047 epoch 14 lr 2.000000e-03
2019-12-03 22:12:08,428 train 000 3.265823e-03 0.875662
2019-12-03 22:12:23,320 train 050 5.380878e-03 0.814778
2019-12-03 22:12:38,211 train 100 5.476926e-03 0.815269
2019-12-03 22:12:53,093 train 150 5.506349e-03 0.810115
2019-12-03 22:13:07,983 train 200 5.618004e-03 0.806150
2019-12-03 22:13:14,680 training loss; R2: 5.568703e-03 0.805945
2019-12-03 22:13:14,825 valid 000 2.914998e-03 0.889649
2019-12-03 22:13:16,367 validation loss; R2: 4.278861e-03 0.854904
2019-12-03 22:13:16,396 epoch 15 lr 2.000000e-03
2019-12-03 22:13:16,779 train 000 8.482416e-03 0.790544
2019-12-03 22:13:31,662 train 050 5.655748e-03 0.807854
2019-12-03 22:13:46,538 train 100 5.136626e-03 0.819075
2019-12-03 22:14:01,410 train 150 5.159823e-03 0.822190
2019-12-03 22:14:16,282 train 200 5.323878e-03 0.819034
2019-12-03 22:14:22,972 training loss; R2: 5.475721e-03 0.814027
2019-12-03 22:14:23,114 valid 000 4.866573e-03 0.884910
2019-12-03 22:14:24,656 validation loss; R2: 3.839936e-03 0.869104
2019-12-03 22:14:24,685 epoch 16 lr 2.000000e-03
2019-12-03 22:14:25,068 train 000 3.706722e-03 0.766722
2019-12-03 22:14:39,947 train 050 6.078148e-03 0.795744
2019-12-03 22:14:54,831 train 100 5.889620e-03 0.795925
2019-12-03 22:15:09,708 train 150 5.889038e-03 0.802196
2019-12-03 22:15:24,583 train 200 5.939008e-03 0.798547
2019-12-03 22:15:31,276 training loss; R2: 5.897027e-03 0.798428
2019-12-03 22:15:31,417 valid 000 4.368343e-03 0.753028
2019-12-03 22:15:32,959 validation loss; R2: 3.811263e-03 0.866507
2019-12-03 22:15:32,987 epoch 17 lr 2.000000e-03
2019-12-03 22:15:33,373 train 000 5.413634e-03 0.793773
2019-12-03 22:15:48,252 train 050 5.596427e-03 0.805906
2019-12-03 22:16:03,128 train 100 5.281528e-03 0.823661
2019-12-03 22:16:18,004 train 150 5.360508e-03 0.821706
2019-12-03 22:16:32,882 train 200 5.349374e-03 0.816335
2019-12-03 22:16:39,574 training loss; R2: 5.379487e-03 0.815882
2019-12-03 22:16:39,715 valid 000 1.763648e-02 0.682445
2019-12-03 22:16:41,257 validation loss; R2: 1.124213e-02 0.627250
2019-12-03 22:16:41,285 epoch 18 lr 2.000000e-03
2019-12-03 22:16:41,665 train 000 4.495427e-03 0.873943
2019-12-03 22:16:56,533 train 050 5.761768e-03 0.802051
2019-12-03 22:17:11,403 train 100 5.500647e-03 0.817223
2019-12-03 22:17:26,268 train 150 5.351216e-03 0.819935
2019-12-03 22:17:41,133 train 200 5.328475e-03 0.817936
2019-12-03 22:17:47,823 training loss; R2: 5.344789e-03 0.816672
2019-12-03 22:17:47,965 valid 000 2.290562e-03 0.856516
2019-12-03 22:17:49,507 validation loss; R2: 2.896564e-03 0.898374
2019-12-03 22:17:49,535 epoch 19 lr 2.000000e-03
2019-12-03 22:17:49,916 train 000 4.542940e-03 0.898586
2019-12-03 22:18:04,785 train 050 5.458854e-03 0.818564
2019-12-03 22:18:19,648 train 100 5.358955e-03 0.818025
2019-12-03 22:18:34,521 train 150 5.527910e-03 0.820885
2019-12-03 22:18:49,377 train 200 5.562631e-03 0.815380
2019-12-03 22:18:56,062 training loss; R2: 5.474965e-03 0.815550
2019-12-03 22:18:56,205 valid 000 1.941059e-03 0.902761
2019-12-03 22:18:57,747 validation loss; R2: 4.024674e-03 0.865192
