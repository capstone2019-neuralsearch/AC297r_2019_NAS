2019-11-13 10:52:42,830 gpu device = 1
2019-11-13 10:52:42,830 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name='galaxy-zoo-cleaned', gpu=1, grad_clip=5, gz_dtree=False, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-galaxy-zoo-EXP-20191113-105242', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-11-13 10:52:54,351 param size = 0.292437MB
2019-11-13 10:52:54,355 epoch 0 lr 1.000000e-03
2019-11-13 10:52:56,594 train 000 1.524372e+00 -795.819713
2019-11-13 10:53:04,015 train 050 8.779139e-02 -37.422031
2019-11-13 10:53:11,266 train 100 5.966125e-02 -19.732552
2019-11-13 10:53:18,623 train 150 4.928337e-02 -13.799108
2019-11-13 10:53:25,866 train 200 4.361665e-02 -10.548286
2019-11-13 10:53:33,103 train 250 4.007520e-02 -8.564349
2019-11-13 10:53:40,350 train 300 3.757790e-02 -7.219252
2019-11-13 10:53:47,575 train 350 3.582874e-02 -6.325225
2019-11-13 10:53:54,786 train 400 3.437565e-02 -5.579420
2019-11-13 10:54:02,096 train 450 3.322366e-02 -5.000248
2019-11-13 10:54:09,520 train 500 3.235149e-02 -4.531935
2019-11-13 10:54:16,850 train 550 3.158471e-02 -4.146544
2019-11-13 10:54:24,040 train 600 3.093111e-02 -3.840489
2019-11-13 10:54:31,255 train 650 3.037197e-02 -3.558974
2019-11-13 10:54:38,419 train 700 2.987264e-02 -3.317967
2019-11-13 10:54:45,597 train 750 2.942912e-02 -3.112820
2019-11-13 10:54:52,905 train 800 2.897796e-02 -2.933700
2019-11-13 10:55:00,107 train 850 2.860628e-02 -2.771085
2019-11-13 10:55:03,052 training loss; R2: 2.849711e-02 -2.728174
2019-11-13 10:55:03,372 valid 000 2.130179e-02 0.009697
2019-11-13 10:55:05,082 valid 050 2.015123e-02 0.028619
2019-11-13 10:55:06,720 validation loss; R2: 2.034574e-02 -0.019869
2019-11-13 10:55:06,737 epoch 1 lr 1.000000e-03
2019-11-13 10:55:07,303 train 000 2.068613e-02 0.007246
2019-11-13 10:55:14,487 train 050 2.194655e-02 -0.094691
2019-11-13 10:55:21,654 train 100 2.170355e-02 -0.114112
2019-11-13 10:55:28,820 train 150 2.160600e-02 -0.098045
2019-11-13 10:55:35,991 train 200 2.151201e-02 -0.098896
2019-11-13 10:55:43,168 train 250 2.145382e-02 -0.101371
2019-11-13 10:55:50,344 train 300 2.141839e-02 -0.099850
2019-11-13 10:55:57,511 train 350 2.131456e-02 -0.099898
2019-11-13 10:56:04,715 train 400 2.121697e-02 -0.087686
2019-11-13 10:56:11,895 train 450 2.107677e-02 -0.095949
2019-11-13 10:56:19,064 train 500 2.099198e-02 -0.086162
2019-11-13 10:56:26,249 train 550 2.094842e-02 -0.078355
2019-11-13 10:56:33,450 train 600 2.091752e-02 -0.078992
2019-11-13 10:56:40,630 train 650 2.081694e-02 -0.235304
2019-11-13 10:56:47,797 train 700 2.072059e-02 -0.221874
2019-11-13 10:56:54,970 train 750 2.068971e-02 -0.218012
2019-11-13 10:57:02,141 train 800 2.058783e-02 -0.202705
2019-11-13 10:57:09,325 train 850 2.052397e-02 -0.188572
2019-11-13 10:57:11,469 training loss; R2: 2.050644e-02 -0.184276
2019-11-13 10:57:11,790 valid 000 1.620347e-02 0.225920
2019-11-13 10:57:13,575 valid 050 1.759234e-02 0.162767
2019-11-13 10:57:15,147 validation loss; R2: 1.743788e-02 0.147683
2019-11-13 10:57:15,166 epoch 2 lr 1.000000e-03
2019-11-13 10:57:15,561 train 000 1.794744e-02 0.148972
2019-11-13 10:57:22,789 train 050 1.903845e-02 -0.007232
2019-11-13 10:57:30,029 train 100 1.920070e-02 0.012754
2019-11-13 10:57:37,209 train 150 1.918061e-02 0.017353
2019-11-13 10:57:44,385 train 200 1.905913e-02 0.026756
2019-11-13 10:57:51,566 train 250 1.905244e-02 0.033490
2019-11-13 10:57:58,758 train 300 1.904482e-02 0.040653
2019-11-13 10:58:05,930 train 350 1.898535e-02 0.041823
2019-11-13 10:58:13,105 train 400 1.889720e-02 0.047505
2019-11-13 10:58:20,281 train 450 1.884364e-02 0.040236
2019-11-13 10:58:27,466 train 500 1.884354e-02 0.042578
2019-11-13 10:58:34,641 train 550 1.880614e-02 0.045284
2019-11-13 10:58:41,817 train 600 1.876805e-02 0.050208
2019-11-13 10:58:48,997 train 650 1.873748e-02 0.038554
2019-11-13 10:58:56,182 train 700 1.865796e-02 0.041434
2019-11-13 10:59:03,359 train 750 1.860750e-02 0.044610
2019-11-13 10:59:10,537 train 800 1.859763e-02 0.045527
2019-11-13 10:59:17,713 train 850 1.855553e-02 0.049620
2019-11-13 10:59:19,860 training loss; R2: 1.854309e-02 0.050171
2019-11-13 10:59:20,163 valid 000 2.315153e-02 -0.234284
2019-11-13 10:59:21,917 valid 050 2.192258e-02 -0.006099
2019-11-13 10:59:23,475 validation loss; R2: 2.204557e-02 -0.024444
2019-11-13 10:59:23,491 epoch 3 lr 1.000000e-03
2019-11-13 10:59:23,898 train 000 1.634702e-02 0.134902
2019-11-13 10:59:31,437 train 050 1.779379e-02 0.110225
2019-11-13 10:59:38,664 train 100 1.780424e-02 0.012989
2019-11-13 10:59:46,038 train 150 1.781455e-02 -0.379304
2019-11-13 10:59:53,247 train 200 1.773622e-02 -0.299139
2019-11-13 11:00:00,467 train 250 1.771934e-02 -0.214875
2019-11-13 11:00:07,672 train 300 1.767101e-02 -0.166286
2019-11-13 11:00:14,892 train 350 1.765126e-02 -0.134298
2019-11-13 11:00:22,101 train 400 1.757383e-02 -0.105956
2019-11-13 11:00:29,309 train 450 1.752930e-02 -0.083633
2019-11-13 11:00:36,699 train 500 1.745149e-02 -0.064423
2019-11-13 11:00:43,910 train 550 1.737395e-02 -0.046770
2019-11-13 11:00:51,179 train 600 1.732236e-02 -0.031517
2019-11-13 11:00:58,387 train 650 1.723817e-02 -0.019544
2019-11-13 11:01:05,628 train 700 1.723869e-02 -0.009121
2019-11-13 11:01:12,863 train 750 1.719788e-02 -0.002763
2019-11-13 11:01:20,127 train 800 1.717938e-02 0.004552
2019-11-13 11:01:27,361 train 850 1.715991e-02 0.012397
2019-11-13 11:01:29,553 training loss; R2: 1.714702e-02 0.014010
2019-11-13 11:01:29,863 valid 000 1.890034e-02 0.163637
2019-11-13 11:01:31,609 valid 050 1.628496e-02 0.147266
2019-11-13 11:01:33,195 validation loss; R2: 1.639860e-02 0.071890
2019-11-13 11:01:33,212 epoch 4 lr 1.000000e-03
2019-11-13 11:01:33,650 train 000 1.662995e-02 0.198665
2019-11-13 11:01:40,831 train 050 1.630673e-02 0.158420
2019-11-13 11:01:48,001 train 100 1.610867e-02 0.142698
2019-11-13 11:01:55,173 train 150 1.615746e-02 -1.505161
2019-11-13 11:02:02,336 train 200 1.622383e-02 -1.095544
2019-11-13 11:02:09,499 train 250 1.624618e-02 -0.853494
2019-11-13 11:02:16,659 train 300 1.622648e-02 -0.704097
2019-11-13 11:02:23,819 train 350 1.624200e-02 -0.582715
2019-11-13 11:02:30,982 train 400 1.629464e-02 -0.491265
2019-11-13 11:02:38,154 train 450 1.626514e-02 -0.417742
2019-11-13 11:02:45,318 train 500 1.617857e-02 -0.359915
2019-11-13 11:02:52,482 train 550 1.618964e-02 -0.738085
2019-11-13 11:02:59,643 train 600 1.619592e-02 -0.662488
2019-11-13 11:03:06,813 train 650 1.614884e-02 -0.599527
2019-11-13 11:03:13,972 train 700 1.612152e-02 -0.549149
2019-11-13 11:03:21,135 train 750 1.609919e-02 -0.501588
2019-11-13 11:03:28,295 train 800 1.610492e-02 -0.459407
2019-11-13 11:03:35,509 train 850 1.608311e-02 -0.424854
2019-11-13 11:03:37,652 training loss; R2: 1.606541e-02 -0.414605
2019-11-13 11:03:37,985 valid 000 1.296646e-02 0.336562
2019-11-13 11:03:39,717 valid 050 1.495901e-02 0.254838
2019-11-13 11:03:41,284 validation loss; R2: 1.480134e-02 0.258917
2019-11-13 11:03:41,306 epoch 5 lr 1.000000e-03
2019-11-13 11:03:41,769 train 000 1.384097e-02 0.259847
2019-11-13 11:03:49,017 train 050 1.562272e-02 0.167469
2019-11-13 11:03:56,191 train 100 1.546061e-02 0.161925
2019-11-13 11:04:03,359 train 150 1.549554e-02 0.156613
2019-11-13 11:04:10,517 train 200 1.545381e-02 0.166522
2019-11-13 11:04:17,681 train 250 1.541572e-02 0.169303
2019-11-13 11:04:24,883 train 300 1.543043e-02 0.174395
2019-11-13 11:04:32,056 train 350 1.544301e-02 0.174563
2019-11-13 11:04:39,217 train 400 1.542774e-02 0.176817
2019-11-13 11:04:46,392 train 450 1.540538e-02 0.177023
2019-11-13 11:04:53,559 train 500 1.537682e-02 0.180368
2019-11-13 11:05:00,725 train 550 1.531808e-02 0.182460
2019-11-13 11:05:07,899 train 600 1.527171e-02 0.181654
2019-11-13 11:05:15,060 train 650 1.526209e-02 0.182662
2019-11-13 11:05:22,222 train 700 1.522976e-02 0.184243
2019-11-13 11:05:29,383 train 750 1.524098e-02 0.183807
2019-11-13 11:05:36,561 train 800 1.522788e-02 -1.174541
2019-11-13 11:05:43,724 train 850 1.519244e-02 -1.093748
2019-11-13 11:05:45,870 training loss; R2: 1.518884e-02 -1.071036
2019-11-13 11:05:46,188 valid 000 1.317258e-02 0.147640
2019-11-13 11:05:47,892 valid 050 1.331406e-02 0.247508
2019-11-13 11:05:49,468 validation loss; R2: 1.322696e-02 0.251190
2019-11-13 11:05:49,484 epoch 6 lr 1.000000e-03
2019-11-13 11:05:49,895 train 000 1.302510e-02 0.208060
2019-11-13 11:05:57,277 train 050 1.442329e-02 0.199242
2019-11-13 11:06:04,620 train 100 1.452105e-02 0.199949
2019-11-13 11:06:11,910 train 150 1.462305e-02 0.206238
2019-11-13 11:06:19,187 train 200 1.457173e-02 0.210548
2019-11-13 11:06:26,437 train 250 1.456053e-02 0.213522
2019-11-13 11:06:33,659 train 300 1.454638e-02 0.210774
2019-11-13 11:06:40,863 train 350 1.457489e-02 0.205110
2019-11-13 11:06:48,066 train 400 1.458795e-02 0.205807
2019-11-13 11:06:55,266 train 450 1.455978e-02 0.207828
2019-11-13 11:07:02,462 train 500 1.457245e-02 0.206941
2019-11-13 11:07:09,659 train 550 1.457495e-02 0.208071
2019-11-13 11:07:16,856 train 600 1.454751e-02 0.211635
2019-11-13 11:07:24,052 train 650 1.456076e-02 0.212077
2019-11-13 11:07:31,246 train 700 1.454179e-02 0.207482
2019-11-13 11:07:38,443 train 750 1.452517e-02 0.208831
2019-11-13 11:07:45,719 train 800 1.450304e-02 0.211074
2019-11-13 11:07:52,930 train 850 1.450221e-02 0.210844
2019-11-13 11:07:55,092 training loss; R2: 1.450623e-02 0.211125
2019-11-13 11:07:55,399 valid 000 1.111277e-02 -0.140961
2019-11-13 11:07:57,137 valid 050 1.310262e-02 0.196565
2019-11-13 11:07:58,708 validation loss; R2: 1.304549e-02 0.186080
2019-11-13 11:07:58,730 epoch 7 lr 1.000000e-03
2019-11-13 11:07:59,175 train 000 1.463318e-02 0.347547
2019-11-13 11:08:06,478 train 050 1.443998e-02 0.218475
2019-11-13 11:08:13,634 train 100 1.395445e-02 0.172275
2019-11-13 11:08:20,776 train 150 1.389572e-02 0.182412
2019-11-13 11:08:27,921 train 200 1.396717e-02 0.175688
2019-11-13 11:08:35,061 train 250 1.401185e-02 0.183213
2019-11-13 11:08:42,216 train 300 1.395794e-02 0.190968
2019-11-13 11:08:49,358 train 350 1.395074e-02 0.198360
2019-11-13 11:08:56,545 train 400 1.398191e-02 0.200870
2019-11-13 11:09:03,685 train 450 1.398979e-02 0.204158
2019-11-13 11:09:10,838 train 500 1.403872e-02 0.205788
2019-11-13 11:09:17,982 train 550 1.406562e-02 0.208313
2019-11-13 11:09:25,124 train 600 1.407967e-02 0.208958
2019-11-13 11:09:32,265 train 650 1.405623e-02 0.209314
2019-11-13 11:09:39,406 train 700 1.404876e-02 0.209929
2019-11-13 11:09:46,562 train 750 1.405610e-02 0.205740
2019-11-13 11:09:53,702 train 800 1.405449e-02 0.208091
2019-11-13 11:10:00,845 train 850 1.403709e-02 0.209521
2019-11-13 11:10:02,980 training loss; R2: 1.402506e-02 0.210354
2019-11-13 11:10:03,299 valid 000 1.373195e-02 0.343653
2019-11-13 11:10:04,983 valid 050 1.336724e-02 0.313939
2019-11-13 11:10:06,522 validation loss; R2: 1.316009e-02 0.308686
2019-11-13 11:10:06,539 epoch 8 lr 1.000000e-03
2019-11-13 11:10:06,960 train 000 1.489501e-02 0.221435
2019-11-13 11:10:14,429 train 050 1.384447e-02 0.227481
2019-11-13 11:10:21,785 train 100 1.387648e-02 0.219378
2019-11-13 11:10:28,989 train 150 1.386120e-02 0.220223
2019-11-13 11:10:36,193 train 200 1.382103e-02 0.225061
2019-11-13 11:10:43,395 train 250 1.371315e-02 0.226698
2019-11-13 11:10:50,608 train 300 1.362921e-02 0.224885
2019-11-13 11:10:57,811 train 350 1.369920e-02 0.157233
2019-11-13 11:11:05,001 train 400 1.377070e-02 0.166590
2019-11-13 11:11:12,203 train 450 1.371218e-02 0.176471
2019-11-13 11:11:19,390 train 500 1.366652e-02 0.180328
2019-11-13 11:11:26,578 train 550 1.368767e-02 0.186214
2019-11-13 11:11:33,770 train 600 1.370630e-02 0.192908
2019-11-13 11:11:40,964 train 650 1.368077e-02 0.193652
2019-11-13 11:11:48,174 train 700 1.367022e-02 0.196418
2019-11-13 11:11:55,405 train 750 1.366258e-02 0.199261
2019-11-13 11:12:02,578 train 800 1.365653e-02 0.201917
2019-11-13 11:12:09,747 train 850 1.364455e-02 0.203587
2019-11-13 11:12:11,892 training loss; R2: 1.364420e-02 0.203938
2019-11-13 11:12:12,209 valid 000 1.151122e-02 0.315336
2019-11-13 11:12:13,878 valid 050 1.190510e-02 0.291421
2019-11-13 11:12:15,421 validation loss; R2: 1.184617e-02 0.264242
2019-11-13 11:12:15,438 epoch 9 lr 1.000000e-03
2019-11-13 11:12:15,865 train 000 1.327429e-02 0.222853
2019-11-13 11:12:23,057 train 050 1.319378e-02 0.268715
2019-11-13 11:12:30,221 train 100 1.341914e-02 0.192163
2019-11-13 11:12:37,378 train 150 1.332029e-02 0.200683
2019-11-13 11:12:44,539 train 200 1.331524e-02 0.206713
2019-11-13 11:12:51,707 train 250 1.334983e-02 0.203164
2019-11-13 11:12:58,903 train 300 1.333437e-02 0.210299
2019-11-13 11:13:06,059 train 350 1.328001e-02 0.214842
2019-11-13 11:13:13,221 train 400 1.325255e-02 0.215268
2019-11-13 11:13:20,374 train 450 1.325070e-02 0.199632
2019-11-13 11:13:27,528 train 500 1.323085e-02 0.203790
2019-11-13 11:13:34,679 train 550 1.321815e-02 0.210739
2019-11-13 11:13:41,836 train 600 1.322662e-02 0.214380
2019-11-13 11:13:48,988 train 650 1.322527e-02 0.215664
2019-11-13 11:13:56,138 train 700 1.323985e-02 0.218231
2019-11-13 11:14:03,296 train 750 1.325366e-02 0.214111
2019-11-13 11:14:10,459 train 800 1.325060e-02 0.216037
2019-11-13 11:14:17,623 train 850 1.325825e-02 0.219984
2019-11-13 11:14:19,765 training loss; R2: 1.326701e-02 0.219725
2019-11-13 11:14:20,086 valid 000 5.170470e-02 -7.278750
2019-11-13 11:14:21,816 valid 050 4.938223e-02 -5.997596
2019-11-13 11:14:23,393 validation loss; R2: 4.931724e-02 -6.939821
2019-11-13 11:14:23,410 epoch 10 lr 1.000000e-03
2019-11-13 11:14:23,842 train 000 1.285057e-02 0.345349
2019-11-13 11:14:31,100 train 050 1.296660e-02 0.018216
2019-11-13 11:14:38,325 train 100 1.304815e-02 0.108813
2019-11-13 11:14:45,551 train 150 1.296162e-02 0.159925
2019-11-13 11:14:52,774 train 200 1.294424e-02 0.187743
2019-11-13 11:14:59,959 train 250 1.292475e-02 0.204084
2019-11-13 11:15:07,147 train 300 1.301589e-02 0.213935
2019-11-13 11:15:14,333 train 350 1.297407e-02 0.166409
2019-11-13 11:15:21,578 train 400 1.291794e-02 0.178889
2019-11-13 11:15:28,775 train 450 1.293806e-02 0.184045
2019-11-13 11:15:35,967 train 500 1.293929e-02 0.194605
2019-11-13 11:15:43,159 train 550 1.295991e-02 0.200053
2019-11-13 11:15:50,349 train 600 1.296235e-02 0.200722
2019-11-13 11:15:57,539 train 650 1.293025e-02 0.205336
2019-11-13 11:16:04,805 train 700 1.293548e-02 0.206011
2019-11-13 11:16:11,996 train 750 1.293750e-02 0.209544
2019-11-13 11:16:19,215 train 800 1.294282e-02 0.208617
2019-11-13 11:16:26,405 train 850 1.293966e-02 0.211864
2019-11-13 11:16:28,556 training loss; R2: 1.292714e-02 0.210574
2019-11-13 11:16:28,868 valid 000 2.435308e-01 -43.412832
2019-11-13 11:16:30,597 valid 050 2.433950e-01 -48.305720
2019-11-13 11:16:32,158 validation loss; R2: 2.435662e-01 -53.152711
2019-11-13 11:16:32,180 epoch 11 lr 1.000000e-03
2019-11-13 11:16:32,634 train 000 1.134946e-02 0.304747
2019-11-13 11:16:40,020 train 050 1.272040e-02 0.280142
2019-11-13 11:16:47,415 train 100 1.256307e-02 0.282602
2019-11-13 11:16:54,688 train 150 1.262773e-02 0.267325
2019-11-13 11:17:01,976 train 200 1.263859e-02 0.266401
2019-11-13 11:17:09,249 train 250 1.263560e-02 0.265286
2019-11-13 11:17:16,546 train 300 1.264737e-02 0.272689
2019-11-13 11:17:23,855 train 350 1.264793e-02 0.269340
2019-11-13 11:17:31,197 train 400 1.272648e-02 0.263037
2019-11-13 11:17:38,483 train 450 1.272275e-02 0.262387
2019-11-13 11:17:45,730 train 500 1.272372e-02 0.262313
2019-11-13 11:17:53,008 train 550 1.270703e-02 0.261817
2019-11-13 11:18:00,222 train 600 1.270853e-02 0.261949
2019-11-13 11:18:07,439 train 650 1.271973e-02 0.261663
2019-11-13 11:18:14,647 train 700 1.271536e-02 0.262745
2019-11-13 11:18:21,856 train 750 1.272865e-02 0.263438
2019-11-13 11:18:29,073 train 800 1.273868e-02 0.262796
2019-11-13 11:18:36,277 train 850 1.272272e-02 0.262449
2019-11-13 11:18:38,433 training loss; R2: 1.271632e-02 0.261321
2019-11-13 11:18:38,740 valid 000 2.666457e+00 -635.976899
2019-11-13 11:18:40,431 valid 050 2.645017e+00 -413.368261
2019-11-13 11:18:41,945 validation loss; R2: 2.641575e+00 -417.900533
2019-11-13 11:18:41,962 epoch 12 lr 1.000000e-03
2019-11-13 11:18:42,388 train 000 1.221328e-02 0.369134
2019-11-13 11:18:49,777 train 050 1.249712e-02 0.268921
2019-11-13 11:18:57,235 train 100 1.247030e-02 0.257301
2019-11-13 11:19:04,695 train 150 1.245517e-02 0.254035
2019-11-13 11:19:12,163 train 200 1.246965e-02 0.242524
2019-11-13 11:19:19,616 train 250 1.245192e-02 0.244058
2019-11-13 11:19:27,070 train 300 1.251531e-02 0.250207
2019-11-13 11:19:34,519 train 350 1.247541e-02 0.253955
2019-11-13 11:19:41,967 train 400 1.250644e-02 0.252271
2019-11-13 11:19:49,415 train 450 1.253980e-02 0.254498
2019-11-13 11:19:56,858 train 500 1.252148e-02 0.259434
2019-11-13 11:20:04,302 train 550 1.255989e-02 0.259460
2019-11-13 11:20:11,740 train 600 1.255981e-02 0.260412
2019-11-13 11:20:19,179 train 650 1.254168e-02 0.263138
2019-11-13 11:20:26,625 train 700 1.254594e-02 0.264405
2019-11-13 11:20:34,065 train 750 1.254633e-02 0.266306
2019-11-13 11:20:41,509 train 800 1.253527e-02 0.268512
2019-11-13 11:20:48,951 train 850 1.253238e-02 0.269549
2019-11-13 11:20:51,176 training loss; R2: 1.252519e-02 0.269011
2019-11-13 11:20:51,489 valid 000 5.478443e+01 -2275.036073
2019-11-13 11:20:53,164 valid 050 5.470404e+01 -2187.072950
2019-11-13 11:20:54,701 validation loss; R2: 5.469379e+01 -2140.439980
2019-11-13 11:20:54,725 epoch 13 lr 1.000000e-03
2019-11-13 11:20:55,148 train 000 1.205338e-02 0.246518
2019-11-13 11:21:02,637 train 050 1.268472e-02 0.280613
2019-11-13 11:21:10,003 train 100 1.253625e-02 0.293978
2019-11-13 11:21:17,412 train 150 1.241138e-02 0.285248
2019-11-13 11:21:24,907 train 200 1.249402e-02 0.261872
2019-11-13 11:21:32,348 train 250 1.247906e-02 0.265465
2019-11-13 11:21:39,705 train 300 1.243568e-02 0.273228
2019-11-13 11:21:46,986 train 350 1.246513e-02 -0.118931
2019-11-13 11:21:54,331 train 400 1.248042e-02 -0.076491
2019-11-13 11:22:01,790 train 450 1.247899e-02 -0.039031
2019-11-13 11:22:09,119 train 500 1.245780e-02 -0.006150
2019-11-13 11:22:16,654 train 550 1.246569e-02 0.011016
2019-11-13 11:22:23,908 train 600 1.244427e-02 0.032367
2019-11-13 11:22:31,223 train 650 1.244700e-02 0.050319
2019-11-13 11:22:38,708 train 700 1.246145e-02 0.060391
2019-11-13 11:22:46,091 train 750 1.244027e-02 0.076286
2019-11-13 11:22:53,513 train 800 1.243631e-02 0.085127
2019-11-13 11:23:00,989 train 850 1.243306e-02 0.097436
2019-11-13 11:23:03,227 training loss; R2: 1.242036e-02 0.101351
2019-11-13 11:23:03,523 valid 000 1.238236e+01 -1116.778993
2019-11-13 11:23:05,271 valid 050 1.233263e+01 -2197.343372
2019-11-13 11:23:06,781 validation loss; R2: 1.233570e+01 -2665.414828
2019-11-13 11:23:06,803 epoch 14 lr 1.000000e-03
2019-11-13 11:23:07,217 train 000 1.346469e-02 0.368549
2019-11-13 11:23:14,571 train 050 1.235407e-02 0.280329
2019-11-13 11:23:22,059 train 100 1.245081e-02 0.260741
2019-11-13 11:23:29,514 train 150 1.228632e-02 0.257141
2019-11-13 11:23:36,776 train 200 1.229302e-02 0.265001
2019-11-13 11:23:44,001 train 250 1.228738e-02 0.267882
2019-11-13 11:23:51,277 train 300 1.226370e-02 0.262924
2019-11-13 11:23:58,764 train 350 1.231039e-02 0.262899
2019-11-13 11:24:06,086 train 400 1.230609e-02 0.264257
2019-11-13 11:24:13,480 train 450 1.228169e-02 0.269400
2019-11-13 11:24:20,971 train 500 1.225074e-02 0.273196
2019-11-13 11:24:28,224 train 550 1.223101e-02 0.273281
2019-11-13 11:24:35,543 train 600 1.224420e-02 0.273966
2019-11-13 11:24:42,814 train 650 1.222578e-02 0.275636
2019-11-13 11:24:50,086 train 700 1.222732e-02 0.276418
2019-11-13 11:24:57,368 train 750 1.224262e-02 0.277569
2019-11-13 11:25:04,651 train 800 1.224643e-02 0.272952
2019-11-13 11:25:12,027 train 850 1.223730e-02 0.273997
2019-11-13 11:25:14,228 training loss; R2: 1.223473e-02 0.274599
2019-11-13 11:25:14,513 valid 000 6.931855e+00 -795.044467
2019-11-13 11:25:16,189 valid 050 6.861472e+00 -1716.458133
2019-11-13 11:25:17,722 validation loss; R2: 6.860444e+00 -1551.691079
2019-11-13 11:25:17,744 epoch 15 lr 1.000000e-03
2019-11-13 11:25:18,142 train 000 1.109846e-02 0.330623
2019-11-13 11:25:25,394 train 050 1.245287e-02 0.260445
2019-11-13 11:25:32,847 train 100 1.229121e-02 0.275611
2019-11-13 11:25:40,033 train 150 1.232007e-02 0.279775
2019-11-13 11:25:47,200 train 200 1.227288e-02 0.285644
2019-11-13 11:25:54,357 train 250 1.228246e-02 0.288243
2019-11-13 11:26:01,526 train 300 1.232050e-02 0.289317
2019-11-13 11:26:08,687 train 350 1.233927e-02 0.288863
2019-11-13 11:26:15,865 train 400 1.235424e-02 0.283775
2019-11-13 11:26:23,046 train 450 1.234048e-02 0.284337
2019-11-13 11:26:30,206 train 500 1.231119e-02 0.284959
2019-11-13 11:26:37,371 train 550 1.229368e-02 0.285895
2019-11-13 11:26:44,534 train 600 1.226605e-02 0.288185
2019-11-13 11:26:51,708 train 650 1.227167e-02 0.288354
2019-11-13 11:26:58,922 train 700 1.224909e-02 0.233754
2019-11-13 11:27:06,077 train 750 1.223777e-02 0.232968
2019-11-13 11:27:13,229 train 800 1.224573e-02 0.232568
2019-11-13 11:27:20,458 train 850 1.225323e-02 0.228560
2019-11-13 11:27:22,601 training loss; R2: 1.225927e-02 0.228491
2019-11-13 11:27:22,916 valid 000 1.358163e+01 -1166.462087
2019-11-13 11:27:24,621 valid 050 1.364669e+01 -2185.882849
2019-11-13 11:27:26,176 validation loss; R2: 1.364678e+01 -2585.880244
2019-11-13 11:27:26,194 epoch 16 lr 1.000000e-03
2019-11-13 11:27:26,617 train 000 1.493911e-02 -0.796207
2019-11-13 11:27:33,799 train 050 1.246279e-02 0.265605
2019-11-13 11:27:40,974 train 100 1.222525e-02 0.285600
2019-11-13 11:27:48,153 train 150 1.239821e-02 0.287441
2019-11-13 11:27:55,334 train 200 1.227330e-02 0.284291
2019-11-13 11:28:02,518 train 250 1.240728e-02 0.282620
2019-11-13 11:28:09,694 train 300 1.235912e-02 0.276921
2019-11-13 11:28:16,868 train 350 1.238633e-02 0.258006
2019-11-13 11:28:24,050 train 400 1.232694e-02 0.262347
2019-11-13 11:28:31,229 train 450 1.234879e-02 0.265526
2019-11-13 11:28:38,407 train 500 1.231735e-02 0.268077
2019-11-13 11:28:45,587 train 550 1.234342e-02 0.270518
2019-11-13 11:28:52,778 train 600 1.233923e-02 0.273322
2019-11-13 11:28:59,964 train 650 1.230896e-02 0.269454
2019-11-13 11:29:07,150 train 700 1.229846e-02 0.268058
2019-11-13 11:29:14,344 train 750 1.230044e-02 0.263001
2019-11-13 11:29:21,610 train 800 1.232733e-02 0.262105
2019-11-13 11:29:28,861 train 850 1.234026e-02 0.263719
2019-11-13 11:29:31,016 training loss; R2: 1.235307e-02 0.264295
2019-11-13 11:29:31,321 valid 000 3.411930e+00 -1707.991986
2019-11-13 11:29:33,045 valid 050 3.462896e+00 -1266.638963
2019-11-13 11:29:34,615 validation loss; R2: 3.463458e+00 -1118.544441
2019-11-13 11:29:34,639 epoch 17 lr 1.000000e-03
2019-11-13 11:29:35,087 train 000 1.098950e-02 0.282768
2019-11-13 11:29:42,486 train 050 1.264658e-02 0.281032
2019-11-13 11:29:49,738 train 100 1.234709e-02 0.282691
2019-11-13 11:29:57,127 train 150 1.229887e-02 0.216517
2019-11-13 11:30:04,403 train 200 1.223756e-02 0.226231
2019-11-13 11:30:11,701 train 250 1.212525e-02 0.243766
2019-11-13 11:30:19,005 train 300 1.210694e-02 0.248908
2019-11-13 11:30:26,245 train 350 1.205728e-02 0.255445
2019-11-13 11:30:33,532 train 400 1.205015e-02 0.260054
2019-11-13 11:30:40,852 train 450 1.200725e-02 -0.478661
2019-11-13 11:30:48,214 train 500 1.198020e-02 -0.401598
2019-11-13 11:30:55,559 train 550 1.197561e-02 -0.347080
2019-11-13 11:31:02,984 train 600 1.199566e-02 -0.292035
2019-11-13 11:31:10,354 train 650 1.200151e-02 -0.245546
2019-11-13 11:31:17,765 train 700 1.200980e-02 -0.207296
2019-11-13 11:31:25,112 train 750 1.201572e-02 -0.173477
2019-11-13 11:31:32,471 train 800 1.203009e-02 -0.146031
2019-11-13 11:31:39,716 train 850 1.203498e-02 -0.122277
2019-11-13 11:31:41,878 training loss; R2: 1.203035e-02 -0.115021
2019-11-13 11:31:42,225 valid 000 5.628736e-01 -49.153870
2019-11-13 11:31:43,911 valid 050 5.932412e-01 -141.942492
2019-11-13 11:31:45,482 validation loss; R2: 5.916825e-01 -116.453757
2019-11-13 11:31:45,500 epoch 18 lr 1.000000e-03
2019-11-13 11:31:45,919 train 000 1.353938e-02 0.307053
2019-11-13 11:31:53,138 train 050 1.221546e-02 0.193879
2019-11-13 11:32:00,356 train 100 1.209296e-02 0.229715
2019-11-13 11:32:07,553 train 150 1.229448e-02 0.238992
2019-11-13 11:32:14,851 train 200 1.232162e-02 0.253450
2019-11-13 11:32:22,109 train 250 1.254222e-02 0.258049
2019-11-13 11:32:29,385 train 300 1.251238e-02 0.256703
2019-11-13 11:32:36,605 train 350 1.251116e-02 0.255270
2019-11-13 11:32:43,966 train 400 1.247994e-02 0.260395
2019-11-13 11:32:51,193 train 450 1.249250e-02 0.259717
2019-11-13 11:32:58,485 train 500 1.249046e-02 0.262085
2019-11-13 11:33:05,851 train 550 1.244196e-02 0.261542
2019-11-13 11:33:13,150 train 600 1.239595e-02 0.261942
2019-11-13 11:33:20,437 train 650 1.238647e-02 0.266130
2019-11-13 11:33:27,739 train 700 1.232532e-02 0.269623
2019-11-13 11:33:34,997 train 750 1.228802e-02 0.269024
2019-11-13 11:33:42,332 train 800 1.226276e-02 0.271211
2019-11-13 11:33:49,707 train 850 1.224591e-02 0.272923
2019-11-13 11:33:51,869 training loss; R2: 1.223964e-02 0.242716
2019-11-13 11:33:52,179 valid 000 2.263831e-01 -17.595688
2019-11-13 11:33:53,911 valid 050 2.319472e-01 -14.191197
2019-11-13 11:33:55,476 validation loss; R2: 2.319497e-01 -19.287308
2019-11-13 11:33:55,499 epoch 19 lr 1.000000e-03
2019-11-13 11:33:55,919 train 000 1.681790e-02 0.299405
2019-11-13 11:34:03,360 train 050 1.248680e-02 0.284749
2019-11-13 11:34:10,637 train 100 1.219797e-02 0.302809
2019-11-13 11:34:17,915 train 150 1.212144e-02 0.299559
2019-11-13 11:34:25,169 train 200 1.198981e-02 0.303322
2019-11-13 11:34:32,464 train 250 1.212417e-02 0.299998
2019-11-13 11:34:39,990 train 300 1.216675e-02 0.293025
2019-11-13 11:34:47,306 train 350 1.230932e-02 0.290200
2019-11-13 11:34:54,737 train 400 1.226574e-02 0.290600
2019-11-13 11:35:02,210 train 450 1.229203e-02 0.290647
2019-11-13 11:35:09,558 train 500 1.224275e-02 0.289785
2019-11-13 11:35:16,722 train 550 1.225192e-02 0.288864
2019-11-13 11:35:23,877 train 600 1.225396e-02 0.289251
2019-11-13 11:35:31,030 train 650 1.224153e-02 0.288631
2019-11-13 11:35:38,192 train 700 1.223671e-02 0.289497
2019-11-13 11:35:45,345 train 750 1.227050e-02 0.233267
2019-11-13 11:35:52,709 train 800 1.225658e-02 0.234864
2019-11-13 11:36:00,161 train 850 1.224608e-02 0.237262
2019-11-13 11:36:02,389 training loss; R2: 1.223647e-02 0.238886
2019-11-13 11:36:02,720 valid 000 1.309163e+01 -564.339899
2019-11-13 11:36:04,363 valid 050 1.309362e+01 -697.780613
2019-11-13 11:36:05,894 validation loss; R2: 1.309793e+01 -710.425257
