2019-12-03 21:37:32,484 gpu device = 1
2019-12-03 21:37:32,484 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=96, cutout=False, cutout_length=16, data='../data', dataset='graphene', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name=None, gpu=1, grad_clip=5, gz_dtree=False, init_channels=36, layers=8, learning_rate=0.002, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-graphene-EXP-20191203-213732', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-12-03 21:37:35,695 param size = 1.173853MB
2019-12-03 21:37:35,698 epoch 0 lr 2.000000e-03
2019-12-03 21:37:38,286 train 000 3.784869e-01 -8.096903
2019-12-03 21:37:50,537 train 050 2.137450e-01 -5.688310
2019-12-03 21:38:02,703 train 100 1.223936e-01 -2.871794
2019-12-03 21:38:14,862 train 150 8.934366e-02 -1.821224
2019-12-03 21:38:27,031 train 200 7.237446e-02 -1.287470
2019-12-03 21:38:33,558 training loss; R2: 6.713997e-02 -1.127144
2019-12-03 21:38:33,697 valid 000 1.230423e-02 0.335721
2019-12-03 21:38:35,277 validation loss; R2: 1.650268e-02 0.473799
2019-12-03 21:38:35,300 epoch 1 lr 2.000000e-03
2019-12-03 21:38:35,697 train 000 1.639745e-02 0.370608
2019-12-03 21:38:47,876 train 050 1.829660e-02 0.419115
2019-12-03 21:39:00,057 train 100 1.773814e-02 0.419284
2019-12-03 21:39:12,240 train 150 1.622296e-02 0.471293
2019-12-03 21:39:24,424 train 200 1.512170e-02 0.494661
2019-12-03 21:39:29,926 training loss; R2: 1.464449e-02 0.510906
2019-12-03 21:39:30,053 valid 000 6.882572e-03 0.761951
2019-12-03 21:39:31,318 validation loss; R2: 7.502335e-03 0.762902
2019-12-03 21:39:31,341 epoch 2 lr 2.000000e-03
2019-12-03 21:39:31,680 train 000 8.537835e-03 0.479212
2019-12-03 21:39:43,576 train 050 9.203191e-03 0.675665
2019-12-03 21:39:55,438 train 100 9.623441e-03 0.681026
2019-12-03 21:40:07,299 train 150 9.394255e-03 0.684993
2019-12-03 21:40:19,163 train 200 9.215453e-03 0.696800
2019-12-03 21:40:24,499 training loss; R2: 9.161577e-03 0.697090
2019-12-03 21:40:24,634 valid 000 6.913743e-03 0.850333
2019-12-03 21:40:25,890 validation loss; R2: 6.127770e-03 0.772983
2019-12-03 21:40:25,913 epoch 3 lr 2.000000e-03
2019-12-03 21:40:26,232 train 000 5.616010e-03 0.776340
2019-12-03 21:40:38,096 train 050 7.715813e-03 0.748915
2019-12-03 21:40:49,959 train 100 7.632749e-03 0.742676
2019-12-03 21:41:01,822 train 150 7.673015e-03 0.747209
2019-12-03 21:41:13,687 train 200 7.503402e-03 0.751530
2019-12-03 21:41:19,023 training loss; R2: 7.480043e-03 0.752730
2019-12-03 21:41:19,152 valid 000 1.637445e-02 0.728780
2019-12-03 21:41:20,408 validation loss; R2: 6.833882e-03 0.786798
2019-12-03 21:41:20,431 epoch 4 lr 2.000000e-03
2019-12-03 21:41:20,748 train 000 8.205466e-03 0.765078
2019-12-03 21:41:32,618 train 050 7.429572e-03 0.780486
2019-12-03 21:41:44,493 train 100 6.920087e-03 0.775315
2019-12-03 21:41:56,363 train 150 6.600103e-03 0.780233
2019-12-03 21:42:08,239 train 200 6.582568e-03 0.777816
2019-12-03 21:42:13,576 training loss; R2: 6.726807e-03 0.776143
2019-12-03 21:42:13,709 valid 000 6.724066e-03 0.840522
2019-12-03 21:42:14,965 validation loss; R2: 7.428588e-03 0.726008
2019-12-03 21:42:14,994 epoch 5 lr 2.000000e-03
2019-12-03 21:42:15,315 train 000 6.814968e-03 0.663484
2019-12-03 21:42:27,179 train 050 5.826839e-03 0.790375
2019-12-03 21:42:39,048 train 100 6.810604e-03 0.760710
2019-12-03 21:42:50,917 train 150 6.686818e-03 0.769835
2019-12-03 21:43:02,786 train 200 6.458881e-03 0.779057
2019-12-03 21:43:08,122 training loss; R2: 6.398233e-03 0.781773
2019-12-03 21:43:08,259 valid 000 2.498682e-03 0.826535
2019-12-03 21:43:09,515 validation loss; R2: 4.512513e-03 0.849272
2019-12-03 21:43:09,537 epoch 6 lr 2.000000e-03
2019-12-03 21:43:09,856 train 000 6.322874e-03 0.842128
2019-12-03 21:43:21,719 train 050 6.325634e-03 0.796137
2019-12-03 21:43:33,604 train 100 5.681118e-03 0.808682
2019-12-03 21:43:45,493 train 150 5.424853e-03 0.812692
2019-12-03 21:43:57,382 train 200 5.348617e-03 0.814346
2019-12-03 21:44:02,725 training loss; R2: 5.415964e-03 0.815759
2019-12-03 21:44:02,853 valid 000 4.113088e-03 0.759584
2019-12-03 21:44:04,111 validation loss; R2: 4.726599e-03 0.826647
2019-12-03 21:44:04,134 epoch 7 lr 2.000000e-03
2019-12-03 21:44:04,451 train 000 7.683729e-03 0.469009
2019-12-03 21:44:16,314 train 050 6.995301e-03 0.751026
2019-12-03 21:44:28,176 train 100 6.555451e-03 0.773233
2019-12-03 21:44:40,050 train 150 6.420344e-03 0.778430
2019-12-03 21:44:51,916 train 200 6.266181e-03 0.785968
2019-12-03 21:44:57,253 training loss; R2: 6.130675e-03 0.789997
2019-12-03 21:44:57,392 valid 000 3.449866e-03 0.885730
2019-12-03 21:44:58,650 validation loss; R2: 3.349849e-03 0.890449
2019-12-03 21:44:58,672 epoch 8 lr 2.000000e-03
2019-12-03 21:44:58,989 train 000 4.451745e-03 0.828224
2019-12-03 21:45:10,858 train 050 5.056689e-03 0.829359
2019-12-03 21:45:22,727 train 100 4.986754e-03 0.828847
2019-12-03 21:45:34,596 train 150 4.877647e-03 0.830620
2019-12-03 21:45:46,462 train 200 4.893416e-03 0.830253
2019-12-03 21:45:51,798 training loss; R2: 4.933514e-03 0.828996
2019-12-03 21:45:51,929 valid 000 5.283892e-03 0.887143
2019-12-03 21:45:53,185 validation loss; R2: 3.608816e-03 0.877784
2019-12-03 21:45:53,207 epoch 9 lr 2.000000e-03
2019-12-03 21:45:53,526 train 000 3.942351e-03 0.853709
2019-12-03 21:46:05,391 train 050 5.342366e-03 0.824476
2019-12-03 21:46:17,255 train 100 5.240634e-03 0.823466
2019-12-03 21:46:29,120 train 150 5.132373e-03 0.823980
2019-12-03 21:46:40,987 train 200 5.017017e-03 0.824058
2019-12-03 21:46:46,328 training loss; R2: 5.018662e-03 0.824934
2019-12-03 21:46:46,458 valid 000 6.067611e-03 0.729302
2019-12-03 21:46:47,714 validation loss; R2: 6.889372e-03 0.762169
2019-12-03 21:46:47,737 epoch 10 lr 2.000000e-03
2019-12-03 21:46:48,057 train 000 4.030554e-03 0.856833
2019-12-03 21:46:59,922 train 050 4.804847e-03 0.828006
2019-12-03 21:47:11,782 train 100 4.786256e-03 0.832414
2019-12-03 21:47:23,647 train 150 4.788234e-03 0.832894
2019-12-03 21:47:35,512 train 200 4.734414e-03 0.838872
2019-12-03 21:47:40,847 training loss; R2: 4.670739e-03 0.838955
2019-12-03 21:47:40,983 valid 000 3.428696e-03 0.868579
2019-12-03 21:47:42,240 validation loss; R2: 4.091825e-03 0.856178
2019-12-03 21:47:42,263 epoch 11 lr 2.000000e-03
2019-12-03 21:47:42,583 train 000 3.758272e-03 0.900352
2019-12-03 21:47:54,450 train 050 4.499867e-03 0.848030
2019-12-03 21:48:06,317 train 100 4.409081e-03 0.846529
2019-12-03 21:48:18,183 train 150 4.549909e-03 0.848381
2019-12-03 21:48:30,052 train 200 4.539177e-03 0.845953
2019-12-03 21:48:35,390 training loss; R2: 4.498355e-03 0.847133
2019-12-03 21:48:35,522 valid 000 2.154497e-02 -0.204391
2019-12-03 21:48:36,779 validation loss; R2: 2.313482e-02 0.197236
2019-12-03 21:48:36,801 epoch 12 lr 2.000000e-03
2019-12-03 21:48:37,118 train 000 3.965189e-03 0.774318
2019-12-03 21:48:48,981 train 050 4.294024e-03 0.856867
2019-12-03 21:49:00,840 train 100 4.596425e-03 0.845446
2019-12-03 21:49:12,701 train 150 4.657827e-03 0.842018
2019-12-03 21:49:24,559 train 200 4.588199e-03 0.841033
2019-12-03 21:49:29,892 training loss; R2: 4.544579e-03 0.842887
2019-12-03 21:49:30,027 valid 000 1.776134e-01 -5.634084
2019-12-03 21:49:31,282 validation loss; R2: 1.741377e-01 -5.287958
2019-12-03 21:49:31,303 epoch 13 lr 2.000000e-03
2019-12-03 21:49:31,621 train 000 3.138861e-03 0.885211
2019-12-03 21:49:43,481 train 050 3.750592e-03 0.865541
2019-12-03 21:49:55,341 train 100 3.856153e-03 0.863135
2019-12-03 21:50:07,200 train 150 4.069015e-03 0.856018
2019-12-03 21:50:19,063 train 200 4.231621e-03 0.853269
2019-12-03 21:50:24,399 training loss; R2: 4.189782e-03 0.853558
2019-12-03 21:50:24,530 valid 000 2.231195e+00 -143.116144
2019-12-03 21:50:25,785 validation loss; R2: 2.157631e+00 -80.102279
2019-12-03 21:50:25,809 epoch 14 lr 2.000000e-03
2019-12-03 21:50:26,129 train 000 3.854842e-03 0.838070
2019-12-03 21:50:37,988 train 050 4.298362e-03 0.864148
2019-12-03 21:50:49,845 train 100 4.094062e-03 0.860075
2019-12-03 21:51:01,700 train 150 4.084030e-03 0.859590
2019-12-03 21:51:13,555 train 200 4.015787e-03 0.861128
2019-12-03 21:51:18,893 training loss; R2: 4.040454e-03 0.858473
2019-12-03 21:51:19,024 valid 000 1.117867e+00 -43.513207
2019-12-03 21:51:20,279 validation loss; R2: 1.118262e+00 -37.081371
2019-12-03 21:51:20,302 epoch 15 lr 2.000000e-03
2019-12-03 21:51:20,620 train 000 3.038102e-03 0.907948
2019-12-03 21:51:32,473 train 050 4.311596e-03 0.859346
2019-12-03 21:51:44,329 train 100 4.141687e-03 0.858212
2019-12-03 21:51:56,177 train 150 4.137038e-03 0.858213
2019-12-03 21:52:08,026 train 200 4.108488e-03 0.857886
2019-12-03 21:52:13,353 training loss; R2: 4.107745e-03 0.858216
2019-12-03 21:52:13,481 valid 000 1.670763e+01 -558.190001
2019-12-03 21:52:14,737 validation loss; R2: 1.659162e+01 -569.813520
2019-12-03 21:52:14,759 epoch 16 lr 2.000000e-03
2019-12-03 21:52:15,079 train 000 2.833209e-03 0.869630
2019-12-03 21:52:26,932 train 050 4.048084e-03 0.858775
2019-12-03 21:52:38,784 train 100 3.898647e-03 0.869886
2019-12-03 21:52:50,635 train 150 3.884682e-03 0.866215
2019-12-03 21:53:02,485 train 200 4.112419e-03 0.859754
2019-12-03 21:53:07,816 training loss; R2: 4.171449e-03 0.857550
2019-12-03 21:53:07,946 valid 000 2.317623e-01 -5.447118
2019-12-03 21:53:09,202 validation loss; R2: 2.350556e-01 -7.896932
2019-12-03 21:53:09,225 epoch 17 lr 2.000000e-03
2019-12-03 21:53:09,542 train 000 2.862522e-03 0.882818
2019-12-03 21:53:21,384 train 050 4.195434e-03 0.849308
2019-12-03 21:53:33,224 train 100 3.966451e-03 0.862906
2019-12-03 21:53:45,060 train 150 3.861517e-03 0.863792
2019-12-03 21:53:56,893 train 200 3.760294e-03 0.869033
2019-12-03 21:54:02,214 training loss; R2: 3.834950e-03 0.867810
2019-12-03 21:54:02,344 valid 000 1.839175e+00 -83.194593
2019-12-03 21:54:03,599 validation loss; R2: 1.868469e+00 -71.351853
2019-12-03 21:54:03,623 epoch 18 lr 2.000000e-03
2019-12-03 21:54:03,940 train 000 2.524992e-03 0.930807
2019-12-03 21:54:15,768 train 050 3.760909e-03 0.868285
2019-12-03 21:54:27,592 train 100 3.991900e-03 0.863282
2019-12-03 21:54:39,417 train 150 3.994489e-03 0.865076
2019-12-03 21:54:51,244 train 200 3.972407e-03 0.864691
2019-12-03 21:54:56,561 training loss; R2: 4.001330e-03 0.863397
2019-12-03 21:54:56,691 valid 000 5.562958e+00 -254.789201
2019-12-03 21:54:57,946 validation loss; R2: 5.434897e+00 -196.179160
2019-12-03 21:54:57,968 epoch 19 lr 2.000000e-03
2019-12-03 21:54:58,287 train 000 5.197745e-03 0.767969
2019-12-03 21:55:10,108 train 050 4.944632e-03 0.816048
2019-12-03 21:55:21,923 train 100 4.315475e-03 0.844155
2019-12-03 21:55:33,737 train 150 4.202314e-03 0.851482
2019-12-03 21:55:45,548 train 200 4.098164e-03 0.856791
2019-12-03 21:55:50,859 training loss; R2: 4.119650e-03 0.856934
2019-12-03 21:55:50,989 valid 000 2.770886e-03 0.897823
2019-12-03 21:55:52,244 validation loss; R2: 3.280005e-03 0.892710
