2019-12-04 03:35:15,116 gpu device = 1
2019-12-04 03:35:15,116 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=96, cutout=False, cutout_length=16, data='../data', dataset='graphene', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name=None, gpu=1, grad_clip=5, gz_dtree=False, init_channels=36, layers=8, learning_rate=0.002, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-graphene-EXP-20191204-033515', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-12-04 03:35:18,320 param size = 0.912061MB
2019-12-04 03:35:18,323 epoch 0 lr 2.000000e-03
2019-12-04 03:35:20,847 train 000 1.078926e+00 -19.139041
2019-12-04 03:35:31,130 train 050 2.545150e-01 -7.034080
2019-12-04 03:35:41,357 train 100 1.465083e-01 -3.636736
2019-12-04 03:35:51,580 train 150 1.086331e-01 -2.434663
2019-12-04 03:36:01,803 train 200 8.823897e-02 -1.792120
2019-12-04 03:36:07,412 training loss; R2: 8.205421e-02 -1.600060
2019-12-04 03:36:07,545 valid 000 3.441070e-02 0.414011
2019-12-04 03:36:08,971 validation loss; R2: 1.821603e-02 0.400181
2019-12-04 03:36:08,987 epoch 1 lr 2.000000e-03
2019-12-04 03:36:09,324 train 000 3.182513e-02 0.110443
2019-12-04 03:36:19,551 train 050 2.341389e-02 0.169119
2019-12-04 03:36:29,776 train 100 2.362120e-02 0.210185
2019-12-04 03:36:40,005 train 150 2.283276e-02 0.255526
2019-12-04 03:36:50,237 train 200 2.177324e-02 0.293777
2019-12-04 03:36:54,839 training loss; R2: 2.163667e-02 0.298771
2019-12-04 03:36:54,970 valid 000 1.225519e-02 0.670512
2019-12-04 03:36:56,081 validation loss; R2: 1.249290e-02 0.615543
2019-12-04 03:36:56,097 epoch 2 lr 2.000000e-03
2019-12-04 03:36:56,381 train 000 1.699492e-02 0.359492
2019-12-04 03:37:06,608 train 050 1.801219e-02 0.430210
2019-12-04 03:37:16,837 train 100 1.759227e-02 0.448049
2019-12-04 03:37:27,066 train 150 1.681170e-02 0.451161
2019-12-04 03:37:37,296 train 200 1.629712e-02 0.464574
2019-12-04 03:37:41,895 training loss; R2: 1.601541e-02 0.474214
2019-12-04 03:37:42,022 valid 000 5.992131e-03 0.737749
2019-12-04 03:37:43,134 validation loss; R2: 7.008351e-03 0.766619
2019-12-04 03:37:43,149 epoch 3 lr 2.000000e-03
2019-12-04 03:37:43,430 train 000 1.141300e-02 0.416556
2019-12-04 03:37:53,657 train 050 1.507870e-02 0.501627
2019-12-04 03:38:03,881 train 100 1.439474e-02 0.533236
2019-12-04 03:38:14,105 train 150 1.400955e-02 0.544698
2019-12-04 03:38:24,339 train 200 1.346529e-02 0.560125
2019-12-04 03:38:28,940 training loss; R2: 1.316963e-02 0.568149
2019-12-04 03:38:29,065 valid 000 5.622705e-03 0.685175
2019-12-04 03:38:30,177 validation loss; R2: 6.316414e-03 0.784429
2019-12-04 03:38:30,193 epoch 4 lr 2.000000e-03
2019-12-04 03:38:30,476 train 000 6.630672e-03 0.638063
2019-12-04 03:38:40,703 train 050 1.092333e-02 0.627652
2019-12-04 03:38:50,929 train 100 1.118155e-02 0.628616
2019-12-04 03:39:01,156 train 150 1.104759e-02 0.634419
2019-12-04 03:39:11,385 train 200 1.085911e-02 0.636343
2019-12-04 03:39:15,987 training loss; R2: 1.074783e-02 0.640846
2019-12-04 03:39:16,116 valid 000 1.008475e-02 0.838890
2019-12-04 03:39:17,228 validation loss; R2: 5.003014e-03 0.828518
2019-12-04 03:39:17,244 epoch 5 lr 2.000000e-03
2019-12-04 03:39:17,525 train 000 7.827177e-03 0.296406
2019-12-04 03:39:27,761 train 050 1.043295e-02 0.631024
2019-12-04 03:39:37,996 train 100 1.001648e-02 0.664512
2019-12-04 03:39:48,228 train 150 9.716907e-03 0.671215
2019-12-04 03:39:58,458 train 200 9.677879e-03 0.679368
2019-12-04 03:40:03,059 training loss; R2: 9.634649e-03 0.678936
2019-12-04 03:40:03,180 valid 000 7.187318e-03 0.875708
2019-12-04 03:40:04,292 validation loss; R2: 4.508784e-03 0.853617
2019-12-04 03:40:04,308 epoch 6 lr 2.000000e-03
2019-12-04 03:40:04,593 train 000 6.081904e-03 0.795122
2019-12-04 03:40:14,822 train 050 1.018614e-02 0.677996
2019-12-04 03:40:25,052 train 100 1.022179e-02 0.674187
2019-12-04 03:40:35,225 train 150 9.829686e-03 0.679071
2019-12-04 03:40:45,254 train 200 9.461437e-03 0.685589
2019-12-04 03:40:49,764 training loss; R2: 9.480032e-03 0.686049
2019-12-04 03:40:49,893 valid 000 6.505858e-03 0.833988
2019-12-04 03:40:51,003 validation loss; R2: 4.940955e-03 0.821970
2019-12-04 03:40:51,020 epoch 7 lr 2.000000e-03
2019-12-04 03:40:51,299 train 000 6.190406e-03 0.796505
2019-12-04 03:41:01,326 train 050 9.490240e-03 0.695155
2019-12-04 03:41:11,349 train 100 9.197197e-03 0.703355
2019-12-04 03:41:21,373 train 150 8.860383e-03 0.713113
2019-12-04 03:41:31,398 train 200 8.897464e-03 0.709400
2019-12-04 03:41:35,903 training loss; R2: 8.831977e-03 0.705777
2019-12-04 03:41:36,033 valid 000 2.290840e-03 0.892603
2019-12-04 03:41:37,143 validation loss; R2: 4.572098e-03 0.853381
2019-12-04 03:41:37,159 epoch 8 lr 2.000000e-03
2019-12-04 03:41:37,435 train 000 4.450301e-03 0.834133
2019-12-04 03:41:47,459 train 050 8.707147e-03 0.716697
2019-12-04 03:41:57,491 train 100 9.420054e-03 0.693602
2019-12-04 03:42:07,521 train 150 8.740276e-03 0.714373
2019-12-04 03:42:17,549 train 200 8.316368e-03 0.720285
2019-12-04 03:42:22,055 training loss; R2: 8.538670e-03 0.717401
2019-12-04 03:42:22,184 valid 000 4.807949e-03 0.758952
2019-12-04 03:42:23,295 validation loss; R2: 5.749323e-03 0.808380
2019-12-04 03:42:23,310 epoch 9 lr 2.000000e-03
2019-12-04 03:42:23,587 train 000 6.103964e-03 0.732759
2019-12-04 03:42:33,619 train 050 8.064680e-03 0.733342
2019-12-04 03:42:43,662 train 100 7.771364e-03 0.729359
2019-12-04 03:42:53,697 train 150 7.845351e-03 0.736407
2019-12-04 03:43:03,734 train 200 7.620845e-03 0.741795
2019-12-04 03:43:08,247 training loss; R2: 7.714585e-03 0.742541
2019-12-04 03:43:08,368 valid 000 8.566034e-03 0.749833
2019-12-04 03:43:09,479 validation loss; R2: 8.915492e-03 0.693546
2019-12-04 03:43:09,495 epoch 10 lr 2.000000e-03
2019-12-04 03:43:09,773 train 000 6.966818e-03 0.790195
2019-12-04 03:43:19,811 train 050 7.699270e-03 0.761302
2019-12-04 03:43:29,845 train 100 7.377639e-03 0.756667
2019-12-04 03:43:39,876 train 150 7.347309e-03 0.749361
2019-12-04 03:43:49,907 train 200 7.517417e-03 0.751472
2019-12-04 03:43:54,418 training loss; R2: 7.518707e-03 0.751972
2019-12-04 03:43:54,542 valid 000 5.911883e-03 0.787899
2019-12-04 03:43:55,652 validation loss; R2: 6.451166e-03 0.772105
2019-12-04 03:43:55,668 epoch 11 lr 2.000000e-03
2019-12-04 03:43:55,948 train 000 7.046980e-03 0.760192
2019-12-04 03:44:05,978 train 050 8.353947e-03 0.732700
2019-12-04 03:44:16,009 train 100 7.808740e-03 0.737461
2019-12-04 03:44:26,040 train 150 7.774144e-03 0.746485
2019-12-04 03:44:36,072 train 200 7.662010e-03 0.747691
2019-12-04 03:44:40,582 training loss; R2: 7.528067e-03 0.749935
2019-12-04 03:44:40,706 valid 000 4.488326e-03 0.934587
2019-12-04 03:44:41,816 validation loss; R2: 3.816403e-03 0.866065
2019-12-04 03:44:41,831 epoch 12 lr 2.000000e-03
2019-12-04 03:44:42,115 train 000 6.930129e-03 0.828819
2019-12-04 03:44:52,141 train 050 6.522078e-03 0.782750
2019-12-04 03:45:02,164 train 100 6.507382e-03 0.776534
2019-12-04 03:45:12,187 train 150 6.555671e-03 0.778432
2019-12-04 03:45:22,217 train 200 6.555344e-03 0.777323
2019-12-04 03:45:26,725 training loss; R2: 6.747670e-03 0.772590
2019-12-04 03:45:26,851 valid 000 8.704499e-03 0.863603
2019-12-04 03:45:27,962 validation loss; R2: 6.888070e-03 0.761131
2019-12-04 03:45:27,977 epoch 13 lr 2.000000e-03
2019-12-04 03:45:28,258 train 000 3.757785e-03 0.856632
2019-12-04 03:45:38,287 train 050 6.016204e-03 0.771128
2019-12-04 03:45:48,313 train 100 6.499952e-03 0.764895
2019-12-04 03:45:58,338 train 150 6.687710e-03 0.775922
2019-12-04 03:46:08,364 train 200 6.777463e-03 0.772818
2019-12-04 03:46:12,873 training loss; R2: 6.725164e-03 0.774616
2019-12-04 03:46:13,003 valid 000 4.923803e-03 0.895402
2019-12-04 03:46:14,114 validation loss; R2: 5.361511e-03 0.804582
2019-12-04 03:46:14,129 epoch 14 lr 2.000000e-03
2019-12-04 03:46:14,412 train 000 6.192949e-03 0.735323
2019-12-04 03:46:24,440 train 050 6.317125e-03 0.775820
2019-12-04 03:46:34,469 train 100 6.495817e-03 0.782953
2019-12-04 03:46:44,495 train 150 6.543240e-03 0.776528
2019-12-04 03:46:54,521 train 200 6.421030e-03 0.782239
2019-12-04 03:46:59,026 training loss; R2: 6.523648e-03 0.780947
2019-12-04 03:46:59,157 valid 000 5.289046e-03 0.868373
2019-12-04 03:47:00,267 validation loss; R2: 3.854410e-03 0.870844
2019-12-04 03:47:00,283 epoch 15 lr 2.000000e-03
2019-12-04 03:47:00,587 train 000 6.267721e-03 0.790843
2019-12-04 03:47:10,618 train 050 6.617804e-03 0.782402
2019-12-04 03:47:20,631 train 100 6.577070e-03 0.775964
2019-12-04 03:47:30,643 train 150 6.476639e-03 0.778644
2019-12-04 03:47:40,656 train 200 6.438579e-03 0.779214
2019-12-04 03:47:45,156 training loss; R2: 6.490131e-03 0.779760
2019-12-04 03:47:45,283 valid 000 2.861977e-03 0.867203
2019-12-04 03:47:46,393 validation loss; R2: 3.730549e-03 0.876775
2019-12-04 03:47:46,408 epoch 16 lr 2.000000e-03
2019-12-04 03:47:46,689 train 000 4.220749e-03 0.843674
2019-12-04 03:47:56,701 train 050 5.364193e-03 0.809061
2019-12-04 03:48:06,716 train 100 5.700254e-03 0.793523
2019-12-04 03:48:16,726 train 150 5.907567e-03 0.789971
2019-12-04 03:48:26,739 train 200 5.973898e-03 0.793878
2019-12-04 03:48:31,241 training loss; R2: 6.047434e-03 0.793902
2019-12-04 03:48:31,367 valid 000 4.283682e-03 0.822985
2019-12-04 03:48:32,478 validation loss; R2: 5.230523e-03 0.830426
2019-12-04 03:48:32,494 epoch 17 lr 2.000000e-03
2019-12-04 03:48:32,781 train 000 1.191441e-02 0.753193
2019-12-04 03:48:42,796 train 050 6.548156e-03 0.783198
2019-12-04 03:48:52,816 train 100 6.313973e-03 0.788349
2019-12-04 03:49:02,836 train 150 6.037209e-03 0.792873
2019-12-04 03:49:12,847 train 200 6.209365e-03 0.787898
2019-12-04 03:49:17,349 training loss; R2: 6.241168e-03 0.790694
2019-12-04 03:49:17,475 valid 000 3.344426e-03 0.888365
2019-12-04 03:49:18,585 validation loss; R2: 3.449531e-03 0.882384
2019-12-04 03:49:18,601 epoch 18 lr 2.000000e-03
2019-12-04 03:49:18,884 train 000 1.517330e-02 0.678794
2019-12-04 03:49:28,893 train 050 6.518643e-03 0.769329
2019-12-04 03:49:38,898 train 100 5.997253e-03 0.787422
2019-12-04 03:49:48,901 train 150 6.014641e-03 0.795396
2019-12-04 03:49:58,907 train 200 6.028650e-03 0.799601
2019-12-04 03:50:03,404 training loss; R2: 6.148935e-03 0.795997
2019-12-04 03:50:03,529 valid 000 2.042559e-03 0.898733
2019-12-04 03:50:04,639 validation loss; R2: 2.965552e-03 0.895244
2019-12-04 03:50:04,656 epoch 19 lr 2.000000e-03
2019-12-04 03:50:04,939 train 000 5.221441e-03 0.804146
2019-12-04 03:50:14,948 train 050 6.767349e-03 0.784886
2019-12-04 03:50:24,960 train 100 6.860966e-03 0.780185
2019-12-04 03:50:34,971 train 150 6.423355e-03 0.787609
2019-12-04 03:50:44,977 train 200 6.349487e-03 0.788389
2019-12-04 03:50:49,476 training loss; R2: 6.238818e-03 0.789619
2019-12-04 03:50:49,603 valid 000 1.286414e-03 0.858958
2019-12-04 03:50:50,713 validation loss; R2: 3.229007e-03 0.890444
