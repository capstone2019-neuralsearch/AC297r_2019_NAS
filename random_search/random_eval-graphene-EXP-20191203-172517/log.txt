2019-12-03 17:25:17,255 gpu device = 1
2019-12-03 17:25:17,256 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=96, cutout=False, cutout_length=16, data='../data', dataset='graphene', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name=None, gpu=1, grad_clip=5, gz_dtree=False, init_channels=36, layers=8, learning_rate=0.002, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-graphene-EXP-20191203-172517', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-12-03 17:25:20,544 param size = 1.110277MB
2019-12-03 17:25:20,547 epoch 0 lr 2.000000e-03
2019-12-03 17:25:23,078 train 000 1.911835e+00 -66.600893
2019-12-03 17:25:35,005 train 050 1.752282e-01 -4.533448
2019-12-03 17:25:46,907 train 100 1.052580e-01 -2.325183
2019-12-03 17:25:58,828 train 150 8.040817e-02 -1.565902
2019-12-03 17:26:10,774 train 200 6.649365e-02 -1.122466
2019-12-03 17:26:17,174 training loss; R2: 6.191595e-02 -0.978193
2019-12-03 17:26:17,291 valid 000 3.357343e-02 0.411126
2019-12-03 17:26:18,893 validation loss; R2: 1.468692e-02 0.535720
2019-12-03 17:26:18,913 epoch 1 lr 2.000000e-03
2019-12-03 17:26:19,259 train 000 1.712084e-02 0.361448
2019-12-03 17:26:31,222 train 050 2.035627e-02 0.355928
2019-12-03 17:26:43,191 train 100 1.944126e-02 0.381384
2019-12-03 17:26:55,222 train 150 1.841009e-02 0.409939
2019-12-03 17:27:07,236 train 200 1.740842e-02 0.439988
2019-12-03 17:27:12,642 training loss; R2: 1.701881e-02 0.447752
2019-12-03 17:27:12,761 valid 000 8.350829e-03 0.775459
2019-12-03 17:27:14,038 validation loss; R2: 8.942223e-03 0.716564
2019-12-03 17:27:14,061 epoch 2 lr 2.000000e-03
2019-12-03 17:27:14,363 train 000 1.071677e-02 0.666121
2019-12-03 17:27:26,366 train 050 1.367001e-02 0.512508
2019-12-03 17:27:38,375 train 100 1.385811e-02 0.535038
2019-12-03 17:27:50,403 train 150 1.372661e-02 0.536462
2019-12-03 17:28:02,437 train 200 1.340302e-02 0.557524
2019-12-03 17:28:07,850 training loss; R2: 1.297649e-02 0.569851
2019-12-03 17:28:07,961 valid 000 1.528756e-02 0.626625
2019-12-03 17:28:09,244 validation loss; R2: 1.251788e-02 0.598535
2019-12-03 17:28:09,267 epoch 3 lr 2.000000e-03
2019-12-03 17:28:09,564 train 000 2.497043e-02 0.498668
2019-12-03 17:28:21,608 train 050 1.004448e-02 0.668481
2019-12-03 17:28:33,647 train 100 1.007062e-02 0.658126
2019-12-03 17:28:45,689 train 150 1.031526e-02 0.660596
2019-12-03 17:28:57,748 train 200 1.022113e-02 0.664262
2019-12-03 17:29:03,168 training loss; R2: 1.015755e-02 0.667940
2019-12-03 17:29:03,283 valid 000 4.873803e-03 0.875676
2019-12-03 17:29:04,566 validation loss; R2: 6.620512e-03 0.763028
2019-12-03 17:29:04,589 epoch 4 lr 2.000000e-03
2019-12-03 17:29:04,888 train 000 9.799499e-03 0.830021
2019-12-03 17:29:16,933 train 050 8.999447e-03 0.647626
2019-12-03 17:29:29,018 train 100 1.021880e-02 0.665828
2019-12-03 17:29:41,094 train 150 9.697341e-03 0.673791
2019-12-03 17:29:53,172 train 200 9.649430e-03 0.679331
2019-12-03 17:29:58,601 training loss; R2: 9.379624e-03 0.683949
2019-12-03 17:29:58,712 valid 000 6.195150e-03 0.794086
2019-12-03 17:29:59,997 validation loss; R2: 6.515852e-03 0.769896
2019-12-03 17:30:00,022 epoch 5 lr 2.000000e-03
2019-12-03 17:30:00,322 train 000 6.850668e-03 0.705327
2019-12-03 17:30:12,397 train 050 7.788415e-03 0.733793
2019-12-03 17:30:24,468 train 100 7.883109e-03 0.731525
2019-12-03 17:30:36,547 train 150 8.149376e-03 0.723425
2019-12-03 17:30:48,620 train 200 7.829375e-03 0.731068
2019-12-03 17:30:54,054 training loss; R2: 7.864148e-03 0.733920
2019-12-03 17:30:54,164 valid 000 2.063303e-02 0.494933
2019-12-03 17:30:55,450 validation loss; R2: 2.018735e-02 0.259506
2019-12-03 17:30:55,473 epoch 6 lr 2.000000e-03
2019-12-03 17:30:55,773 train 000 5.745657e-03 0.875873
2019-12-03 17:31:07,866 train 050 7.564683e-03 0.703724
2019-12-03 17:31:19,963 train 100 7.961806e-03 0.720562
2019-12-03 17:31:32,053 train 150 7.751729e-03 0.729933
2019-12-03 17:31:44,146 train 200 8.616486e-03 0.706164
2019-12-03 17:31:49,582 training loss; R2: 8.583425e-03 0.707562
2019-12-03 17:31:49,693 valid 000 1.410698e-02 0.790414
2019-12-03 17:31:50,983 validation loss; R2: 8.804567e-03 0.678059
2019-12-03 17:31:51,006 epoch 7 lr 2.000000e-03
2019-12-03 17:31:51,309 train 000 8.820673e-03 0.619564
2019-12-03 17:32:03,405 train 050 8.129474e-03 0.719429
2019-12-03 17:32:15,500 train 100 7.643416e-03 0.738781
2019-12-03 17:32:27,591 train 150 7.431297e-03 0.752331
2019-12-03 17:32:39,688 train 200 7.524506e-03 0.744607
2019-12-03 17:32:45,130 training loss; R2: 7.548430e-03 0.738614
2019-12-03 17:32:45,241 valid 000 3.921637e-03 0.822937
2019-12-03 17:32:46,528 validation loss; R2: 5.970439e-03 0.801173
2019-12-03 17:32:46,561 epoch 8 lr 2.000000e-03
2019-12-03 17:32:46,863 train 000 1.332422e-02 0.744870
2019-12-03 17:32:58,967 train 050 7.285309e-03 0.735124
2019-12-03 17:33:11,070 train 100 7.460933e-03 0.735107
2019-12-03 17:33:23,176 train 150 7.389913e-03 0.745897
2019-12-03 17:33:35,278 train 200 7.220066e-03 0.751449
2019-12-03 17:33:40,722 training loss; R2: 7.036815e-03 0.756524
2019-12-03 17:33:40,831 valid 000 1.024842e-02 0.675440
2019-12-03 17:33:42,119 validation loss; R2: 1.403848e-02 0.519479
2019-12-03 17:33:42,152 epoch 9 lr 2.000000e-03
2019-12-03 17:33:42,465 train 000 4.100160e-03 0.859163
2019-12-03 17:33:54,563 train 050 6.125011e-03 0.791467
2019-12-03 17:34:06,657 train 100 6.122097e-03 0.794566
2019-12-03 17:34:18,755 train 150 6.223534e-03 0.789618
2019-12-03 17:34:30,860 train 200 6.294506e-03 0.787654
2019-12-03 17:34:36,298 training loss; R2: 6.265346e-03 0.788664
2019-12-03 17:34:36,409 valid 000 8.669512e-03 0.809017
2019-12-03 17:34:37,700 validation loss; R2: 3.614816e-03 0.874335
2019-12-03 17:34:37,724 epoch 10 lr 2.000000e-03
2019-12-03 17:34:38,025 train 000 4.245656e-03 0.754167
2019-12-03 17:34:50,127 train 050 6.504381e-03 0.771479
2019-12-03 17:35:02,228 train 100 6.627130e-03 0.782604
2019-12-03 17:35:14,324 train 150 6.626253e-03 0.788606
2019-12-03 17:35:26,430 train 200 6.423741e-03 0.787251
2019-12-03 17:35:31,877 training loss; R2: 6.340705e-03 0.787640
2019-12-03 17:35:31,986 valid 000 5.684765e-01 -16.324035
2019-12-03 17:35:33,275 validation loss; R2: 5.688195e-01 -19.275316
2019-12-03 17:35:33,298 epoch 11 lr 2.000000e-03
2019-12-03 17:35:33,598 train 000 3.551556e-03 0.796421
2019-12-03 17:35:45,696 train 050 6.132298e-03 0.777491
2019-12-03 17:35:57,789 train 100 5.797123e-03 0.786665
2019-12-03 17:36:09,883 train 150 5.746082e-03 0.800258
2019-12-03 17:36:21,970 train 200 5.677208e-03 0.806289
2019-12-03 17:36:27,406 training loss; R2: 5.776441e-03 0.803884
2019-12-03 17:36:27,516 valid 000 2.594838e-01 -14.461476
2019-12-03 17:36:28,802 validation loss; R2: 2.660855e-01 -8.807033
2019-12-03 17:36:28,825 epoch 12 lr 2.000000e-03
2019-12-03 17:36:29,125 train 000 8.404258e-03 0.836709
2019-12-03 17:36:41,204 train 050 5.690890e-03 0.814259
2019-12-03 17:36:53,280 train 100 6.161442e-03 0.790150
2019-12-03 17:37:05,354 train 150 6.464869e-03 0.779084
2019-12-03 17:37:17,424 train 200 6.883876e-03 0.770803
2019-12-03 17:37:22,849 training loss; R2: 6.797509e-03 0.773120
2019-12-03 17:37:22,956 valid 000 5.634068e-02 -0.089450
2019-12-03 17:37:24,243 validation loss; R2: 3.994351e-02 -0.400345
2019-12-03 17:37:24,276 epoch 13 lr 2.000000e-03
2019-12-03 17:37:24,582 train 000 3.295411e-03 0.887691
2019-12-03 17:37:36,658 train 050 5.179048e-03 0.817126
2019-12-03 17:37:48,738 train 100 6.263915e-03 0.795776
2019-12-03 17:38:00,816 train 150 6.316546e-03 0.790578
2019-12-03 17:38:12,895 train 200 6.343902e-03 0.786923
2019-12-03 17:38:18,323 training loss; R2: 6.449469e-03 0.785816
2019-12-03 17:38:18,432 valid 000 2.549779e-03 0.758193
2019-12-03 17:38:19,719 validation loss; R2: 3.782363e-03 0.861244
2019-12-03 17:38:19,742 epoch 14 lr 2.000000e-03
2019-12-03 17:38:20,044 train 000 5.610345e-03 0.734036
2019-12-03 17:38:32,115 train 050 5.387567e-03 0.810373
2019-12-03 17:38:44,187 train 100 6.088079e-03 0.794920
2019-12-03 17:38:56,256 train 150 6.006313e-03 0.794444
2019-12-03 17:39:08,327 train 200 6.078043e-03 0.792230
2019-12-03 17:39:13,755 training loss; R2: 6.006753e-03 0.796771
2019-12-03 17:39:13,865 valid 000 9.163705e-02 -4.246234
2019-12-03 17:39:15,151 validation loss; R2: 8.203001e-02 -2.257643
2019-12-03 17:39:15,181 epoch 15 lr 2.000000e-03
2019-12-03 17:39:15,483 train 000 8.201445e-03 0.850622
2019-12-03 17:39:27,545 train 050 5.729798e-03 0.808320
2019-12-03 17:39:39,611 train 100 5.495913e-03 0.822448
2019-12-03 17:39:51,682 train 150 5.608946e-03 0.817927
2019-12-03 17:40:03,748 train 200 5.898261e-03 0.807117
2019-12-03 17:40:09,172 training loss; R2: 5.791825e-03 0.807409
2019-12-03 17:40:09,280 valid 000 5.140534e-01 -14.243474
2019-12-03 17:40:10,565 validation loss; R2: 5.155187e-01 -16.776904
2019-12-03 17:40:10,588 epoch 16 lr 2.000000e-03
2019-12-03 17:40:10,890 train 000 9.439423e-03 0.884911
2019-12-03 17:40:22,958 train 050 5.501737e-03 0.806689
2019-12-03 17:40:35,022 train 100 5.105991e-03 0.812585
2019-12-03 17:40:47,094 train 150 5.418668e-03 0.811256
2019-12-03 17:40:59,157 train 200 5.512306e-03 0.811424
2019-12-03 17:41:04,583 training loss; R2: 5.544724e-03 0.811477
2019-12-03 17:41:04,693 valid 000 1.273435e-02 0.733755
2019-12-03 17:41:05,979 validation loss; R2: 8.459492e-03 0.723616
2019-12-03 17:41:06,010 epoch 17 lr 2.000000e-03
2019-12-03 17:41:06,310 train 000 5.229225e-03 0.880289
2019-12-03 17:41:18,365 train 050 6.463205e-03 0.780888
2019-12-03 17:41:30,420 train 100 6.211832e-03 0.782285
2019-12-03 17:41:42,475 train 150 5.834290e-03 0.799432
2019-12-03 17:41:54,529 train 200 5.737257e-03 0.804176
2019-12-03 17:41:59,947 training loss; R2: 5.677837e-03 0.806097
2019-12-03 17:42:00,054 valid 000 1.790140e-01 -6.539630
2019-12-03 17:42:01,340 validation loss; R2: 1.913079e-01 -6.248026
2019-12-03 17:42:01,364 epoch 18 lr 2.000000e-03
2019-12-03 17:42:01,668 train 000 2.245634e-03 0.949333
2019-12-03 17:42:13,707 train 050 5.487680e-03 0.834931
2019-12-03 17:42:25,749 train 100 5.647029e-03 0.816831
2019-12-03 17:42:37,792 train 150 5.457943e-03 0.819053
2019-12-03 17:42:49,827 train 200 5.540401e-03 0.812371
2019-12-03 17:42:55,239 training loss; R2: 5.503884e-03 0.810703
2019-12-03 17:42:55,350 valid 000 8.810332e-02 -0.510957
2019-12-03 17:42:56,637 validation loss; R2: 6.800040e-02 -1.445871
2019-12-03 17:42:56,668 epoch 19 lr 2.000000e-03
2019-12-03 17:42:56,977 train 000 4.037454e-03 0.848362
2019-12-03 17:43:09,020 train 050 5.372981e-03 0.825835
2019-12-03 17:43:21,056 train 100 5.416711e-03 0.822110
2019-12-03 17:43:33,099 train 150 5.454127e-03 0.818142
2019-12-03 17:43:45,135 train 200 5.559997e-03 0.810747
2019-12-03 17:43:50,552 training loss; R2: 5.627905e-03 0.806269
2019-12-03 17:43:50,663 valid 000 1.204655e-01 -2.113609
2019-12-03 17:43:51,948 validation loss; R2: 9.843857e-02 -2.345085
