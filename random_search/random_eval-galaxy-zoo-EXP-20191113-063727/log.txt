2019-11-13 06:37:28,152 gpu device = 1
2019-11-13 06:37:28,152 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name='galaxy-zoo-cleaned', gpu=1, grad_clip=5, gz_dtree=False, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-galaxy-zoo-EXP-20191113-063727', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-11-13 06:37:39,638 param size = 0.335669MB
2019-11-13 06:37:39,643 epoch 0 lr 1.000000e-03
2019-11-13 06:37:41,975 train 000 2.631627e-01 -68.374555
2019-11-13 06:37:50,249 train 050 4.395592e-02 -5.083817
2019-11-13 06:37:58,311 train 100 3.525702e-02 -2.781032
2019-11-13 06:38:06,465 train 150 3.176754e-02 -1.976430
2019-11-13 06:38:14,575 train 200 2.992870e-02 -1.534933
2019-11-13 06:38:22,649 train 250 2.867639e-02 -1.261532
2019-11-13 06:38:30,853 train 300 2.765230e-02 -1.067903
2019-11-13 06:38:39,094 train 350 2.685999e-02 -0.936059
2019-11-13 06:38:47,560 train 400 2.623417e-02 -0.844713
2019-11-13 06:38:56,033 train 450 2.569663e-02 -0.755742
2019-11-13 06:39:04,500 train 500 2.521457e-02 -0.711881
2019-11-13 06:39:12,966 train 550 2.476878e-02 -0.643664
2019-11-13 06:39:21,065 train 600 2.441589e-02 -0.587946
2019-11-13 06:39:29,170 train 650 2.416214e-02 -0.541773
2019-11-13 06:39:37,280 train 700 2.388180e-02 -0.499380
2019-11-13 06:39:45,381 train 750 2.362852e-02 -0.464189
2019-11-13 06:39:53,476 train 800 2.333506e-02 -0.518858
2019-11-13 06:40:01,589 train 850 2.308095e-02 -0.485098
2019-11-13 06:40:04,942 training loss; R2: 2.300739e-02 -0.474807
2019-11-13 06:40:05,220 valid 000 1.940222e-02 0.011881
2019-11-13 06:40:06,943 valid 050 1.960828e-02 0.047776
2019-11-13 06:40:08,551 validation loss; R2: 1.977557e-02 0.010294
2019-11-13 06:40:08,571 epoch 1 lr 1.000000e-03
2019-11-13 06:40:09,182 train 000 1.707076e-02 0.042398
2019-11-13 06:40:17,651 train 050 1.885260e-02 -0.002637
2019-11-13 06:40:26,119 train 100 1.881327e-02 0.044617
2019-11-13 06:40:34,543 train 150 1.880254e-02 0.044421
2019-11-13 06:40:42,679 train 200 1.864240e-02 0.062833
2019-11-13 06:40:50,814 train 250 1.851131e-02 0.071417
2019-11-13 06:40:59,041 train 300 1.844818e-02 0.077934
2019-11-13 06:41:07,483 train 350 1.826724e-02 0.059776
2019-11-13 06:41:15,562 train 400 1.822491e-02 0.066111
2019-11-13 06:41:23,603 train 450 1.816989e-02 0.070628
2019-11-13 06:41:31,643 train 500 1.805445e-02 0.077687
2019-11-13 06:41:39,680 train 550 1.792822e-02 0.085312
2019-11-13 06:41:47,712 train 600 1.783017e-02 0.089065
2019-11-13 06:41:55,747 train 650 1.774010e-02 0.095043
2019-11-13 06:42:03,776 train 700 1.769455e-02 0.088801
2019-11-13 06:42:11,801 train 750 1.758878e-02 0.089730
2019-11-13 06:42:19,825 train 800 1.749173e-02 0.094100
2019-11-13 06:42:27,851 train 850 1.741201e-02 0.098856
2019-11-13 06:42:30,252 training loss; R2: 1.738451e-02 0.099920
2019-11-13 06:42:30,552 valid 000 1.533855e-02 0.098538
2019-11-13 06:42:32,270 valid 050 1.492485e-02 0.193593
2019-11-13 06:42:33,816 validation loss; R2: 1.508793e-02 0.192614
2019-11-13 06:42:33,837 epoch 2 lr 1.000000e-03
2019-11-13 06:42:34,232 train 000 1.674250e-02 0.255273
2019-11-13 06:42:42,261 train 050 1.602382e-02 0.196771
2019-11-13 06:42:50,363 train 100 1.589182e-02 0.194282
2019-11-13 06:42:58,400 train 150 1.574578e-02 0.200722
2019-11-13 06:43:06,439 train 200 1.571010e-02 0.185503
2019-11-13 06:43:14,473 train 250 1.567285e-02 0.188863
2019-11-13 06:43:22,496 train 300 1.563419e-02 0.189359
2019-11-13 06:43:30,520 train 350 1.555667e-02 0.187622
2019-11-13 06:43:38,540 train 400 1.550398e-02 0.183752
2019-11-13 06:43:46,560 train 450 1.547144e-02 0.182215
2019-11-13 06:43:54,578 train 500 1.543443e-02 0.183261
2019-11-13 06:44:02,606 train 550 1.536227e-02 0.184908
2019-11-13 06:44:10,631 train 600 1.531253e-02 0.188165
2019-11-13 06:44:18,655 train 650 1.525605e-02 0.189144
2019-11-13 06:44:26,682 train 700 1.521234e-02 0.185307
2019-11-13 06:44:34,723 train 750 1.515398e-02 0.184729
2019-11-13 06:44:42,743 train 800 1.510956e-02 0.186770
2019-11-13 06:44:50,776 train 850 1.506132e-02 0.187473
2019-11-13 06:44:53,175 training loss; R2: 1.504741e-02 0.188316
2019-11-13 06:44:53,499 valid 000 1.290246e-02 0.306290
2019-11-13 06:44:55,204 valid 050 1.353993e-02 0.108910
2019-11-13 06:44:56,781 validation loss; R2: 1.357054e-02 0.184005
2019-11-13 06:44:56,802 epoch 3 lr 1.000000e-03
2019-11-13 06:44:57,200 train 000 1.552710e-02 0.038582
2019-11-13 06:45:05,246 train 050 1.423757e-02 0.226265
2019-11-13 06:45:13,289 train 100 1.425335e-02 0.216901
2019-11-13 06:45:21,318 train 150 1.429137e-02 0.214711
2019-11-13 06:45:29,346 train 200 1.409713e-02 0.224567
2019-11-13 06:45:37,380 train 250 1.403824e-02 0.228945
2019-11-13 06:45:45,407 train 300 1.402611e-02 0.231691
2019-11-13 06:45:53,432 train 350 1.395214e-02 0.227778
2019-11-13 06:46:01,460 train 400 1.394968e-02 0.228289
2019-11-13 06:46:09,490 train 450 1.396354e-02 0.226317
2019-11-13 06:46:17,515 train 500 1.391811e-02 0.226381
2019-11-13 06:46:25,535 train 550 1.389060e-02 0.228100
2019-11-13 06:46:33,564 train 600 1.389066e-02 0.228873
2019-11-13 06:46:41,589 train 650 1.387875e-02 0.228400
2019-11-13 06:46:49,608 train 700 1.383190e-02 -0.436414
2019-11-13 06:46:57,639 train 750 1.382253e-02 -0.394790
2019-11-13 06:47:05,664 train 800 1.380616e-02 -0.354103
2019-11-13 06:47:13,687 train 850 1.377538e-02 -0.317775
2019-11-13 06:47:16,085 training loss; R2: 1.377374e-02 -0.308299
2019-11-13 06:47:16,395 valid 000 1.014417e-02 0.205537
2019-11-13 06:47:18,174 valid 050 1.195154e-02 0.234493
2019-11-13 06:47:19,717 validation loss; R2: 1.197839e-02 0.267997
2019-11-13 06:47:19,738 epoch 4 lr 1.000000e-03
2019-11-13 06:47:20,138 train 000 1.380354e-02 0.163638
2019-11-13 06:47:28,183 train 050 1.340678e-02 0.163231
2019-11-13 06:47:36,214 train 100 1.324404e-02 0.215141
2019-11-13 06:47:44,237 train 150 1.318613e-02 0.228201
2019-11-13 06:47:52,267 train 200 1.315530e-02 0.221226
2019-11-13 06:48:00,295 train 250 1.316454e-02 0.202670
2019-11-13 06:48:08,321 train 300 1.310973e-02 0.210487
2019-11-13 06:48:16,347 train 350 1.313314e-02 0.221303
2019-11-13 06:48:24,376 train 400 1.316043e-02 0.219408
2019-11-13 06:48:32,406 train 450 1.316286e-02 0.224271
2019-11-13 06:48:40,433 train 500 1.313389e-02 0.206734
2019-11-13 06:48:48,461 train 550 1.310918e-02 0.211678
2019-11-13 06:48:56,491 train 600 1.308635e-02 0.216929
2019-11-13 06:49:04,523 train 650 1.308733e-02 0.220305
2019-11-13 06:49:12,551 train 700 1.305786e-02 0.222237
2019-11-13 06:49:20,590 train 750 1.304951e-02 0.221544
2019-11-13 06:49:28,644 train 800 1.304506e-02 0.225570
2019-11-13 06:49:36,691 train 850 1.301734e-02 0.225754
2019-11-13 06:49:39,098 training loss; R2: 1.301121e-02 0.227340
2019-11-13 06:49:39,399 valid 000 9.645392e-03 0.391320
2019-11-13 06:49:41,146 valid 050 1.097410e-02 0.334701
2019-11-13 06:49:42,733 validation loss; R2: 1.087666e-02 0.279774
2019-11-13 06:49:42,752 epoch 5 lr 1.000000e-03
2019-11-13 06:49:43,168 train 000 1.339509e-02 0.127630
2019-11-13 06:49:51,249 train 050 1.302369e-02 0.258587
2019-11-13 06:49:59,306 train 100 1.276263e-02 0.252248
2019-11-13 06:50:07,359 train 150 1.263714e-02 0.251954
2019-11-13 06:50:15,413 train 200 1.255165e-02 0.258886
2019-11-13 06:50:23,475 train 250 1.257627e-02 0.262668
2019-11-13 06:50:31,529 train 300 1.258201e-02 0.265712
2019-11-13 06:50:39,573 train 350 1.257572e-02 0.263180
2019-11-13 06:50:47,621 train 400 1.255543e-02 0.260264
2019-11-13 06:50:55,680 train 450 1.258420e-02 0.253351
2019-11-13 06:51:03,731 train 500 1.258011e-02 0.251898
2019-11-13 06:51:11,783 train 550 1.261300e-02 0.251667
2019-11-13 06:51:19,831 train 600 1.259154e-02 0.253785
2019-11-13 06:51:27,882 train 650 1.256616e-02 0.251501
2019-11-13 06:51:35,928 train 700 1.256534e-02 0.255272
2019-11-13 06:51:43,974 train 750 1.254646e-02 0.256075
2019-11-13 06:51:52,025 train 800 1.251985e-02 0.259444
2019-11-13 06:52:00,076 train 850 1.250259e-02 0.260070
2019-11-13 06:52:02,481 training loss; R2: 1.250432e-02 0.259232
2019-11-13 06:52:02,791 valid 000 1.169669e-02 0.428649
2019-11-13 06:52:04,539 valid 050 1.035587e-02 0.385866
2019-11-13 06:52:06,097 validation loss; R2: 1.060789e-02 0.384051
2019-11-13 06:52:06,118 epoch 6 lr 1.000000e-03
2019-11-13 06:52:06,546 train 000 1.295809e-02 0.270030
2019-11-13 06:52:14,811 train 050 1.226761e-02 0.284865
2019-11-13 06:52:22,911 train 100 1.222405e-02 0.262755
2019-11-13 06:52:31,021 train 150 1.231180e-02 0.263661
2019-11-13 06:52:39,115 train 200 1.220152e-02 0.269402
2019-11-13 06:52:47,194 train 250 1.220177e-02 0.277200
2019-11-13 06:52:55,341 train 300 1.221437e-02 0.274962
2019-11-13 06:53:03,411 train 350 1.216087e-02 0.262757
2019-11-13 06:53:11,473 train 400 1.219995e-02 0.264766
2019-11-13 06:53:19,518 train 450 1.221150e-02 0.263098
2019-11-13 06:53:27,560 train 500 1.218801e-02 0.259869
2019-11-13 06:53:35,595 train 550 1.217233e-02 0.262199
2019-11-13 06:53:43,641 train 600 1.217964e-02 0.262575
2019-11-13 06:53:51,683 train 650 1.216279e-02 0.263940
2019-11-13 06:53:59,723 train 700 1.215586e-02 0.266879
2019-11-13 06:54:07,776 train 750 1.216107e-02 0.268695
2019-11-13 06:54:15,815 train 800 1.216066e-02 0.267610
2019-11-13 06:54:23,855 train 850 1.215678e-02 0.268969
2019-11-13 06:54:26,261 training loss; R2: 1.214798e-02 0.269640
2019-11-13 06:54:26,576 valid 000 8.803599e-02 -4.217896
2019-11-13 06:54:28,280 valid 050 9.353023e-02 -4.483959
2019-11-13 06:54:29,859 validation loss; R2: 9.391273e-02 -4.742149
2019-11-13 06:54:29,880 epoch 7 lr 1.000000e-03
2019-11-13 06:54:30,298 train 000 1.093639e-02 0.356908
2019-11-13 06:54:38,560 train 050 1.183177e-02 0.298795
2019-11-13 06:54:46,682 train 100 1.180119e-02 0.296615
2019-11-13 06:54:54,987 train 150 1.178204e-02 0.292953
2019-11-13 06:55:03,294 train 200 1.174955e-02 0.299564
2019-11-13 06:55:11,378 train 250 1.176229e-02 0.303316
2019-11-13 06:55:19,435 train 300 1.188266e-02 0.285999
2019-11-13 06:55:27,506 train 350 1.187426e-02 0.284183
2019-11-13 06:55:35,545 train 400 1.187093e-02 0.286566
2019-11-13 06:55:43,590 train 450 1.190369e-02 0.280479
2019-11-13 06:55:51,639 train 500 1.189766e-02 0.275472
2019-11-13 06:55:59,688 train 550 1.186118e-02 0.274013
2019-11-13 06:56:07,736 train 600 1.186054e-02 0.276809
2019-11-13 06:56:15,793 train 650 1.184654e-02 0.279990
2019-11-13 06:56:23,839 train 700 1.185393e-02 0.283559
2019-11-13 06:56:31,890 train 750 1.183887e-02 0.285292
2019-11-13 06:56:39,938 train 800 1.181737e-02 0.285910
2019-11-13 06:56:47,996 train 850 1.182092e-02 0.286439
2019-11-13 06:56:50,405 training loss; R2: 1.181485e-02 0.286557
2019-11-13 06:56:50,705 valid 000 6.472331e+00 -1751.423479
2019-11-13 06:56:52,409 valid 050 6.462051e+00 -652.442559
2019-11-13 06:56:53,982 validation loss; R2: 6.458170e+00 -764.498289
2019-11-13 06:56:54,003 epoch 8 lr 1.000000e-03
2019-11-13 06:56:54,422 train 000 1.365496e-02 0.246391
2019-11-13 06:57:02,631 train 050 1.149004e-02 0.339316
2019-11-13 06:57:10,905 train 100 1.168102e-02 0.296283
2019-11-13 06:57:19,148 train 150 1.173812e-02 0.303042
2019-11-13 06:57:27,432 train 200 1.176187e-02 0.306286
2019-11-13 06:57:35,727 train 250 1.168209e-02 0.291541
2019-11-13 06:57:44,056 train 300 1.172158e-02 0.297137
2019-11-13 06:57:52,527 train 350 1.170557e-02 0.283815
2019-11-13 06:58:00,665 train 400 1.169465e-02 0.285884
2019-11-13 06:58:09,139 train 450 1.169824e-02 0.289556
2019-11-13 06:58:17,631 train 500 1.171164e-02 0.287543
2019-11-13 06:58:26,130 train 550 1.169544e-02 0.287194
2019-11-13 06:58:34,608 train 600 1.165683e-02 0.288792
2019-11-13 06:58:43,085 train 650 1.167616e-02 0.290434
2019-11-13 06:58:51,580 train 700 1.166892e-02 0.289234
2019-11-13 06:59:00,080 train 750 1.167573e-02 0.286662
2019-11-13 06:59:08,570 train 800 1.167104e-02 0.288765
2019-11-13 06:59:17,054 train 850 1.165270e-02 0.287461
2019-11-13 06:59:19,588 training loss; R2: 1.164483e-02 0.288010
2019-11-13 06:59:19,875 valid 000 2.654015e+00 -130.535169
2019-11-13 06:59:21,568 valid 050 2.617393e+00 -130.182772
2019-11-13 06:59:23,107 validation loss; R2: 2.617997e+00 -137.924276
2019-11-13 06:59:23,128 epoch 9 lr 1.000000e-03
2019-11-13 06:59:23,527 train 000 1.034336e-02 0.312180
2019-11-13 06:59:31,642 train 050 1.162650e-02 0.222849
2019-11-13 06:59:39,742 train 100 1.153162e-02 0.282289
2019-11-13 06:59:47,842 train 150 1.155800e-02 0.297240
2019-11-13 06:59:55,942 train 200 1.160943e-02 0.227561
2019-11-13 07:00:04,037 train 250 1.153700e-02 0.242569
2019-11-13 07:00:12,135 train 300 1.152198e-02 0.248900
2019-11-13 07:00:20,230 train 350 1.146184e-02 0.250086
2019-11-13 07:00:28,327 train 400 1.141100e-02 0.261049
2019-11-13 07:00:36,424 train 450 1.143773e-02 0.262019
2019-11-13 07:00:44,537 train 500 1.143587e-02 0.264015
2019-11-13 07:00:52,640 train 550 1.145409e-02 0.265294
2019-11-13 07:01:00,739 train 600 1.146347e-02 0.265541
2019-11-13 07:01:08,842 train 650 1.146906e-02 0.269675
2019-11-13 07:01:16,942 train 700 1.148268e-02 0.270948
2019-11-13 07:01:25,113 train 750 1.146673e-02 0.273015
2019-11-13 07:01:33,246 train 800 1.146381e-02 0.274199
2019-11-13 07:01:41,348 train 850 1.147144e-02 0.271858
2019-11-13 07:01:43,767 training loss; R2: 1.146506e-02 0.273440
2019-11-13 07:01:44,034 valid 000 1.567787e+00 -115.236318
2019-11-13 07:01:45,712 valid 050 1.553822e+00 -110.799530
2019-11-13 07:01:47,240 validation loss; R2: 1.555795e+00 -107.201178
2019-11-13 07:01:47,260 epoch 10 lr 1.000000e-03
2019-11-13 07:01:47,645 train 000 1.351680e-02 0.294755
2019-11-13 07:01:55,951 train 050 1.142698e-02 -0.320980
2019-11-13 07:02:04,083 train 100 1.129434e-02 -0.004426
2019-11-13 07:02:12,452 train 150 1.134234e-02 -1.695523
2019-11-13 07:02:20,665 train 200 1.138742e-02 -1.204669
2019-11-13 07:02:29,054 train 250 1.137407e-02 -0.911943
2019-11-13 07:02:37,531 train 300 1.139258e-02 -0.707975
2019-11-13 07:02:45,893 train 350 1.137565e-02 -0.573361
2019-11-13 07:02:54,213 train 400 1.139361e-02 -0.461971
2019-11-13 07:03:02,715 train 450 1.140041e-02 -0.388317
2019-11-13 07:03:11,206 train 500 1.137722e-02 -0.317414
2019-11-13 07:03:19,715 train 550 1.135691e-02 -0.259121
2019-11-13 07:03:28,211 train 600 1.135305e-02 -0.213269
2019-11-13 07:03:36,547 train 650 1.138198e-02 -0.174350
2019-11-13 07:03:44,982 train 700 1.135560e-02 -0.152086
2019-11-13 07:03:53,487 train 750 1.135608e-02 -0.120921
2019-11-13 07:04:01,620 train 800 1.133717e-02 -0.092936
2019-11-13 07:04:09,684 train 850 1.131125e-02 -0.068089
2019-11-13 07:04:12,089 training loss; R2: 1.130684e-02 -0.062119
2019-11-13 07:04:12,392 valid 000 6.165194e+00 -915.158881
2019-11-13 07:04:14,067 valid 050 6.222961e+00 -426.981966
2019-11-13 07:04:15,639 validation loss; R2: 6.220666e+00 -410.207669
2019-11-13 07:04:15,659 epoch 11 lr 1.000000e-03
2019-11-13 07:04:16,081 train 000 1.025844e-02 0.360849
2019-11-13 07:04:24,125 train 050 1.106406e-02 0.295116
2019-11-13 07:04:32,276 train 100 1.100602e-02 0.324560
2019-11-13 07:04:40,426 train 150 1.105835e-02 0.327435
2019-11-13 07:04:48,454 train 200 1.110629e-02 0.325969
2019-11-13 07:04:56,491 train 250 1.110901e-02 0.308752
2019-11-13 07:05:04,530 train 300 1.111365e-02 0.302645
2019-11-13 07:05:12,570 train 350 1.114700e-02 0.302772
2019-11-13 07:05:20,600 train 400 1.118935e-02 0.301929
2019-11-13 07:05:28,631 train 450 1.116335e-02 0.302086
2019-11-13 07:05:36,661 train 500 1.117926e-02 0.303174
2019-11-13 07:05:44,689 train 550 1.119919e-02 0.302753
2019-11-13 07:05:52,721 train 600 1.120117e-02 0.303102
2019-11-13 07:06:00,757 train 650 1.122326e-02 0.301744
2019-11-13 07:06:08,789 train 700 1.123611e-02 0.302063
2019-11-13 07:06:16,823 train 750 1.123805e-02 0.287715
2019-11-13 07:06:24,856 train 800 1.125214e-02 0.289734
2019-11-13 07:06:32,890 train 850 1.123876e-02 0.277494
2019-11-13 07:06:35,297 training loss; R2: 1.123941e-02 0.278299
2019-11-13 07:06:35,612 valid 000 2.129299e+00 -96.979958
2019-11-13 07:06:37,363 valid 050 2.137965e+00 -89.563923
2019-11-13 07:06:38,948 validation loss; R2: 2.138345e+00 -91.679792
2019-11-13 07:06:38,969 epoch 12 lr 1.000000e-03
2019-11-13 07:06:39,373 train 000 1.065227e-02 0.284686
2019-11-13 07:06:47,494 train 050 1.125235e-02 0.319928
2019-11-13 07:06:55,628 train 100 1.116049e-02 0.278584
2019-11-13 07:07:03,761 train 150 1.119823e-02 0.290933
2019-11-13 07:07:12,017 train 200 1.119099e-02 0.299008
2019-11-13 07:07:20,189 train 250 1.117973e-02 0.304119
2019-11-13 07:07:28,323 train 300 1.117567e-02 0.303849
2019-11-13 07:07:36,450 train 350 1.116171e-02 0.297001
2019-11-13 07:07:44,577 train 400 1.115581e-02 0.295592
2019-11-13 07:07:52,773 train 450 1.114495e-02 0.299211
2019-11-13 07:08:00,872 train 500 1.110903e-02 0.301466
2019-11-13 07:08:08,949 train 550 1.109504e-02 0.304976
2019-11-13 07:08:17,073 train 600 1.108596e-02 0.304182
2019-11-13 07:08:25,169 train 650 1.108414e-02 0.298599
2019-11-13 07:08:33,238 train 700 1.106686e-02 0.297884
2019-11-13 07:08:41,384 train 750 1.104916e-02 0.300240
2019-11-13 07:08:49,529 train 800 1.104631e-02 0.303348
2019-11-13 07:08:57,659 train 850 1.104158e-02 0.303174
2019-11-13 07:09:00,067 training loss; R2: 1.105013e-02 0.303221
2019-11-13 07:09:00,384 valid 000 4.079974e+00 -181.493596
2019-11-13 07:09:02,123 valid 050 4.047531e+00 -328.147262
2019-11-13 07:09:03,715 validation loss; R2: 4.050204e+00 -294.985256
2019-11-13 07:09:03,737 epoch 13 lr 1.000000e-03
2019-11-13 07:09:04,169 train 000 1.130560e-02 0.383761
2019-11-13 07:09:12,278 train 050 1.166017e-02 0.245887
2019-11-13 07:09:20,487 train 100 1.179321e-02 0.275730
2019-11-13 07:09:28,645 train 150 1.185857e-02 0.209760
2019-11-13 07:09:36,765 train 200 1.178041e-02 0.229513
2019-11-13 07:09:44,890 train 250 1.169195e-02 0.242562
2019-11-13 07:09:53,049 train 300 1.168281e-02 0.253136
2019-11-13 07:10:01,206 train 350 1.161770e-02 0.258795
2019-11-13 07:10:09,416 train 400 1.157775e-02 0.262984
2019-11-13 07:10:17,543 train 450 1.150556e-02 0.253897
2019-11-13 07:10:25,653 train 500 1.149181e-02 0.260739
2019-11-13 07:10:33,709 train 550 1.145866e-02 0.266415
2019-11-13 07:10:41,730 train 600 1.141855e-02 0.269896
2019-11-13 07:10:49,757 train 650 1.138676e-02 0.273351
2019-11-13 07:10:57,772 train 700 1.134913e-02 0.276955
2019-11-13 07:11:05,784 train 750 1.131786e-02 0.243260
2019-11-13 07:11:13,803 train 800 1.129182e-02 0.242684
2019-11-13 07:11:21,820 train 850 1.128677e-02 0.248045
2019-11-13 07:11:24,216 training loss; R2: 1.129134e-02 0.248124
2019-11-13 07:11:24,528 valid 000 1.330384e-01 -12.876333
2019-11-13 07:11:26,200 valid 050 1.219479e-01 -5.521690
2019-11-13 07:11:27,736 validation loss; R2: 1.220664e-01 -5.419882
2019-11-13 07:11:27,755 epoch 14 lr 1.000000e-03
2019-11-13 07:11:28,171 train 000 1.195328e-02 0.373495
2019-11-13 07:11:36,320 train 050 1.087926e-02 0.295871
2019-11-13 07:11:44,479 train 100 1.090910e-02 0.296245
2019-11-13 07:11:52,582 train 150 1.095428e-02 0.296467
2019-11-13 07:12:00,673 train 200 1.104826e-02 0.303607
2019-11-13 07:12:08,935 train 250 1.103006e-02 0.302737
2019-11-13 07:12:17,372 train 300 1.103516e-02 0.309451
2019-11-13 07:12:25,630 train 350 1.106729e-02 0.311863
2019-11-13 07:12:34,106 train 400 1.107863e-02 0.308186
2019-11-13 07:12:42,571 train 450 1.106690e-02 0.308730
2019-11-13 07:12:51,009 train 500 1.101656e-02 0.307853
2019-11-13 07:12:59,085 train 550 1.100886e-02 0.297638
2019-11-13 07:13:07,163 train 600 1.100415e-02 0.298890
2019-11-13 07:13:15,234 train 650 1.101060e-02 0.290374
2019-11-13 07:13:23,312 train 700 1.098142e-02 0.294730
2019-11-13 07:13:31,388 train 750 1.097851e-02 0.298310
2019-11-13 07:13:39,528 train 800 1.096525e-02 0.299719
2019-11-13 07:13:47,995 train 850 1.095063e-02 0.301829
2019-11-13 07:13:50,527 training loss; R2: 1.094573e-02 0.302421
2019-11-13 07:13:50,821 valid 000 3.296015e+00 -288.011084
2019-11-13 07:13:52,539 valid 050 3.332857e+00 -261.279185
2019-11-13 07:13:54,088 validation loss; R2: 3.334889e+00 -370.774330
2019-11-13 07:13:54,108 epoch 15 lr 1.000000e-03
2019-11-13 07:13:54,515 train 000 9.803482e-03 0.379067
2019-11-13 07:14:02,759 train 050 1.063448e-02 0.275840
2019-11-13 07:14:10,971 train 100 1.073059e-02 0.316917
2019-11-13 07:14:19,086 train 150 1.094436e-02 0.316228
2019-11-13 07:14:27,188 train 200 1.105364e-02 0.316022
2019-11-13 07:14:35,335 train 250 1.106873e-02 0.316212
2019-11-13 07:14:43,560 train 300 1.111512e-02 0.317199
2019-11-13 07:14:52,014 train 350 1.115554e-02 0.318044
2019-11-13 07:15:00,154 train 400 1.112828e-02 0.297378
2019-11-13 07:15:08,257 train 450 1.106201e-02 0.302429
2019-11-13 07:15:16,348 train 500 1.103580e-02 0.302551
2019-11-13 07:15:24,419 train 550 1.103354e-02 0.304985
2019-11-13 07:15:32,504 train 600 1.102079e-02 0.308257
2019-11-13 07:15:40,577 train 650 1.100199e-02 0.311732
2019-11-13 07:15:48,667 train 700 1.097901e-02 0.311035
2019-11-13 07:15:56,780 train 750 1.096294e-02 0.308466
2019-11-13 07:16:04,897 train 800 1.095697e-02 0.309341
2019-11-13 07:16:13,004 train 850 1.095711e-02 0.307850
2019-11-13 07:16:15,432 training loss; R2: 1.096078e-02 0.308915
2019-11-13 07:16:15,711 valid 000 4.714681e-01 -33.155022
2019-11-13 07:16:17,392 valid 050 4.655759e-01 -46.329880
2019-11-13 07:16:18,945 validation loss; R2: 4.657310e-01 -45.676057
2019-11-13 07:16:18,965 epoch 16 lr 1.000000e-03
2019-11-13 07:16:19,395 train 000 9.869050e-03 0.421528
2019-11-13 07:16:27,621 train 050 1.095503e-02 0.336182
2019-11-13 07:16:35,947 train 100 1.101581e-02 0.331749
2019-11-13 07:16:44,007 train 150 1.096721e-02 0.334557
2019-11-13 07:16:52,130 train 200 1.104203e-02 0.327001
2019-11-13 07:17:00,266 train 250 1.109559e-02 0.325139
2019-11-13 07:17:08,378 train 300 1.103511e-02 0.312252
2019-11-13 07:17:16,513 train 350 1.100426e-02 0.317192
2019-11-13 07:17:24,638 train 400 1.100793e-02 0.313460
2019-11-13 07:17:33,062 train 450 1.098693e-02 0.308752
2019-11-13 07:17:41,477 train 500 1.097121e-02 0.311299
2019-11-13 07:17:49,658 train 550 1.094126e-02 0.313494
2019-11-13 07:17:57,808 train 600 1.094004e-02 0.315489
2019-11-13 07:18:05,986 train 650 1.092784e-02 0.314442
2019-11-13 07:18:14,146 train 700 1.092554e-02 0.315203
2019-11-13 07:18:22,252 train 750 1.091464e-02 0.316065
2019-11-13 07:18:30,447 train 800 1.089902e-02 0.316775
2019-11-13 07:18:38,817 train 850 1.088815e-02 0.318462
2019-11-13 07:18:41,218 training loss; R2: 1.088062e-02 0.319099
2019-11-13 07:18:41,536 valid 000 1.093526e+00 -530.068563
2019-11-13 07:18:43,245 valid 050 1.104059e+00 -77.972935
2019-11-13 07:18:44,792 validation loss; R2: 1.103375e+00 -78.019122
2019-11-13 07:18:44,811 epoch 17 lr 1.000000e-03
2019-11-13 07:18:45,232 train 000 1.194692e-02 0.420820
2019-11-13 07:18:53,307 train 050 1.076395e-02 0.331095
2019-11-13 07:19:01,673 train 100 1.070519e-02 0.326973
2019-11-13 07:19:10,119 train 150 1.067921e-02 0.309445
2019-11-13 07:19:18,498 train 200 1.071638e-02 0.306565
2019-11-13 07:19:26,607 train 250 1.073871e-02 0.310486
2019-11-13 07:19:34,658 train 300 1.077577e-02 0.316242
2019-11-13 07:19:42,797 train 350 1.082457e-02 0.310622
2019-11-13 07:19:51,002 train 400 1.084937e-02 0.309700
2019-11-13 07:19:59,117 train 450 1.082354e-02 0.302929
2019-11-13 07:20:07,169 train 500 1.082414e-02 0.306647
2019-11-13 07:20:15,231 train 550 1.078615e-02 0.308607
2019-11-13 07:20:23,291 train 600 1.077354e-02 0.307519
2019-11-13 07:20:31,746 train 650 1.077052e-02 0.305163
2019-11-13 07:20:40,185 train 700 1.078676e-02 0.304130
2019-11-13 07:20:48,619 train 750 1.079258e-02 0.305270
2019-11-13 07:20:57,056 train 800 1.079437e-02 0.305295
2019-11-13 07:21:05,489 train 850 1.079849e-02 0.306388
2019-11-13 07:21:08,010 training loss; R2: 1.080659e-02 0.305582
2019-11-13 07:21:08,309 valid 000 6.012926e-01 -297.174050
2019-11-13 07:21:09,999 valid 050 6.082934e-01 -92.916785
2019-11-13 07:21:11,538 validation loss; R2: 6.088114e-01 -85.790198
2019-11-13 07:21:11,558 epoch 18 lr 1.000000e-03
2019-11-13 07:21:11,991 train 000 1.024061e-02 0.158322
2019-11-13 07:21:20,137 train 050 1.072762e-02 0.283438
2019-11-13 07:21:28,269 train 100 1.084900e-02 0.312155
2019-11-13 07:21:36,358 train 150 1.078445e-02 0.312481
2019-11-13 07:21:44,431 train 200 1.069990e-02 0.313887
2019-11-13 07:21:52,496 train 250 1.071172e-02 0.319004
2019-11-13 07:22:00,565 train 300 1.071310e-02 0.318381
2019-11-13 07:22:08,619 train 350 1.072699e-02 0.312947
2019-11-13 07:22:16,643 train 400 1.076894e-02 0.317921
2019-11-13 07:22:24,673 train 450 1.075910e-02 0.322515
2019-11-13 07:22:32,700 train 500 1.074304e-02 0.314800
2019-11-13 07:22:40,730 train 550 1.073312e-02 0.316746
2019-11-13 07:22:48,764 train 600 1.071094e-02 0.318459
2019-11-13 07:22:56,810 train 650 1.070675e-02 0.319085
2019-11-13 07:23:04,868 train 700 1.070697e-02 0.321396
2019-11-13 07:23:12,891 train 750 1.070945e-02 0.323008
2019-11-13 07:23:20,920 train 800 1.071228e-02 0.319746
2019-11-13 07:23:28,947 train 850 1.072037e-02 0.319126
2019-11-13 07:23:31,344 training loss; R2: 1.071139e-02 0.320039
2019-11-13 07:23:31,668 valid 000 6.634375e-01 -71.650434
2019-11-13 07:23:33,346 valid 050 6.636777e-01 -66.232371
2019-11-13 07:23:34,890 validation loss; R2: 6.627918e-01 -69.807827
2019-11-13 07:23:34,911 epoch 19 lr 1.000000e-03
2019-11-13 07:23:35,337 train 000 8.905734e-03 0.415748
2019-11-13 07:23:43,542 train 050 1.066513e-02 0.338107
2019-11-13 07:23:51,679 train 100 1.079328e-02 0.320177
2019-11-13 07:23:59,737 train 150 1.077088e-02 0.329449
2019-11-13 07:24:07,802 train 200 1.080681e-02 0.315819
2019-11-13 07:24:15,859 train 250 1.074836e-02 0.316767
2019-11-13 07:24:23,938 train 300 1.068421e-02 0.321053
2019-11-13 07:24:31,996 train 350 1.072506e-02 0.324580
2019-11-13 07:24:40,052 train 400 1.070176e-02 0.310319
2019-11-13 07:24:48,117 train 450 1.068723e-02 0.302829
2019-11-13 07:24:56,175 train 500 1.067668e-02 0.305584
2019-11-13 07:25:04,239 train 550 1.070579e-02 0.308034
2019-11-13 07:25:12,300 train 600 1.074224e-02 0.308667
2019-11-13 07:25:20,489 train 650 1.077087e-02 0.308387
2019-11-13 07:25:28,678 train 700 1.079623e-02 0.304543
2019-11-13 07:25:36,768 train 750 1.080095e-02 0.305491
2019-11-13 07:25:44,876 train 800 1.079730e-02 0.305543
2019-11-13 07:25:52,968 train 850 1.079353e-02 0.306024
2019-11-13 07:25:55,385 training loss; R2: 1.079343e-02 0.306323
2019-11-13 07:25:55,700 valid 000 2.269423e+00 -203.131705
2019-11-13 07:25:57,441 valid 050 2.256841e+00 -255.712131
2019-11-13 07:25:59,000 validation loss; R2: 2.257696e+00 -285.581818
