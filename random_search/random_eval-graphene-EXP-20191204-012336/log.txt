2019-12-04 01:23:36,868 gpu device = 1
2019-12-04 01:23:36,868 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=96, cutout=False, cutout_length=16, data='../data', dataset='graphene', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name=None, gpu=1, grad_clip=5, gz_dtree=False, init_channels=36, layers=8, learning_rate=0.002, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-graphene-EXP-20191204-012336', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-12-04 01:23:40,087 param size = 1.119205MB
2019-12-04 01:23:40,090 epoch 0 lr 2.000000e-03
2019-12-04 01:23:42,688 train 000 4.367994e-01 -13.582681
2019-12-04 01:23:55,316 train 050 1.632484e-01 -4.433607
2019-12-04 01:24:07,878 train 100 1.005106e-01 -2.373608
2019-12-04 01:24:20,442 train 150 7.674469e-02 -1.561511
2019-12-04 01:24:33,015 train 200 6.414540e-02 -1.139684
2019-12-04 01:24:39,744 training loss; R2: 6.046844e-02 -1.005688
2019-12-04 01:24:39,884 valid 000 1.377182e-02 0.465932
2019-12-04 01:24:41,499 validation loss; R2: 1.863356e-02 0.392320
2019-12-04 01:24:41,520 epoch 1 lr 2.000000e-03
2019-12-04 01:24:41,926 train 000 1.944698e-02 0.232488
2019-12-04 01:24:54,395 train 050 2.478699e-02 0.192255
2019-12-04 01:25:06,694 train 100 2.332698e-02 0.226793
2019-12-04 01:25:18,995 train 150 2.240346e-02 0.260854
2019-12-04 01:25:31,299 train 200 2.218787e-02 0.284376
2019-12-04 01:25:36,831 training loss; R2: 2.231775e-02 0.284618
2019-12-04 01:25:36,969 valid 000 5.418724e-02 0.068737
2019-12-04 01:25:38,271 validation loss; R2: 2.933370e-02 0.039079
2019-12-04 01:25:38,292 epoch 2 lr 2.000000e-03
2019-12-04 01:25:38,620 train 000 3.710968e-02 0.147696
2019-12-04 01:25:50,916 train 050 1.874873e-02 0.386916
2019-12-04 01:26:03,219 train 100 1.640445e-02 0.433127
2019-12-04 01:26:15,525 train 150 1.663895e-02 0.452942
2019-12-04 01:26:27,833 train 200 1.615556e-02 0.469262
2019-12-04 01:26:33,369 training loss; R2: 1.619197e-02 0.474811
2019-12-04 01:26:33,501 valid 000 6.738196e-03 0.699569
2019-12-04 01:26:34,804 validation loss; R2: 9.682074e-03 0.695540
2019-12-04 01:26:34,825 epoch 3 lr 2.000000e-03
2019-12-04 01:26:35,152 train 000 1.208827e-02 0.548648
2019-12-04 01:26:47,455 train 050 1.418457e-02 0.578705
2019-12-04 01:26:59,758 train 100 1.359561e-02 0.564524
2019-12-04 01:27:12,062 train 150 1.326825e-02 0.572468
2019-12-04 01:27:24,363 train 200 1.278680e-02 0.581945
2019-12-04 01:27:29,897 training loss; R2: 1.259942e-02 0.587221
2019-12-04 01:27:30,029 valid 000 5.186650e-03 0.767416
2019-12-04 01:27:31,332 validation loss; R2: 7.716832e-03 0.722004
2019-12-04 01:27:31,354 epoch 4 lr 2.000000e-03
2019-12-04 01:27:31,684 train 000 6.307737e-03 0.834174
2019-12-04 01:27:43,994 train 050 1.188304e-02 0.630438
2019-12-04 01:27:56,295 train 100 1.076244e-02 0.646564
2019-12-04 01:28:08,606 train 150 1.088097e-02 0.642136
2019-12-04 01:28:20,915 train 200 1.104961e-02 0.642255
2019-12-04 01:28:26,449 training loss; R2: 1.083669e-02 0.647383
2019-12-04 01:28:26,583 valid 000 6.228039e-03 0.811810
2019-12-04 01:28:27,886 validation loss; R2: 5.416580e-03 0.818543
2019-12-04 01:28:27,907 epoch 5 lr 2.000000e-03
2019-12-04 01:28:28,237 train 000 5.526766e-03 0.753105
2019-12-04 01:28:40,547 train 050 9.648383e-03 0.668570
2019-12-04 01:28:52,864 train 100 9.983469e-03 0.669912
2019-12-04 01:29:05,175 train 150 9.761247e-03 0.676104
2019-12-04 01:29:17,491 train 200 9.524988e-03 0.682599
2019-12-04 01:29:23,028 training loss; R2: 9.540355e-03 0.679071
2019-12-04 01:29:23,161 valid 000 1.250108e-02 0.766820
2019-12-04 01:29:24,464 validation loss; R2: 5.840923e-03 0.810408
2019-12-04 01:29:24,485 epoch 6 lr 2.000000e-03
2019-12-04 01:29:24,814 train 000 7.396940e-03 0.691709
2019-12-04 01:29:37,126 train 050 1.003221e-02 0.660813
2019-12-04 01:29:49,432 train 100 9.115021e-03 0.694300
2019-12-04 01:30:01,740 train 150 8.836696e-03 0.701420
2019-12-04 01:30:14,042 train 200 9.023101e-03 0.701005
2019-12-04 01:30:19,577 training loss; R2: 9.177122e-03 0.700523
2019-12-04 01:30:19,711 valid 000 5.659623e-03 0.712737
2019-12-04 01:30:21,014 validation loss; R2: 8.630140e-03 0.709069
2019-12-04 01:30:21,037 epoch 7 lr 2.000000e-03
2019-12-04 01:30:21,365 train 000 1.349059e-02 0.784855
2019-12-04 01:30:33,678 train 050 1.074431e-02 0.678572
2019-12-04 01:30:45,985 train 100 9.434508e-03 0.696371
2019-12-04 01:30:58,295 train 150 9.117615e-03 0.705597
2019-12-04 01:31:10,604 train 200 8.697676e-03 0.713560
2019-12-04 01:31:16,144 training loss; R2: 8.590232e-03 0.717732
2019-12-04 01:31:16,276 valid 000 4.816677e-03 0.825733
2019-12-04 01:31:17,579 validation loss; R2: 5.423717e-03 0.815124
2019-12-04 01:31:17,601 epoch 8 lr 2.000000e-03
2019-12-04 01:31:17,930 train 000 7.214803e-03 0.757891
2019-12-04 01:31:30,232 train 050 8.142174e-03 0.741524
2019-12-04 01:31:42,536 train 100 8.166124e-03 0.728809
2019-12-04 01:31:54,838 train 150 9.300941e-03 0.695014
2019-12-04 01:32:07,140 train 200 9.085008e-03 0.698424
2019-12-04 01:32:12,672 training loss; R2: 8.976530e-03 0.698977
2019-12-04 01:32:12,805 valid 000 2.991762e-03 0.849627
2019-12-04 01:32:14,108 validation loss; R2: 4.326421e-03 0.845493
2019-12-04 01:32:14,129 epoch 9 lr 2.000000e-03
2019-12-04 01:32:14,455 train 000 6.458679e-03 0.688513
2019-12-04 01:32:26,768 train 050 7.860730e-03 0.731045
2019-12-04 01:32:39,074 train 100 7.629436e-03 0.743679
2019-12-04 01:32:51,382 train 150 7.960284e-03 0.735425
2019-12-04 01:33:03,689 train 200 7.709177e-03 0.735018
2019-12-04 01:33:09,225 training loss; R2: 7.798964e-03 0.736184
2019-12-04 01:33:09,357 valid 000 8.374777e-03 0.865792
2019-12-04 01:33:10,660 validation loss; R2: 4.814100e-03 0.845675
2019-12-04 01:33:10,681 epoch 10 lr 2.000000e-03
2019-12-04 01:33:11,009 train 000 7.209020e-03 0.715234
2019-12-04 01:33:23,328 train 050 7.579373e-03 0.736519
2019-12-04 01:33:35,641 train 100 7.354128e-03 0.747297
2019-12-04 01:33:47,958 train 150 7.691585e-03 0.735944
2019-12-04 01:34:00,273 train 200 7.800647e-03 0.736626
2019-12-04 01:34:05,811 training loss; R2: 7.748210e-03 0.737666
2019-12-04 01:34:05,943 valid 000 7.167019e-03 0.850695
2019-12-04 01:34:07,247 validation loss; R2: 4.655722e-03 0.845487
2019-12-04 01:34:07,268 epoch 11 lr 2.000000e-03
2019-12-04 01:34:07,599 train 000 6.301901e-03 0.849822
2019-12-04 01:34:19,909 train 050 6.795049e-03 0.779093
2019-12-04 01:34:32,222 train 100 7.571361e-03 0.749735
2019-12-04 01:34:44,530 train 150 7.467716e-03 0.747469
2019-12-04 01:34:56,843 train 200 7.500619e-03 0.748178
2019-12-04 01:35:02,380 training loss; R2: 7.518427e-03 0.747658
2019-12-04 01:35:02,513 valid 000 4.069355e-03 0.914404
2019-12-04 01:35:03,816 validation loss; R2: 4.582282e-03 0.831002
2019-12-04 01:35:03,837 epoch 12 lr 2.000000e-03
2019-12-04 01:35:04,164 train 000 5.720880e-03 0.702381
2019-12-04 01:35:16,485 train 050 6.714268e-03 0.766413
2019-12-04 01:35:28,806 train 100 6.500520e-03 0.771529
2019-12-04 01:35:41,129 train 150 6.953027e-03 0.763348
2019-12-04 01:35:53,451 train 200 7.052553e-03 0.761994
2019-12-04 01:35:58,988 training loss; R2: 7.101610e-03 0.760779
2019-12-04 01:35:59,133 valid 000 5.227638e-03 0.713724
2019-12-04 01:36:00,437 validation loss; R2: 5.302725e-03 0.824845
2019-12-04 01:36:00,457 epoch 13 lr 2.000000e-03
2019-12-04 01:36:00,781 train 000 6.180393e-03 0.641715
2019-12-04 01:36:13,077 train 050 6.759384e-03 0.760797
2019-12-04 01:36:25,376 train 100 6.782472e-03 0.763877
2019-12-04 01:36:37,674 train 150 6.952569e-03 0.770214
2019-12-04 01:36:49,973 train 200 6.976049e-03 0.766703
2019-12-04 01:36:55,503 training loss; R2: 7.016995e-03 0.764907
2019-12-04 01:36:55,635 valid 000 2.566076e-03 0.866755
2019-12-04 01:36:56,938 validation loss; R2: 3.982896e-03 0.861429
2019-12-04 01:36:56,960 epoch 14 lr 2.000000e-03
2019-12-04 01:36:57,287 train 000 5.683576e-03 0.775025
2019-12-04 01:37:09,592 train 050 6.461663e-03 0.763873
2019-12-04 01:37:21,893 train 100 6.573122e-03 0.773420
2019-12-04 01:37:34,193 train 150 6.570359e-03 0.775375
2019-12-04 01:37:46,491 train 200 6.490020e-03 0.782232
2019-12-04 01:37:52,023 training loss; R2: 6.530289e-03 0.781799
2019-12-04 01:37:52,157 valid 000 3.755617e-03 0.884847
2019-12-04 01:37:53,459 validation loss; R2: 3.907477e-03 0.864216
2019-12-04 01:37:53,481 epoch 15 lr 2.000000e-03
2019-12-04 01:37:53,806 train 000 3.818888e-03 0.884939
2019-12-04 01:38:06,103 train 050 6.529544e-03 0.797680
2019-12-04 01:38:18,398 train 100 6.435081e-03 0.794160
2019-12-04 01:38:30,689 train 150 6.602953e-03 0.779783
2019-12-04 01:38:42,984 train 200 6.648770e-03 0.774668
2019-12-04 01:38:48,511 training loss; R2: 6.611855e-03 0.775411
2019-12-04 01:38:48,645 valid 000 1.851959e-03 0.908340
2019-12-04 01:38:49,948 validation loss; R2: 3.986857e-03 0.860046
2019-12-04 01:38:49,970 epoch 16 lr 2.000000e-03
2019-12-04 01:38:50,298 train 000 5.505064e-03 0.868711
2019-12-04 01:39:02,593 train 050 6.542588e-03 0.771428
2019-12-04 01:39:14,886 train 100 6.895188e-03 0.775700
2019-12-04 01:39:27,176 train 150 6.441219e-03 0.781605
2019-12-04 01:39:39,469 train 200 6.530850e-03 0.783058
2019-12-04 01:39:44,996 training loss; R2: 6.616327e-03 0.782829
2019-12-04 01:39:45,127 valid 000 5.426434e-03 0.832454
2019-12-04 01:39:46,430 validation loss; R2: 4.950076e-03 0.831328
2019-12-04 01:39:46,452 epoch 17 lr 2.000000e-03
2019-12-04 01:39:46,785 train 000 4.795929e-03 0.779414
2019-12-04 01:39:59,074 train 050 6.889861e-03 0.755100
2019-12-04 01:40:11,365 train 100 6.790179e-03 0.777555
2019-12-04 01:40:23,649 train 150 6.879002e-03 0.778675
2019-12-04 01:40:35,936 train 200 6.883063e-03 0.777293
2019-12-04 01:40:41,460 training loss; R2: 6.786054e-03 0.776910
2019-12-04 01:40:41,593 valid 000 1.137251e-02 0.624921
2019-12-04 01:40:42,896 validation loss; R2: 1.011010e-02 0.642248
2019-12-04 01:40:42,918 epoch 18 lr 2.000000e-03
2019-12-04 01:40:43,244 train 000 5.753850e-03 0.778977
2019-12-04 01:40:55,529 train 050 7.034689e-03 0.763693
2019-12-04 01:41:07,808 train 100 7.484096e-03 0.743219
2019-12-04 01:41:20,087 train 150 7.266397e-03 0.750792
2019-12-04 01:41:32,368 train 200 7.380353e-03 0.745911
2019-12-04 01:41:37,892 training loss; R2: 7.522172e-03 0.744162
2019-12-04 01:41:38,026 valid 000 3.306875e-03 0.895568
2019-12-04 01:41:39,328 validation loss; R2: 3.810274e-03 0.870790
2019-12-04 01:41:39,350 epoch 19 lr 2.000000e-03
2019-12-04 01:41:39,674 train 000 5.885573e-03 0.751307
2019-12-04 01:41:51,956 train 050 6.819756e-03 0.774151
2019-12-04 01:42:04,247 train 100 7.122149e-03 0.772046
2019-12-04 01:42:16,531 train 150 7.188324e-03 0.772137
2019-12-04 01:42:28,854 train 200 7.067555e-03 0.761228
2019-12-04 01:42:34,512 training loss; R2: 7.014780e-03 0.759422
2019-12-04 01:42:34,645 valid 000 3.738267e-03 0.808020
2019-12-04 01:42:35,949 validation loss; R2: 4.772691e-03 0.832336
