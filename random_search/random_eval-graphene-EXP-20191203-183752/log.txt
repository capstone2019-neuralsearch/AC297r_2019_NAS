2019-12-03 18:37:52,550 gpu device = 1
2019-12-03 18:37:52,550 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=96, cutout=False, cutout_length=16, data='../data', dataset='graphene', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name=None, gpu=1, grad_clip=5, gz_dtree=False, init_channels=36, layers=8, learning_rate=0.002, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-graphene-EXP-20191203-183752', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-12-03 18:37:55,799 param size = 1.034749MB
2019-12-03 18:37:55,802 epoch 0 lr 2.000000e-03
2019-12-03 18:37:58,297 train 000 2.209355e-01 -5.036104
2019-12-03 18:38:09,726 train 050 2.459184e-01 -11.930066
2019-12-03 18:38:21,103 train 100 1.397284e-01 -6.038521
2019-12-03 18:38:32,478 train 150 1.024923e-01 -3.984450
2019-12-03 18:38:43,808 train 200 8.215134e-02 -2.908546
2019-12-03 18:38:49,776 training loss; R2: 7.587284e-02 -2.575330
2019-12-03 18:38:49,899 valid 000 4.425557e-02 0.448079
2019-12-03 18:38:51,436 validation loss; R2: 1.557120e-02 0.507334
2019-12-03 18:38:51,455 epoch 1 lr 2.000000e-03
2019-12-03 18:38:51,782 train 000 1.246695e-02 0.509379
2019-12-03 18:39:02,887 train 050 1.793267e-02 0.389226
2019-12-03 18:39:13,993 train 100 1.655413e-02 0.447956
2019-12-03 18:39:25,098 train 150 1.706203e-02 0.448510
2019-12-03 18:39:36,204 train 200 1.581470e-02 0.488498
2019-12-03 18:39:41,201 training loss; R2: 1.518820e-02 0.505002
2019-12-03 18:39:41,319 valid 000 6.240411e-03 0.764352
2019-12-03 18:39:42,540 validation loss; R2: 7.097161e-03 0.756831
2019-12-03 18:39:42,560 epoch 2 lr 2.000000e-03
2019-12-03 18:39:42,849 train 000 8.665652e-03 0.737292
2019-12-03 18:39:53,962 train 050 1.123772e-02 0.639498
2019-12-03 18:40:05,070 train 100 1.157386e-02 0.627605
2019-12-03 18:40:16,177 train 150 1.117338e-02 0.626436
2019-12-03 18:40:27,279 train 200 1.095990e-02 0.637432
2019-12-03 18:40:32,281 training loss; R2: 1.076813e-02 0.645142
2019-12-03 18:40:32,399 valid 000 6.123923e-03 0.754295
2019-12-03 18:40:33,620 validation loss; R2: 7.111913e-03 0.764457
2019-12-03 18:40:33,641 epoch 3 lr 2.000000e-03
2019-12-03 18:40:33,937 train 000 8.634443e-03 0.462477
2019-12-03 18:40:45,058 train 050 1.065459e-02 0.677864
2019-12-03 18:40:56,174 train 100 9.754415e-03 0.687807
2019-12-03 18:41:07,288 train 150 9.283365e-03 0.698525
2019-12-03 18:41:18,399 train 200 8.945461e-03 0.703435
2019-12-03 18:41:23,403 training loss; R2: 8.957216e-03 0.705372
2019-12-03 18:41:23,519 valid 000 2.955396e-03 0.896277
2019-12-03 18:41:24,739 validation loss; R2: 4.682501e-03 0.841315
2019-12-03 18:41:24,759 epoch 4 lr 2.000000e-03
2019-12-03 18:41:25,046 train 000 8.246996e-03 0.703544
2019-12-03 18:41:36,154 train 050 8.741772e-03 0.689836
2019-12-03 18:41:47,263 train 100 7.988396e-03 0.717027
2019-12-03 18:41:58,366 train 150 8.283881e-03 0.709392
2019-12-03 18:42:09,476 train 200 8.462732e-03 0.706798
2019-12-03 18:42:14,469 training loss; R2: 8.293138e-03 0.713478
2019-12-03 18:42:14,584 valid 000 9.663648e-03 0.710677
2019-12-03 18:42:15,806 validation loss; R2: 1.047658e-02 0.624233
2019-12-03 18:42:15,827 epoch 5 lr 2.000000e-03
2019-12-03 18:42:16,114 train 000 7.738956e-03 0.829479
2019-12-03 18:42:27,235 train 050 8.128456e-03 0.733499
2019-12-03 18:42:38,347 train 100 7.870231e-03 0.727023
2019-12-03 18:42:49,456 train 150 7.692043e-03 0.740439
2019-12-03 18:43:00,574 train 200 7.361220e-03 0.751606
2019-12-03 18:43:05,573 training loss; R2: 7.389733e-03 0.751274
2019-12-03 18:43:05,690 valid 000 5.694997e-03 0.804551
2019-12-03 18:43:06,913 validation loss; R2: 5.488847e-03 0.806451
2019-12-03 18:43:06,935 epoch 6 lr 2.000000e-03
2019-12-03 18:43:07,224 train 000 3.279214e-03 0.852986
2019-12-03 18:43:18,327 train 050 7.117758e-03 0.772130
2019-12-03 18:43:29,438 train 100 6.826558e-03 0.768670
2019-12-03 18:43:40,545 train 150 6.648210e-03 0.779984
2019-12-03 18:43:51,643 train 200 6.527317e-03 0.784452
2019-12-03 18:43:56,635 training loss; R2: 6.591906e-03 0.779092
2019-12-03 18:43:56,755 valid 000 1.214139e-02 0.541920
2019-12-03 18:43:57,976 validation loss; R2: 1.138390e-02 0.612308
2019-12-03 18:43:57,997 epoch 7 lr 2.000000e-03
2019-12-03 18:43:58,288 train 000 4.828200e-03 0.833709
2019-12-03 18:44:09,392 train 050 6.620796e-03 0.782000
2019-12-03 18:44:20,497 train 100 6.357279e-03 0.782798
2019-12-03 18:44:31,597 train 150 6.294182e-03 0.786208
2019-12-03 18:44:42,695 train 200 6.380351e-03 0.785030
2019-12-03 18:44:47,688 training loss; R2: 6.322353e-03 0.786798
2019-12-03 18:44:47,804 valid 000 3.555404e-03 0.903722
2019-12-03 18:44:49,025 validation loss; R2: 4.312783e-03 0.854396
2019-12-03 18:44:49,053 epoch 8 lr 2.000000e-03
2019-12-03 18:44:49,350 train 000 3.926504e-03 0.815318
2019-12-03 18:45:00,461 train 050 6.022988e-03 0.770594
2019-12-03 18:45:11,570 train 100 5.740927e-03 0.796285
2019-12-03 18:45:22,669 train 150 5.993663e-03 0.797722
2019-12-03 18:45:33,767 train 200 5.898357e-03 0.802530
2019-12-03 18:45:38,760 training loss; R2: 6.011592e-03 0.799339
2019-12-03 18:45:38,877 valid 000 4.002379e-03 0.871916
2019-12-03 18:45:40,098 validation loss; R2: 3.523926e-03 0.882316
2019-12-03 18:45:40,119 epoch 9 lr 2.000000e-03
2019-12-03 18:45:40,405 train 000 5.550826e-03 0.860347
2019-12-03 18:45:51,507 train 050 6.020124e-03 0.808644
2019-12-03 18:46:02,604 train 100 5.785784e-03 0.802783
2019-12-03 18:46:13,703 train 150 5.650691e-03 0.810048
2019-12-03 18:46:24,807 train 200 5.893440e-03 0.804715
2019-12-03 18:46:29,798 training loss; R2: 5.836024e-03 0.805450
2019-12-03 18:46:29,912 valid 000 3.698064e-03 0.850334
2019-12-03 18:46:31,134 validation loss; R2: 3.881319e-03 0.861739
2019-12-03 18:46:31,155 epoch 10 lr 2.000000e-03
2019-12-03 18:46:31,443 train 000 4.387614e-03 0.899257
2019-12-03 18:46:42,548 train 050 5.622044e-03 0.815958
2019-12-03 18:46:53,650 train 100 5.488029e-03 0.812146
2019-12-03 18:47:04,754 train 150 5.295026e-03 0.813332
2019-12-03 18:47:15,855 train 200 5.264373e-03 0.818227
2019-12-03 18:47:20,845 training loss; R2: 5.262232e-03 0.818244
2019-12-03 18:47:20,968 valid 000 1.067563e-02 0.779282
2019-12-03 18:47:22,190 validation loss; R2: 5.116052e-03 0.826717
2019-12-03 18:47:22,211 epoch 11 lr 2.000000e-03
2019-12-03 18:47:22,500 train 000 7.973972e-03 0.782046
2019-12-03 18:47:33,594 train 050 5.333787e-03 0.817642
2019-12-03 18:47:44,689 train 100 4.973021e-03 0.829173
2019-12-03 18:47:55,783 train 150 4.787816e-03 0.832925
2019-12-03 18:48:06,877 train 200 4.755361e-03 0.835473
2019-12-03 18:48:11,866 training loss; R2: 4.864439e-03 0.833497
2019-12-03 18:48:11,979 valid 000 1.086678e-02 0.881654
2019-12-03 18:48:13,199 validation loss; R2: 3.955946e-03 0.865742
2019-12-03 18:48:13,220 epoch 12 lr 2.000000e-03
2019-12-03 18:48:13,508 train 000 2.956763e-03 0.914221
2019-12-03 18:48:24,606 train 050 4.349024e-03 0.841299
2019-12-03 18:48:35,701 train 100 4.439295e-03 0.842324
2019-12-03 18:48:46,795 train 150 4.392641e-03 0.844766
2019-12-03 18:48:57,895 train 200 4.502030e-03 0.845472
2019-12-03 18:49:02,880 training loss; R2: 4.530874e-03 0.845703
2019-12-03 18:49:03,003 valid 000 4.056022e-03 0.812385
2019-12-03 18:49:04,226 validation loss; R2: 4.441433e-03 0.837661
2019-12-03 18:49:04,253 epoch 13 lr 2.000000e-03
2019-12-03 18:49:04,545 train 000 4.105298e-03 0.839769
2019-12-03 18:49:15,637 train 050 5.238356e-03 0.826582
2019-12-03 18:49:26,727 train 100 4.948601e-03 0.829338
2019-12-03 18:49:37,823 train 150 4.881440e-03 0.832611
2019-12-03 18:49:48,913 train 200 4.824175e-03 0.832884
2019-12-03 18:49:53,907 training loss; R2: 4.752417e-03 0.834557
2019-12-03 18:49:54,021 valid 000 2.186263e-03 0.919448
2019-12-03 18:49:55,245 validation loss; R2: 2.462012e-03 0.917690
2019-12-03 18:49:55,264 epoch 14 lr 2.000000e-03
2019-12-03 18:49:55,549 train 000 2.192844e-03 0.911767
2019-12-03 18:50:06,647 train 050 4.774295e-03 0.854304
2019-12-03 18:50:17,742 train 100 4.555978e-03 0.849611
2019-12-03 18:50:28,831 train 150 4.416596e-03 0.855408
2019-12-03 18:50:39,927 train 200 4.491547e-03 0.843355
2019-12-03 18:50:44,911 training loss; R2: 4.567061e-03 0.840650
2019-12-03 18:50:45,025 valid 000 3.794170e-03 0.891872
2019-12-03 18:50:46,247 validation loss; R2: 3.529377e-03 0.882590
2019-12-03 18:50:46,267 epoch 15 lr 2.000000e-03
2019-12-03 18:50:46,555 train 000 3.679323e-03 0.894408
2019-12-03 18:50:57,637 train 050 3.912298e-03 0.855672
2019-12-03 18:51:08,727 train 100 4.046723e-03 0.857186
2019-12-03 18:51:19,817 train 150 4.096882e-03 0.855930
2019-12-03 18:51:30,912 train 200 4.357740e-03 0.847311
2019-12-03 18:51:35,896 training loss; R2: 4.322237e-03 0.848924
2019-12-03 18:51:36,011 valid 000 1.473142e-03 0.939109
2019-12-03 18:51:37,237 validation loss; R2: 2.482296e-03 0.918751
2019-12-03 18:51:37,257 epoch 16 lr 2.000000e-03
2019-12-03 18:51:37,555 train 000 2.710066e-03 0.926498
2019-12-03 18:51:48,643 train 050 3.879273e-03 0.867395
2019-12-03 18:51:59,739 train 100 4.593195e-03 0.845047
2019-12-03 18:52:10,839 train 150 4.816962e-03 0.839189
2019-12-03 18:52:21,947 train 200 4.843200e-03 0.837260
2019-12-03 18:52:26,931 training loss; R2: 4.825674e-03 0.836880
2019-12-03 18:52:27,045 valid 000 3.278714e-03 0.907086
2019-12-03 18:52:28,267 validation loss; R2: 3.245111e-03 0.886638
2019-12-03 18:52:28,287 epoch 17 lr 2.000000e-03
2019-12-03 18:52:28,575 train 000 5.822931e-03 0.849176
2019-12-03 18:52:39,665 train 050 4.300254e-03 0.846131
2019-12-03 18:52:50,758 train 100 4.258543e-03 0.853627
2019-12-03 18:53:01,856 train 150 4.310545e-03 0.849750
2019-12-03 18:53:12,947 train 200 4.365638e-03 0.850520
2019-12-03 18:53:17,931 training loss; R2: 4.377642e-03 0.850640
2019-12-03 18:53:18,045 valid 000 1.729827e-03 0.936099
2019-12-03 18:53:19,269 validation loss; R2: 3.120476e-03 0.893159
2019-12-03 18:53:19,290 epoch 18 lr 2.000000e-03
2019-12-03 18:53:19,585 train 000 4.176477e-03 0.731257
2019-12-03 18:53:30,675 train 050 4.748893e-03 0.851102
2019-12-03 18:53:41,766 train 100 4.526210e-03 0.847988
2019-12-03 18:53:52,852 train 150 4.368124e-03 0.851634
2019-12-03 18:54:03,937 train 200 4.381903e-03 0.851765
2019-12-03 18:54:08,919 training loss; R2: 4.332611e-03 0.852991
2019-12-03 18:54:09,039 valid 000 4.245910e-02 -1.432828
2019-12-03 18:54:10,263 validation loss; R2: 5.193291e-02 -0.837038
2019-12-03 18:54:10,283 epoch 19 lr 2.000000e-03
2019-12-03 18:54:10,575 train 000 3.517721e-03 0.915735
2019-12-03 18:54:21,655 train 050 4.893420e-03 0.847744
2019-12-03 18:54:32,732 train 100 4.528032e-03 0.850612
2019-12-03 18:54:43,808 train 150 4.334589e-03 0.856889
2019-12-03 18:54:54,878 train 200 4.205548e-03 0.857322
2019-12-03 18:54:59,854 training loss; R2: 4.364768e-03 0.853904
2019-12-03 18:54:59,969 valid 000 3.963985e-02 -0.436211
2019-12-03 18:55:01,193 validation loss; R2: 4.281889e-02 -0.567961
