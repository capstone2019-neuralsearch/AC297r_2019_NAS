2019-11-13 05:21:49,756 gpu device = 1
2019-11-13 05:21:49,756 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name='galaxy-zoo-cleaned', gpu=1, grad_clip=5, gz_dtree=False, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-galaxy-zoo-EXP-20191113-052149', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-11-13 05:22:01,311 param size = 0.189461MB
2019-11-13 05:22:01,315 epoch 0 lr 1.000000e-03
2019-11-13 05:22:03,416 train 000 9.734563e-01 -210.854867
2019-11-13 05:22:08,774 train 050 7.932461e-02 -17.022940
2019-11-13 05:22:14,081 train 100 5.616975e-02 -9.429406
2019-11-13 05:22:19,336 train 150 4.719997e-02 -6.604832
2019-11-13 05:22:24,542 train 200 4.238548e-02 -5.115258
2019-11-13 05:22:29,752 train 250 3.927973e-02 -4.197434
2019-11-13 05:22:34,950 train 300 3.713361e-02 -3.574712
2019-11-13 05:22:40,164 train 350 3.553442e-02 -3.119970
2019-11-13 05:22:45,374 train 400 3.430496e-02 -2.780075
2019-11-13 05:22:50,602 train 450 3.336194e-02 -2.527667
2019-11-13 05:22:55,784 train 500 3.244108e-02 -2.300777
2019-11-13 05:23:00,986 train 550 3.172215e-02 -2.112494
2019-11-13 05:23:06,190 train 600 3.108493e-02 -1.984785
2019-11-13 05:23:11,395 train 650 3.052036e-02 -1.855186
2019-11-13 05:23:16,616 train 700 2.997992e-02 -1.734296
2019-11-13 05:23:21,840 train 750 2.955554e-02 -1.631667
2019-11-13 05:23:27,053 train 800 2.914521e-02 -1.538652
2019-11-13 05:23:32,280 train 850 2.877032e-02 -1.473692
2019-11-13 05:23:34,482 training loss; R2: 2.866445e-02 -1.449129
2019-11-13 05:23:34,784 valid 000 2.250811e-02 0.021008
2019-11-13 05:23:36,482 valid 050 2.174419e-02 0.018035
2019-11-13 05:23:38,096 validation loss; R2: 2.146606e-02 0.010077
2019-11-13 05:23:38,107 epoch 1 lr 1.000000e-03
2019-11-13 05:23:38,580 train 000 2.342910e-02 -0.218576
2019-11-13 05:23:43,523 train 050 2.257304e-02 -0.169963
2019-11-13 05:23:48,487 train 100 2.228729e-02 -0.327739
2019-11-13 05:23:53,524 train 150 2.208495e-02 -0.407408
2019-11-13 05:23:58,744 train 200 2.190583e-02 -0.339686
2019-11-13 05:24:03,964 train 250 2.183947e-02 -0.285831
2019-11-13 05:24:09,179 train 300 2.159780e-02 -0.251755
2019-11-13 05:24:14,406 train 350 2.138176e-02 -0.220963
2019-11-13 05:24:19,610 train 400 2.129820e-02 -0.195840
2019-11-13 05:24:24,837 train 450 2.119092e-02 -0.172925
2019-11-13 05:24:30,075 train 500 2.109062e-02 -0.154283
2019-11-13 05:24:35,298 train 550 2.104765e-02 -0.146853
2019-11-13 05:24:40,516 train 600 2.095656e-02 -0.134271
2019-11-13 05:24:45,718 train 650 2.084282e-02 -0.122998
2019-11-13 05:24:50,932 train 700 2.073787e-02 -0.132856
2019-11-13 05:24:56,131 train 750 2.063227e-02 -0.121342
2019-11-13 05:25:01,335 train 800 2.056262e-02 -0.138281
2019-11-13 05:25:06,539 train 850 2.050833e-02 -0.128413
2019-11-13 05:25:08,090 training loss; R2: 2.048824e-02 -0.125687
2019-11-13 05:25:08,404 valid 000 1.614838e-02 0.215756
2019-11-13 05:25:10,086 valid 050 1.703318e-02 0.165991
2019-11-13 05:25:11,633 validation loss; R2: 1.703672e-02 0.154261
2019-11-13 05:25:11,653 epoch 2 lr 1.000000e-03
2019-11-13 05:25:12,042 train 000 1.940282e-02 0.079725
2019-11-13 05:25:17,218 train 050 1.912679e-02 0.046897
2019-11-13 05:25:22,441 train 100 1.886066e-02 -0.001535
2019-11-13 05:25:27,596 train 150 1.901778e-02 0.016142
2019-11-13 05:25:32,741 train 200 1.911961e-02 -0.012954
2019-11-13 05:25:37,882 train 250 1.902588e-02 0.007077
2019-11-13 05:25:43,038 train 300 1.896951e-02 0.016272
2019-11-13 05:25:48,196 train 350 1.889821e-02 0.026857
2019-11-13 05:25:53,339 train 400 1.886101e-02 0.027216
2019-11-13 05:25:58,489 train 450 1.886424e-02 0.033722
2019-11-13 05:26:03,671 train 500 1.880903e-02 0.041328
2019-11-13 05:26:08,923 train 550 1.871042e-02 0.046332
2019-11-13 05:26:14,154 train 600 1.865418e-02 0.050663
2019-11-13 05:26:19,515 train 650 1.862700e-02 0.053895
2019-11-13 05:26:24,851 train 700 1.855514e-02 0.053035
2019-11-13 05:26:30,082 train 750 1.847369e-02 0.056311
2019-11-13 05:26:35,297 train 800 1.840936e-02 0.060229
2019-11-13 05:26:40,512 train 850 1.836206e-02 0.061516
2019-11-13 05:26:42,065 training loss; R2: 1.834862e-02 0.062866
2019-11-13 05:26:42,381 valid 000 1.583179e-02 0.236136
2019-11-13 05:26:44,031 valid 050 1.563437e-02 0.220913
2019-11-13 05:26:45,541 validation loss; R2: 1.577896e-02 0.220721
2019-11-13 05:26:45,556 epoch 3 lr 1.000000e-03
2019-11-13 05:26:45,931 train 000 1.728170e-02 0.226585
2019-11-13 05:26:51,123 train 050 1.747074e-02 0.123036
2019-11-13 05:26:56,349 train 100 1.714904e-02 0.125840
2019-11-13 05:27:01,472 train 150 1.729956e-02 0.115937
2019-11-13 05:27:06,598 train 200 1.732943e-02 0.114956
2019-11-13 05:27:11,755 train 250 1.730851e-02 0.113650
2019-11-13 05:27:16,881 train 300 1.734823e-02 0.115117
2019-11-13 05:27:22,017 train 350 1.733031e-02 0.109801
2019-11-13 05:27:27,143 train 400 1.730324e-02 0.113375
2019-11-13 05:27:32,274 train 450 1.720515e-02 0.117390
2019-11-13 05:27:37,389 train 500 1.720776e-02 0.115332
2019-11-13 05:27:42,510 train 550 1.717921e-02 0.119924
2019-11-13 05:27:47,633 train 600 1.708809e-02 0.124178
2019-11-13 05:27:52,742 train 650 1.704667e-02 0.122155
2019-11-13 05:27:57,861 train 700 1.699305e-02 0.123571
2019-11-13 05:28:02,994 train 750 1.697353e-02 0.124435
2019-11-13 05:28:08,123 train 800 1.696356e-02 0.125363
2019-11-13 05:28:13,247 train 850 1.694966e-02 0.126736
2019-11-13 05:28:14,780 training loss; R2: 1.693374e-02 0.126777
2019-11-13 05:28:15,108 valid 000 1.408607e-02 0.240726
2019-11-13 05:28:16,756 valid 050 1.461837e-02 0.240043
2019-11-13 05:28:18,259 validation loss; R2: 1.458275e-02 0.179197
2019-11-13 05:28:18,274 epoch 4 lr 1.000000e-03
2019-11-13 05:28:18,707 train 000 1.763947e-02 0.289202
2019-11-13 05:28:23,906 train 050 1.644815e-02 0.022875
2019-11-13 05:28:29,242 train 100 1.646203e-02 0.092664
2019-11-13 05:28:34,547 train 150 1.626881e-02 0.106279
2019-11-13 05:28:39,811 train 200 1.624216e-02 0.101024
2019-11-13 05:28:45,066 train 250 1.623775e-02 0.100492
2019-11-13 05:28:50,300 train 300 1.614658e-02 0.112291
2019-11-13 05:28:55,533 train 350 1.611512e-02 0.120692
2019-11-13 05:29:00,779 train 400 1.611134e-02 0.124920
2019-11-13 05:29:06,043 train 450 1.611912e-02 0.120144
2019-11-13 05:29:11,292 train 500 1.606259e-02 0.122991
2019-11-13 05:29:16,535 train 550 1.603534e-02 0.121772
2019-11-13 05:29:21,763 train 600 1.600800e-02 0.124684
2019-11-13 05:29:27,019 train 650 1.594533e-02 0.130172
2019-11-13 05:29:32,241 train 700 1.590279e-02 0.134738
2019-11-13 05:29:37,462 train 750 1.590471e-02 0.137467
2019-11-13 05:29:42,724 train 800 1.588109e-02 0.126031
2019-11-13 05:29:47,970 train 850 1.587105e-02 0.128738
2019-11-13 05:29:49,528 training loss; R2: 1.585983e-02 0.127267
2019-11-13 05:29:49,843 valid 000 1.720780e-02 0.252879
2019-11-13 05:29:51,627 valid 050 1.487198e-02 0.188159
2019-11-13 05:29:53,142 validation loss; R2: 1.497564e-02 0.186212
2019-11-13 05:29:53,153 epoch 5 lr 1.000000e-03
2019-11-13 05:29:53,498 train 000 1.448874e-02 -1.152817
2019-11-13 05:29:58,463 train 050 1.517197e-02 0.152058
2019-11-13 05:30:03,626 train 100 1.535843e-02 -9.395444
2019-11-13 05:30:08,902 train 150 1.521513e-02 -6.230582
2019-11-13 05:30:14,253 train 200 1.522928e-02 -4.732897
2019-11-13 05:30:19,525 train 250 1.526297e-02 -3.759204
2019-11-13 05:30:24,793 train 300 1.523819e-02 -3.101243
2019-11-13 05:30:30,027 train 350 1.518550e-02 -2.640004
2019-11-13 05:30:35,301 train 400 1.516989e-02 -2.284037
2019-11-13 05:30:40,681 train 450 1.515063e-02 -2.014387
2019-11-13 05:30:46,054 train 500 1.512571e-02 -1.797548
2019-11-13 05:30:51,474 train 550 1.513995e-02 -1.617560
2019-11-13 05:30:56,856 train 600 1.511566e-02 -1.471233
2019-11-13 05:31:02,064 train 650 1.512710e-02 -1.347030
2019-11-13 05:31:07,188 train 700 1.511422e-02 -1.235970
2019-11-13 05:31:12,322 train 750 1.508044e-02 -1.142142
2019-11-13 05:31:17,439 train 800 1.506442e-02 -1.060291
2019-11-13 05:31:22,562 train 850 1.505267e-02 -0.986168
2019-11-13 05:31:24,098 training loss; R2: 1.504375e-02 -0.965526
2019-11-13 05:31:24,418 valid 000 5.478564e-02 -1.280103
2019-11-13 05:31:26,067 valid 050 5.844122e-02 -1.860291
2019-11-13 05:31:27,581 validation loss; R2: 5.830708e-02 -1.883272
2019-11-13 05:31:27,601 epoch 6 lr 1.000000e-03
2019-11-13 05:31:27,986 train 000 1.546740e-02 0.184539
2019-11-13 05:31:33,179 train 050 1.487133e-02 -0.300726
2019-11-13 05:31:38,475 train 100 1.481711e-02 -0.074595
2019-11-13 05:31:43,697 train 150 1.460527e-02 0.024068
2019-11-13 05:31:48,875 train 200 1.459307e-02 0.067338
2019-11-13 05:31:54,095 train 250 1.458327e-02 0.097046
2019-11-13 05:31:59,297 train 300 1.460423e-02 0.114141
2019-11-13 05:32:04,492 train 350 1.461076e-02 0.128570
2019-11-13 05:32:09,688 train 400 1.464027e-02 0.137964
2019-11-13 05:32:14,875 train 450 1.463283e-02 0.144243
2019-11-13 05:32:20,069 train 500 1.456854e-02 0.145570
2019-11-13 05:32:25,282 train 550 1.455308e-02 0.152444
2019-11-13 05:32:30,515 train 600 1.451184e-02 0.155384
2019-11-13 05:32:35,730 train 650 1.451195e-02 0.158535
2019-11-13 05:32:40,955 train 700 1.452585e-02 0.160230
2019-11-13 05:32:46,171 train 750 1.447445e-02 0.164010
2019-11-13 05:32:51,396 train 800 1.444797e-02 0.168024
2019-11-13 05:32:56,594 train 850 1.444387e-02 0.171926
2019-11-13 05:32:58,167 training loss; R2: 1.442910e-02 0.173172
2019-11-13 05:32:58,484 valid 000 2.734491e-02 -0.124495
2019-11-13 05:33:00,164 valid 050 2.683729e-02 -0.253083
2019-11-13 05:33:01,723 validation loss; R2: 2.668021e-02 -0.243749
2019-11-13 05:33:01,737 epoch 7 lr 1.000000e-03
2019-11-13 05:33:02,143 train 000 1.530643e-02 0.278878
2019-11-13 05:33:07,413 train 050 1.430535e-02 0.189153
2019-11-13 05:33:12,680 train 100 1.443487e-02 0.213862
2019-11-13 05:33:17,960 train 150 1.431434e-02 0.207033
2019-11-13 05:33:23,248 train 200 1.426904e-02 0.199525
2019-11-13 05:33:28,524 train 250 1.421556e-02 0.208688
2019-11-13 05:33:33,812 train 300 1.420901e-02 0.202553
2019-11-13 05:33:39,110 train 350 1.414988e-02 0.200763
2019-11-13 05:33:44,466 train 400 1.416869e-02 0.140550
2019-11-13 05:33:49,704 train 450 1.414438e-02 0.151920
2019-11-13 05:33:54,989 train 500 1.412097e-02 0.155992
2019-11-13 05:34:00,375 train 550 1.409051e-02 0.155880
2019-11-13 05:34:05,601 train 600 1.407085e-02 0.162502
2019-11-13 05:34:10,794 train 650 1.404838e-02 0.156843
2019-11-13 05:34:16,102 train 700 1.402229e-02 0.157972
2019-11-13 05:34:21,509 train 750 1.400928e-02 0.161035
2019-11-13 05:34:26,821 train 800 1.401080e-02 0.164330
2019-11-13 05:34:32,178 train 850 1.400506e-02 0.168565
2019-11-13 05:34:33,802 training loss; R2: 1.399734e-02 0.169948
2019-11-13 05:34:34,132 valid 000 1.166193e-02 0.267992
2019-11-13 05:34:35,772 valid 050 1.249563e-02 0.301624
2019-11-13 05:34:37,259 validation loss; R2: 1.241387e-02 0.296800
2019-11-13 05:34:37,272 epoch 8 lr 1.000000e-03
2019-11-13 05:34:37,661 train 000 1.317322e-02 -0.132168
2019-11-13 05:34:42,887 train 050 1.369935e-02 0.200287
2019-11-13 05:34:48,187 train 100 1.373812e-02 -0.045805
2019-11-13 05:34:53,396 train 150 1.370540e-02 0.053966
2019-11-13 05:34:58,613 train 200 1.373702e-02 0.099084
2019-11-13 05:35:03,819 train 250 1.373246e-02 0.126944
2019-11-13 05:35:08,994 train 300 1.372478e-02 0.143885
2019-11-13 05:35:14,139 train 350 1.379911e-02 0.153936
2019-11-13 05:35:19,311 train 400 1.374736e-02 0.159355
2019-11-13 05:35:24,485 train 450 1.373073e-02 0.167400
2019-11-13 05:35:29,630 train 500 1.373411e-02 0.172688
2019-11-13 05:35:34,774 train 550 1.371359e-02 0.180754
2019-11-13 05:35:39,926 train 600 1.370170e-02 0.182220
2019-11-13 05:35:45,083 train 650 1.370386e-02 0.185607
2019-11-13 05:35:50,242 train 700 1.370729e-02 0.185830
2019-11-13 05:35:55,403 train 750 1.367469e-02 0.188460
2019-11-13 05:36:00,548 train 800 1.366842e-02 0.192518
2019-11-13 05:36:05,685 train 850 1.365611e-02 0.195281
2019-11-13 05:36:07,224 training loss; R2: 1.364630e-02 0.196434
2019-11-13 05:36:07,568 valid 000 6.834730e-02 -3.277305
2019-11-13 05:36:09,207 valid 050 6.698941e-02 -4.881749
2019-11-13 05:36:10,731 validation loss; R2: 6.680296e-02 -4.604716
2019-11-13 05:36:10,751 epoch 9 lr 1.000000e-03
2019-11-13 05:36:11,195 train 000 1.309708e-02 0.249694
2019-11-13 05:36:16,325 train 050 1.345005e-02 0.254854
2019-11-13 05:36:21,473 train 100 1.342323e-02 0.249360
2019-11-13 05:36:26,601 train 150 1.348103e-02 0.249143
2019-11-13 05:36:31,738 train 200 1.346988e-02 0.240118
2019-11-13 05:36:36,876 train 250 1.345968e-02 0.238802
2019-11-13 05:36:42,029 train 300 1.355163e-02 0.237359
2019-11-13 05:36:47,219 train 350 1.351157e-02 0.243513
2019-11-13 05:36:52,402 train 400 1.349071e-02 0.196114
2019-11-13 05:36:57,594 train 450 1.346917e-02 0.204218
2019-11-13 05:37:02,775 train 500 1.348880e-02 0.202845
2019-11-13 05:37:07,969 train 550 1.348469e-02 0.201996
2019-11-13 05:37:13,163 train 600 1.349042e-02 0.199814
2019-11-13 05:37:18,355 train 650 1.345600e-02 0.202125
2019-11-13 05:37:23,554 train 700 1.344225e-02 0.204326
2019-11-13 05:37:28,738 train 750 1.344443e-02 0.181927
2019-11-13 05:37:33,924 train 800 1.343922e-02 0.187360
2019-11-13 05:37:39,108 train 850 1.343319e-02 0.192483
2019-11-13 05:37:40,653 training loss; R2: 1.343275e-02 0.193653
2019-11-13 05:37:40,975 valid 000 5.647108e-01 -18.416415
2019-11-13 05:37:42,668 valid 050 5.662266e-01 -28.108577
2019-11-13 05:37:44,222 validation loss; R2: 5.666361e-01 -33.939491
2019-11-13 05:37:44,241 epoch 10 lr 1.000000e-03
2019-11-13 05:37:44,651 train 000 1.224326e-02 0.145034
2019-11-13 05:37:49,855 train 050 1.357941e-02 0.224398
2019-11-13 05:37:55,142 train 100 1.340924e-02 0.221455
2019-11-13 05:38:00,391 train 150 1.327280e-02 0.211115
2019-11-13 05:38:05,714 train 200 1.315227e-02 0.224285
2019-11-13 05:38:10,997 train 250 1.317102e-02 0.226078
2019-11-13 05:38:16,295 train 300 1.314221e-02 0.232923
2019-11-13 05:38:21,546 train 350 1.310145e-02 0.231777
2019-11-13 05:38:26,863 train 400 1.314049e-02 0.230637
2019-11-13 05:38:32,153 train 450 1.314918e-02 0.235080
2019-11-13 05:38:37,413 train 500 1.314815e-02 0.237391
2019-11-13 05:38:42,698 train 550 1.315607e-02 0.241604
2019-11-13 05:38:47,943 train 600 1.315268e-02 0.243980
2019-11-13 05:38:53,201 train 650 1.315840e-02 0.241449
2019-11-13 05:38:58,560 train 700 1.315629e-02 0.242732
2019-11-13 05:39:03,861 train 750 1.315097e-02 0.242007
2019-11-13 05:39:09,205 train 800 1.314472e-02 0.242032
2019-11-13 05:39:14,551 train 850 1.312312e-02 0.241389
2019-11-13 05:39:16,122 training loss; R2: 1.311681e-02 0.242059
2019-11-13 05:39:16,455 valid 000 1.966634e+00 -162.885711
2019-11-13 05:39:18,164 valid 050 1.943316e+00 -148.834337
2019-11-13 05:39:19,700 validation loss; R2: 1.943532e+00 -149.134489
2019-11-13 05:39:19,711 epoch 11 lr 1.000000e-03
2019-11-13 05:39:20,081 train 000 1.193326e-02 0.416941
2019-11-13 05:39:25,071 train 050 1.274590e-02 0.273178
2019-11-13 05:39:30,028 train 100 1.295315e-02 0.270049
2019-11-13 05:39:34,971 train 150 1.302718e-02 0.259094
2019-11-13 05:39:39,933 train 200 1.299951e-02 0.256508
2019-11-13 05:39:44,926 train 250 1.297138e-02 0.262909
2019-11-13 05:39:50,102 train 300 1.297096e-02 0.248430
2019-11-13 05:39:55,301 train 350 1.299199e-02 0.245580
2019-11-13 05:40:00,500 train 400 1.298571e-02 0.249349
2019-11-13 05:40:05,714 train 450 1.294104e-02 0.250640
2019-11-13 05:40:11,027 train 500 1.296129e-02 0.248190
2019-11-13 05:40:16,320 train 550 1.291291e-02 0.251009
2019-11-13 05:40:21,514 train 600 1.290993e-02 0.251595
2019-11-13 05:40:26,716 train 650 1.289901e-02 0.253173
2019-11-13 05:40:31,962 train 700 1.289462e-02 0.252378
2019-11-13 05:40:37,196 train 750 1.288744e-02 0.254318
2019-11-13 05:40:42,412 train 800 1.289627e-02 0.251510
2019-11-13 05:40:47,826 train 850 1.289820e-02 0.251538
2019-11-13 05:40:49,470 training loss; R2: 1.290590e-02 0.251579
2019-11-13 05:40:49,789 valid 000 4.014835e-02 -1.026923
2019-11-13 05:40:51,430 valid 050 4.142169e-02 -2.393453
2019-11-13 05:40:52,952 validation loss; R2: 4.115479e-02 -1.996720
2019-11-13 05:40:52,965 epoch 12 lr 1.000000e-03
2019-11-13 05:40:53,340 train 000 1.374169e-02 0.314814
2019-11-13 05:40:58,574 train 050 1.269057e-02 0.237059
2019-11-13 05:41:03,834 train 100 1.263156e-02 0.244077
2019-11-13 05:41:09,073 train 150 1.274007e-02 0.247989
2019-11-13 05:41:14,337 train 200 1.278260e-02 0.254047
2019-11-13 05:41:19,577 train 250 1.271677e-02 0.260418
2019-11-13 05:41:24,867 train 300 1.268909e-02 0.258988
2019-11-13 05:41:30,175 train 350 1.269049e-02 0.262452
2019-11-13 05:41:35,415 train 400 1.267066e-02 0.264009
2019-11-13 05:41:40,706 train 450 1.269076e-02 0.259761
2019-11-13 05:41:45,959 train 500 1.268149e-02 0.261946
2019-11-13 05:41:51,323 train 550 1.268856e-02 0.259875
2019-11-13 05:41:56,573 train 600 1.267952e-02 0.261283
2019-11-13 05:42:01,791 train 650 1.270295e-02 0.263883
2019-11-13 05:42:07,103 train 700 1.271208e-02 0.264522
2019-11-13 05:42:12,370 train 750 1.269369e-02 0.264181
2019-11-13 05:42:17,634 train 800 1.269613e-02 0.264008
2019-11-13 05:42:22,922 train 850 1.269885e-02 0.264265
2019-11-13 05:42:24,479 training loss; R2: 1.269323e-02 0.265223
2019-11-13 05:42:24,808 valid 000 1.299918e-01 -4.173802
2019-11-13 05:42:26,485 valid 050 1.295163e-01 -6.851339
2019-11-13 05:42:27,978 validation loss; R2: 1.292082e-01 -6.842957
2019-11-13 05:42:27,995 epoch 13 lr 1.000000e-03
2019-11-13 05:42:28,391 train 000 1.251445e-02 0.374010
2019-11-13 05:42:33,552 train 050 1.257600e-02 0.271693
2019-11-13 05:42:38,918 train 100 1.270218e-02 0.278140
2019-11-13 05:42:44,184 train 150 1.270150e-02 0.269144
2019-11-13 05:42:49,452 train 200 1.265741e-02 0.267481
2019-11-13 05:42:54,697 train 250 1.258895e-02 0.265323
2019-11-13 05:42:59,926 train 300 1.261542e-02 0.253216
2019-11-13 05:43:05,179 train 350 1.262849e-02 0.258755
2019-11-13 05:43:10,438 train 400 1.259720e-02 0.257060
2019-11-13 05:43:15,715 train 450 1.258736e-02 0.250400
2019-11-13 05:43:20,991 train 500 1.259834e-02 0.252828
2019-11-13 05:43:26,248 train 550 1.255038e-02 0.255662
2019-11-13 05:43:31,606 train 600 1.255341e-02 0.256590
2019-11-13 05:43:36,993 train 650 1.253649e-02 0.258858
2019-11-13 05:43:42,405 train 700 1.252389e-02 0.256808
2019-11-13 05:43:47,720 train 750 1.250712e-02 0.256505
2019-11-13 05:43:53,031 train 800 1.250567e-02 0.255617
2019-11-13 05:43:58,344 train 850 1.251675e-02 0.253589
2019-11-13 05:43:59,967 training loss; R2: 1.251233e-02 0.253406
2019-11-13 05:44:00,320 valid 000 1.457657e+00 -65.610432
2019-11-13 05:44:02,006 valid 050 1.451274e+00 -85.283133
2019-11-13 05:44:03,549 validation loss; R2: 1.452208e+00 -83.202500
2019-11-13 05:44:03,563 epoch 14 lr 1.000000e-03
2019-11-13 05:44:03,962 train 000 1.240372e-02 0.296277
2019-11-13 05:44:09,132 train 050 1.230996e-02 0.257235
2019-11-13 05:44:14,382 train 100 1.234322e-02 0.253297
2019-11-13 05:44:19,692 train 150 1.243253e-02 0.257035
2019-11-13 05:44:25,135 train 200 1.242982e-02 0.248517
2019-11-13 05:44:30,549 train 250 1.240321e-02 0.256236
2019-11-13 05:44:35,925 train 300 1.239090e-02 0.255943
2019-11-13 05:44:41,149 train 350 1.235665e-02 0.260500
2019-11-13 05:44:46,466 train 400 1.235524e-02 0.237125
2019-11-13 05:44:51,837 train 450 1.235759e-02 0.243107
2019-11-13 05:44:57,296 train 500 1.237474e-02 0.246911
2019-11-13 05:45:02,523 train 550 1.235217e-02 0.251408
2019-11-13 05:45:07,743 train 600 1.234551e-02 0.252910
2019-11-13 05:45:13,007 train 650 1.234100e-02 0.242129
2019-11-13 05:45:18,242 train 700 1.233974e-02 0.244551
2019-11-13 05:45:23,627 train 750 1.232686e-02 0.247586
2019-11-13 05:45:29,104 train 800 1.230692e-02 0.251453
2019-11-13 05:45:34,586 train 850 1.229157e-02 0.254628
2019-11-13 05:45:36,208 training loss; R2: 1.228148e-02 0.254906
2019-11-13 05:45:36,535 valid 000 1.962965e-02 -0.076909
2019-11-13 05:45:38,172 valid 050 2.093252e-02 -0.510676
2019-11-13 05:45:39,677 validation loss; R2: 2.073371e-02 -0.604605
2019-11-13 05:45:39,689 epoch 15 lr 1.000000e-03
2019-11-13 05:45:40,051 train 000 1.130155e-02 0.376382
2019-11-13 05:45:45,351 train 050 1.232618e-02 0.225515
2019-11-13 05:45:50,654 train 100 1.227045e-02 0.252746
2019-11-13 05:45:55,889 train 150 1.224281e-02 0.211722
2019-11-13 05:46:01,117 train 200 1.224742e-02 0.225755
2019-11-13 05:46:06,353 train 250 1.228844e-02 0.238001
2019-11-13 05:46:11,585 train 300 1.225795e-02 0.244750
2019-11-13 05:46:16,810 train 350 1.223248e-02 0.246118
2019-11-13 05:46:22,042 train 400 1.224598e-02 0.251796
2019-11-13 05:46:27,264 train 450 1.224864e-02 0.256187
2019-11-13 05:46:32,481 train 500 1.224571e-02 0.256524
2019-11-13 05:46:37,721 train 550 1.222015e-02 0.258608
2019-11-13 05:46:42,951 train 600 1.223860e-02 0.257914
2019-11-13 05:46:48,207 train 650 1.224184e-02 0.260872
2019-11-13 05:46:53,452 train 700 1.222372e-02 0.262204
2019-11-13 05:46:58,705 train 750 1.222011e-02 0.263488
2019-11-13 05:47:03,952 train 800 1.222015e-02 0.264332
2019-11-13 05:47:09,178 train 850 1.221259e-02 0.264601
2019-11-13 05:47:10,731 training loss; R2: 1.221244e-02 0.265637
2019-11-13 05:47:11,061 valid 000 1.064875e+00 -68.508102
2019-11-13 05:47:12,738 valid 050 1.047863e+00 -147.662452
2019-11-13 05:47:14,265 validation loss; R2: 1.047074e+00 -129.938134
2019-11-13 05:47:14,282 epoch 16 lr 1.000000e-03
2019-11-13 05:47:14,694 train 000 1.093220e-02 0.154204
2019-11-13 05:47:19,967 train 050 1.209998e-02 0.315823
2019-11-13 05:47:25,265 train 100 1.189874e-02 0.318825
2019-11-13 05:47:30,604 train 150 1.202586e-02 0.303137
2019-11-13 05:47:36,024 train 200 1.202766e-02 0.296318
2019-11-13 05:47:41,384 train 250 1.203378e-02 0.284180
2019-11-13 05:47:46,764 train 300 1.205461e-02 0.278746
2019-11-13 05:47:52,050 train 350 1.204775e-02 0.248066
2019-11-13 05:47:57,419 train 400 1.202364e-02 0.249687
2019-11-13 05:48:02,691 train 450 1.206091e-02 0.254793
2019-11-13 05:48:08,047 train 500 1.207701e-02 0.260819
2019-11-13 05:48:13,403 train 550 1.204824e-02 0.265663
2019-11-13 05:48:18,701 train 600 1.203176e-02 0.269938
2019-11-13 05:48:23,974 train 650 1.203110e-02 0.269990
2019-11-13 05:48:29,358 train 700 1.201023e-02 0.270946
2019-11-13 05:48:34,665 train 750 1.201042e-02 0.275210
2019-11-13 05:48:39,909 train 800 1.202666e-02 0.276562
2019-11-13 05:48:45,240 train 850 1.202566e-02 0.276789
2019-11-13 05:48:46,860 training loss; R2: 1.202657e-02 0.277281
2019-11-13 05:48:47,189 valid 000 1.855943e-01 -12.786377
2019-11-13 05:48:48,845 valid 050 1.952513e-01 -15.360916
2019-11-13 05:48:50,351 validation loss; R2: 1.953000e-01 -15.865767
2019-11-13 05:48:50,363 epoch 17 lr 1.000000e-03
2019-11-13 05:48:50,748 train 000 1.467121e-02 0.293735
2019-11-13 05:48:55,976 train 050 1.215233e-02 0.261173
2019-11-13 05:49:01,215 train 100 1.198269e-02 0.279971
2019-11-13 05:49:06,559 train 150 1.192428e-02 0.292202
2019-11-13 05:49:11,882 train 200 1.189958e-02 0.296815
2019-11-13 05:49:17,261 train 250 1.191896e-02 0.295265
2019-11-13 05:49:22,500 train 300 1.194110e-02 0.282335
2019-11-13 05:49:27,739 train 350 1.195375e-02 0.284087
2019-11-13 05:49:33,146 train 400 1.194693e-02 0.283191
2019-11-13 05:49:38,389 train 450 1.194809e-02 0.277976
2019-11-13 05:49:43,625 train 500 1.193793e-02 0.277635
2019-11-13 05:49:48,849 train 550 1.198166e-02 0.278394
2019-11-13 05:49:54,068 train 600 1.196364e-02 0.279660
2019-11-13 05:49:59,273 train 650 1.194998e-02 0.275668
2019-11-13 05:50:04,481 train 700 1.196877e-02 0.278044
2019-11-13 05:50:09,671 train 750 1.195924e-02 0.274488
2019-11-13 05:50:14,849 train 800 1.196780e-02 0.275745
2019-11-13 05:50:20,050 train 850 1.198527e-02 0.276729
2019-11-13 05:50:21,607 training loss; R2: 1.198879e-02 0.277660
2019-11-13 05:50:21,941 valid 000 3.009683e-01 -19.030367
2019-11-13 05:50:23,583 valid 050 3.018646e-01 -24.305450
2019-11-13 05:50:25,061 validation loss; R2: 3.025312e-01 -22.532292
2019-11-13 05:50:25,078 epoch 18 lr 1.000000e-03
2019-11-13 05:50:25,422 train 000 1.094591e-02 0.241424
2019-11-13 05:50:30,360 train 050 1.189610e-02 0.252153
2019-11-13 05:50:35,299 train 100 1.187839e-02 0.246369
2019-11-13 05:50:40,241 train 150 1.187985e-02 0.270107
2019-11-13 05:50:45,215 train 200 1.181871e-02 0.279630
2019-11-13 05:50:50,435 train 250 1.184190e-02 0.281626
2019-11-13 05:50:55,685 train 300 1.179125e-02 0.284977
2019-11-13 05:51:00,868 train 350 1.184013e-02 0.285348
2019-11-13 05:51:06,070 train 400 1.184782e-02 0.283343
2019-11-13 05:51:11,276 train 450 1.185800e-02 0.282438
2019-11-13 05:51:16,482 train 500 1.187942e-02 0.283446
2019-11-13 05:51:21,702 train 550 1.186747e-02 0.285660
2019-11-13 05:51:26,913 train 600 1.187793e-02 0.286373
2019-11-13 05:51:32,128 train 650 1.186569e-02 0.282475
2019-11-13 05:51:37,364 train 700 1.187460e-02 0.280925
2019-11-13 05:51:42,585 train 750 1.186166e-02 0.279249
2019-11-13 05:51:47,812 train 800 1.184086e-02 0.281366
2019-11-13 05:51:53,016 train 850 1.183891e-02 0.226142
2019-11-13 05:51:54,570 training loss; R2: 1.182722e-02 0.227454
2019-11-13 05:51:54,916 valid 000 2.857968e-01 -9.670630
2019-11-13 05:51:56,569 valid 050 2.857859e-01 -41.770110
2019-11-13 05:51:58,103 validation loss; R2: 2.851735e-01 -28.501607
2019-11-13 05:51:58,122 epoch 19 lr 1.000000e-03
2019-11-13 05:51:58,522 train 000 1.276915e-02 0.306078
2019-11-13 05:52:03,751 train 050 1.166513e-02 0.247449
2019-11-13 05:52:09,064 train 100 1.182934e-02 0.263425
2019-11-13 05:52:14,411 train 150 1.176803e-02 0.273927
2019-11-13 05:52:19,674 train 200 1.182932e-02 0.209976
2019-11-13 05:52:24,986 train 250 1.184743e-02 0.174913
2019-11-13 05:52:30,314 train 300 1.180834e-02 0.194423
2019-11-13 05:52:35,592 train 350 1.174426e-02 0.209299
2019-11-13 05:52:40,881 train 400 1.174538e-02 0.220358
2019-11-13 05:52:46,193 train 450 1.173508e-02 0.059849
2019-11-13 05:52:51,614 train 500 1.175347e-02 0.078203
2019-11-13 05:52:56,827 train 550 1.175396e-02 0.101293
2019-11-13 05:53:02,137 train 600 1.175567e-02 0.118000
2019-11-13 05:53:07,514 train 650 1.174017e-02 0.129977
2019-11-13 05:53:12,891 train 700 1.175114e-02 0.141899
2019-11-13 05:53:18,269 train 750 1.174607e-02 0.152485
2019-11-13 05:53:23,645 train 800 1.174228e-02 0.161135
2019-11-13 05:53:29,055 train 850 1.172958e-02 0.169583
2019-11-13 05:53:30,653 training loss; R2: 1.173828e-02 0.170267
2019-11-13 05:53:30,976 valid 000 3.193430e-01 -15.970430
2019-11-13 05:53:32,734 valid 050 3.299050e-01 -30.609451
2019-11-13 05:53:34,292 validation loss; R2: 3.305637e-01 -26.247431
