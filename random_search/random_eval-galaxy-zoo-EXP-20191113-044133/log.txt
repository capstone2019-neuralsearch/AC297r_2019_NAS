2019-11-13 04:41:33,917 gpu device = 1
2019-11-13 04:41:33,917 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name='galaxy-zoo-cleaned', gpu=1, grad_clip=5, gz_dtree=False, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-galaxy-zoo-EXP-20191113-044133', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-11-13 04:41:45,560 param size = 0.271157MB
2019-11-13 04:41:45,564 epoch 0 lr 1.000000e-03
2019-11-13 04:41:47,716 train 000 2.020905e-01 -26.516923
2019-11-13 04:41:54,732 train 050 4.865586e-02 -5.152283
2019-11-13 04:42:01,612 train 100 3.724625e-02 -2.929561
2019-11-13 04:42:08,155 train 150 3.304449e-02 -2.050543
2019-11-13 04:42:14,656 train 200 3.049518e-02 -1.669647
2019-11-13 04:42:21,156 train 250 2.876277e-02 -1.358557
2019-11-13 04:42:27,660 train 300 2.748974e-02 -1.137805
2019-11-13 04:42:34,162 train 350 2.653018e-02 -0.982450
2019-11-13 04:42:40,656 train 400 2.569941e-02 -0.877675
2019-11-13 04:42:47,158 train 450 2.505526e-02 -0.777771
2019-11-13 04:42:53,671 train 500 2.445751e-02 -0.709145
2019-11-13 04:43:00,185 train 550 2.397657e-02 -0.639507
2019-11-13 04:43:06,699 train 600 2.349699e-02 -0.581780
2019-11-13 04:43:13,214 train 650 2.311414e-02 -0.530638
2019-11-13 04:43:19,731 train 700 2.276322e-02 -0.489052
2019-11-13 04:43:26,246 train 750 2.247322e-02 -0.450564
2019-11-13 04:43:32,758 train 800 2.215688e-02 -0.424828
2019-11-13 04:43:39,272 train 850 2.186337e-02 -0.394594
2019-11-13 04:43:41,921 training loss; R2: 2.177857e-02 -0.385348
2019-11-13 04:43:42,228 valid 000 1.208142e-02 0.229761
2019-11-13 04:43:43,948 valid 050 1.654283e-02 0.095143
2019-11-13 04:43:45,585 validation loss; R2: 1.666489e-02 0.084882
2019-11-13 04:43:45,601 epoch 1 lr 1.000000e-03
2019-11-13 04:43:46,117 train 000 1.816106e-02 0.143632
2019-11-13 04:43:52,640 train 050 1.688855e-02 0.145085
2019-11-13 04:43:59,157 train 100 1.691125e-02 0.117705
2019-11-13 04:44:05,675 train 150 1.673326e-02 0.131813
2019-11-13 04:44:12,187 train 200 1.662876e-02 0.125110
2019-11-13 04:44:18,698 train 250 1.655081e-02 0.121972
2019-11-13 04:44:25,218 train 300 1.645617e-02 0.125106
2019-11-13 04:44:31,735 train 350 1.635857e-02 0.131195
2019-11-13 04:44:38,251 train 400 1.629650e-02 0.133723
2019-11-13 04:44:44,763 train 450 1.622283e-02 0.129286
2019-11-13 04:44:51,283 train 500 1.609266e-02 0.137061
2019-11-13 04:44:57,808 train 550 1.601138e-02 0.126869
2019-11-13 04:45:04,330 train 600 1.595618e-02 0.131329
2019-11-13 04:45:10,849 train 650 1.592093e-02 0.131659
2019-11-13 04:45:17,371 train 700 1.584887e-02 0.125443
2019-11-13 04:45:23,893 train 750 1.577283e-02 0.132176
2019-11-13 04:45:30,422 train 800 1.570752e-02 0.136074
2019-11-13 04:45:36,945 train 850 1.562664e-02 0.140425
2019-11-13 04:45:38,896 training loss; R2: 1.561485e-02 0.141949
2019-11-13 04:45:39,183 valid 000 1.309964e-02 -0.149176
2019-11-13 04:45:40,860 valid 050 1.304341e-02 0.187908
2019-11-13 04:45:42,378 validation loss; R2: 1.305364e-02 0.206924
2019-11-13 04:45:42,393 epoch 2 lr 1.000000e-03
2019-11-13 04:45:42,802 train 000 1.203012e-02 0.252208
2019-11-13 04:45:49,515 train 050 1.404755e-02 -0.040131
2019-11-13 04:45:56,093 train 100 1.403511e-02 0.075939
2019-11-13 04:46:02,727 train 150 1.411518e-02 0.120606
2019-11-13 04:46:09,284 train 200 1.410603e-02 0.140957
2019-11-13 04:46:15,840 train 250 1.401042e-02 0.126184
2019-11-13 04:46:22,405 train 300 1.401521e-02 0.142350
2019-11-13 04:46:28,985 train 350 1.396915e-02 0.157027
2019-11-13 04:46:35,562 train 400 1.389140e-02 0.165983
2019-11-13 04:46:42,130 train 450 1.388141e-02 0.170262
2019-11-13 04:46:48,700 train 500 1.386564e-02 0.174945
2019-11-13 04:46:55,284 train 550 1.383864e-02 0.182185
2019-11-13 04:47:01,856 train 600 1.377253e-02 0.188012
2019-11-13 04:47:08,426 train 650 1.373462e-02 0.194235
2019-11-13 04:47:14,997 train 700 1.373472e-02 0.196391
2019-11-13 04:47:21,573 train 750 1.371579e-02 0.199333
2019-11-13 04:47:28,146 train 800 1.368649e-02 0.204048
2019-11-13 04:47:34,819 train 850 1.364921e-02 0.201881
2019-11-13 04:47:36,782 training loss; R2: 1.364904e-02 0.201655
2019-11-13 04:47:37,073 valid 000 1.373241e-02 -0.168569
2019-11-13 04:47:38,792 valid 050 1.292606e-02 0.262929
2019-11-13 04:47:40,335 validation loss; R2: 1.307555e-02 0.150599
2019-11-13 04:47:40,356 epoch 3 lr 1.000000e-03
2019-11-13 04:47:40,729 train 000 1.261023e-02 0.286575
2019-11-13 04:47:47,513 train 050 1.335599e-02 0.259348
2019-11-13 04:47:54,163 train 100 1.304296e-02 0.259285
2019-11-13 04:48:00,941 train 150 1.309987e-02 0.244407
2019-11-13 04:48:07,836 train 200 1.312205e-02 0.247196
2019-11-13 04:48:14,620 train 250 1.303353e-02 0.249261
2019-11-13 04:48:21,424 train 300 1.301868e-02 0.254145
2019-11-13 04:48:28,240 train 350 1.301435e-02 0.255461
2019-11-13 04:48:35,079 train 400 1.295696e-02 0.252707
2019-11-13 04:48:41,892 train 450 1.290515e-02 0.254009
2019-11-13 04:48:48,706 train 500 1.284103e-02 0.249732
2019-11-13 04:48:55,539 train 550 1.279012e-02 -0.108613
2019-11-13 04:49:02,352 train 600 1.276552e-02 -0.074678
2019-11-13 04:49:09,167 train 650 1.277487e-02 -0.051072
2019-11-13 04:49:15,978 train 700 1.276742e-02 -0.027001
2019-11-13 04:49:22,787 train 750 1.273265e-02 -0.006107
2019-11-13 04:49:29,612 train 800 1.270364e-02 0.012162
2019-11-13 04:49:36,425 train 850 1.269071e-02 0.026468
2019-11-13 04:49:38,464 training loss; R2: 1.267234e-02 0.030326
2019-11-13 04:49:38,751 valid 000 1.201773e-02 0.278828
2019-11-13 04:49:40,421 valid 050 1.128425e-02 0.326937
2019-11-13 04:49:41,962 validation loss; R2: 1.120040e-02 0.334801
2019-11-13 04:49:41,984 epoch 4 lr 1.000000e-03
2019-11-13 04:49:42,391 train 000 1.227971e-02 0.313882
2019-11-13 04:49:49,141 train 050 1.265773e-02 0.290565
2019-11-13 04:49:55,688 train 100 1.252942e-02 0.199797
2019-11-13 04:50:02,221 train 150 1.258173e-02 0.227308
2019-11-13 04:50:08,763 train 200 1.246115e-02 0.242860
2019-11-13 04:50:15,298 train 250 1.236290e-02 0.252467
2019-11-13 04:50:21,827 train 300 1.232211e-02 0.253194
2019-11-13 04:50:28,370 train 350 1.228852e-02 0.255658
2019-11-13 04:50:34,908 train 400 1.227296e-02 0.259018
2019-11-13 04:50:41,481 train 450 1.223289e-02 0.262619
2019-11-13 04:50:47,996 train 500 1.222699e-02 0.261939
2019-11-13 04:50:54,515 train 550 1.222741e-02 0.262957
2019-11-13 04:51:01,033 train 600 1.218712e-02 0.264428
2019-11-13 04:51:07,550 train 650 1.215775e-02 0.265534
2019-11-13 04:51:14,069 train 700 1.212559e-02 0.267958
2019-11-13 04:51:20,592 train 750 1.211227e-02 0.270090
2019-11-13 04:51:27,113 train 800 1.208917e-02 0.270551
2019-11-13 04:51:33,635 train 850 1.207494e-02 0.269546
2019-11-13 04:51:35,586 training loss; R2: 1.207247e-02 0.270898
2019-11-13 04:51:35,900 valid 000 1.052843e-02 0.419185
2019-11-13 04:51:37,591 valid 050 1.066980e-02 0.368724
2019-11-13 04:51:39,113 validation loss; R2: 1.074883e-02 0.358971
2019-11-13 04:51:39,128 epoch 5 lr 1.000000e-03
2019-11-13 04:51:39,519 train 000 1.308656e-02 0.329336
2019-11-13 04:51:46,032 train 050 1.171475e-02 0.280613
2019-11-13 04:51:52,540 train 100 1.179843e-02 0.288239
2019-11-13 04:51:59,050 train 150 1.180768e-02 0.261193
2019-11-13 04:52:05,561 train 200 1.177521e-02 0.265363
2019-11-13 04:52:12,068 train 250 1.182670e-02 0.266903
2019-11-13 04:52:18,578 train 300 1.181633e-02 0.264362
2019-11-13 04:52:25,085 train 350 1.175448e-02 0.261998
2019-11-13 04:52:31,590 train 400 1.177298e-02 0.267579
2019-11-13 04:52:38,108 train 450 1.174220e-02 0.265661
2019-11-13 04:52:44,620 train 500 1.173579e-02 0.267103
2019-11-13 04:52:51,136 train 550 1.172318e-02 0.261794
2019-11-13 04:52:57,648 train 600 1.170467e-02 0.266659
2019-11-13 04:53:04,159 train 650 1.169991e-02 0.270602
2019-11-13 04:53:10,673 train 700 1.168574e-02 0.273404
2019-11-13 04:53:17,182 train 750 1.167717e-02 0.275141
2019-11-13 04:53:23,687 train 800 1.164891e-02 0.276333
2019-11-13 04:53:30,199 train 850 1.164711e-02 0.278354
2019-11-13 04:53:32,148 training loss; R2: 1.164352e-02 0.279599
2019-11-13 04:53:32,443 valid 000 1.135986e-02 -0.060813
2019-11-13 04:53:34,166 valid 050 1.108944e-02 0.196811
2019-11-13 04:53:35,741 validation loss; R2: 1.088060e-02 0.232710
2019-11-13 04:53:35,763 epoch 6 lr 1.000000e-03
2019-11-13 04:53:36,149 train 000 9.339069e-03 0.268978
2019-11-13 04:53:42,693 train 050 1.103263e-02 0.328473
2019-11-13 04:53:49,217 train 100 1.126572e-02 0.331503
2019-11-13 04:53:55,747 train 150 1.124881e-02 0.336833
2019-11-13 04:54:02,280 train 200 1.127644e-02 0.330152
2019-11-13 04:54:08,812 train 250 1.119931e-02 0.329450
2019-11-13 04:54:15,335 train 300 1.122924e-02 0.321831
2019-11-13 04:54:21,886 train 350 1.122291e-02 0.320628
2019-11-13 04:54:28,411 train 400 1.124833e-02 0.317869
2019-11-13 04:54:34,932 train 450 1.129258e-02 0.316235
2019-11-13 04:54:41,457 train 500 1.127113e-02 0.316698
2019-11-13 04:54:47,998 train 550 1.126936e-02 0.297018
2019-11-13 04:54:54,521 train 600 1.129135e-02 0.298597
2019-11-13 04:55:01,045 train 650 1.129579e-02 0.298140
2019-11-13 04:55:07,604 train 700 1.130979e-02 0.297141
2019-11-13 04:55:14,130 train 750 1.130542e-02 0.298730
2019-11-13 04:55:20,654 train 800 1.129522e-02 0.298622
2019-11-13 04:55:27,181 train 850 1.127359e-02 0.300489
2019-11-13 04:55:29,131 training loss; R2: 1.127452e-02 0.298784
2019-11-13 04:55:29,430 valid 000 6.212748e-01 -166.968348
2019-11-13 04:55:31,143 valid 050 6.221801e-01 -140.905921
2019-11-13 04:55:32,701 validation loss; R2: 6.210216e-01 -139.261325
2019-11-13 04:55:32,716 epoch 7 lr 1.000000e-03
2019-11-13 04:55:33,070 train 000 1.550148e-02 0.232670
2019-11-13 04:55:39,862 train 050 1.143353e-02 0.287111
2019-11-13 04:55:46,533 train 100 1.123081e-02 0.280856
2019-11-13 04:55:53,079 train 150 1.119606e-02 0.295768
2019-11-13 04:55:59,644 train 200 1.118389e-02 0.293276
2019-11-13 04:56:06,203 train 250 1.114878e-02 0.301915
2019-11-13 04:56:12,746 train 300 1.112760e-02 0.289133
2019-11-13 04:56:19,344 train 350 1.113436e-02 0.283306
2019-11-13 04:56:25,936 train 400 1.111466e-02 0.288515
2019-11-13 04:56:32,491 train 450 1.114554e-02 0.290790
2019-11-13 04:56:39,063 train 500 1.116641e-02 0.290144
2019-11-13 04:56:45,761 train 550 1.112683e-02 0.281816
2019-11-13 04:56:52,375 train 600 1.109221e-02 0.286164
2019-11-13 04:56:58,956 train 650 1.107141e-02 0.288352
2019-11-13 04:57:05,653 train 700 1.107319e-02 0.287051
2019-11-13 04:57:12,244 train 750 1.106118e-02 0.288609
2019-11-13 04:57:18,813 train 800 1.105899e-02 0.287380
2019-11-13 04:57:25,413 train 850 1.103459e-02 0.291076
2019-11-13 04:57:27,404 training loss; R2: 1.103626e-02 0.292208
2019-11-13 04:57:27,720 valid 000 9.206743e-03 0.471919
2019-11-13 04:57:29,445 valid 050 1.070546e-02 0.356042
2019-11-13 04:57:31,014 validation loss; R2: 1.091840e-02 0.369746
2019-11-13 04:57:31,029 epoch 8 lr 1.000000e-03
2019-11-13 04:57:31,422 train 000 9.603507e-03 0.355031
2019-11-13 04:57:38,132 train 050 1.071616e-02 0.359442
2019-11-13 04:57:44,816 train 100 1.072799e-02 0.336480
2019-11-13 04:57:51,555 train 150 1.067957e-02 0.314706
2019-11-13 04:57:58,327 train 200 1.060169e-02 0.323282
2019-11-13 04:58:05,008 train 250 1.068855e-02 0.241225
2019-11-13 04:58:11,679 train 300 1.069085e-02 0.258403
2019-11-13 04:58:18,343 train 350 1.072907e-02 0.266401
2019-11-13 04:58:24,944 train 400 1.073753e-02 0.274185
2019-11-13 04:58:31,678 train 450 1.072630e-02 0.241084
2019-11-13 04:58:38,453 train 500 1.076094e-02 0.247939
2019-11-13 04:58:45,220 train 550 1.075911e-02 -1.232669
2019-11-13 04:58:51,910 train 600 1.075354e-02 -1.118580
2019-11-13 04:58:58,573 train 650 1.075798e-02 -1.010347
2019-11-13 04:59:05,219 train 700 1.076407e-02 -0.918551
2019-11-13 04:59:11,921 train 750 1.076992e-02 -0.838683
2019-11-13 04:59:18,637 train 800 1.077313e-02 -0.767205
2019-11-13 04:59:25,435 train 850 1.077011e-02 -0.705749
2019-11-13 04:59:27,465 training loss; R2: 1.076896e-02 -0.687676
2019-11-13 04:59:27,760 valid 000 6.961954e-02 -9.604173
2019-11-13 04:59:29,423 valid 050 7.381938e-02 -11.402354
2019-11-13 04:59:30,940 validation loss; R2: 7.390148e-02 -11.467986
2019-11-13 04:59:30,966 epoch 9 lr 1.000000e-03
2019-11-13 04:59:31,422 train 000 9.474046e-03 0.392396
2019-11-13 04:59:38,345 train 050 1.034989e-02 0.295601
2019-11-13 04:59:45,090 train 100 1.051317e-02 0.300441
2019-11-13 04:59:51,811 train 150 1.057015e-02 0.309574
2019-11-13 04:59:58,779 train 200 1.065066e-02 0.318631
2019-11-13 05:00:05,728 train 250 1.068854e-02 0.317886
2019-11-13 05:00:12,688 train 300 1.064003e-02 0.318127
2019-11-13 05:00:19,658 train 350 1.063074e-02 0.320706
2019-11-13 05:00:26,453 train 400 1.066227e-02 0.319729
2019-11-13 05:00:33,138 train 450 1.066150e-02 0.315494
2019-11-13 05:00:39,716 train 500 1.062164e-02 0.321075
2019-11-13 05:00:46,276 train 550 1.062630e-02 0.323475
2019-11-13 05:00:52,824 train 600 1.060319e-02 0.301920
2019-11-13 05:00:59,375 train 650 1.060012e-02 0.304969
2019-11-13 05:01:05,951 train 700 1.058053e-02 0.308810
2019-11-13 05:01:12,562 train 750 1.058629e-02 0.310462
2019-11-13 05:01:19,128 train 800 1.056918e-02 0.311610
2019-11-13 05:01:25,674 train 850 1.057409e-02 0.312712
2019-11-13 05:01:27,631 training loss; R2: 1.057243e-02 0.313124
2019-11-13 05:01:27,949 valid 000 2.390662e-01 -23.443407
2019-11-13 05:01:29,634 valid 050 2.478161e-01 -48.111347
2019-11-13 05:01:31,153 validation loss; R2: 2.471988e-01 -45.010364
2019-11-13 05:01:31,167 epoch 10 lr 1.000000e-03
2019-11-13 05:01:31,548 train 000 8.133987e-03 0.437388
2019-11-13 05:01:38,472 train 050 1.055904e-02 0.325678
2019-11-13 05:01:45,294 train 100 1.047783e-02 0.293092
2019-11-13 05:01:52,009 train 150 1.052041e-02 0.310938
2019-11-13 05:01:58,722 train 200 1.049651e-02 0.320296
2019-11-13 05:02:05,552 train 250 1.046321e-02 0.304335
2019-11-13 05:02:12,482 train 300 1.042709e-02 0.308930
2019-11-13 05:02:19,390 train 350 1.043890e-02 0.304121
2019-11-13 05:02:26,170 train 400 1.046719e-02 0.306467
2019-11-13 05:02:32,867 train 450 1.044370e-02 0.302347
2019-11-13 05:02:39,577 train 500 1.043929e-02 0.308502
2019-11-13 05:02:46,407 train 550 1.048091e-02 0.308747
2019-11-13 05:02:53,332 train 600 1.047636e-02 0.312155
2019-11-13 05:03:00,148 train 650 1.046858e-02 0.314189
2019-11-13 05:03:06,948 train 700 1.047573e-02 -0.421978
2019-11-13 05:03:13,900 train 750 1.048006e-02 -0.372079
2019-11-13 05:03:20,815 train 800 1.048702e-02 -0.326988
2019-11-13 05:03:27,878 train 850 1.049384e-02 -0.287891
2019-11-13 05:03:29,957 training loss; R2: 1.049490e-02 -0.276768
2019-11-13 05:03:30,273 valid 000 4.331461e-01 -49.201845
2019-11-13 05:03:31,984 valid 050 4.227044e-01 -128.872021
2019-11-13 05:03:33,551 validation loss; R2: 4.227573e-01 -129.899164
2019-11-13 05:03:33,570 epoch 11 lr 1.000000e-03
2019-11-13 05:03:34,000 train 000 9.601626e-03 0.393233
2019-11-13 05:03:40,965 train 050 1.042162e-02 0.365446
2019-11-13 05:03:47,929 train 100 1.039792e-02 0.343936
2019-11-13 05:03:54,783 train 150 1.024748e-02 0.353072
2019-11-13 05:04:01,637 train 200 1.022565e-02 0.355596
2019-11-13 05:04:08,497 train 250 1.012067e-02 0.324675
2019-11-13 05:04:15,324 train 300 1.017425e-02 0.321610
2019-11-13 05:04:22,162 train 350 1.020515e-02 0.329067
2019-11-13 05:04:28,989 train 400 1.017442e-02 0.328829
2019-11-13 05:04:35,803 train 450 1.021631e-02 0.331736
2019-11-13 05:04:42,586 train 500 1.023945e-02 0.326290
2019-11-13 05:04:49,385 train 550 1.026740e-02 0.326895
2019-11-13 05:04:56,195 train 600 1.029448e-02 0.323181
2019-11-13 05:05:03,033 train 650 1.032383e-02 0.325227
2019-11-13 05:05:09,869 train 700 1.032344e-02 0.327964
2019-11-13 05:05:16,706 train 750 1.029366e-02 0.329930
2019-11-13 05:05:23,544 train 800 1.028395e-02 0.332239
2019-11-13 05:05:30,424 train 850 1.027442e-02 0.335397
2019-11-13 05:05:32,448 training loss; R2: 1.027057e-02 0.335725
2019-11-13 05:05:32,747 valid 000 2.594480e-02 -1.211520
2019-11-13 05:05:34,376 valid 050 3.158756e-02 -0.802405
2019-11-13 05:05:35,888 validation loss; R2: 3.147204e-02 -0.784831
2019-11-13 05:05:35,906 epoch 12 lr 1.000000e-03
2019-11-13 05:05:36,376 train 000 1.178800e-02 0.366302
2019-11-13 05:05:43,035 train 050 1.061533e-02 0.364641
2019-11-13 05:05:49,820 train 100 1.054947e-02 0.361792
2019-11-13 05:05:56,596 train 150 1.057050e-02 0.359203
2019-11-13 05:06:03,522 train 200 1.054953e-02 0.352161
2019-11-13 05:06:10,359 train 250 1.049521e-02 0.354651
2019-11-13 05:06:17,199 train 300 1.045080e-02 0.342822
2019-11-13 05:06:24,017 train 350 1.049242e-02 0.340198
2019-11-13 05:06:30,749 train 400 1.048293e-02 0.339231
2019-11-13 05:06:37,620 train 450 1.045806e-02 0.341323
2019-11-13 05:06:44,562 train 500 1.042500e-02 0.341375
2019-11-13 05:06:51,520 train 550 1.039731e-02 0.341135
2019-11-13 05:06:58,453 train 600 1.034000e-02 0.342387
2019-11-13 05:07:05,395 train 650 1.035733e-02 0.337843
2019-11-13 05:07:12,272 train 700 1.038178e-02 0.332520
2019-11-13 05:07:19,174 train 750 1.037179e-02 0.333521
2019-11-13 05:07:26,100 train 800 1.036210e-02 0.335886
2019-11-13 05:07:32,868 train 850 1.037795e-02 0.332909
2019-11-13 05:07:34,884 training loss; R2: 1.038904e-02 0.333279
2019-11-13 05:07:35,167 valid 000 7.544262e-01 -282.286535
2019-11-13 05:07:36,846 valid 050 7.456282e-01 -211.554608
2019-11-13 05:07:38,366 validation loss; R2: 7.466457e-01 -222.478740
2019-11-13 05:07:38,385 epoch 13 lr 1.000000e-03
2019-11-13 05:07:38,787 train 000 8.619014e-03 0.467056
2019-11-13 05:07:45,688 train 050 1.051547e-02 0.342911
2019-11-13 05:07:52,608 train 100 1.052240e-02 0.342108
2019-11-13 05:07:59,594 train 150 1.060629e-02 0.320453
2019-11-13 05:08:06,363 train 200 1.053891e-02 0.315558
2019-11-13 05:08:13,146 train 250 1.050002e-02 0.322749
2019-11-13 05:08:19,965 train 300 1.051795e-02 0.323505
2019-11-13 05:08:26,700 train 350 1.049246e-02 0.326274
2019-11-13 05:08:33,519 train 400 1.045993e-02 0.282293
2019-11-13 05:08:40,402 train 450 1.040206e-02 0.291367
2019-11-13 05:08:47,177 train 500 1.039592e-02 0.298223
2019-11-13 05:08:54,092 train 550 1.040086e-02 0.294040
2019-11-13 05:09:00,911 train 600 1.039329e-02 0.293160
2019-11-13 05:09:07,725 train 650 1.036755e-02 0.299149
2019-11-13 05:09:14,580 train 700 1.036271e-02 0.300832
2019-11-13 05:09:21,365 train 750 1.036697e-02 0.303773
2019-11-13 05:09:28,052 train 800 1.040416e-02 0.304283
2019-11-13 05:09:34,980 train 850 1.042879e-02 0.306315
2019-11-13 05:09:37,068 training loss; R2: 1.043110e-02 0.307390
2019-11-13 05:09:37,394 valid 000 3.187920e-01 -75.612266
2019-11-13 05:09:39,096 valid 050 3.241931e-01 -96.378131
2019-11-13 05:09:40,625 validation loss; R2: 3.243962e-01 -102.068528
2019-11-13 05:09:40,649 epoch 14 lr 1.000000e-03
2019-11-13 05:09:41,060 train 000 1.385321e-02 0.378563
2019-11-13 05:09:47,764 train 050 1.028154e-02 0.361589
2019-11-13 05:09:54,712 train 100 1.044633e-02 0.336759
2019-11-13 05:10:01,600 train 150 1.044599e-02 0.326405
2019-11-13 05:10:08,269 train 200 1.037260e-02 0.334395
2019-11-13 05:10:14,911 train 250 1.038121e-02 0.342962
2019-11-13 05:10:21,579 train 300 1.039018e-02 0.338080
2019-11-13 05:10:28,434 train 350 1.037005e-02 0.344180
2019-11-13 05:10:35,165 train 400 1.040087e-02 0.345071
2019-11-13 05:10:41,819 train 450 1.038726e-02 0.340270
2019-11-13 05:10:48,472 train 500 1.036826e-02 0.332005
2019-11-13 05:10:55,430 train 550 1.036683e-02 0.330911
2019-11-13 05:11:02,018 train 600 1.035809e-02 0.329938
2019-11-13 05:11:08,594 train 650 1.035049e-02 0.332616
2019-11-13 05:11:15,147 train 700 1.033202e-02 0.333840
2019-11-13 05:11:21,712 train 750 1.033116e-02 0.333508
2019-11-13 05:11:28,275 train 800 1.032878e-02 0.335086
2019-11-13 05:11:34,828 train 850 1.033759e-02 0.337501
2019-11-13 05:11:36,781 training loss; R2: 1.032948e-02 0.337458
2019-11-13 05:11:37,095 valid 000 4.304914e-02 -1.773310
2019-11-13 05:11:38,792 valid 050 4.184213e-02 -1.037090
2019-11-13 05:11:40,318 validation loss; R2: 4.196632e-02 -1.012866
2019-11-13 05:11:40,332 epoch 15 lr 1.000000e-03
2019-11-13 05:11:40,746 train 000 9.165197e-03 0.481252
2019-11-13 05:11:47,633 train 050 1.058090e-02 0.364910
2019-11-13 05:11:54,261 train 100 1.054760e-02 0.346675
2019-11-13 05:12:00,979 train 150 1.045116e-02 0.342179
2019-11-13 05:12:07,600 train 200 1.035050e-02 0.345418
2019-11-13 05:12:14,258 train 250 1.033293e-02 0.345453
2019-11-13 05:12:20,942 train 300 1.033340e-02 0.337308
2019-11-13 05:12:27,647 train 350 1.032101e-02 0.339026
2019-11-13 05:12:34,478 train 400 1.031154e-02 0.333714
2019-11-13 05:12:41,030 train 450 1.031811e-02 0.335666
2019-11-13 05:12:47,530 train 500 1.030283e-02 0.334843
2019-11-13 05:12:54,043 train 550 1.028961e-02 0.336138
2019-11-13 05:13:00,536 train 600 1.028565e-02 0.336201
2019-11-13 05:13:07,039 train 650 1.026265e-02 0.337178
2019-11-13 05:13:13,531 train 700 1.025451e-02 0.340417
2019-11-13 05:13:20,111 train 750 1.023849e-02 0.342627
2019-11-13 05:13:26,606 train 800 1.019882e-02 0.343027
2019-11-13 05:13:33,100 train 850 1.019339e-02 0.344700
2019-11-13 05:13:35,041 training loss; R2: 1.019055e-02 0.344910
2019-11-13 05:13:35,350 valid 000 6.960195e-02 -6.295115
2019-11-13 05:13:37,026 valid 050 7.318444e-02 -20.662296
2019-11-13 05:13:38,546 validation loss; R2: 7.296003e-02 -15.852754
2019-11-13 05:13:38,561 epoch 16 lr 1.000000e-03
2019-11-13 05:13:38,949 train 000 8.701188e-03 0.438269
2019-11-13 05:13:45,816 train 050 9.955862e-03 0.342675
2019-11-13 05:13:52,697 train 100 9.969201e-03 0.343004
2019-11-13 05:13:59,488 train 150 1.004484e-02 0.314121
2019-11-13 05:14:06,446 train 200 1.004776e-02 0.313129
2019-11-13 05:14:13,186 train 250 1.005325e-02 0.324602
2019-11-13 05:14:20,026 train 300 1.005234e-02 0.333048
2019-11-13 05:14:27,032 train 350 1.000392e-02 0.329053
2019-11-13 05:14:33,949 train 400 1.000400e-02 0.332009
2019-11-13 05:14:40,681 train 450 9.989498e-03 0.335906
2019-11-13 05:14:47,460 train 500 1.000897e-02 0.337702
2019-11-13 05:14:54,409 train 550 1.002120e-02 0.337918
2019-11-13 05:15:01,286 train 600 1.003226e-02 0.332608
2019-11-13 05:15:08,072 train 650 1.000615e-02 0.303989
2019-11-13 05:15:14,816 train 700 1.000743e-02 0.305311
2019-11-13 05:15:21,687 train 750 9.993230e-03 0.243183
2019-11-13 05:15:28,523 train 800 9.998896e-03 0.249495
2019-11-13 05:15:35,407 train 850 1.000327e-02 0.253478
2019-11-13 05:15:37,398 training loss; R2: 9.994486e-03 0.255223
2019-11-13 05:15:37,695 valid 000 1.713756e-01 -13.718728
2019-11-13 05:15:39,378 valid 050 1.686713e-01 -14.183061
2019-11-13 05:15:40,889 validation loss; R2: 1.684850e-01 -15.356147
2019-11-13 05:15:40,911 epoch 17 lr 1.000000e-03
2019-11-13 05:15:41,280 train 000 9.530762e-03 0.374851
2019-11-13 05:15:48,158 train 050 9.703273e-03 0.387260
2019-11-13 05:15:54,947 train 100 9.750260e-03 0.380004
2019-11-13 05:16:01,736 train 150 9.837742e-03 0.374347
2019-11-13 05:16:08,747 train 200 9.794340e-03 0.366676
2019-11-13 05:16:15,725 train 250 9.796787e-03 0.359064
2019-11-13 05:16:22,628 train 300 9.777968e-03 0.358306
2019-11-13 05:16:29,604 train 350 9.757246e-03 0.360524
2019-11-13 05:16:36,480 train 400 9.749974e-03 0.360812
2019-11-13 05:16:43,271 train 450 9.746851e-03 0.359224
2019-11-13 05:16:49,914 train 500 9.747925e-03 0.361175
2019-11-13 05:16:56,774 train 550 9.762216e-03 0.357330
2019-11-13 05:17:03,576 train 600 9.750718e-03 0.356399
2019-11-13 05:17:10,167 train 650 9.746571e-03 0.353773
2019-11-13 05:17:16,750 train 700 9.748984e-03 0.354154
2019-11-13 05:17:23,329 train 750 9.751929e-03 0.355512
2019-11-13 05:17:29,913 train 800 9.766857e-03 0.357787
2019-11-13 05:17:36,509 train 850 9.751577e-03 0.357526
2019-11-13 05:17:38,476 training loss; R2: 9.748355e-03 0.357468
2019-11-13 05:17:38,785 valid 000 4.534844e-02 -1.198423
2019-11-13 05:17:40,482 valid 050 4.456017e-02 -1.046483
2019-11-13 05:17:42,015 validation loss; R2: 4.509595e-02 -1.058153
2019-11-13 05:17:42,040 epoch 18 lr 1.000000e-03
2019-11-13 05:17:42,475 train 000 9.822547e-03 0.294279
2019-11-13 05:17:49,399 train 050 9.503876e-03 0.295873
2019-11-13 05:17:56,280 train 100 9.565781e-03 0.296818
2019-11-13 05:18:03,218 train 150 9.622840e-03 0.315805
2019-11-13 05:18:10,012 train 200 9.608275e-03 0.336460
2019-11-13 05:18:16,841 train 250 9.635286e-03 0.322454
2019-11-13 05:18:23,657 train 300 9.644282e-03 0.334082
2019-11-13 05:18:30,446 train 350 9.716407e-03 0.340799
2019-11-13 05:18:37,388 train 400 9.749142e-03 0.343888
2019-11-13 05:18:44,134 train 450 9.737911e-03 0.345757
2019-11-13 05:18:51,115 train 500 9.746542e-03 0.349997
2019-11-13 05:18:57,970 train 550 9.749968e-03 0.347673
2019-11-13 05:19:04,796 train 600 9.738703e-03 0.351785
2019-11-13 05:19:11,706 train 650 9.714027e-03 0.350538
2019-11-13 05:19:18,382 train 700 9.729001e-03 0.352654
2019-11-13 05:19:25,236 train 750 9.705416e-03 0.355116
2019-11-13 05:19:32,170 train 800 9.729556e-03 0.354054
2019-11-13 05:19:38,987 train 850 9.737730e-03 0.356101
2019-11-13 05:19:41,022 training loss; R2: 9.739973e-03 0.355889
2019-11-13 05:19:41,356 valid 000 1.191631e-01 -22.620042
2019-11-13 05:19:43,069 valid 050 1.220060e-01 -17.077739
2019-11-13 05:19:44,617 validation loss; R2: 1.217564e-01 -18.820848
2019-11-13 05:19:44,632 epoch 19 lr 1.000000e-03
2019-11-13 05:19:45,029 train 000 1.009775e-02 0.434355
2019-11-13 05:19:52,073 train 050 9.656443e-03 0.269933
2019-11-13 05:19:59,058 train 100 9.701917e-03 0.319818
2019-11-13 05:20:06,060 train 150 9.711487e-03 0.320867
2019-11-13 05:20:12,906 train 200 9.743221e-03 0.338619
2019-11-13 05:20:19,692 train 250 9.763264e-03 0.344526
2019-11-13 05:20:26,691 train 300 9.718192e-03 0.345610
2019-11-13 05:20:33,519 train 350 9.694354e-03 0.351126
2019-11-13 05:20:40,314 train 400 9.708226e-03 0.355047
2019-11-13 05:20:47,353 train 450 9.691695e-03 0.356771
2019-11-13 05:20:54,147 train 500 9.703034e-03 0.357199
2019-11-13 05:21:00,920 train 550 9.690044e-03 0.358661
2019-11-13 05:21:07,576 train 600 9.665128e-03 0.361002
2019-11-13 05:21:14,167 train 650 9.657547e-03 0.362327
2019-11-13 05:21:21,179 train 700 9.683513e-03 0.358225
2019-11-13 05:21:28,070 train 750 9.678192e-03 0.356475
2019-11-13 05:21:34,959 train 800 9.700321e-03 0.356308
2019-11-13 05:21:41,836 train 850 9.694803e-03 0.356062
2019-11-13 05:21:43,890 training loss; R2: 9.691037e-03 0.348517
2019-11-13 05:21:44,191 valid 000 2.562416e+01 -53812.849845
2019-11-13 05:21:45,855 valid 050 2.564781e+01 -11253.244870
2019-11-13 05:21:47,431 validation loss; R2: 2.563723e+01 -9679.669421
