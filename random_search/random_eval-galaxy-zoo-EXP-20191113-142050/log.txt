2019-11-13 14:20:50,622 gpu device = 1
2019-11-13 14:20:50,622 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name='galaxy-zoo-cleaned', gpu=1, grad_clip=5, gz_dtree=False, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-galaxy-zoo-EXP-20191113-142050', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-11-13 14:21:02,181 param size = 0.287381MB
2019-11-13 14:21:02,185 epoch 0 lr 1.000000e-03
2019-11-13 14:21:04,423 train 000 3.049505e-01 -579.314420
2019-11-13 14:21:11,869 train 050 4.658690e-02 -19.974273
2019-11-13 14:21:19,041 train 100 3.695213e-02 -10.604814
2019-11-13 14:21:26,264 train 150 3.317314e-02 -7.252671
2019-11-13 14:21:33,411 train 200 3.116468e-02 -5.501937
2019-11-13 14:21:40,498 train 250 2.976539e-02 -4.437174
2019-11-13 14:21:47,732 train 300 2.879819e-02 -3.717791
2019-11-13 14:21:54,837 train 350 2.802545e-02 -3.204201
2019-11-13 14:22:02,102 train 400 2.744793e-02 -2.813467
2019-11-13 14:22:09,185 train 450 2.694546e-02 -2.566775
2019-11-13 14:22:16,224 train 500 2.648922e-02 -2.326136
2019-11-13 14:22:23,262 train 550 2.606637e-02 -2.124667
2019-11-13 14:22:30,324 train 600 2.569127e-02 -1.950598
2019-11-13 14:22:37,309 train 650 2.534293e-02 -1.803384
2019-11-13 14:22:44,400 train 700 2.506877e-02 -1.675200
2019-11-13 14:22:51,609 train 750 2.480770e-02 -1.560211
2019-11-13 14:22:58,788 train 800 2.457533e-02 -1.461195
2019-11-13 14:23:05,760 train 850 2.436645e-02 -1.388700
2019-11-13 14:23:08,640 training loss; R2: 2.430683e-02 -1.364842
2019-11-13 14:23:08,971 valid 000 1.936841e-02 0.144938
2019-11-13 14:23:10,654 valid 050 1.890079e-02 0.069834
2019-11-13 14:23:12,302 validation loss; R2: 1.905353e-02 0.084173
2019-11-13 14:23:12,325 epoch 1 lr 1.000000e-03
2019-11-13 14:23:12,944 train 000 1.886808e-02 0.054742
2019-11-13 14:23:20,039 train 050 2.062384e-02 -0.011749
2019-11-13 14:23:27,133 train 100 2.043790e-02 -0.083232
2019-11-13 14:23:34,131 train 150 2.042495e-02 -0.046409
2019-11-13 14:23:41,038 train 200 2.039655e-02 -0.025802
2019-11-13 14:23:47,941 train 250 2.020541e-02 -0.002516
2019-11-13 14:23:54,847 train 300 2.011462e-02 0.012229
2019-11-13 14:24:01,770 train 350 2.006314e-02 0.017977
2019-11-13 14:24:08,673 train 400 1.997535e-02 0.021753
2019-11-13 14:24:15,579 train 450 1.993041e-02 0.029463
2019-11-13 14:24:22,475 train 500 1.987777e-02 0.021859
2019-11-13 14:24:29,399 train 550 1.979483e-02 0.030188
2019-11-13 14:24:36,306 train 600 1.977034e-02 0.035793
2019-11-13 14:24:43,200 train 650 1.967712e-02 0.040008
2019-11-13 14:24:50,100 train 700 1.961046e-02 0.044172
2019-11-13 14:24:56,997 train 750 1.957655e-02 0.045306
2019-11-13 14:25:03,892 train 800 1.951441e-02 0.038047
2019-11-13 14:25:10,824 train 850 1.948392e-02 0.041204
2019-11-13 14:25:12,888 training loss; R2: 1.948097e-02 0.042804
2019-11-13 14:25:13,208 valid 000 1.710333e-02 0.192498
2019-11-13 14:25:14,900 valid 050 1.710389e-02 -0.493656
2019-11-13 14:25:16,450 validation loss; R2: 1.709876e-02 -0.425410
2019-11-13 14:25:16,470 epoch 2 lr 1.000000e-03
2019-11-13 14:25:16,911 train 000 1.902644e-02 0.111468
2019-11-13 14:25:23,852 train 050 1.865210e-02 0.128845
2019-11-13 14:25:30,767 train 100 1.847129e-02 0.120791
2019-11-13 14:25:37,677 train 150 1.844656e-02 0.126492
2019-11-13 14:25:44,584 train 200 1.849345e-02 0.118544
2019-11-13 14:25:51,490 train 250 1.838431e-02 0.119723
2019-11-13 14:25:58,393 train 300 1.830583e-02 0.120628
2019-11-13 14:26:05,300 train 350 1.819853e-02 0.123314
2019-11-13 14:26:12,204 train 400 1.809619e-02 0.120633
2019-11-13 14:26:19,125 train 450 1.798212e-02 0.119073
2019-11-13 14:26:26,025 train 500 1.796351e-02 0.118152
2019-11-13 14:26:32,943 train 550 1.791307e-02 0.118458
2019-11-13 14:26:39,870 train 600 1.789192e-02 0.119915
2019-11-13 14:26:46,764 train 650 1.785085e-02 0.117001
2019-11-13 14:26:53,659 train 700 1.781131e-02 0.118105
2019-11-13 14:27:00,554 train 750 1.774372e-02 0.120637
2019-11-13 14:27:07,451 train 800 1.767770e-02 0.120468
2019-11-13 14:27:14,344 train 850 1.764040e-02 0.122408
2019-11-13 14:27:16,410 training loss; R2: 1.762913e-02 0.123054
2019-11-13 14:27:16,711 valid 000 1.468652e-02 0.246437
2019-11-13 14:27:18,443 valid 050 1.541573e-02 0.163110
2019-11-13 14:27:19,984 validation loss; R2: 1.544960e-02 0.188362
2019-11-13 14:27:20,002 epoch 3 lr 1.000000e-03
2019-11-13 14:27:20,399 train 000 1.834271e-02 0.184641
2019-11-13 14:27:27,376 train 050 1.689988e-02 -0.872174
2019-11-13 14:27:34,302 train 100 1.664399e-02 -0.358543
2019-11-13 14:27:41,203 train 150 1.660188e-02 -0.183106
2019-11-13 14:27:48,106 train 200 1.647133e-02 -0.096662
2019-11-13 14:27:55,012 train 250 1.650706e-02 -0.351334
2019-11-13 14:28:01,914 train 300 1.648557e-02 -0.262100
2019-11-13 14:28:08,814 train 350 1.647010e-02 -0.203015
2019-11-13 14:28:15,729 train 400 1.638698e-02 -0.152780
2019-11-13 14:28:22,696 train 450 1.632861e-02 -0.115987
2019-11-13 14:28:29,601 train 500 1.629643e-02 -0.084687
2019-11-13 14:28:36,495 train 550 1.627190e-02 -0.064597
2019-11-13 14:28:43,392 train 600 1.622615e-02 -0.043559
2019-11-13 14:28:50,300 train 650 1.617657e-02 -0.032337
2019-11-13 14:28:57,201 train 700 1.610284e-02 -0.020661
2019-11-13 14:29:04,097 train 750 1.603492e-02 -0.008195
2019-11-13 14:29:10,993 train 800 1.600462e-02 0.004035
2019-11-13 14:29:17,885 train 850 1.597060e-02 0.015062
2019-11-13 14:29:19,949 training loss; R2: 1.596946e-02 0.017883
2019-11-13 14:29:20,269 valid 000 1.323608e-02 0.308783
2019-11-13 14:29:21,979 valid 050 1.350501e-02 0.235142
2019-11-13 14:29:23,552 validation loss; R2: 1.375765e-02 0.233161
2019-11-13 14:29:23,569 epoch 4 lr 1.000000e-03
2019-11-13 14:29:23,969 train 000 1.380780e-02 0.127091
2019-11-13 14:29:31,094 train 050 1.555201e-02 0.208617
2019-11-13 14:29:38,137 train 100 1.535504e-02 0.217734
2019-11-13 14:29:45,050 train 150 1.541295e-02 0.205142
2019-11-13 14:29:51,963 train 200 1.536643e-02 0.201497
2019-11-13 14:29:58,867 train 250 1.530555e-02 0.201288
2019-11-13 14:30:05,766 train 300 1.525023e-02 0.205025
2019-11-13 14:30:12,664 train 350 1.513529e-02 0.194993
2019-11-13 14:30:19,572 train 400 1.510491e-02 0.194364
2019-11-13 14:30:26,475 train 450 1.507387e-02 0.196240
2019-11-13 14:30:33,372 train 500 1.506155e-02 0.186148
2019-11-13 14:30:40,313 train 550 1.500368e-02 0.189651
2019-11-13 14:30:47,203 train 600 1.495524e-02 0.190638
2019-11-13 14:30:54,100 train 650 1.492326e-02 0.188841
2019-11-13 14:31:00,996 train 700 1.492278e-02 0.190976
2019-11-13 14:31:07,887 train 750 1.488838e-02 0.192793
2019-11-13 14:31:14,787 train 800 1.486280e-02 0.193264
2019-11-13 14:31:21,683 train 850 1.483528e-02 0.194405
2019-11-13 14:31:23,748 training loss; R2: 1.482212e-02 0.192357
2019-11-13 14:31:24,072 valid 000 1.394104e-02 0.241791
2019-11-13 14:31:25,771 valid 050 1.297874e-02 0.257628
2019-11-13 14:31:27,318 validation loss; R2: 1.300550e-02 0.238202
2019-11-13 14:31:27,337 epoch 5 lr 1.000000e-03
2019-11-13 14:31:27,727 train 000 1.452086e-02 0.312220
2019-11-13 14:31:34,715 train 050 1.407026e-02 0.054177
2019-11-13 14:31:41,617 train 100 1.415048e-02 0.145891
2019-11-13 14:31:48,528 train 150 1.422479e-02 0.159879
2019-11-13 14:31:55,425 train 200 1.423428e-02 0.177010
2019-11-13 14:32:02,316 train 250 1.417282e-02 0.189997
2019-11-13 14:32:09,257 train 300 1.417156e-02 0.184866
2019-11-13 14:32:16,211 train 350 1.417570e-02 0.182834
2019-11-13 14:32:23,102 train 400 1.414507e-02 0.188941
2019-11-13 14:32:29,989 train 450 1.410235e-02 0.193825
2019-11-13 14:32:36,924 train 500 1.410425e-02 0.176154
2019-11-13 14:32:43,874 train 550 1.410102e-02 0.181495
2019-11-13 14:32:50,757 train 600 1.405900e-02 0.187259
2019-11-13 14:32:57,644 train 650 1.407893e-02 0.188950
2019-11-13 14:33:04,538 train 700 1.406778e-02 0.191288
2019-11-13 14:33:11,420 train 750 1.405600e-02 0.194490
2019-11-13 14:33:18,310 train 800 1.402747e-02 0.196982
2019-11-13 14:33:25,193 train 850 1.399820e-02 0.200282
2019-11-13 14:33:27,252 training loss; R2: 1.398483e-02 0.199548
2019-11-13 14:33:27,561 valid 000 8.148614e-02 -1.939325
2019-11-13 14:33:29,280 valid 050 8.663833e-02 -2.782142
2019-11-13 14:33:30,846 validation loss; R2: 8.670901e-02 -2.907351
2019-11-13 14:33:30,863 epoch 6 lr 1.000000e-03
2019-11-13 14:33:31,250 train 000 1.440853e-02 0.223914
2019-11-13 14:33:38,328 train 050 1.391740e-02 0.181613
2019-11-13 14:33:45,244 train 100 1.383937e-02 0.195037
2019-11-13 14:33:52,138 train 150 1.387096e-02 0.202987
2019-11-13 14:33:59,038 train 200 1.372802e-02 0.217636
2019-11-13 14:34:05,931 train 250 1.359033e-02 0.226007
2019-11-13 14:34:12,830 train 300 1.356338e-02 0.229031
2019-11-13 14:34:19,728 train 350 1.359576e-02 0.227492
2019-11-13 14:34:26,622 train 400 1.366601e-02 0.228683
2019-11-13 14:34:33,524 train 450 1.363386e-02 0.233149
2019-11-13 14:34:40,422 train 500 1.359581e-02 0.237214
2019-11-13 14:34:47,343 train 550 1.354408e-02 0.239033
2019-11-13 14:34:54,252 train 600 1.352686e-02 0.239392
2019-11-13 14:35:01,155 train 650 1.351623e-02 0.240510
2019-11-13 14:35:08,054 train 700 1.350132e-02 0.239942
2019-11-13 14:35:14,970 train 750 1.348249e-02 0.242579
2019-11-13 14:35:21,860 train 800 1.344698e-02 0.243591
2019-11-13 14:35:28,749 train 850 1.345595e-02 0.234608
2019-11-13 14:35:30,813 training loss; R2: 1.345465e-02 0.233986
2019-11-13 14:35:31,136 valid 000 1.968239e-01 -10.254695
2019-11-13 14:35:32,854 valid 050 2.018132e-01 -11.468903
2019-11-13 14:35:34,401 validation loss; R2: 2.022604e-01 -10.996717
2019-11-13 14:35:34,419 epoch 7 lr 1.000000e-03
2019-11-13 14:35:34,831 train 000 1.542109e-02 0.282701
2019-11-13 14:35:41,947 train 050 1.316166e-02 0.257274
2019-11-13 14:35:48,998 train 100 1.320778e-02 0.119646
2019-11-13 14:35:55,951 train 150 1.319565e-02 0.160438
2019-11-13 14:36:02,859 train 200 1.313152e-02 0.183392
2019-11-13 14:36:09,784 train 250 1.307165e-02 0.192319
2019-11-13 14:36:16,708 train 300 1.304889e-02 0.202752
2019-11-13 14:36:23,624 train 350 1.304336e-02 0.211668
2019-11-13 14:36:30,594 train 400 1.303534e-02 0.219432
2019-11-13 14:36:37,524 train 450 1.302877e-02 0.226298
2019-11-13 14:36:44,512 train 500 1.302405e-02 0.228520
2019-11-13 14:36:51,432 train 550 1.300400e-02 0.229312
2019-11-13 14:36:58,345 train 600 1.303379e-02 0.233075
2019-11-13 14:37:05,276 train 650 1.303075e-02 0.235760
2019-11-13 14:37:12,195 train 700 1.301184e-02 0.238090
2019-11-13 14:37:19,115 train 750 1.299557e-02 0.240797
2019-11-13 14:37:26,034 train 800 1.298399e-02 0.242717
2019-11-13 14:37:32,956 train 850 1.295704e-02 0.245373
2019-11-13 14:37:35,029 training loss; R2: 1.295870e-02 0.244829
2019-11-13 14:37:35,340 valid 000 4.715814e-01 -19.197413
2019-11-13 14:37:37,018 valid 050 4.721555e-01 -23.977278
2019-11-13 14:37:38,576 validation loss; R2: 4.730858e-01 -25.089213
2019-11-13 14:37:38,594 epoch 8 lr 1.000000e-03
2019-11-13 14:37:39,009 train 000 1.271369e-02 0.240202
2019-11-13 14:37:46,089 train 050 1.275472e-02 0.242227
2019-11-13 14:37:53,201 train 100 1.276935e-02 0.253785
2019-11-13 14:38:00,145 train 150 1.267214e-02 0.243260
2019-11-13 14:38:07,074 train 200 1.259801e-02 0.243807
2019-11-13 14:38:13,995 train 250 1.268438e-02 0.249358
2019-11-13 14:38:20,913 train 300 1.275755e-02 0.249436
2019-11-13 14:38:27,838 train 350 1.273917e-02 0.251374
2019-11-13 14:38:34,864 train 400 1.273615e-02 0.250210
2019-11-13 14:38:41,802 train 450 1.272155e-02 0.252055
2019-11-13 14:38:48,727 train 500 1.273139e-02 0.257391
2019-11-13 14:38:55,655 train 550 1.269661e-02 0.258960
2019-11-13 14:39:02,586 train 600 1.264841e-02 0.260491
2019-11-13 14:39:09,510 train 650 1.267349e-02 0.259516
2019-11-13 14:39:16,441 train 700 1.267952e-02 0.260879
2019-11-13 14:39:23,380 train 750 1.268198e-02 0.259161
2019-11-13 14:39:30,334 train 800 1.266912e-02 0.261302
2019-11-13 14:39:37,354 train 850 1.266520e-02 0.261516
2019-11-13 14:39:39,428 training loss; R2: 1.266008e-02 0.262035
2019-11-13 14:39:39,750 valid 000 5.520637e+01 -38596.139765
2019-11-13 14:39:41,430 valid 050 5.523090e+01 -12709.917791
2019-11-13 14:39:42,975 validation loss; R2: 5.522854e+01 -11876.123269
2019-11-13 14:39:42,993 epoch 9 lr 1.000000e-03
2019-11-13 14:39:43,399 train 000 1.029464e-02 0.209201
2019-11-13 14:39:50,475 train 050 1.247559e-02 0.254735
2019-11-13 14:39:57,579 train 100 1.239188e-02 0.266078
2019-11-13 14:40:04,534 train 150 1.247387e-02 0.260927
2019-11-13 14:40:11,455 train 200 1.255915e-02 0.269035
2019-11-13 14:40:18,385 train 250 1.250033e-02 0.274997
2019-11-13 14:40:25,348 train 300 1.248095e-02 0.274554
2019-11-13 14:40:32,420 train 350 1.249245e-02 0.273007
2019-11-13 14:40:39,324 train 400 1.248944e-02 0.263593
2019-11-13 14:40:46,331 train 450 1.248287e-02 0.266293
2019-11-13 14:40:53,256 train 500 1.246675e-02 0.267182
2019-11-13 14:41:00,167 train 550 1.245153e-02 0.269018
2019-11-13 14:41:07,080 train 600 1.246507e-02 0.267224
2019-11-13 14:41:13,989 train 650 1.244844e-02 0.269854
2019-11-13 14:41:20,898 train 700 1.245598e-02 0.268210
2019-11-13 14:41:27,807 train 750 1.246691e-02 0.262414
2019-11-13 14:41:34,724 train 800 1.245045e-02 0.265591
2019-11-13 14:41:41,635 train 850 1.244248e-02 0.265499
2019-11-13 14:41:43,704 training loss; R2: 1.244828e-02 0.265960
2019-11-13 14:41:44,033 valid 000 4.917780e+01 -2854.780243
2019-11-13 14:41:45,739 valid 050 4.917651e+01 -2565.783820
2019-11-13 14:41:47,285 validation loss; R2: 4.916728e+01 -2578.820904
2019-11-13 14:41:47,302 epoch 10 lr 1.000000e-03
2019-11-13 14:41:47,699 train 000 1.339035e-02 0.337491
2019-11-13 14:41:54,875 train 050 1.247561e-02 0.231508
2019-11-13 14:42:02,074 train 100 1.256542e-02 0.230855
2019-11-13 14:42:09,265 train 150 1.264147e-02 0.244566
2019-11-13 14:42:16,453 train 200 1.298665e-02 0.239908
2019-11-13 14:42:23,631 train 250 1.295680e-02 0.248026
2019-11-13 14:42:30,813 train 300 1.293124e-02 0.251678
2019-11-13 14:42:37,999 train 350 1.283451e-02 0.255006
2019-11-13 14:42:45,173 train 400 1.273501e-02 0.257733
2019-11-13 14:42:52,350 train 450 1.269828e-02 0.255646
2019-11-13 14:42:59,530 train 500 1.263186e-02 0.259478
2019-11-13 14:43:06,709 train 550 1.261393e-02 0.259730
2019-11-13 14:43:13,885 train 600 1.258640e-02 0.263446
2019-11-13 14:43:21,057 train 650 1.258515e-02 0.266656
2019-11-13 14:43:28,231 train 700 1.256796e-02 0.265466
2019-11-13 14:43:35,419 train 750 1.254221e-02 0.267625
2019-11-13 14:43:42,593 train 800 1.251545e-02 0.264884
2019-11-13 14:43:49,768 train 850 1.248530e-02 0.268368
2019-11-13 14:43:51,915 training loss; R2: 1.246528e-02 0.269419
2019-11-13 14:43:52,229 valid 000 1.565879e+00 -191.503863
2019-11-13 14:43:53,917 valid 050 1.560617e+00 -147.875615
2019-11-13 14:43:55,423 validation loss; R2: 1.561508e+00 -167.111177
2019-11-13 14:43:55,441 epoch 11 lr 1.000000e-03
2019-11-13 14:43:55,875 train 000 1.226605e-02 0.365282
2019-11-13 14:44:03,173 train 050 1.220967e-02 0.283125
2019-11-13 14:44:10,462 train 100 1.229507e-02 0.197423
2019-11-13 14:44:17,581 train 150 1.224781e-02 0.234528
2019-11-13 14:44:24,795 train 200 1.229915e-02 0.247602
2019-11-13 14:44:32,031 train 250 1.227534e-02 0.252766
2019-11-13 14:44:39,151 train 300 1.224166e-02 0.249695
2019-11-13 14:44:46,216 train 350 1.223180e-02 0.252053
2019-11-13 14:44:53,277 train 400 1.223928e-02 0.250392
2019-11-13 14:45:00,385 train 450 1.231620e-02 0.255078
2019-11-13 14:45:07,606 train 500 1.235487e-02 0.254538
2019-11-13 14:45:14,715 train 550 1.236816e-02 0.234534
2019-11-13 14:45:21,893 train 600 1.236086e-02 0.238087
2019-11-13 14:45:28,946 train 650 1.236980e-02 0.238355
2019-11-13 14:45:36,091 train 700 1.236083e-02 0.239633
2019-11-13 14:45:43,159 train 750 1.234953e-02 0.243930
2019-11-13 14:45:50,411 train 800 1.236982e-02 0.243921
2019-11-13 14:45:57,735 train 850 1.232393e-02 0.244334
2019-11-13 14:45:59,901 training loss; R2: 1.231994e-02 0.246133
2019-11-13 14:46:00,210 valid 000 9.417028e+01 -5587.086040
2019-11-13 14:46:01,914 valid 050 9.428980e+01 -9964.341325
2019-11-13 14:46:03,435 validation loss; R2: 9.430255e+01 -10159.966440
2019-11-13 14:46:03,464 epoch 12 lr 1.000000e-03
2019-11-13 14:46:03,851 train 000 1.212964e-02 0.335596
2019-11-13 14:46:11,009 train 050 1.250812e-02 0.291464
2019-11-13 14:46:17,947 train 100 1.232274e-02 0.296286
2019-11-13 14:46:25,000 train 150 1.227582e-02 0.288261
2019-11-13 14:46:32,069 train 200 1.227027e-02 0.292531
2019-11-13 14:46:38,985 train 250 1.259573e-02 0.195024
2019-11-13 14:46:45,880 train 300 1.274472e-02 0.199007
2019-11-13 14:46:52,799 train 350 1.270353e-02 0.211680
2019-11-13 14:46:59,700 train 400 1.269248e-02 0.221599
2019-11-13 14:47:06,613 train 450 1.265765e-02 0.226267
2019-11-13 14:47:13,510 train 500 1.267741e-02 0.229207
2019-11-13 14:47:20,400 train 550 1.264985e-02 0.229382
2019-11-13 14:47:27,290 train 600 1.262186e-02 0.229923
2019-11-13 14:47:34,261 train 650 1.256141e-02 0.235398
2019-11-13 14:47:41,180 train 700 1.253595e-02 0.239926
2019-11-13 14:47:48,073 train 750 1.251092e-02 0.236205
2019-11-13 14:47:54,973 train 800 1.249013e-02 0.239272
2019-11-13 14:48:01,874 train 850 1.245231e-02 0.240878
2019-11-13 14:48:03,936 training loss; R2: 1.244067e-02 0.242118
2019-11-13 14:48:04,260 valid 000 2.018198e-01 -49.873456
2019-11-13 14:48:05,924 valid 050 2.041454e-01 -60.907536
2019-11-13 14:48:07,445 validation loss; R2: 2.044013e-01 -64.351177
2019-11-13 14:48:07,463 epoch 13 lr 1.000000e-03
2019-11-13 14:48:07,880 train 000 1.178601e-02 0.382430
2019-11-13 14:48:14,796 train 050 1.210716e-02 0.306389
2019-11-13 14:48:21,697 train 100 1.218455e-02 0.290538
2019-11-13 14:48:28,669 train 150 1.202842e-02 0.295534
2019-11-13 14:48:35,581 train 200 1.204533e-02 0.287020
2019-11-13 14:48:42,489 train 250 1.205231e-02 0.291643
2019-11-13 14:48:49,404 train 300 1.203235e-02 0.291999
2019-11-13 14:48:56,333 train 350 1.194960e-02 0.291604
2019-11-13 14:49:03,242 train 400 1.189330e-02 0.291638
2019-11-13 14:49:10,159 train 450 1.190856e-02 0.291631
2019-11-13 14:49:17,063 train 500 1.189953e-02 0.291939
2019-11-13 14:49:24,000 train 550 1.191842e-02 0.292826
2019-11-13 14:49:30,902 train 600 1.191661e-02 0.294031
2019-11-13 14:49:37,806 train 650 1.192777e-02 0.286820
2019-11-13 14:49:44,721 train 700 1.192719e-02 0.289244
2019-11-13 14:49:51,644 train 750 1.192721e-02 0.290826
2019-11-13 14:49:58,585 train 800 1.192027e-02 0.293170
2019-11-13 14:50:05,487 train 850 1.190883e-02 0.293582
2019-11-13 14:50:07,548 training loss; R2: 1.189702e-02 0.294534
2019-11-13 14:50:07,858 valid 000 3.890508e+00 -234.643835
2019-11-13 14:50:09,564 valid 050 3.889059e+00 -394.044816
2019-11-13 14:50:11,123 validation loss; R2: 3.884322e+00 -391.112547
2019-11-13 14:50:11,146 epoch 14 lr 1.000000e-03
2019-11-13 14:50:11,563 train 000 9.820155e-03 0.452646
2019-11-13 14:50:18,612 train 050 1.168090e-02 0.310019
2019-11-13 14:50:25,758 train 100 1.173968e-02 0.295033
2019-11-13 14:50:32,725 train 150 1.172189e-02 0.298928
2019-11-13 14:50:39,686 train 200 1.174809e-02 0.291756
2019-11-13 14:50:46,670 train 250 1.188286e-02 0.290351
2019-11-13 14:50:53,644 train 300 1.190122e-02 0.220676
2019-11-13 14:51:00,607 train 350 1.192576e-02 0.230848
2019-11-13 14:51:07,581 train 400 1.195067e-02 0.240245
2019-11-13 14:51:14,569 train 450 1.189669e-02 0.246595
2019-11-13 14:51:21,697 train 500 1.188778e-02 0.249932
2019-11-13 14:51:28,665 train 550 1.186792e-02 0.254676
2019-11-13 14:51:35,601 train 600 1.185406e-02 0.259726
2019-11-13 14:51:42,500 train 650 1.186716e-02 0.262099
2019-11-13 14:51:49,416 train 700 1.184868e-02 0.265319
2019-11-13 14:51:56,333 train 750 1.184932e-02 0.267746
2019-11-13 14:52:03,232 train 800 1.184867e-02 0.269269
2019-11-13 14:52:10,137 train 850 1.184346e-02 0.271997
2019-11-13 14:52:12,197 training loss; R2: 1.184587e-02 0.273102
2019-11-13 14:52:12,524 valid 000 5.423179e+00 -549.199001
2019-11-13 14:52:14,257 valid 050 5.440021e+00 -785.267017
2019-11-13 14:52:15,816 validation loss; R2: 5.440192e+00 -758.205253
2019-11-13 14:52:15,840 epoch 15 lr 1.000000e-03
2019-11-13 14:52:16,242 train 000 9.717152e-03 0.360670
2019-11-13 14:52:23,185 train 050 1.188266e-02 0.315201
2019-11-13 14:52:30,237 train 100 1.176617e-02 0.312421
2019-11-13 14:52:37,212 train 150 1.180266e-02 0.315924
2019-11-13 14:52:44,140 train 200 1.179661e-02 0.165563
2019-11-13 14:52:51,075 train 250 1.182263e-02 0.187135
2019-11-13 14:52:58,002 train 300 1.180166e-02 0.205399
2019-11-13 14:53:05,005 train 350 1.187348e-02 0.184272
2019-11-13 14:53:11,947 train 400 1.197620e-02 0.191079
2019-11-13 14:53:18,912 train 450 1.199821e-02 0.200860
2019-11-13 14:53:25,887 train 500 1.201401e-02 0.208391
2019-11-13 14:53:32,832 train 550 1.199290e-02 0.216973
2019-11-13 14:53:39,770 train 600 1.198974e-02 0.224350
2019-11-13 14:53:46,710 train 650 1.199258e-02 0.228797
2019-11-13 14:53:53,665 train 700 1.196874e-02 0.234805
2019-11-13 14:54:00,592 train 750 1.196717e-02 0.234080
2019-11-13 14:54:07,521 train 800 1.195590e-02 0.236395
2019-11-13 14:54:14,436 train 850 1.193546e-02 0.240835
2019-11-13 14:54:16,501 training loss; R2: 1.193121e-02 0.242477
2019-11-13 14:54:16,821 valid 000 1.426641e+00 -76.352886
2019-11-13 14:54:18,551 valid 050 1.414111e+00 -110.545299
2019-11-13 14:54:20,108 validation loss; R2: 1.414635e+00 -115.149625
2019-11-13 14:54:20,126 epoch 16 lr 1.000000e-03
2019-11-13 14:54:20,541 train 000 1.200884e-02 0.323805
2019-11-13 14:54:27,543 train 050 1.157679e-02 0.321763
2019-11-13 14:54:34,676 train 100 1.160944e-02 0.293183
2019-11-13 14:54:41,723 train 150 1.164542e-02 0.298600
2019-11-13 14:54:48,844 train 200 1.168252e-02 0.298159
2019-11-13 14:54:56,034 train 250 1.167274e-02 0.291519
2019-11-13 14:55:03,131 train 300 1.165022e-02 0.286584
2019-11-13 14:55:10,170 train 350 1.165055e-02 0.283199
2019-11-13 14:55:17,271 train 400 1.167616e-02 0.275500
2019-11-13 14:55:24,332 train 450 1.167675e-02 0.277867
2019-11-13 14:55:31,435 train 500 1.165338e-02 0.277113
2019-11-13 14:55:38,484 train 550 1.162010e-02 0.282477
2019-11-13 14:55:45,545 train 600 1.161973e-02 0.283137
2019-11-13 14:55:52,644 train 650 1.162470e-02 0.282775
2019-11-13 14:55:59,732 train 700 1.163764e-02 0.282889
2019-11-13 14:56:06,821 train 750 1.163189e-02 0.283226
2019-11-13 14:56:13,937 train 800 1.162605e-02 0.286201
2019-11-13 14:56:21,099 train 850 1.160030e-02 0.287444
2019-11-13 14:56:23,208 training loss; R2: 1.159642e-02 0.288433
2019-11-13 14:56:23,554 valid 000 6.057144e-02 -2.065203
2019-11-13 14:56:25,216 valid 050 5.796139e-02 -4.510482
2019-11-13 14:56:26,758 validation loss; R2: 5.790000e-02 -4.064996
2019-11-13 14:56:26,788 epoch 17 lr 1.000000e-03
2019-11-13 14:56:27,276 train 000 9.971162e-03 0.382146
2019-11-13 14:56:34,317 train 050 1.144161e-02 0.296632
2019-11-13 14:56:41,252 train 100 1.143257e-02 0.314379
2019-11-13 14:56:48,182 train 150 1.142766e-02 0.283299
2019-11-13 14:56:55,111 train 200 1.155641e-02 0.285811
2019-11-13 14:57:02,035 train 250 1.162588e-02 0.286388
2019-11-13 14:57:08,966 train 300 1.163366e-02 0.290970
2019-11-13 14:57:15,894 train 350 1.164978e-02 0.294070
2019-11-13 14:57:22,847 train 400 1.161549e-02 0.298010
2019-11-13 14:57:29,799 train 450 1.157497e-02 0.299355
2019-11-13 14:57:36,724 train 500 1.156856e-02 0.301900
2019-11-13 14:57:43,652 train 550 1.155804e-02 0.299534
2019-11-13 14:57:50,526 train 600 1.154557e-02 0.298866
2019-11-13 14:57:57,399 train 650 1.154482e-02 0.290061
2019-11-13 14:58:04,276 train 700 1.154959e-02 0.289781
2019-11-13 14:58:11,157 train 750 1.155368e-02 0.292178
2019-11-13 14:58:18,031 train 800 1.154944e-02 0.293045
2019-11-13 14:58:24,902 train 850 1.154766e-02 0.292617
2019-11-13 14:58:26,959 training loss; R2: 1.153951e-02 0.293707
2019-11-13 14:58:27,299 valid 000 2.867151e-01 -31.627255
2019-11-13 14:58:28,991 valid 050 2.910993e-01 -31.482479
2019-11-13 14:58:30,508 validation loss; R2: 2.912286e-01 -27.511184
2019-11-13 14:58:30,532 epoch 18 lr 1.000000e-03
2019-11-13 14:58:30,988 train 000 9.583590e-03 0.456439
2019-11-13 14:58:37,907 train 050 1.104431e-02 0.335870
2019-11-13 14:58:44,826 train 100 1.129141e-02 0.319442
2019-11-13 14:58:51,746 train 150 1.128788e-02 0.315801
2019-11-13 14:58:58,669 train 200 1.130611e-02 0.318487
2019-11-13 14:59:05,590 train 250 1.128452e-02 0.316779
2019-11-13 14:59:12,503 train 300 1.129907e-02 0.318719
2019-11-13 14:59:19,417 train 350 1.128011e-02 0.318529
2019-11-13 14:59:26,333 train 400 1.132582e-02 0.311959
2019-11-13 14:59:33,246 train 450 1.138017e-02 0.310339
2019-11-13 14:59:40,168 train 500 1.138663e-02 0.310860
2019-11-13 14:59:47,077 train 550 1.139251e-02 0.309465
2019-11-13 14:59:53,988 train 600 1.141449e-02 0.308092
2019-11-13 15:00:00,894 train 650 1.142804e-02 0.287526
2019-11-13 15:00:08,014 train 700 1.143542e-02 -0.800090
2019-11-13 15:00:15,026 train 750 1.151647e-02 -0.732488
2019-11-13 15:00:21,981 train 800 1.152817e-02 -0.665566
2019-11-13 15:00:28,980 train 850 1.153153e-02 -0.609033
2019-11-13 15:00:31,054 training loss; R2: 1.152782e-02 -0.593056
2019-11-13 15:00:31,377 valid 000 1.020936e+00 -83.408381
2019-11-13 15:00:33,101 valid 050 1.019152e+00 -101.403937
2019-11-13 15:00:34,655 validation loss; R2: 1.019024e+00 -94.076433
2019-11-13 15:00:34,673 epoch 19 lr 1.000000e-03
2019-11-13 15:00:35,103 train 000 1.324169e-02 0.314058
2019-11-13 15:00:42,035 train 050 1.216862e-02 0.292747
2019-11-13 15:00:48,952 train 100 1.291841e-02 0.247908
2019-11-13 15:00:55,864 train 150 1.276917e-02 0.228561
2019-11-13 15:01:02,832 train 200 1.282503e-02 0.236867
2019-11-13 15:01:09,748 train 250 1.322682e-02 0.224211
2019-11-13 15:01:16,658 train 300 1.353632e-02 0.213307
2019-11-13 15:01:23,569 train 350 1.348969e-02 0.213879
2019-11-13 15:01:30,477 train 400 1.336598e-02 0.223059
2019-11-13 15:01:37,386 train 450 1.322930e-02 0.230145
2019-11-13 15:01:44,366 train 500 1.312120e-02 0.238050
2019-11-13 15:01:51,277 train 550 1.299969e-02 0.239419
2019-11-13 15:01:58,219 train 600 1.291674e-02 0.244032
2019-11-13 15:02:05,219 train 650 1.284420e-02 0.247622
2019-11-13 15:02:12,173 train 700 1.278223e-02 0.237786
2019-11-13 15:02:19,092 train 750 1.268839e-02 0.237584
2019-11-13 15:02:26,023 train 800 1.264992e-02 0.241379
2019-11-13 15:02:32,941 train 850 1.257614e-02 0.246454
2019-11-13 15:02:35,004 training loss; R2: 1.256342e-02 0.247328
2019-11-13 15:02:35,340 valid 000 7.459161e+00 -465.502004
2019-11-13 15:02:37,086 valid 050 7.437872e+00 -579.520632
2019-11-13 15:02:38,631 validation loss; R2: 7.436228e+00 -578.724576
