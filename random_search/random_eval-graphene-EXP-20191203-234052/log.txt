2019-12-03 23:40:52,099 gpu device = 1
2019-12-03 23:40:52,099 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=96, cutout=False, cutout_length=16, data='../data', dataset='graphene', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name=None, gpu=1, grad_clip=5, gz_dtree=False, init_channels=36, layers=8, learning_rate=0.002, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-graphene-EXP-20191203-234052', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-12-03 23:40:55,331 param size = 1.254565MB
2019-12-03 23:40:55,334 epoch 0 lr 2.000000e-03
2019-12-03 23:40:58,051 train 000 6.058325e-01 -9.930772
2019-12-03 23:41:12,144 train 050 1.927751e-01 -6.135855
2019-12-03 23:41:26,151 train 100 1.210358e-01 -3.406794
2019-12-03 23:41:40,157 train 150 9.006355e-02 -2.276390
2019-12-03 23:41:54,164 train 200 7.456586e-02 -1.667236
2019-12-03 23:42:01,565 training loss; R2: 7.033623e-02 -1.497634
2019-12-03 23:42:01,715 valid 000 1.560434e-02 0.275618
2019-12-03 23:42:03,481 validation loss; R2: 2.406966e-02 0.234151
2019-12-03 23:42:03,504 epoch 1 lr 2.000000e-03
2019-12-03 23:42:03,943 train 000 2.163637e-02 0.102557
2019-12-03 23:42:17,952 train 050 2.595029e-02 0.179479
2019-12-03 23:42:31,965 train 100 2.529315e-02 0.214085
2019-12-03 23:42:45,982 train 150 2.458763e-02 0.228278
2019-12-03 23:43:00,004 train 200 2.322207e-02 0.260126
2019-12-03 23:43:06,314 training loss; R2: 2.268650e-02 0.269942
2019-12-03 23:43:06,459 valid 000 7.802485e-03 0.722003
2019-12-03 23:43:07,893 validation loss; R2: 1.277146e-02 0.596084
2019-12-03 23:43:07,917 epoch 2 lr 2.000000e-03
2019-12-03 23:43:08,280 train 000 1.966637e-02 0.340250
2019-12-03 23:43:22,300 train 050 1.875084e-02 0.419652
2019-12-03 23:43:36,323 train 100 1.696734e-02 0.443727
2019-12-03 23:43:50,346 train 150 1.606846e-02 0.459768
2019-12-03 23:44:04,370 train 200 1.597875e-02 0.468069
2019-12-03 23:44:10,683 training loss; R2: 1.632841e-02 0.472711
2019-12-03 23:44:10,826 valid 000 2.344350e-02 0.256788
2019-12-03 23:44:12,261 validation loss; R2: 2.037904e-02 0.346258
2019-12-03 23:44:12,285 epoch 3 lr 2.000000e-03
2019-12-03 23:44:12,651 train 000 1.549808e-02 0.710567
2019-12-03 23:44:26,686 train 050 1.295390e-02 0.530200
2019-12-03 23:44:40,720 train 100 1.305322e-02 0.550069
2019-12-03 23:44:54,747 train 150 1.308502e-02 0.552691
2019-12-03 23:45:08,778 train 200 1.304234e-02 0.565039
2019-12-03 23:45:15,093 training loss; R2: 1.288335e-02 0.570453
2019-12-03 23:45:15,230 valid 000 5.758267e-03 0.737330
2019-12-03 23:45:16,665 validation loss; R2: 9.946026e-03 0.678719
2019-12-03 23:45:16,690 epoch 4 lr 2.000000e-03
2019-12-03 23:45:17,053 train 000 1.063193e-02 0.776891
2019-12-03 23:45:31,072 train 050 1.247183e-02 0.611060
2019-12-03 23:45:45,088 train 100 1.152246e-02 0.618603
2019-12-03 23:45:59,107 train 150 1.121129e-02 0.633500
2019-12-03 23:46:13,125 train 200 1.092291e-02 0.647760
2019-12-03 23:46:19,435 training loss; R2: 1.083495e-02 0.652221
2019-12-03 23:46:19,572 valid 000 5.643312e-03 0.720178
2019-12-03 23:46:21,007 validation loss; R2: 5.992826e-03 0.806641
2019-12-03 23:46:21,032 epoch 5 lr 2.000000e-03
2019-12-03 23:46:21,398 train 000 6.083183e-03 0.741531
2019-12-03 23:46:35,429 train 050 1.057168e-02 0.643721
2019-12-03 23:46:49,464 train 100 1.006174e-02 0.657253
2019-12-03 23:47:03,495 train 150 1.016755e-02 0.662986
2019-12-03 23:47:17,526 train 200 1.001934e-02 0.666224
2019-12-03 23:47:23,840 training loss; R2: 1.000976e-02 0.671376
2019-12-03 23:47:23,977 valid 000 4.118743e-03 0.850659
2019-12-03 23:47:25,412 validation loss; R2: 5.335590e-03 0.825957
2019-12-03 23:47:25,437 epoch 6 lr 2.000000e-03
2019-12-03 23:47:25,802 train 000 5.916320e-03 0.807332
2019-12-03 23:47:39,826 train 050 1.104815e-02 0.657410
2019-12-03 23:47:53,849 train 100 1.052082e-02 0.666395
2019-12-03 23:48:07,875 train 150 9.685516e-03 0.682085
2019-12-03 23:48:21,899 train 200 9.241274e-03 0.697318
2019-12-03 23:48:28,211 training loss; R2: 9.206480e-03 0.697903
2019-12-03 23:48:28,354 valid 000 9.699685e-03 0.803563
2019-12-03 23:48:29,789 validation loss; R2: 6.646059e-03 0.784560
2019-12-03 23:48:29,815 epoch 7 lr 2.000000e-03
2019-12-03 23:48:30,179 train 000 4.600679e-03 0.808891
2019-12-03 23:48:44,203 train 050 8.360594e-03 0.708460
2019-12-03 23:48:58,229 train 100 8.997293e-03 0.704380
2019-12-03 23:49:12,252 train 150 8.696429e-03 0.708849
2019-12-03 23:49:26,277 train 200 8.581630e-03 0.710039
2019-12-03 23:49:32,585 training loss; R2: 8.493476e-03 0.712538
2019-12-03 23:49:32,723 valid 000 1.382461e-02 0.765809
2019-12-03 23:49:34,157 validation loss; R2: 7.248499e-03 0.763418
2019-12-03 23:49:34,182 epoch 8 lr 2.000000e-03
2019-12-03 23:49:34,550 train 000 9.464531e-03 0.756191
2019-12-03 23:49:48,577 train 050 7.800508e-03 0.727241
2019-12-03 23:50:02,605 train 100 8.151162e-03 0.723478
2019-12-03 23:50:16,627 train 150 8.121189e-03 0.723772
2019-12-03 23:50:30,648 train 200 8.389686e-03 0.724467
2019-12-03 23:50:36,959 training loss; R2: 8.419062e-03 0.721419
2019-12-03 23:50:37,100 valid 000 5.496928e-03 0.753766
2019-12-03 23:50:38,534 validation loss; R2: 4.930436e-03 0.840048
2019-12-03 23:50:38,559 epoch 9 lr 2.000000e-03
2019-12-03 23:50:38,926 train 000 5.224066e-03 0.862678
2019-12-03 23:50:52,954 train 050 7.976048e-03 0.732038
2019-12-03 23:51:06,988 train 100 7.440564e-03 0.738272
2019-12-03 23:51:21,022 train 150 7.473878e-03 0.745570
2019-12-03 23:51:35,057 train 200 7.501656e-03 0.746355
2019-12-03 23:51:41,372 training loss; R2: 7.491266e-03 0.747812
2019-12-03 23:51:41,511 valid 000 1.282986e-02 0.614143
2019-12-03 23:51:42,946 validation loss; R2: 1.192302e-02 0.620814
2019-12-03 23:51:42,970 epoch 10 lr 2.000000e-03
2019-12-03 23:51:43,338 train 000 5.221818e-03 0.705452
2019-12-03 23:51:57,372 train 050 8.096309e-03 0.715682
2019-12-03 23:52:11,409 train 100 7.914610e-03 0.735301
2019-12-03 23:52:25,448 train 150 7.552744e-03 0.740248
2019-12-03 23:52:39,487 train 200 7.257672e-03 0.745984
2019-12-03 23:52:45,801 training loss; R2: 7.485151e-03 0.744442
2019-12-03 23:52:45,947 valid 000 3.596921e-02 -1.034599
2019-12-03 23:52:47,381 validation loss; R2: 5.547995e-02 -0.848956
2019-12-03 23:52:47,406 epoch 11 lr 2.000000e-03
2019-12-03 23:52:47,770 train 000 6.681389e-03 0.798667
2019-12-03 23:53:01,790 train 050 8.203851e-03 0.712946
2019-12-03 23:53:15,811 train 100 7.883979e-03 0.736561
2019-12-03 23:53:29,832 train 150 7.965597e-03 0.730350
2019-12-03 23:53:43,855 train 200 7.639018e-03 0.743563
2019-12-03 23:53:50,164 training loss; R2: 7.611187e-03 0.740992
2019-12-03 23:53:50,314 valid 000 9.742763e-03 0.628709
2019-12-03 23:53:51,748 validation loss; R2: 6.564450e-03 0.779639
2019-12-03 23:53:51,773 epoch 12 lr 2.000000e-03
2019-12-03 23:53:52,139 train 000 5.653101e-03 0.752583
2019-12-03 23:54:06,168 train 050 6.497546e-03 0.753138
2019-12-03 23:54:20,194 train 100 6.438499e-03 0.768008
2019-12-03 23:54:34,217 train 150 7.139384e-03 0.757646
2019-12-03 23:54:48,246 train 200 6.993092e-03 0.764523
2019-12-03 23:54:54,557 training loss; R2: 6.919607e-03 0.769552
2019-12-03 23:54:54,696 valid 000 7.294511e-03 0.790582
2019-12-03 23:54:56,131 validation loss; R2: 3.869869e-03 0.874836
2019-12-03 23:54:56,156 epoch 13 lr 2.000000e-03
2019-12-03 23:54:56,528 train 000 4.699898e-03 0.783166
2019-12-03 23:55:10,546 train 050 6.992216e-03 0.774683
2019-12-03 23:55:24,562 train 100 6.882401e-03 0.768609
2019-12-03 23:55:38,579 train 150 6.792957e-03 0.768762
2019-12-03 23:55:52,594 train 200 6.722613e-03 0.770890
2019-12-03 23:55:58,901 training loss; R2: 6.643074e-03 0.772138
2019-12-03 23:55:59,040 valid 000 5.953879e-03 0.760746
2019-12-03 23:56:00,474 validation loss; R2: 8.049750e-03 0.738718
2019-12-03 23:56:00,499 epoch 14 lr 2.000000e-03
2019-12-03 23:56:00,865 train 000 4.820844e-03 0.821674
2019-12-03 23:56:14,881 train 050 6.330176e-03 0.785445
2019-12-03 23:56:28,703 train 100 6.254073e-03 0.782320
2019-12-03 23:56:42,388 train 150 6.312253e-03 0.783481
2019-12-03 23:56:56,069 train 200 6.494645e-03 0.779435
2019-12-03 23:57:02,221 training loss; R2: 6.522116e-03 0.780427
2019-12-03 23:57:02,357 valid 000 3.777878e-03 0.867796
2019-12-03 23:57:03,790 validation loss; R2: 4.358680e-03 0.846766
2019-12-03 23:57:03,815 epoch 15 lr 2.000000e-03
2019-12-03 23:57:04,172 train 000 3.572658e-03 0.806198
2019-12-03 23:57:17,870 train 050 7.563208e-03 0.759602
2019-12-03 23:57:31,567 train 100 6.699038e-03 0.775899
2019-12-03 23:57:45,260 train 150 6.939336e-03 0.765736
2019-12-03 23:57:58,958 train 200 6.728671e-03 0.775250
2019-12-03 23:58:05,115 training loss; R2: 6.758263e-03 0.775760
2019-12-03 23:58:05,253 valid 000 3.017277e-03 0.841915
2019-12-03 23:58:06,687 validation loss; R2: 3.552795e-03 0.875399
2019-12-03 23:58:06,711 epoch 16 lr 2.000000e-03
2019-12-03 23:58:07,070 train 000 5.085036e-03 0.892183
2019-12-03 23:58:20,760 train 050 6.096583e-03 0.785581
2019-12-03 23:58:34,445 train 100 6.364655e-03 0.780565
2019-12-03 23:58:48,133 train 150 6.350540e-03 0.785077
2019-12-03 23:59:01,812 train 200 6.335184e-03 0.785885
2019-12-03 23:59:07,966 training loss; R2: 6.234276e-03 0.787173
2019-12-03 23:59:08,112 valid 000 2.621462e-02 0.100594
2019-12-03 23:59:09,545 validation loss; R2: 2.830697e-02 0.106472
2019-12-03 23:59:09,570 epoch 17 lr 2.000000e-03
2019-12-03 23:59:09,928 train 000 5.524816e-03 0.892959
2019-12-03 23:59:23,617 train 050 7.000771e-03 0.767289
2019-12-03 23:59:37,312 train 100 6.646456e-03 0.780234
2019-12-03 23:59:50,996 train 150 6.553878e-03 0.781443
2019-12-04 00:00:04,684 train 200 6.502244e-03 0.783967
2019-12-04 00:00:10,841 training loss; R2: 6.522560e-03 0.785032
2019-12-04 00:00:10,984 valid 000 3.078579e-03 0.880463
2019-12-04 00:00:12,418 validation loss; R2: 4.132908e-03 0.852709
2019-12-04 00:00:12,441 epoch 18 lr 2.000000e-03
2019-12-04 00:00:12,800 train 000 5.434599e-03 0.742820
2019-12-04 00:00:26,489 train 050 5.903850e-03 0.803499
2019-12-04 00:00:40,176 train 100 6.366091e-03 0.789022
2019-12-04 00:00:53,867 train 150 6.420920e-03 0.783590
2019-12-04 00:01:07,550 train 200 6.249501e-03 0.787402
2019-12-04 00:01:13,705 training loss; R2: 6.154094e-03 0.789893
2019-12-04 00:01:13,843 valid 000 2.987980e-03 0.909054
2019-12-04 00:01:15,275 validation loss; R2: 4.452449e-03 0.849482
2019-12-04 00:01:15,300 epoch 19 lr 2.000000e-03
2019-12-04 00:01:15,658 train 000 8.712488e-03 0.753076
2019-12-04 00:01:29,338 train 050 6.030465e-03 0.797106
2019-12-04 00:01:43,013 train 100 5.777159e-03 0.797061
2019-12-04 00:01:56,693 train 150 5.914941e-03 0.792779
2019-12-04 00:02:10,367 train 200 6.022471e-03 0.788769
2019-12-04 00:02:16,520 training loss; R2: 6.206982e-03 0.788823
2019-12-04 00:02:16,659 valid 000 3.983603e-03 0.860488
2019-12-04 00:02:18,092 validation loss; R2: 4.433678e-03 0.848438
