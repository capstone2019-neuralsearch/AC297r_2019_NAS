2019-12-03 20:08:56,469 gpu device = 1
2019-12-03 20:08:56,469 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=96, cutout=False, cutout_length=16, data='../data', dataset='graphene', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name=None, gpu=1, grad_clip=5, gz_dtree=False, init_channels=36, layers=8, learning_rate=0.002, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-graphene-EXP-20191203-200856', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-12-03 20:08:59,693 param size = 0.908101MB
2019-12-03 20:08:59,696 epoch 0 lr 2.000000e-03
2019-12-03 20:09:02,065 train 000 8.756737e-01 -50.479259
2019-12-03 20:09:12,101 train 050 2.525923e-01 -7.186737
2019-12-03 20:09:22,108 train 100 1.421808e-01 -3.578648
2019-12-03 20:09:32,120 train 150 1.036065e-01 -2.345611
2019-12-03 20:09:42,125 train 200 8.377904e-02 -1.714528
2019-12-03 20:09:47,561 training loss; R2: 7.800169e-02 -1.523701
2019-12-03 20:09:47,686 valid 000 3.961333e-02 0.375554
2019-12-03 20:09:49,062 validation loss; R2: 1.911073e-02 0.398214
2019-12-03 20:09:49,080 epoch 1 lr 2.000000e-03
2019-12-03 20:09:49,385 train 000 4.756479e-02 0.264751
2019-12-03 20:09:59,396 train 050 2.546899e-02 0.215127
2019-12-03 20:10:09,406 train 100 2.541889e-02 0.226419
2019-12-03 20:10:19,397 train 150 2.362496e-02 0.249862
2019-12-03 20:10:29,175 train 200 2.230308e-02 0.293776
2019-12-03 20:10:33,571 training loss; R2: 2.158946e-02 0.311218
2019-12-03 20:10:33,683 valid 000 1.623348e-02 0.577334
2019-12-03 20:10:34,739 validation loss; R2: 1.266635e-02 0.579616
2019-12-03 20:10:34,757 epoch 2 lr 2.000000e-03
2019-12-03 20:10:35,022 train 000 2.113155e-02 0.415662
2019-12-03 20:10:44,808 train 050 1.484759e-02 0.495173
2019-12-03 20:10:54,588 train 100 1.484042e-02 0.516000
2019-12-03 20:11:04,375 train 150 1.561104e-02 0.507048
2019-12-03 20:11:14,164 train 200 1.504080e-02 0.517478
2019-12-03 20:11:18,563 training loss; R2: 1.480551e-02 0.523227
2019-12-03 20:11:18,680 valid 000 7.531449e-03 0.760467
2019-12-03 20:11:19,734 validation loss; R2: 6.909686e-03 0.778601
2019-12-03 20:11:19,751 epoch 3 lr 2.000000e-03
2019-12-03 20:11:20,015 train 000 1.348307e-02 0.656307
2019-12-03 20:11:29,800 train 050 1.444703e-02 0.551994
2019-12-03 20:11:39,590 train 100 1.416443e-02 0.543895
2019-12-03 20:11:49,378 train 150 1.391133e-02 0.545519
2019-12-03 20:11:59,165 train 200 1.332221e-02 0.566737
2019-12-03 20:12:03,566 training loss; R2: 1.314728e-02 0.572002
2019-12-03 20:12:03,681 valid 000 9.133243e-03 0.775209
2019-12-03 20:12:04,738 validation loss; R2: 7.409596e-03 0.765386
2019-12-03 20:12:04,755 epoch 4 lr 2.000000e-03
2019-12-03 20:12:05,023 train 000 8.991194e-03 0.779593
2019-12-03 20:12:14,805 train 050 1.264516e-02 0.587532
2019-12-03 20:12:24,619 train 100 1.193387e-02 0.612987
2019-12-03 20:12:34,409 train 150 1.110106e-02 0.633383
2019-12-03 20:12:44,203 train 200 1.107089e-02 0.637818
2019-12-03 20:12:48,603 training loss; R2: 1.114739e-02 0.630441
2019-12-03 20:12:48,716 valid 000 7.301886e-03 0.786141
2019-12-03 20:12:49,773 validation loss; R2: 8.890227e-03 0.696206
2019-12-03 20:12:49,791 epoch 5 lr 2.000000e-03
2019-12-03 20:12:50,056 train 000 7.054383e-03 0.643198
2019-12-03 20:12:59,839 train 050 1.114069e-02 0.632548
2019-12-03 20:13:09,617 train 100 1.129916e-02 0.612471
2019-12-03 20:13:19,401 train 150 1.052255e-02 0.636688
2019-12-03 20:13:29,185 train 200 1.054479e-02 0.647280
2019-12-03 20:13:33,584 training loss; R2: 1.036456e-02 0.650715
2019-12-03 20:13:33,696 valid 000 1.175370e-02 0.732598
2019-12-03 20:13:34,751 validation loss; R2: 7.545001e-03 0.748905
2019-12-03 20:13:34,769 epoch 6 lr 2.000000e-03
2019-12-03 20:13:35,032 train 000 1.094082e-02 0.817234
2019-12-03 20:13:44,820 train 050 1.020221e-02 0.687804
2019-12-03 20:13:54,608 train 100 9.881150e-03 0.683131
2019-12-03 20:14:04,393 train 150 9.512762e-03 0.694480
2019-12-03 20:14:14,181 train 200 9.173646e-03 0.698052
2019-12-03 20:14:18,582 training loss; R2: 9.167926e-03 0.696794
2019-12-03 20:14:18,706 valid 000 6.662698e-03 0.811002
2019-12-03 20:14:19,762 validation loss; R2: 6.479265e-03 0.761026
2019-12-03 20:14:19,779 epoch 7 lr 2.000000e-03
2019-12-03 20:14:20,045 train 000 1.853266e-02 -0.069613
2019-12-03 20:14:29,825 train 050 9.393825e-03 0.658402
2019-12-03 20:14:39,600 train 100 9.495711e-03 0.682575
2019-12-03 20:14:49,387 train 150 8.874058e-03 0.702424
2019-12-03 20:14:59,171 train 200 8.785745e-03 0.707962
2019-12-03 20:15:03,567 training loss; R2: 8.629620e-03 0.710528
2019-12-03 20:15:03,683 valid 000 4.991298e-03 0.866901
2019-12-03 20:15:04,740 validation loss; R2: 5.353290e-03 0.828262
2019-12-03 20:15:04,759 epoch 8 lr 2.000000e-03
2019-12-03 20:15:05,030 train 000 8.254856e-03 0.759627
2019-12-03 20:15:14,810 train 050 8.337898e-03 0.720971
2019-12-03 20:15:24,593 train 100 8.644263e-03 0.718001
2019-12-03 20:15:34,374 train 150 8.477948e-03 0.722594
2019-12-03 20:15:44,161 train 200 8.557882e-03 0.719606
2019-12-03 20:15:48,558 training loss; R2: 8.492924e-03 0.720327
2019-12-03 20:15:48,673 valid 000 6.255936e-03 0.839834
2019-12-03 20:15:49,728 validation loss; R2: 5.640881e-03 0.820467
2019-12-03 20:15:49,752 epoch 9 lr 2.000000e-03
2019-12-03 20:15:50,018 train 000 1.713199e-02 0.627718
2019-12-03 20:15:59,806 train 050 8.103321e-03 0.720085
2019-12-03 20:16:09,591 train 100 7.951089e-03 0.726364
2019-12-03 20:16:19,369 train 150 8.065847e-03 0.732096
2019-12-03 20:16:29,153 train 200 8.044712e-03 0.729484
2019-12-03 20:16:33,556 training loss; R2: 8.097134e-03 0.728241
2019-12-03 20:16:33,669 valid 000 6.236847e-03 0.646359
2019-12-03 20:16:34,725 validation loss; R2: 5.811775e-03 0.809058
2019-12-03 20:16:34,741 epoch 10 lr 2.000000e-03
2019-12-03 20:16:35,007 train 000 6.161082e-03 0.668172
2019-12-03 20:16:44,794 train 050 8.082261e-03 0.749344
2019-12-03 20:16:54,579 train 100 7.731163e-03 0.743590
2019-12-03 20:17:04,360 train 150 7.723417e-03 0.743747
2019-12-03 20:17:14,143 train 200 7.520279e-03 0.748768
2019-12-03 20:17:18,541 training loss; R2: 7.469894e-03 0.750159
2019-12-03 20:17:18,651 valid 000 2.827592e-03 0.903393
2019-12-03 20:17:19,707 validation loss; R2: 3.671689e-03 0.876128
2019-12-03 20:17:19,724 epoch 11 lr 2.000000e-03
2019-12-03 20:17:19,991 train 000 8.664869e-03 0.739458
2019-12-03 20:17:29,772 train 050 8.083202e-03 0.743149
2019-12-03 20:17:39,546 train 100 7.769064e-03 0.741894
2019-12-03 20:17:49,320 train 150 7.578522e-03 0.753516
2019-12-03 20:17:59,101 train 200 7.464165e-03 0.753658
2019-12-03 20:18:03,499 training loss; R2: 7.328490e-03 0.756440
2019-12-03 20:18:03,605 valid 000 5.241998e-03 0.819436
2019-12-03 20:18:04,661 validation loss; R2: 4.635576e-03 0.852412
2019-12-03 20:18:04,679 epoch 12 lr 2.000000e-03
2019-12-03 20:18:04,952 train 000 6.182456e-03 0.684871
2019-12-03 20:18:14,727 train 050 6.575578e-03 0.765402
2019-12-03 20:18:24,506 train 100 6.891933e-03 0.753941
2019-12-03 20:18:34,281 train 150 7.430014e-03 0.747670
2019-12-03 20:18:44,054 train 200 7.616219e-03 0.744129
2019-12-03 20:18:48,444 training loss; R2: 7.593321e-03 0.746213
2019-12-03 20:18:48,553 valid 000 2.679644e-03 0.943269
2019-12-03 20:18:49,611 validation loss; R2: 3.843316e-03 0.870228
2019-12-03 20:18:49,629 epoch 13 lr 2.000000e-03
2019-12-03 20:18:49,892 train 000 4.516263e-03 0.796693
2019-12-03 20:18:59,658 train 050 7.412503e-03 0.727561
2019-12-03 20:19:09,434 train 100 7.176410e-03 0.750504
2019-12-03 20:19:19,217 train 150 7.154411e-03 0.753168
2019-12-03 20:19:29,005 train 200 7.166693e-03 0.757126
2019-12-03 20:19:33,403 training loss; R2: 7.066835e-03 0.760676
2019-12-03 20:19:33,510 valid 000 2.717337e-03 0.906682
2019-12-03 20:19:34,565 validation loss; R2: 4.012962e-03 0.867513
2019-12-03 20:19:34,582 epoch 14 lr 2.000000e-03
2019-12-03 20:19:34,846 train 000 1.011363e-02 0.829246
2019-12-03 20:19:44,615 train 050 7.877152e-03 0.734157
2019-12-03 20:19:54,395 train 100 7.072607e-03 0.756744
2019-12-03 20:20:04,169 train 150 6.667882e-03 0.772041
2019-12-03 20:20:13,943 train 200 6.605164e-03 0.773510
2019-12-03 20:20:18,333 training loss; R2: 6.566029e-03 0.775220
2019-12-03 20:20:18,445 valid 000 3.545048e-03 0.860893
2019-12-03 20:20:19,502 validation loss; R2: 4.500606e-03 0.857508
2019-12-03 20:20:19,519 epoch 15 lr 2.000000e-03
2019-12-03 20:20:19,788 train 000 4.270524e-03 0.812486
2019-12-03 20:20:29,552 train 050 6.429607e-03 0.784674
2019-12-03 20:20:39,321 train 100 6.187633e-03 0.796815
2019-12-03 20:20:49,085 train 150 6.200182e-03 0.792800
2019-12-03 20:20:58,852 train 200 6.078755e-03 0.794342
2019-12-03 20:21:03,241 training loss; R2: 6.048249e-03 0.795310
2019-12-03 20:21:03,352 valid 000 5.569136e-03 0.827142
2019-12-03 20:21:04,409 validation loss; R2: 4.125045e-03 0.859453
2019-12-03 20:21:04,427 epoch 16 lr 2.000000e-03
2019-12-03 20:21:04,693 train 000 6.301360e-03 0.818771
2019-12-03 20:21:14,459 train 050 5.970844e-03 0.809862
2019-12-03 20:21:24,221 train 100 6.003420e-03 0.801273
2019-12-03 20:21:33,980 train 150 6.078312e-03 0.798078
2019-12-03 20:21:43,736 train 200 5.900392e-03 0.801670
2019-12-03 20:21:48,123 training loss; R2: 5.910739e-03 0.800384
2019-12-03 20:21:48,234 valid 000 2.809870e-03 0.879079
2019-12-03 20:21:49,289 validation loss; R2: 3.382052e-03 0.881443
2019-12-03 20:21:49,306 epoch 17 lr 2.000000e-03
2019-12-03 20:21:49,568 train 000 5.044166e-03 0.866963
2019-12-03 20:21:59,326 train 050 5.183449e-03 0.813191
2019-12-03 20:22:09,085 train 100 5.694515e-03 0.801557
2019-12-03 20:22:18,844 train 150 6.105465e-03 0.795010
2019-12-03 20:22:28,602 train 200 6.217204e-03 0.791081
2019-12-03 20:22:32,985 training loss; R2: 6.191863e-03 0.793641
2019-12-03 20:22:33,095 valid 000 2.504942e-03 0.871039
2019-12-03 20:22:34,150 validation loss; R2: 4.729296e-03 0.843753
2019-12-03 20:22:34,168 epoch 18 lr 2.000000e-03
2019-12-03 20:22:34,438 train 000 4.213937e-03 0.673467
2019-12-03 20:22:44,189 train 050 6.678348e-03 0.776991
2019-12-03 20:22:53,958 train 100 6.110325e-03 0.792949
2019-12-03 20:23:03,717 train 150 5.732280e-03 0.804633
2019-12-03 20:23:13,477 train 200 5.783011e-03 0.801567
2019-12-03 20:23:17,868 training loss; R2: 5.861566e-03 0.800980
2019-12-03 20:23:17,980 valid 000 3.138472e-03 0.879722
2019-12-03 20:23:19,036 validation loss; R2: 3.875492e-03 0.872523
2019-12-03 20:23:19,054 epoch 19 lr 2.000000e-03
2019-12-03 20:23:19,322 train 000 4.999775e-03 0.874771
2019-12-03 20:23:29,085 train 050 7.534562e-03 0.749478
2019-12-03 20:23:38,849 train 100 7.271094e-03 0.758956
2019-12-03 20:23:48,608 train 150 7.025717e-03 0.767844
2019-12-03 20:23:58,368 train 200 6.695809e-03 0.776804
2019-12-03 20:24:02,755 training loss; R2: 6.642993e-03 0.777402
2019-12-03 20:24:02,868 valid 000 3.399434e-03 0.871961
2019-12-03 20:24:03,925 validation loss; R2: 4.308832e-03 0.857353
