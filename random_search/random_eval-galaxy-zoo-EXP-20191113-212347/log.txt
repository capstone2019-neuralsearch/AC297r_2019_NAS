2019-11-13 21:23:48,241 gpu device = 1
2019-11-13 21:23:48,242 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name='galaxy-zoo-cleaned', gpu=1, grad_clip=5, gz_dtree=False, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-galaxy-zoo-EXP-20191113-212347', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-11-13 21:23:59,951 param size = 0.276021MB
2019-11-13 21:23:59,955 epoch 0 lr 1.000000e-03
2019-11-13 21:24:02,164 train 000 1.987292e-01 -92.013456
2019-11-13 21:24:09,260 train 050 4.580226e-02 -7.074515
2019-11-13 21:24:16,375 train 100 3.572150e-02 -3.759086
2019-11-13 21:24:23,367 train 150 3.196888e-02 -2.597420
2019-11-13 21:24:30,451 train 200 2.981594e-02 -1.994851
2019-11-13 21:24:37,567 train 250 2.836350e-02 -1.604241
2019-11-13 21:24:44,612 train 300 2.718631e-02 -1.350908
2019-11-13 21:24:51,685 train 350 2.625533e-02 -1.159831
2019-11-13 21:24:58,780 train 400 2.550140e-02 -1.058736
2019-11-13 21:25:05,676 train 450 2.493518e-02 -0.936540
2019-11-13 21:25:12,610 train 500 2.439704e-02 -0.836395
2019-11-13 21:25:19,620 train 550 2.393614e-02 -0.758043
2019-11-13 21:25:26,821 train 600 2.349641e-02 -0.690520
2019-11-13 21:25:33,861 train 650 2.309613e-02 -0.629188
2019-11-13 21:25:40,916 train 700 2.280063e-02 -0.610218
2019-11-13 21:25:47,939 train 750 2.246427e-02 -0.562561
2019-11-13 21:25:54,961 train 800 2.216503e-02 -0.520042
2019-11-13 21:26:01,854 train 850 2.188380e-02 -0.482044
2019-11-13 21:26:04,681 training loss; R2: 2.180653e-02 -0.471354
2019-11-13 21:26:04,990 valid 000 1.950744e-02 0.050365
2019-11-13 21:26:06,721 valid 050 1.686748e-02 0.051780
2019-11-13 21:26:08,324 validation loss; R2: 1.672618e-02 -0.003670
2019-11-13 21:26:08,356 epoch 1 lr 1.000000e-03
2019-11-13 21:26:08,964 train 000 1.686235e-02 0.168565
2019-11-13 21:26:16,033 train 050 1.698246e-02 0.132313
2019-11-13 21:26:23,116 train 100 1.690945e-02 0.128058
2019-11-13 21:26:30,142 train 150 1.675784e-02 -1.346512
2019-11-13 21:26:37,076 train 200 1.667157e-02 -0.977313
2019-11-13 21:26:44,053 train 250 1.653831e-02 -0.750658
2019-11-13 21:26:51,265 train 300 1.642397e-02 -0.600342
2019-11-13 21:26:58,454 train 350 1.630913e-02 -0.493468
2019-11-13 21:27:05,521 train 400 1.616559e-02 -0.412183
2019-11-13 21:27:12,540 train 450 1.609410e-02 -0.345788
2019-11-13 21:27:19,598 train 500 1.599188e-02 -0.290105
2019-11-13 21:27:26,762 train 550 1.589360e-02 -0.248159
2019-11-13 21:27:33,890 train 600 1.579264e-02 -0.211522
2019-11-13 21:27:40,920 train 650 1.572768e-02 -0.179486
2019-11-13 21:27:48,020 train 700 1.563258e-02 -0.155008
2019-11-13 21:27:55,054 train 750 1.556229e-02 -0.131796
2019-11-13 21:28:02,080 train 800 1.550130e-02 -0.113031
2019-11-13 21:28:09,229 train 850 1.542117e-02 -0.093602
2019-11-13 21:28:11,369 training loss; R2: 1.539284e-02 -0.087666
2019-11-13 21:28:11,660 valid 000 1.210103e-02 0.327242
2019-11-13 21:28:13,365 valid 050 1.412701e-02 0.260558
2019-11-13 21:28:14,940 validation loss; R2: 1.394442e-02 0.261871
2019-11-13 21:28:14,966 epoch 2 lr 1.000000e-03
2019-11-13 21:28:15,373 train 000 1.580604e-02 0.246524
2019-11-13 21:28:22,310 train 050 1.435171e-02 0.221948
2019-11-13 21:28:29,321 train 100 1.411190e-02 0.190845
2019-11-13 21:28:36,285 train 150 1.407117e-02 0.033511
2019-11-13 21:28:43,316 train 200 1.398404e-02 0.073179
2019-11-13 21:28:50,319 train 250 1.391050e-02 0.101405
2019-11-13 21:28:57,584 train 300 1.381479e-02 0.125009
2019-11-13 21:29:04,669 train 350 1.378381e-02 0.135433
2019-11-13 21:29:11,753 train 400 1.376416e-02 0.147534
2019-11-13 21:29:18,904 train 450 1.378383e-02 0.157710
2019-11-13 21:29:25,993 train 500 1.376606e-02 0.160738
2019-11-13 21:29:33,135 train 550 1.372884e-02 0.167054
2019-11-13 21:29:40,157 train 600 1.373118e-02 0.171468
2019-11-13 21:29:47,070 train 650 1.369774e-02 0.178668
2019-11-13 21:29:53,931 train 700 1.364669e-02 0.186537
2019-11-13 21:30:00,870 train 750 1.361247e-02 0.189064
2019-11-13 21:30:07,957 train 800 1.358378e-02 0.190453
2019-11-13 21:30:14,940 train 850 1.354931e-02 0.193661
2019-11-13 21:30:17,030 training loss; R2: 1.353160e-02 0.194879
2019-11-13 21:30:17,325 valid 000 1.061312e-02 0.321224
2019-11-13 21:30:19,044 valid 050 1.200601e-02 0.307034
2019-11-13 21:30:20,594 validation loss; R2: 1.171952e-02 0.308275
2019-11-13 21:30:20,615 epoch 3 lr 1.000000e-03
2019-11-13 21:30:21,045 train 000 1.266863e-02 0.270531
2019-11-13 21:30:28,255 train 050 1.258235e-02 0.264676
2019-11-13 21:30:35,152 train 100 1.277971e-02 0.261493
2019-11-13 21:30:41,948 train 150 1.296256e-02 0.209350
2019-11-13 21:30:48,738 train 200 1.291467e-02 0.211576
2019-11-13 21:30:55,534 train 250 1.287645e-02 0.136812
2019-11-13 21:31:02,331 train 300 1.282058e-02 0.157001
2019-11-13 21:31:09,120 train 350 1.280467e-02 0.173990
2019-11-13 21:31:15,905 train 400 1.280744e-02 0.132588
2019-11-13 21:31:22,696 train 450 1.282776e-02 0.143629
2019-11-13 21:31:29,478 train 500 1.279457e-02 0.153156
2019-11-13 21:31:36,270 train 550 1.279309e-02 0.154841
2019-11-13 21:31:43,062 train 600 1.277340e-02 0.161874
2019-11-13 21:31:49,862 train 650 1.275744e-02 0.168891
2019-11-13 21:31:56,664 train 700 1.274581e-02 0.173654
2019-11-13 21:32:03,453 train 750 1.272883e-02 0.177182
2019-11-13 21:32:10,251 train 800 1.271568e-02 0.184628
2019-11-13 21:32:17,046 train 850 1.270297e-02 0.187320
2019-11-13 21:32:19,079 training loss; R2: 1.269272e-02 0.189360
2019-11-13 21:32:19,415 valid 000 1.158466e-02 0.350867
2019-11-13 21:32:21,110 valid 050 1.136483e-02 0.358223
2019-11-13 21:32:22,647 validation loss; R2: 1.117757e-02 0.346449
2019-11-13 21:32:22,663 epoch 4 lr 1.000000e-03
2019-11-13 21:32:23,081 train 000 1.073331e-02 0.386312
2019-11-13 21:32:30,170 train 050 1.204502e-02 0.310249
2019-11-13 21:32:37,077 train 100 1.206715e-02 0.299853
2019-11-13 21:32:43,889 train 150 1.223580e-02 0.289793
2019-11-13 21:32:50,700 train 200 1.213506e-02 0.290377
2019-11-13 21:32:57,511 train 250 1.215006e-02 0.263890
2019-11-13 21:33:04,347 train 300 1.209662e-02 0.270210
2019-11-13 21:33:11,161 train 350 1.205295e-02 0.274604
2019-11-13 21:33:17,975 train 400 1.202835e-02 0.275883
2019-11-13 21:33:24,786 train 450 1.205932e-02 0.275502
2019-11-13 21:33:31,663 train 500 1.205943e-02 0.277634
2019-11-13 21:33:38,466 train 550 1.204859e-02 0.279481
2019-11-13 21:33:45,265 train 600 1.205812e-02 0.278459
2019-11-13 21:33:52,073 train 650 1.204783e-02 0.279398
2019-11-13 21:33:58,951 train 700 1.201437e-02 0.281892
2019-11-13 21:34:05,762 train 750 1.199564e-02 0.280750
2019-11-13 21:34:12,575 train 800 1.200559e-02 0.277755
2019-11-13 21:34:19,386 train 850 1.199970e-02 0.279507
2019-11-13 21:34:21,425 training loss; R2: 1.201281e-02 0.279541
2019-11-13 21:34:21,780 valid 000 8.605937e-03 -0.013188
2019-11-13 21:34:23,523 valid 050 1.112009e-02 0.307341
2019-11-13 21:34:25,100 validation loss; R2: 1.119303e-02 0.323680
2019-11-13 21:34:25,117 epoch 5 lr 1.000000e-03
2019-11-13 21:34:25,539 train 000 1.340814e-02 0.305096
2019-11-13 21:34:32,574 train 050 1.185766e-02 0.304446
2019-11-13 21:34:39,481 train 100 1.168506e-02 0.306378
2019-11-13 21:34:46,301 train 150 1.172200e-02 0.295312
2019-11-13 21:34:53,114 train 200 1.174907e-02 0.297402
2019-11-13 21:34:59,931 train 250 1.181839e-02 0.296421
2019-11-13 21:35:06,746 train 300 1.177570e-02 0.297747
2019-11-13 21:35:13,578 train 350 1.177148e-02 0.297734
2019-11-13 21:35:20,391 train 400 1.176970e-02 0.295479
2019-11-13 21:35:27,204 train 450 1.176798e-02 0.294749
2019-11-13 21:35:34,022 train 500 1.175253e-02 0.288849
2019-11-13 21:35:40,846 train 550 1.173592e-02 0.284194
2019-11-13 21:35:47,666 train 600 1.172777e-02 0.284093
2019-11-13 21:35:54,479 train 650 1.171271e-02 0.285703
2019-11-13 21:36:01,301 train 700 1.169775e-02 0.287624
2019-11-13 21:36:08,272 train 750 1.168861e-02 0.280532
2019-11-13 21:36:15,080 train 800 1.168694e-02 0.279914
2019-11-13 21:36:21,879 train 850 1.165863e-02 0.281371
2019-11-13 21:36:23,913 training loss; R2: 1.165031e-02 0.282239
2019-11-13 21:36:24,268 valid 000 1.211916e-02 0.310026
2019-11-13 21:36:26,020 valid 050 1.055864e-02 0.262911
2019-11-13 21:36:27,600 validation loss; R2: 1.053030e-02 0.309310
2019-11-13 21:36:27,622 epoch 6 lr 1.000000e-03
2019-11-13 21:36:28,095 train 000 1.116399e-02 0.326818
2019-11-13 21:36:34,971 train 050 1.117074e-02 0.298094
2019-11-13 21:36:41,786 train 100 1.134143e-02 0.303405
2019-11-13 21:36:48,590 train 150 1.134826e-02 0.303004
2019-11-13 21:36:55,394 train 200 1.133297e-02 0.302414
2019-11-13 21:37:02,200 train 250 1.132758e-02 0.304886
2019-11-13 21:37:09,002 train 300 1.135344e-02 0.290965
2019-11-13 21:37:15,820 train 350 1.139401e-02 0.294731
2019-11-13 21:37:22,614 train 400 1.140285e-02 0.292002
2019-11-13 21:37:29,417 train 450 1.138649e-02 0.294396
2019-11-13 21:37:36,217 train 500 1.138346e-02 0.288067
2019-11-13 21:37:43,013 train 550 1.137461e-02 0.292200
2019-11-13 21:37:49,813 train 600 1.136784e-02 0.288333
2019-11-13 21:37:56,614 train 650 1.135503e-02 0.290329
2019-11-13 21:38:03,428 train 700 1.135595e-02 0.293473
2019-11-13 21:38:10,229 train 750 1.133676e-02 0.294637
2019-11-13 21:38:17,025 train 800 1.132493e-02 0.297274
2019-11-13 21:38:23,826 train 850 1.131716e-02 0.296316
2019-11-13 21:38:25,857 training loss; R2: 1.131834e-02 0.297232
2019-11-13 21:38:26,199 valid 000 5.197184e-02 -2.593869
2019-11-13 21:38:27,947 valid 050 5.405370e-02 -2.789807
2019-11-13 21:38:29,528 validation loss; R2: 5.396605e-02 -2.906778
2019-11-13 21:38:29,545 epoch 7 lr 1.000000e-03
2019-11-13 21:38:29,990 train 000 1.060731e-02 0.165765
2019-11-13 21:38:36,807 train 050 1.089107e-02 0.287617
2019-11-13 21:38:43,609 train 100 1.093805e-02 0.288541
2019-11-13 21:38:50,408 train 150 1.100915e-02 0.295513
2019-11-13 21:38:57,202 train 200 1.101225e-02 0.299276
2019-11-13 21:39:04,016 train 250 1.106966e-02 0.294636
2019-11-13 21:39:10,814 train 300 1.112177e-02 0.291240
2019-11-13 21:39:17,612 train 350 1.110560e-02 0.296886
2019-11-13 21:39:24,407 train 400 1.108388e-02 0.293896
2019-11-13 21:39:31,203 train 450 1.106076e-02 0.298709
2019-11-13 21:39:38,006 train 500 1.106553e-02 0.299596
2019-11-13 21:39:44,799 train 550 1.105425e-02 0.303480
2019-11-13 21:39:51,599 train 600 1.106224e-02 0.303492
2019-11-13 21:39:58,396 train 650 1.105889e-02 0.307122
2019-11-13 21:40:05,193 train 700 1.107806e-02 0.308457
2019-11-13 21:40:11,991 train 750 1.105719e-02 0.309779
2019-11-13 21:40:18,793 train 800 1.103764e-02 0.310564
2019-11-13 21:40:25,725 train 850 1.102472e-02 0.311174
2019-11-13 21:40:27,853 training loss; R2: 1.102249e-02 0.311522
2019-11-13 21:40:28,185 valid 000 1.126250e-01 -4.811751
2019-11-13 21:40:29,945 valid 050 1.120586e-01 -5.298748
2019-11-13 21:40:31,534 validation loss; R2: 1.118505e-01 -5.386991
2019-11-13 21:40:31,550 epoch 8 lr 1.000000e-03
2019-11-13 21:40:31,982 train 000 1.140941e-02 0.311861
2019-11-13 21:40:38,863 train 050 1.109281e-02 0.258101
2019-11-13 21:40:45,737 train 100 1.119564e-02 0.278371
2019-11-13 21:40:52,538 train 150 1.105482e-02 0.292025
2019-11-13 21:40:59,337 train 200 1.096253e-02 0.303175
2019-11-13 21:41:06,129 train 250 1.100309e-02 0.301777
2019-11-13 21:41:12,914 train 300 1.095970e-02 0.305346
2019-11-13 21:41:19,712 train 350 1.097057e-02 0.304323
2019-11-13 21:41:26,559 train 400 1.096375e-02 0.300606
2019-11-13 21:41:33,349 train 450 1.094325e-02 0.284383
2019-11-13 21:41:40,151 train 500 1.092063e-02 0.288825
2019-11-13 21:41:46,932 train 550 1.089979e-02 0.293798
2019-11-13 21:41:53,725 train 600 1.090249e-02 0.196505
2019-11-13 21:42:00,523 train 650 1.088700e-02 0.203970
2019-11-13 21:42:07,308 train 700 1.088536e-02 0.213897
2019-11-13 21:42:14,090 train 750 1.087428e-02 0.218678
2019-11-13 21:42:20,875 train 800 1.085322e-02 0.221992
2019-11-13 21:42:27,664 train 850 1.084939e-02 0.227914
2019-11-13 21:42:29,692 training loss; R2: 1.084265e-02 0.228754
2019-11-13 21:42:30,052 valid 000 7.746294e-02 -2.762652
2019-11-13 21:42:31,773 valid 050 8.230740e-02 -3.536622
2019-11-13 21:42:33,379 validation loss; R2: 8.200696e-02 -3.332685
2019-11-13 21:42:33,396 epoch 9 lr 1.000000e-03
2019-11-13 21:42:33,838 train 000 1.206171e-02 0.381679
2019-11-13 21:42:40,638 train 050 1.068698e-02 0.148599
2019-11-13 21:42:47,483 train 100 1.070601e-02 0.184524
2019-11-13 21:42:54,279 train 150 1.069579e-02 0.239191
2019-11-13 21:43:01,064 train 200 1.068758e-02 0.259756
2019-11-13 21:43:07,846 train 250 1.066643e-02 0.274496
2019-11-13 21:43:14,698 train 300 1.066637e-02 0.284056
2019-11-13 21:43:21,575 train 350 1.068056e-02 0.281290
2019-11-13 21:43:28,430 train 400 1.071261e-02 0.288364
2019-11-13 21:43:35,228 train 450 1.071579e-02 0.290348
2019-11-13 21:43:42,006 train 500 1.069820e-02 0.230846
2019-11-13 21:43:48,779 train 550 1.069548e-02 0.237452
2019-11-13 21:43:55,642 train 600 1.066531e-02 0.246451
2019-11-13 21:44:02,438 train 650 1.066798e-02 0.249008
2019-11-13 21:44:09,220 train 700 1.067609e-02 0.253317
2019-11-13 21:44:15,997 train 750 1.068178e-02 0.252644
2019-11-13 21:44:22,777 train 800 1.070021e-02 0.247748
2019-11-13 21:44:29,555 train 850 1.070001e-02 0.253975
2019-11-13 21:44:31,584 training loss; R2: 1.070255e-02 0.254424
2019-11-13 21:44:31,941 valid 000 8.357842e-02 -5.892747
2019-11-13 21:44:33,687 valid 050 8.729988e-02 -5.548970
2019-11-13 21:44:35,273 validation loss; R2: 8.731586e-02 -5.524421
2019-11-13 21:44:35,290 epoch 10 lr 1.000000e-03
2019-11-13 21:44:35,702 train 000 1.215862e-02 0.293735
2019-11-13 21:44:42,693 train 050 1.083885e-02 0.305105
2019-11-13 21:44:49,606 train 100 1.072432e-02 -0.081473
2019-11-13 21:44:56,420 train 150 1.068285e-02 0.062806
2019-11-13 21:45:03,242 train 200 1.064827e-02 0.126893
2019-11-13 21:45:10,040 train 250 1.058271e-02 0.170797
2019-11-13 21:45:16,834 train 300 1.057011e-02 0.200269
2019-11-13 21:45:23,637 train 350 1.057066e-02 0.201800
2019-11-13 21:45:30,435 train 400 1.055575e-02 0.190577
2019-11-13 21:45:37,238 train 450 1.058399e-02 0.196865
2019-11-13 21:45:44,033 train 500 1.057901e-02 0.207980
2019-11-13 21:45:50,831 train 550 1.054892e-02 0.222109
2019-11-13 21:45:57,633 train 600 1.053549e-02 0.228056
2019-11-13 21:46:04,430 train 650 1.051821e-02 0.234545
2019-11-13 21:46:11,224 train 700 1.051005e-02 0.240104
2019-11-13 21:46:18,019 train 750 1.051371e-02 0.242754
2019-11-13 21:46:24,816 train 800 1.051761e-02 0.247817
2019-11-13 21:46:31,612 train 850 1.050936e-02 0.251726
2019-11-13 21:46:33,647 training loss; R2: 1.051362e-02 0.253762
2019-11-13 21:46:34,012 valid 000 2.809461e-02 -0.387077
2019-11-13 21:46:35,776 valid 050 2.828426e-02 -0.616401
2019-11-13 21:46:37,369 validation loss; R2: 2.822270e-02 -0.661714
2019-11-13 21:46:37,386 epoch 11 lr 1.000000e-03
2019-11-13 21:46:37,828 train 000 1.190616e-02 0.407298
2019-11-13 21:46:44,648 train 050 1.035015e-02 0.180640
2019-11-13 21:46:51,438 train 100 1.025079e-02 0.192592
2019-11-13 21:46:58,216 train 150 1.037099e-02 0.247409
2019-11-13 21:47:05,008 train 200 1.036594e-02 0.270607
2019-11-13 21:47:11,787 train 250 1.038823e-02 0.280115
2019-11-13 21:47:18,562 train 300 1.041479e-02 0.281927
2019-11-13 21:47:25,333 train 350 1.043631e-02 0.290009
2019-11-13 21:47:32,107 train 400 1.046108e-02 0.264100
2019-11-13 21:47:38,880 train 450 1.045443e-02 0.263935
2019-11-13 21:47:45,652 train 500 1.047589e-02 0.272075
2019-11-13 21:47:52,429 train 550 1.048458e-02 0.275492
2019-11-13 21:47:59,201 train 600 1.048342e-02 0.278727
2019-11-13 21:48:05,990 train 650 1.049253e-02 0.283262
2019-11-13 21:48:12,757 train 700 1.049895e-02 0.285604
2019-11-13 21:48:19,527 train 750 1.049464e-02 0.287467
2019-11-13 21:48:26,294 train 800 1.048195e-02 0.290249
2019-11-13 21:48:33,076 train 850 1.048744e-02 0.292482
2019-11-13 21:48:35,103 training loss; R2: 1.047640e-02 0.291451
2019-11-13 21:48:35,459 valid 000 2.025584e-01 -8.070505
2019-11-13 21:48:37,217 valid 050 1.960557e-01 -14.390191
2019-11-13 21:48:38,801 validation loss; R2: 1.956941e-01 -11.832180
2019-11-13 21:48:38,818 epoch 12 lr 1.000000e-03
2019-11-13 21:48:39,249 train 000 9.736812e-03 0.373478
2019-11-13 21:48:46,204 train 050 1.041656e-02 0.331060
2019-11-13 21:48:53,100 train 100 1.028684e-02 0.312346
2019-11-13 21:48:59,910 train 150 1.025316e-02 0.323777
2019-11-13 21:49:06,814 train 200 1.032214e-02 0.328700
2019-11-13 21:49:13,614 train 250 1.029279e-02 0.308327
2019-11-13 21:49:20,395 train 300 1.036917e-02 0.305074
2019-11-13 21:49:27,203 train 350 1.037642e-02 0.308512
2019-11-13 21:49:33,983 train 400 1.032972e-02 0.313797
2019-11-13 21:49:40,774 train 450 1.033803e-02 0.317308
2019-11-13 21:49:47,564 train 500 1.034435e-02 0.315313
2019-11-13 21:49:54,342 train 550 1.037181e-02 0.315098
2019-11-13 21:50:01,121 train 600 1.038968e-02 0.307673
2019-11-13 21:50:07,905 train 650 1.037039e-02 0.307089
2019-11-13 21:50:14,680 train 700 1.037953e-02 0.306338
2019-11-13 21:50:21,457 train 750 1.039296e-02 0.309036
2019-11-13 21:50:28,315 train 800 1.038479e-02 0.311055
2019-11-13 21:50:35,104 train 850 1.038491e-02 0.311891
2019-11-13 21:50:37,136 training loss; R2: 1.037927e-02 0.313234
2019-11-13 21:50:37,491 valid 000 9.643140e+01 -46975.740566
2019-11-13 21:50:39,251 valid 050 9.629228e+01 -15485.821771
2019-11-13 21:50:40,825 validation loss; R2: 9.629284e+01 -15370.783901
2019-11-13 21:50:40,842 epoch 13 lr 1.000000e-03
2019-11-13 21:50:41,299 train 000 1.050338e-02 0.416006
2019-11-13 21:50:48,278 train 050 1.023425e-02 0.344653
2019-11-13 21:50:55,154 train 100 1.030108e-02 0.339476
2019-11-13 21:51:02,022 train 150 1.027603e-02 0.152365
2019-11-13 21:51:08,872 train 200 1.027056e-02 0.202961
2019-11-13 21:51:15,677 train 250 1.032627e-02 0.226310
2019-11-13 21:51:22,461 train 300 1.033904e-02 0.234643
2019-11-13 21:51:29,237 train 350 1.034753e-02 0.253355
2019-11-13 21:51:36,013 train 400 1.035313e-02 0.260707
2019-11-13 21:51:42,790 train 450 1.036843e-02 0.258805
2019-11-13 21:51:49,567 train 500 1.037370e-02 0.259687
2019-11-13 21:51:56,341 train 550 1.036538e-02 0.261272
2019-11-13 21:52:03,140 train 600 1.035008e-02 0.266349
2019-11-13 21:52:09,946 train 650 1.036341e-02 0.273416
2019-11-13 21:52:16,761 train 700 1.038016e-02 0.276512
2019-11-13 21:52:23,537 train 750 1.039712e-02 -0.018965
2019-11-13 21:52:30,322 train 800 1.040306e-02 0.002818
2019-11-13 21:52:37,100 train 850 1.040620e-02 0.010028
2019-11-13 21:52:39,135 training loss; R2: 1.040887e-02 0.015148
2019-11-13 21:52:39,489 valid 000 7.594392e+03 -595004.808341
2019-11-13 21:52:41,246 valid 050 7.594044e+03 -887070.980608
2019-11-13 21:52:42,817 validation loss; R2: 7.593821e+03 -904969.640062
2019-11-13 21:52:42,834 epoch 14 lr 1.000000e-03
2019-11-13 21:52:43,254 train 000 1.191667e-02 0.370911
2019-11-13 21:52:50,200 train 050 1.007686e-02 0.363588
2019-11-13 21:52:57,000 train 100 1.020005e-02 0.347846
2019-11-13 21:53:03,843 train 150 1.011597e-02 0.342980
2019-11-13 21:53:10,621 train 200 1.019130e-02 0.321185
2019-11-13 21:53:17,414 train 250 1.022225e-02 0.323851
2019-11-13 21:53:24,203 train 300 1.026496e-02 0.321262
2019-11-13 21:53:31,034 train 350 1.025459e-02 0.322684
2019-11-13 21:53:37,909 train 400 1.022713e-02 0.326046
2019-11-13 21:53:44,711 train 450 1.022610e-02 0.320824
2019-11-13 21:53:51,513 train 500 1.026147e-02 0.319347
2019-11-13 21:53:58,308 train 550 1.024734e-02 0.322762
2019-11-13 21:54:05,091 train 600 1.025283e-02 0.324252
2019-11-13 21:54:11,906 train 650 1.026986e-02 0.324748
2019-11-13 21:54:18,719 train 700 1.028655e-02 0.321497
2019-11-13 21:54:25,518 train 750 1.028782e-02 0.323494
2019-11-13 21:54:32,345 train 800 1.028320e-02 0.321033
2019-11-13 21:54:39,230 train 850 1.030231e-02 0.321008
2019-11-13 21:54:41,286 training loss; R2: 1.030479e-02 0.321746
2019-11-13 21:54:41,704 valid 000 1.588259e+04 -938651.893141
2019-11-13 21:54:43,485 valid 050 1.588476e+04 -1986023.669813
2019-11-13 21:54:45,075 validation loss; R2: 1.588484e+04 -1519105.163306
2019-11-13 21:54:45,092 epoch 15 lr 1.000000e-03
2019-11-13 21:54:45,527 train 000 1.074033e-02 0.370358
2019-11-13 21:54:52,571 train 050 1.039522e-02 0.287837
2019-11-13 21:54:59,478 train 100 1.033239e-02 0.317821
2019-11-13 21:55:06,359 train 150 1.020284e-02 0.327644
2019-11-13 21:55:13,298 train 200 1.024021e-02 0.334698
2019-11-13 21:55:20,255 train 250 1.027442e-02 0.327516
2019-11-13 21:55:27,111 train 300 1.024860e-02 0.326557
2019-11-13 21:55:33,971 train 350 1.021163e-02 0.329980
2019-11-13 21:55:40,813 train 400 1.017406e-02 0.331639
2019-11-13 21:55:47,661 train 450 1.022092e-02 0.223369
2019-11-13 21:55:54,509 train 500 1.021845e-02 0.230307
2019-11-13 21:56:01,413 train 550 1.019351e-02 0.239855
2019-11-13 21:56:08,252 train 600 1.018266e-02 0.244165
2019-11-13 21:56:15,083 train 650 1.016236e-02 0.252375
2019-11-13 21:56:21,916 train 700 1.018120e-02 0.255990
2019-11-13 21:56:28,761 train 750 1.017638e-02 0.262836
2019-11-13 21:56:35,679 train 800 1.018365e-02 0.265985
2019-11-13 21:56:42,515 train 850 1.018172e-02 0.269926
2019-11-13 21:56:44,562 training loss; R2: 1.019781e-02 0.271214
2019-11-13 21:56:44,932 valid 000 4.881465e+03 -375607.748383
2019-11-13 21:56:46,674 valid 050 4.882529e+03 -585048.040209
2019-11-13 21:56:48,257 validation loss; R2: 4.882400e+03 -566191.255563
2019-11-13 21:56:48,274 epoch 16 lr 1.000000e-03
2019-11-13 21:56:48,699 train 000 1.170057e-02 0.393957
2019-11-13 21:56:55,821 train 050 9.716026e-03 0.206407
2019-11-13 21:57:02,676 train 100 9.824772e-03 0.286710
2019-11-13 21:57:09,567 train 150 9.831112e-03 0.301244
2019-11-13 21:57:16,409 train 200 9.954900e-03 0.311809
2019-11-13 21:57:23,255 train 250 1.003569e-02 0.320473
2019-11-13 21:57:30,096 train 300 9.991082e-03 0.327512
2019-11-13 21:57:36,924 train 350 1.007791e-02 0.296965
2019-11-13 21:57:43,767 train 400 1.010568e-02 0.300093
2019-11-13 21:57:50,607 train 450 1.010510e-02 0.305415
2019-11-13 21:57:57,447 train 500 1.010237e-02 0.310325
2019-11-13 21:58:04,295 train 550 1.010955e-02 0.311107
2019-11-13 21:58:11,137 train 600 1.010268e-02 0.314896
2019-11-13 21:58:17,986 train 650 1.011059e-02 0.316354
2019-11-13 21:58:24,839 train 700 1.008802e-02 0.320423
2019-11-13 21:58:31,684 train 750 1.007124e-02 0.322322
2019-11-13 21:58:38,580 train 800 1.002586e-02 0.325888
2019-11-13 21:58:45,435 train 850 9.997541e-03 0.330133
2019-11-13 21:58:47,496 training loss; R2: 9.988845e-03 0.331477
2019-11-13 21:58:47,861 valid 000 6.219235e+03 -455625.345392
2019-11-13 21:58:49,602 valid 050 6.218181e+03 -537341.940226
2019-11-13 21:58:51,189 validation loss; R2: 6.218331e+03 -570709.434472
2019-11-13 21:58:51,206 epoch 17 lr 1.000000e-03
2019-11-13 21:58:51,647 train 000 1.067072e-02 0.399290
2019-11-13 21:58:58,762 train 050 1.016292e-02 0.346757
2019-11-13 21:59:05,698 train 100 1.006840e-02 0.343268
2019-11-13 21:59:12,661 train 150 1.009861e-02 0.295327
2019-11-13 21:59:19,599 train 200 1.011825e-02 0.307114
2019-11-13 21:59:26,544 train 250 1.011198e-02 0.318667
2019-11-13 21:59:33,427 train 300 1.003796e-02 0.263685
2019-11-13 21:59:40,295 train 350 1.002909e-02 0.278250
2019-11-13 21:59:47,160 train 400 1.000260e-02 0.289044
2019-11-13 21:59:54,027 train 450 9.974527e-03 0.296872
2019-11-13 22:00:00,881 train 500 9.953533e-03 0.300979
2019-11-13 22:00:07,732 train 550 9.927572e-03 0.309444
2019-11-13 22:00:14,581 train 600 9.894171e-03 0.313089
2019-11-13 22:00:21,492 train 650 9.873962e-03 0.316550
2019-11-13 22:00:28,343 train 700 9.894095e-03 0.320872
2019-11-13 22:00:35,201 train 750 9.895534e-03 0.322632
2019-11-13 22:00:42,052 train 800 9.906085e-03 0.322395
2019-11-13 22:00:48,910 train 850 9.947422e-03 0.321259
2019-11-13 22:00:50,961 training loss; R2: 9.958419e-03 0.320500
2019-11-13 22:00:51,333 valid 000 7.395664e+03 -398166.035606
2019-11-13 22:00:53,104 valid 050 7.393726e+03 -700405.587065
2019-11-13 22:00:54,683 validation loss; R2: 7.393831e+03 -701504.865605
2019-11-13 22:00:54,699 epoch 18 lr 1.000000e-03
2019-11-13 22:00:55,143 train 000 8.449504e-03 0.445757
2019-11-13 22:01:02,207 train 050 1.016518e-02 0.323397
2019-11-13 22:01:09,085 train 100 9.978979e-03 0.349970
2019-11-13 22:01:15,967 train 150 9.865143e-03 0.342571
2019-11-13 22:01:22,845 train 200 9.800308e-03 0.345341
2019-11-13 22:01:29,820 train 250 9.847722e-03 0.340855
2019-11-13 22:01:36,687 train 300 9.889681e-03 0.339000
2019-11-13 22:01:43,554 train 350 9.916456e-03 0.335835
2019-11-13 22:01:50,396 train 400 9.949829e-03 0.330219
2019-11-13 22:01:57,264 train 450 9.998100e-03 0.329309
2019-11-13 22:02:04,130 train 500 1.003635e-02 0.329327
2019-11-13 22:02:10,993 train 550 1.011961e-02 0.324462
2019-11-13 22:02:17,847 train 600 1.015371e-02 0.321081
2019-11-13 22:02:24,692 train 650 1.022704e-02 0.312974
2019-11-13 22:02:31,538 train 700 1.025243e-02 0.313495
2019-11-13 22:02:38,380 train 750 1.023693e-02 0.310372
2019-11-13 22:02:45,249 train 800 1.026818e-02 0.306108
2019-11-13 22:02:52,096 train 850 1.026020e-02 0.305397
2019-11-13 22:02:54,148 training loss; R2: 1.025809e-02 0.306149
2019-11-13 22:02:54,501 valid 000 9.629104e+03 -377198.624065
2019-11-13 22:02:56,241 valid 050 9.630452e+03 -476228.966844
2019-11-13 22:02:57,780 validation loss; R2: 9.630284e+03 -489989.727260
2019-11-13 22:02:57,796 epoch 19 lr 1.000000e-03
2019-11-13 22:02:58,197 train 000 1.152158e-02 0.378446
2019-11-13 22:03:05,157 train 050 1.009616e-02 0.353944
2019-11-13 22:03:12,034 train 100 1.038821e-02 0.349289
2019-11-13 22:03:18,922 train 150 1.028108e-02 0.331191
2019-11-13 22:03:25,780 train 200 1.021067e-02 0.321886
2019-11-13 22:03:32,630 train 250 1.027385e-02 0.315936
2019-11-13 22:03:39,477 train 300 1.021505e-02 0.320199
2019-11-13 22:03:46,347 train 350 1.020868e-02 0.298008
2019-11-13 22:03:53,196 train 400 1.020933e-02 0.257244
2019-11-13 22:04:00,127 train 450 1.020331e-02 0.268661
2019-11-13 22:04:06,981 train 500 1.018386e-02 0.275791
2019-11-13 22:04:13,826 train 550 1.019474e-02 0.279563
2019-11-13 22:04:20,669 train 600 1.020641e-02 0.286279
2019-11-13 22:04:27,512 train 650 1.029564e-02 0.286594
2019-11-13 22:04:34,372 train 700 1.033726e-02 0.288186
2019-11-13 22:04:41,215 train 750 1.039392e-02 0.287398
2019-11-13 22:04:48,049 train 800 1.045385e-02 0.286311
2019-11-13 22:04:54,891 train 850 1.052748e-02 0.280409
2019-11-13 22:04:56,941 training loss; R2: 1.054698e-02 0.279380
2019-11-13 22:04:57,296 valid 000 1.575243e+03 -246520.967776
2019-11-13 22:04:59,049 valid 050 1.574771e+03 -520236.022363
2019-11-13 22:05:00,629 validation loss; R2: 1.574801e+03 -477965.637014
