2019-11-13 09:41:24,576 gpu device = 1
2019-11-13 09:41:24,576 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name='galaxy-zoo-cleaned', gpu=1, grad_clip=5, gz_dtree=False, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-galaxy-zoo-EXP-20191113-094124', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-11-13 09:41:36,132 param size = 0.208469MB
2019-11-13 09:41:36,136 epoch 0 lr 1.000000e-03
2019-11-13 09:41:38,220 train 000 9.599858e-01 -266.639393
2019-11-13 09:41:43,593 train 050 6.910274e-02 -16.922538
2019-11-13 09:41:48,912 train 100 4.993174e-02 -9.168811
2019-11-13 09:41:54,212 train 150 4.232212e-02 -6.402183
2019-11-13 09:41:59,533 train 200 3.826993e-02 -4.903170
2019-11-13 09:42:04,914 train 250 3.570197e-02 -4.001312
2019-11-13 09:42:10,175 train 300 3.386090e-02 -3.424497
2019-11-13 09:42:15,407 train 350 3.245552e-02 -2.969553
2019-11-13 09:42:20,635 train 400 3.143546e-02 -2.630004
2019-11-13 09:42:25,847 train 450 3.054753e-02 -2.355473
2019-11-13 09:42:31,069 train 500 2.982897e-02 -2.138321
2019-11-13 09:42:36,292 train 550 2.917166e-02 -1.956082
2019-11-13 09:42:41,504 train 600 2.859140e-02 -1.811192
2019-11-13 09:42:46,718 train 650 2.808690e-02 -1.678677
2019-11-13 09:42:51,952 train 700 2.760616e-02 -1.571138
2019-11-13 09:42:57,279 train 750 2.719387e-02 -1.470751
2019-11-13 09:43:02,591 train 800 2.679535e-02 -1.377965
2019-11-13 09:43:07,913 train 850 2.642490e-02 -1.299774
2019-11-13 09:43:10,177 training loss; R2: 2.631492e-02 -1.277531
2019-11-13 09:43:10,492 valid 000 2.330883e-02 0.058370
2019-11-13 09:43:12,173 valid 050 2.223606e-02 -0.012151
2019-11-13 09:43:13,779 validation loss; R2: 2.211472e-02 -0.035101
2019-11-13 09:43:13,791 epoch 1 lr 1.000000e-03
2019-11-13 09:43:14,307 train 000 2.139685e-02 0.098944
2019-11-13 09:43:19,287 train 050 2.021387e-02 -0.083731
2019-11-13 09:43:24,250 train 100 1.987895e-02 -0.051865
2019-11-13 09:43:29,213 train 150 1.986893e-02 -0.030797
2019-11-13 09:43:34,168 train 200 1.984018e-02 -0.027508
2019-11-13 09:43:39,123 train 250 1.963719e-02 -0.015752
2019-11-13 09:43:44,088 train 300 1.962828e-02 -0.010051
2019-11-13 09:43:49,066 train 350 1.950875e-02 -0.004654
2019-11-13 09:43:54,026 train 400 1.947478e-02 -0.044951
2019-11-13 09:43:58,992 train 450 1.940149e-02 -0.035653
2019-11-13 09:44:03,958 train 500 1.933291e-02 -0.032581
2019-11-13 09:44:08,916 train 550 1.924187e-02 -0.033255
2019-11-13 09:44:13,877 train 600 1.919166e-02 -0.023528
2019-11-13 09:44:18,848 train 650 1.918695e-02 -0.018142
2019-11-13 09:44:23,831 train 700 1.912286e-02 -0.016157
2019-11-13 09:44:28,820 train 750 1.905540e-02 -0.009440
2019-11-13 09:44:33,817 train 800 1.897892e-02 -0.010761
2019-11-13 09:44:39,020 train 850 1.887903e-02 -0.005492
2019-11-13 09:44:40,585 training loss; R2: 1.886510e-02 -0.003869
2019-11-13 09:44:40,915 valid 000 1.485949e-02 0.230356
2019-11-13 09:44:42,625 valid 050 1.618411e-02 0.148287
2019-11-13 09:44:44,183 validation loss; R2: 1.586482e-02 0.162552
2019-11-13 09:44:44,195 epoch 2 lr 1.000000e-03
2019-11-13 09:44:44,561 train 000 1.460289e-02 0.191444
2019-11-13 09:44:49,844 train 050 1.725221e-02 0.091142
2019-11-13 09:44:55,155 train 100 1.747024e-02 -0.040157
2019-11-13 09:45:00,441 train 150 1.739878e-02 -0.018189
2019-11-13 09:45:05,653 train 200 1.735571e-02 0.012228
2019-11-13 09:45:10,866 train 250 1.722670e-02 0.025765
2019-11-13 09:45:16,077 train 300 1.712915e-02 0.035606
2019-11-13 09:45:21,299 train 350 1.702571e-02 0.046546
2019-11-13 09:45:26,523 train 400 1.701226e-02 0.053128
2019-11-13 09:45:31,732 train 450 1.692816e-02 0.064893
2019-11-13 09:45:36,947 train 500 1.688273e-02 0.069647
2019-11-13 09:45:42,159 train 550 1.679708e-02 0.066259
2019-11-13 09:45:47,372 train 600 1.673989e-02 0.072074
2019-11-13 09:45:52,581 train 650 1.669049e-02 0.069409
2019-11-13 09:45:57,815 train 700 1.661667e-02 0.076849
2019-11-13 09:46:03,057 train 750 1.656317e-02 0.079587
2019-11-13 09:46:08,294 train 800 1.651735e-02 0.082540
2019-11-13 09:46:13,524 train 850 1.648647e-02 0.084131
2019-11-13 09:46:15,089 training loss; R2: 1.648038e-02 0.085854
2019-11-13 09:46:15,421 valid 000 1.449052e-02 0.206168
2019-11-13 09:46:17,130 valid 050 1.477304e-02 0.209859
2019-11-13 09:46:18,676 validation loss; R2: 1.475095e-02 0.219324
2019-11-13 09:46:18,688 epoch 3 lr 1.000000e-03
2019-11-13 09:46:19,035 train 000 1.716308e-02 0.264722
2019-11-13 09:46:24,328 train 050 1.498424e-02 0.075042
2019-11-13 09:46:29,721 train 100 1.513848e-02 0.119616
2019-11-13 09:46:35,056 train 150 1.520758e-02 0.131350
2019-11-13 09:46:40,336 train 200 1.527879e-02 0.124352
2019-11-13 09:46:45,605 train 250 1.525006e-02 -0.335877
2019-11-13 09:46:50,834 train 300 1.521495e-02 -0.249574
2019-11-13 09:46:56,052 train 350 1.520331e-02 -0.194297
2019-11-13 09:47:01,270 train 400 1.513423e-02 -0.153008
2019-11-13 09:47:06,489 train 450 1.507607e-02 -0.115227
2019-11-13 09:47:11,712 train 500 1.507173e-02 -0.090664
2019-11-13 09:47:16,929 train 550 1.504032e-02 -0.064840
2019-11-13 09:47:22,149 train 600 1.503493e-02 -0.044938
2019-11-13 09:47:27,389 train 650 1.501071e-02 -0.029513
2019-11-13 09:47:32,628 train 700 1.500734e-02 -0.017661
2019-11-13 09:47:37,857 train 750 1.498455e-02 -0.003077
2019-11-13 09:47:43,079 train 800 1.491325e-02 0.010435
2019-11-13 09:47:48,312 train 850 1.489842e-02 0.022087
2019-11-13 09:47:49,872 training loss; R2: 1.488992e-02 0.025544
2019-11-13 09:47:50,198 valid 000 1.388218e-02 0.320283
2019-11-13 09:47:51,922 valid 050 1.322609e-02 0.263559
2019-11-13 09:47:53,457 validation loss; R2: 1.317880e-02 0.257164
2019-11-13 09:47:53,469 epoch 4 lr 1.000000e-03
2019-11-13 09:47:53,803 train 000 1.297733e-02 -0.008201
2019-11-13 09:47:58,913 train 050 1.394291e-02 0.179391
2019-11-13 09:48:04,128 train 100 1.410253e-02 0.165261
2019-11-13 09:48:09,335 train 150 1.413488e-02 -4.413194
2019-11-13 09:48:14,545 train 200 1.400371e-02 -3.260079
2019-11-13 09:48:19,771 train 250 1.403079e-02 -2.585028
2019-11-13 09:48:24,986 train 300 1.399539e-02 -2.140664
2019-11-13 09:48:30,210 train 350 1.399996e-02 -1.810141
2019-11-13 09:48:35,424 train 400 1.399948e-02 -1.555666
2019-11-13 09:48:40,629 train 450 1.396827e-02 -1.360440
2019-11-13 09:48:45,849 train 500 1.393583e-02 -1.207700
2019-11-13 09:48:51,062 train 550 1.394251e-02 -1.078983
2019-11-13 09:48:56,275 train 600 1.393656e-02 -0.970629
2019-11-13 09:49:01,488 train 650 1.394454e-02 -0.877185
2019-11-13 09:49:06,711 train 700 1.393566e-02 -0.797629
2019-11-13 09:49:11,923 train 750 1.391894e-02 -0.731131
2019-11-13 09:49:17,130 train 800 1.390940e-02 -0.671008
2019-11-13 09:49:22,347 train 850 1.389054e-02 -0.621083
2019-11-13 09:49:23,907 training loss; R2: 1.389366e-02 -0.606156
2019-11-13 09:49:24,223 valid 000 1.229246e-02 0.295822
2019-11-13 09:49:25,948 valid 050 1.264787e-02 0.267395
2019-11-13 09:49:27,488 validation loss; R2: 1.248914e-02 0.227064
2019-11-13 09:49:27,499 epoch 5 lr 1.000000e-03
2019-11-13 09:49:27,861 train 000 1.451411e-02 0.338220
2019-11-13 09:49:33,048 train 050 1.300090e-02 0.187096
2019-11-13 09:49:38,305 train 100 1.332805e-02 0.210818
2019-11-13 09:49:43,508 train 150 1.322827e-02 0.191675
2019-11-13 09:49:48,742 train 200 1.318089e-02 0.207286
2019-11-13 09:49:53,981 train 250 1.318818e-02 0.204057
2019-11-13 09:49:59,185 train 300 1.318574e-02 0.208816
2019-11-13 09:50:04,401 train 350 1.324697e-02 0.207367
2019-11-13 09:50:09,609 train 400 1.324779e-02 0.152745
2019-11-13 09:50:14,816 train 450 1.328274e-02 0.159087
2019-11-13 09:50:20,026 train 500 1.327596e-02 0.168700
2019-11-13 09:50:25,239 train 550 1.326353e-02 0.176096
2019-11-13 09:50:30,443 train 600 1.327251e-02 0.182118
2019-11-13 09:50:35,662 train 650 1.326368e-02 0.185658
2019-11-13 09:50:40,868 train 700 1.326237e-02 0.190121
2019-11-13 09:50:46,076 train 750 1.322110e-02 0.186426
2019-11-13 09:50:51,279 train 800 1.320439e-02 0.181714
2019-11-13 09:50:56,485 train 850 1.321310e-02 0.182488
2019-11-13 09:50:58,045 training loss; R2: 1.321141e-02 0.182363
2019-11-13 09:50:58,370 valid 000 9.650262e-03 0.337858
2019-11-13 09:51:00,104 valid 050 1.172644e-02 0.290852
2019-11-13 09:51:01,658 validation loss; R2: 1.165865e-02 0.295015
2019-11-13 09:51:01,669 epoch 6 lr 1.000000e-03
2019-11-13 09:51:02,035 train 000 1.163159e-02 -0.883089
2019-11-13 09:51:07,351 train 050 1.300177e-02 0.244454
2019-11-13 09:51:12,747 train 100 1.293282e-02 0.254394
2019-11-13 09:51:18,243 train 150 1.286266e-02 0.248472
2019-11-13 09:51:23,610 train 200 1.288407e-02 0.242497
2019-11-13 09:51:28,971 train 250 1.291519e-02 0.246500
2019-11-13 09:51:34,302 train 300 1.289953e-02 0.251210
2019-11-13 09:51:39,648 train 350 1.286412e-02 0.258298
2019-11-13 09:51:45,049 train 400 1.283053e-02 0.259220
2019-11-13 09:51:50,431 train 450 1.284803e-02 0.247152
2019-11-13 09:51:55,814 train 500 1.283389e-02 0.247711
2019-11-13 09:52:01,156 train 550 1.282221e-02 0.250301
2019-11-13 09:52:06,504 train 600 1.280850e-02 0.249984
2019-11-13 09:52:11,908 train 650 1.277487e-02 0.250327
2019-11-13 09:52:17,280 train 700 1.274283e-02 0.252021
2019-11-13 09:52:22,521 train 750 1.273267e-02 0.249772
2019-11-13 09:52:27,770 train 800 1.273909e-02 0.247918
2019-11-13 09:52:33,036 train 850 1.273865e-02 0.245207
2019-11-13 09:52:34,597 training loss; R2: 1.273231e-02 0.244969
2019-11-13 09:52:34,925 valid 000 2.144046e-02 0.002436
2019-11-13 09:52:36,671 valid 050 2.219568e-02 -0.114907
2019-11-13 09:52:38,222 validation loss; R2: 2.211987e-02 -0.086628
2019-11-13 09:52:38,234 epoch 7 lr 1.000000e-03
2019-11-13 09:52:38,615 train 000 1.499163e-02 0.294282
2019-11-13 09:52:44,144 train 050 1.210453e-02 0.218180
2019-11-13 09:52:49,531 train 100 1.228017e-02 0.225477
2019-11-13 09:52:54,981 train 150 1.243223e-02 0.214288
2019-11-13 09:53:00,577 train 200 1.239395e-02 0.213602
2019-11-13 09:53:06,173 train 250 1.240358e-02 0.208776
2019-11-13 09:53:11,735 train 300 1.242151e-02 0.216064
2019-11-13 09:53:17,119 train 350 1.238901e-02 0.228540
2019-11-13 09:53:22,452 train 400 1.237599e-02 0.230551
2019-11-13 09:53:27,774 train 450 1.237958e-02 0.235907
2019-11-13 09:53:33,195 train 500 1.237930e-02 -1.768197
2019-11-13 09:53:38,773 train 550 1.238007e-02 -1.585885
2019-11-13 09:53:44,350 train 600 1.236784e-02 -1.431387
2019-11-13 09:53:49,911 train 650 1.235397e-02 -1.301500
2019-11-13 09:53:55,496 train 700 1.233797e-02 -1.191391
2019-11-13 09:54:01,057 train 750 1.235246e-02 -1.094243
2019-11-13 09:54:06,620 train 800 1.234190e-02 -1.009539
2019-11-13 09:54:12,173 train 850 1.232675e-02 -0.933546
2019-11-13 09:54:13,825 training loss; R2: 1.231663e-02 -0.912843
2019-11-13 09:54:14,143 valid 000 2.897941e-02 -0.515495
2019-11-13 09:54:15,807 valid 050 3.275993e-02 -0.547172
2019-11-13 09:54:17,310 validation loss; R2: 3.283367e-02 -0.594848
2019-11-13 09:54:17,327 epoch 8 lr 1.000000e-03
2019-11-13 09:54:17,657 train 000 1.248878e-02 0.359566
2019-11-13 09:54:22,948 train 050 1.236963e-02 0.304103
2019-11-13 09:54:28,245 train 100 1.226701e-02 0.307371
2019-11-13 09:54:33,637 train 150 1.216545e-02 0.300638
2019-11-13 09:54:39,038 train 200 1.203410e-02 0.292241
2019-11-13 09:54:44,303 train 250 1.196565e-02 0.291785
2019-11-13 09:54:49,571 train 300 1.197051e-02 0.290077
2019-11-13 09:54:54,863 train 350 1.193013e-02 0.295307
2019-11-13 09:55:00,068 train 400 1.192389e-02 0.290856
2019-11-13 09:55:05,175 train 450 1.190654e-02 0.286726
2019-11-13 09:55:10,359 train 500 1.191627e-02 0.284385
2019-11-13 09:55:15,613 train 550 1.194027e-02 0.278665
2019-11-13 09:55:20,867 train 600 1.193553e-02 0.277976
2019-11-13 09:55:26,121 train 650 1.194720e-02 0.278400
2019-11-13 09:55:31,401 train 700 1.194556e-02 0.280167
2019-11-13 09:55:36,577 train 750 1.193763e-02 0.279782
2019-11-13 09:55:41,675 train 800 1.192552e-02 0.170814
2019-11-13 09:55:46,781 train 850 1.192499e-02 0.177918
2019-11-13 09:55:48,330 training loss; R2: 1.192512e-02 0.180003
2019-11-13 09:55:48,662 valid 000 1.343883e-02 0.235477
2019-11-13 09:55:50,320 valid 050 1.524159e-02 0.226131
2019-11-13 09:55:51,867 validation loss; R2: 1.517356e-02 0.209308
2019-11-13 09:55:51,882 epoch 9 lr 1.000000e-03
2019-11-13 09:55:52,257 train 000 1.144484e-02 -0.020372
2019-11-13 09:55:57,611 train 050 1.174918e-02 0.283721
2019-11-13 09:56:02,987 train 100 1.167411e-02 0.273841
2019-11-13 09:56:08,287 train 150 1.164859e-02 0.288417
2019-11-13 09:56:13,546 train 200 1.154693e-02 0.295127
2019-11-13 09:56:18,744 train 250 1.159867e-02 0.296411
2019-11-13 09:56:23,962 train 300 1.163429e-02 0.297171
2019-11-13 09:56:29,220 train 350 1.166776e-02 0.289874
2019-11-13 09:56:34,508 train 400 1.165923e-02 0.290872
2019-11-13 09:56:39,768 train 450 1.170151e-02 0.289923
2019-11-13 09:56:45,228 train 500 1.170990e-02 0.286875
2019-11-13 09:56:50,670 train 550 1.170310e-02 0.273048
2019-11-13 09:56:56,029 train 600 1.168438e-02 0.263467
2019-11-13 09:57:01,412 train 650 1.166165e-02 0.267867
2019-11-13 09:57:06,649 train 700 1.165778e-02 0.272008
2019-11-13 09:57:11,893 train 750 1.166821e-02 0.273280
2019-11-13 09:57:17,166 train 800 1.165805e-02 0.274391
2019-11-13 09:57:22,397 train 850 1.166135e-02 0.262910
2019-11-13 09:57:23,962 training loss; R2: 1.166332e-02 0.262400
2019-11-13 09:57:24,297 valid 000 1.176193e-02 0.349466
2019-11-13 09:57:25,969 valid 050 1.129589e-02 0.349933
2019-11-13 09:57:27,494 validation loss; R2: 1.118746e-02 0.356031
2019-11-13 09:57:27,506 epoch 10 lr 1.000000e-03
2019-11-13 09:57:27,873 train 000 1.198438e-02 0.354344
2019-11-13 09:57:32,945 train 050 1.138432e-02 0.223957
2019-11-13 09:57:37,943 train 100 1.148254e-02 0.260862
2019-11-13 09:57:42,929 train 150 1.152267e-02 0.278191
2019-11-13 09:57:47,937 train 200 1.151455e-02 0.267432
2019-11-13 09:57:52,926 train 250 1.152776e-02 0.269816
2019-11-13 09:57:58,031 train 300 1.157802e-02 0.275678
2019-11-13 09:58:03,273 train 350 1.155842e-02 0.284569
2019-11-13 09:58:08,508 train 400 1.152532e-02 0.286400
2019-11-13 09:58:13,762 train 450 1.153126e-02 0.288403
2019-11-13 09:58:18,987 train 500 1.150467e-02 0.290655
2019-11-13 09:58:24,241 train 550 1.151158e-02 0.291888
2019-11-13 09:58:29,463 train 600 1.149942e-02 0.291420
2019-11-13 09:58:34,682 train 650 1.148337e-02 0.282374
2019-11-13 09:58:39,910 train 700 1.149341e-02 0.282919
2019-11-13 09:58:45,146 train 750 1.147602e-02 0.255658
2019-11-13 09:58:50,370 train 800 1.146346e-02 0.255986
2019-11-13 09:58:55,591 train 850 1.145985e-02 0.255604
2019-11-13 09:58:57,164 training loss; R2: 1.145756e-02 0.256851
2019-11-13 09:58:57,502 valid 000 1.182781e-02 0.371907
2019-11-13 09:58:59,198 valid 050 1.048010e-02 0.356238
2019-11-13 09:59:00,745 validation loss; R2: 1.027860e-02 0.325978
2019-11-13 09:59:00,765 epoch 11 lr 1.000000e-03
2019-11-13 09:59:01,123 train 000 1.229932e-02 0.306380
2019-11-13 09:59:06,416 train 050 1.139145e-02 0.313848
2019-11-13 09:59:11,648 train 100 1.128971e-02 0.299624
2019-11-13 09:59:16,884 train 150 1.135015e-02 0.279063
2019-11-13 09:59:22,131 train 200 1.127781e-02 0.284586
2019-11-13 09:59:27,356 train 250 1.130487e-02 0.277492
2019-11-13 09:59:32,582 train 300 1.132128e-02 0.279246
2019-11-13 09:59:37,827 train 350 1.134520e-02 0.266527
2019-11-13 09:59:43,054 train 400 1.133718e-02 0.272604
2019-11-13 09:59:48,281 train 450 1.136344e-02 0.274037
2019-11-13 09:59:53,527 train 500 1.135494e-02 0.278123
2019-11-13 09:59:58,757 train 550 1.135762e-02 0.280741
2019-11-13 10:00:03,982 train 600 1.133877e-02 0.276257
2019-11-13 10:00:09,215 train 650 1.130975e-02 0.279816
2019-11-13 10:00:14,436 train 700 1.130242e-02 0.281333
2019-11-13 10:00:19,673 train 750 1.130323e-02 0.280878
2019-11-13 10:00:24,904 train 800 1.128866e-02 0.281888
2019-11-13 10:00:30,154 train 850 1.128068e-02 0.282956
2019-11-13 10:00:31,722 training loss; R2: 1.128255e-02 0.283723
2019-11-13 10:00:32,054 valid 000 1.067850e-02 0.348686
2019-11-13 10:00:33,699 valid 050 9.806311e-03 0.378241
2019-11-13 10:00:35,271 validation loss; R2: 9.577682e-03 0.361734
2019-11-13 10:00:35,282 epoch 12 lr 1.000000e-03
2019-11-13 10:00:35,691 train 000 1.260345e-02 0.209077
2019-11-13 10:00:41,024 train 050 1.143363e-02 0.309532
2019-11-13 10:00:46,357 train 100 1.116791e-02 0.319356
2019-11-13 10:00:51,671 train 150 1.124769e-02 0.320092
2019-11-13 10:00:56,982 train 200 1.115327e-02 0.301756
2019-11-13 10:01:02,284 train 250 1.117605e-02 0.270737
2019-11-13 10:01:07,580 train 300 1.111931e-02 0.279378
2019-11-13 10:01:12,834 train 350 1.112773e-02 0.283813
2019-11-13 10:01:18,081 train 400 1.114043e-02 0.288221
2019-11-13 10:01:23,318 train 450 1.112493e-02 0.292181
2019-11-13 10:01:28,550 train 500 1.109669e-02 0.291542
2019-11-13 10:01:33,770 train 550 1.109436e-02 0.288778
2019-11-13 10:01:39,002 train 600 1.108948e-02 0.272601
2019-11-13 10:01:44,229 train 650 1.109262e-02 0.276994
2019-11-13 10:01:49,451 train 700 1.111688e-02 0.280320
2019-11-13 10:01:54,689 train 750 1.111263e-02 0.281598
2019-11-13 10:01:59,921 train 800 1.113047e-02 0.279733
2019-11-13 10:02:05,149 train 850 1.111126e-02 0.283381
2019-11-13 10:02:06,710 training loss; R2: 1.110840e-02 0.283722
2019-11-13 10:02:07,040 valid 000 1.135125e-02 0.442761
2019-11-13 10:02:08,761 valid 050 1.011448e-02 0.289252
2019-11-13 10:02:10,318 validation loss; R2: 1.005185e-02 0.305569
2019-11-13 10:02:10,329 epoch 13 lr 1.000000e-03
2019-11-13 10:02:10,696 train 000 8.587048e-03 0.413145
2019-11-13 10:02:15,990 train 050 1.109519e-02 0.328134
2019-11-13 10:02:21,317 train 100 1.105001e-02 0.324056
2019-11-13 10:02:26,634 train 150 1.099531e-02 0.323274
2019-11-13 10:02:31,939 train 200 1.094956e-02 0.325993
2019-11-13 10:02:37,250 train 250 1.090056e-02 0.329520
2019-11-13 10:02:42,492 train 300 1.092608e-02 0.320070
2019-11-13 10:02:47,723 train 350 1.098152e-02 0.318964
2019-11-13 10:02:52,969 train 400 1.097016e-02 0.315836
2019-11-13 10:02:58,195 train 450 1.099931e-02 0.316063
2019-11-13 10:03:03,418 train 500 1.094821e-02 0.317214
2019-11-13 10:03:08,646 train 550 1.096371e-02 0.317622
2019-11-13 10:03:13,865 train 600 1.094752e-02 0.318958
2019-11-13 10:03:19,089 train 650 1.093461e-02 0.319726
2019-11-13 10:03:24,313 train 700 1.094587e-02 0.121647
2019-11-13 10:03:29,536 train 750 1.094716e-02 0.134983
2019-11-13 10:03:34,757 train 800 1.094429e-02 0.146533
2019-11-13 10:03:39,982 train 850 1.094283e-02 0.156568
2019-11-13 10:03:41,541 training loss; R2: 1.094335e-02 0.160150
2019-11-13 10:03:41,862 valid 000 1.048364e-02 0.314662
2019-11-13 10:03:43,560 valid 050 1.000246e-02 0.398222
2019-11-13 10:03:45,114 validation loss; R2: 9.829852e-03 0.383817
2019-11-13 10:03:45,131 epoch 14 lr 1.000000e-03
2019-11-13 10:03:45,462 train 000 9.476976e-03 0.130363
2019-11-13 10:03:50,493 train 050 1.078099e-02 0.323308
2019-11-13 10:03:55,529 train 100 1.084954e-02 0.318528
2019-11-13 10:04:00,760 train 150 1.087716e-02 0.278772
2019-11-13 10:04:06,044 train 200 1.081055e-02 0.289244
2019-11-13 10:04:11,315 train 250 1.084947e-02 0.290966
2019-11-13 10:04:16,586 train 300 1.083331e-02 -1.725959
2019-11-13 10:04:21,853 train 350 1.081209e-02 -1.448786
2019-11-13 10:04:27,126 train 400 1.078513e-02 -1.226903
2019-11-13 10:04:32,388 train 450 1.082149e-02 -1.053747
2019-11-13 10:04:37,657 train 500 1.083647e-02 -0.915479
2019-11-13 10:04:42,926 train 550 1.082105e-02 -0.802619
2019-11-13 10:04:48,237 train 600 1.082566e-02 -0.708680
2019-11-13 10:04:53,579 train 650 1.083375e-02 -0.638342
2019-11-13 10:04:58,898 train 700 1.086844e-02 -0.572063
2019-11-13 10:05:04,241 train 750 1.087525e-02 -0.512039
2019-11-13 10:05:09,574 train 800 1.086464e-02 -0.458213
2019-11-13 10:05:14,894 train 850 1.086865e-02 -0.417664
2019-11-13 10:05:16,500 training loss; R2: 1.086128e-02 -0.424289
2019-11-13 10:05:16,815 valid 000 1.038301e-02 0.405367
2019-11-13 10:05:18,536 valid 050 1.043802e-02 0.383243
2019-11-13 10:05:20,082 validation loss; R2: 1.042692e-02 0.385006
2019-11-13 10:05:20,093 epoch 15 lr 1.000000e-03
2019-11-13 10:05:20,445 train 000 1.159044e-02 0.405206
2019-11-13 10:05:25,665 train 050 1.081016e-02 0.324304
2019-11-13 10:05:30,999 train 100 1.071624e-02 0.331336
2019-11-13 10:05:36,415 train 150 1.057616e-02 0.339476
2019-11-13 10:05:41,817 train 200 1.063599e-02 0.335194
2019-11-13 10:05:47,145 train 250 1.064925e-02 0.329596
2019-11-13 10:05:52,454 train 300 1.066199e-02 0.323726
2019-11-13 10:05:57,762 train 350 1.065070e-02 0.317512
2019-11-13 10:06:03,076 train 400 1.064526e-02 0.319176
2019-11-13 10:06:08,385 train 450 1.070337e-02 0.322562
2019-11-13 10:06:13,700 train 500 1.074558e-02 0.321131
2019-11-13 10:06:19,007 train 550 1.078990e-02 0.319406
2019-11-13 10:06:24,321 train 600 1.079587e-02 0.319094
2019-11-13 10:06:29,656 train 650 1.079269e-02 0.319529
2019-11-13 10:06:35,035 train 700 1.079634e-02 0.317954
2019-11-13 10:06:40,365 train 750 1.081531e-02 0.318748
2019-11-13 10:06:45,695 train 800 1.082803e-02 0.319179
2019-11-13 10:06:51,009 train 850 1.083106e-02 0.319959
2019-11-13 10:06:52,587 training loss; R2: 1.083866e-02 0.319584
2019-11-13 10:06:52,909 valid 000 9.377399e-03 0.433550
2019-11-13 10:06:54,629 valid 050 9.987045e-03 0.376782
2019-11-13 10:06:56,169 validation loss; R2: 1.004109e-02 0.242108
2019-11-13 10:06:56,184 epoch 16 lr 1.000000e-03
2019-11-13 10:06:56,589 train 000 9.857577e-03 0.360511
2019-11-13 10:07:01,924 train 050 1.084481e-02 0.312785
2019-11-13 10:07:07,383 train 100 1.086484e-02 0.323077
2019-11-13 10:07:12,886 train 150 1.086090e-02 0.312884
2019-11-13 10:07:18,506 train 200 1.079817e-02 0.320628
2019-11-13 10:07:24,116 train 250 1.081485e-02 0.326559
2019-11-13 10:07:29,719 train 300 1.079235e-02 0.325519
2019-11-13 10:07:35,326 train 350 1.078719e-02 0.316936
2019-11-13 10:07:40,921 train 400 1.078932e-02 0.322601
2019-11-13 10:07:46,515 train 450 1.081441e-02 0.322938
2019-11-13 10:07:52,126 train 500 1.079863e-02 0.324607
2019-11-13 10:07:57,505 train 550 1.078651e-02 0.322658
2019-11-13 10:08:02,836 train 600 1.076001e-02 0.324956
2019-11-13 10:08:08,158 train 650 1.077041e-02 0.323056
2019-11-13 10:08:13,486 train 700 1.076503e-02 0.315621
2019-11-13 10:08:18,809 train 750 1.074244e-02 0.313894
2019-11-13 10:08:24,145 train 800 1.074424e-02 0.314291
2019-11-13 10:08:29,469 train 850 1.072890e-02 0.315349
2019-11-13 10:08:31,057 training loss; R2: 1.072889e-02 0.316166
2019-11-13 10:08:31,379 valid 000 9.784590e-03 0.371978
2019-11-13 10:08:33,034 valid 050 9.514873e-03 0.350012
2019-11-13 10:08:34,534 validation loss; R2: 9.565600e-03 0.262729
2019-11-13 10:08:34,545 epoch 17 lr 1.000000e-03
2019-11-13 10:08:34,889 train 000 1.202691e-02 0.133660
2019-11-13 10:08:40,176 train 050 1.063442e-02 0.358502
2019-11-13 10:08:45,437 train 100 1.058115e-02 0.351811
2019-11-13 10:08:50,657 train 150 1.062708e-02 0.346839
2019-11-13 10:08:55,940 train 200 1.064605e-02 0.341821
2019-11-13 10:09:01,226 train 250 1.062603e-02 0.341696
2019-11-13 10:09:06,470 train 300 1.064419e-02 0.338652
2019-11-13 10:09:11,804 train 350 1.070405e-02 0.341463
2019-11-13 10:09:17,143 train 400 1.074573e-02 0.342435
2019-11-13 10:09:22,502 train 450 1.073458e-02 0.339751
2019-11-13 10:09:27,801 train 500 1.075166e-02 0.338611
2019-11-13 10:09:33,036 train 550 1.075653e-02 0.334407
2019-11-13 10:09:38,274 train 600 1.076814e-02 0.334114
2019-11-13 10:09:43,520 train 650 1.074459e-02 0.325177
2019-11-13 10:09:48,763 train 700 1.072187e-02 0.326932
2019-11-13 10:09:53,993 train 750 1.071550e-02 0.280511
2019-11-13 10:09:59,234 train 800 1.069834e-02 0.284555
2019-11-13 10:10:04,459 train 850 1.069046e-02 0.287089
2019-11-13 10:10:06,023 training loss; R2: 1.069651e-02 0.287296
2019-11-13 10:10:06,372 valid 000 1.112591e-02 0.380877
2019-11-13 10:10:08,021 valid 050 1.071344e-02 0.246718
2019-11-13 10:10:09,558 validation loss; R2: 1.069393e-02 0.295560
2019-11-13 10:10:09,579 epoch 18 lr 1.000000e-03
2019-11-13 10:10:09,990 train 000 1.025449e-02 0.396471
2019-11-13 10:10:15,452 train 050 1.053783e-02 0.340713
2019-11-13 10:10:20,792 train 100 1.061157e-02 0.328581
2019-11-13 10:10:26,126 train 150 1.060995e-02 0.327262
2019-11-13 10:10:31,436 train 200 1.061783e-02 0.321950
2019-11-13 10:10:36,742 train 250 1.060195e-02 0.318399
2019-11-13 10:10:42,035 train 300 1.056804e-02 0.319097
2019-11-13 10:10:47,287 train 350 1.056565e-02 0.321982
2019-11-13 10:10:52,518 train 400 1.053968e-02 0.314367
2019-11-13 10:10:57,746 train 450 1.056034e-02 0.315941
2019-11-13 10:11:02,974 train 500 1.054362e-02 0.316238
2019-11-13 10:11:08,221 train 550 1.054826e-02 0.317299
2019-11-13 10:11:13,451 train 600 1.053114e-02 0.312200
2019-11-13 10:11:18,691 train 650 1.055521e-02 0.310452
2019-11-13 10:11:23,906 train 700 1.055458e-02 0.312708
2019-11-13 10:11:29,132 train 750 1.057383e-02 0.299514
2019-11-13 10:11:34,371 train 800 1.055714e-02 0.298194
2019-11-13 10:11:39,602 train 850 1.054921e-02 0.300644
2019-11-13 10:11:41,171 training loss; R2: 1.054308e-02 0.301796
2019-11-13 10:11:41,518 valid 000 1.121123e-02 0.412466
2019-11-13 10:11:43,255 valid 050 1.120889e-02 -0.111807
2019-11-13 10:11:44,811 validation loss; R2: 1.109409e-02 0.088343
2019-11-13 10:11:44,825 epoch 19 lr 1.000000e-03
2019-11-13 10:11:45,226 train 000 1.181261e-02 0.404281
2019-11-13 10:11:50,564 train 050 1.053373e-02 0.349338
2019-11-13 10:11:56,072 train 100 1.052287e-02 0.340615
2019-11-13 10:12:01,488 train 150 1.043073e-02 0.334155
2019-11-13 10:12:06,848 train 200 1.050240e-02 0.330891
2019-11-13 10:12:12,172 train 250 1.056926e-02 0.323292
2019-11-13 10:12:17,508 train 300 1.052227e-02 0.312041
2019-11-13 10:12:22,827 train 350 1.050909e-02 0.312641
2019-11-13 10:12:28,155 train 400 1.050150e-02 0.312891
2019-11-13 10:12:33,468 train 450 1.051103e-02 0.315553
2019-11-13 10:12:38,769 train 500 1.054475e-02 0.316531
2019-11-13 10:12:44,079 train 550 1.059448e-02 0.314992
2019-11-13 10:12:49,434 train 600 1.061444e-02 0.315382
2019-11-13 10:12:54,747 train 650 1.063950e-02 0.313693
2019-11-13 10:13:00,075 train 700 1.069612e-02 0.308832
2019-11-13 10:13:05,377 train 750 1.070938e-02 0.308079
2019-11-13 10:13:10,696 train 800 1.071940e-02 0.304761
2019-11-13 10:13:15,993 train 850 1.073876e-02 0.305795
2019-11-13 10:13:17,582 training loss; R2: 1.073277e-02 0.305124
2019-11-13 10:13:17,910 valid 000 1.036009e-02 0.442435
2019-11-13 10:13:19,626 valid 050 1.087807e-02 0.336701
2019-11-13 10:13:21,159 validation loss; R2: 1.087845e-02 0.345000
