2019-11-12 13:39:54,162 gpu device = 1
2019-11-12 13:39:54,162 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name='galaxy-zoo-cleaned', gpu=1, grad_clip=5, gz_dtree=False, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-galaxy-zoo-EXP-20191112-133953', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-11-12 13:40:05,704 param size = 0.238613MB
2019-11-12 13:40:05,708 epoch 0 lr 1.000000e-03
2019-11-12 13:40:07,857 train 000 8.401857e-01 -780.758210
2019-11-12 13:40:14,200 train 050 6.108489e-02 -25.086629
2019-11-12 13:40:20,515 train 100 4.476963e-02 -13.110977
2019-11-12 13:40:26,535 train 150 3.880217e-02 -9.039514
2019-11-12 13:40:32,606 train 200 3.549090e-02 -6.962571
2019-11-12 13:40:38,655 train 250 3.326748e-02 -22.477894
2019-11-12 13:40:44,701 train 300 3.179132e-02 -18.788689
2019-11-12 13:40:50,737 train 350 3.063794e-02 -16.155232
2019-11-12 13:40:56,802 train 400 2.971428e-02 -14.184995
2019-11-12 13:41:02,848 train 450 2.892829e-02 -12.636223
2019-11-12 13:41:08,879 train 500 2.824659e-02 -11.394595
2019-11-12 13:41:14,908 train 550 2.764819e-02 -10.374073
2019-11-12 13:41:20,945 train 600 2.710692e-02 -9.532301
2019-11-12 13:41:26,982 train 650 2.665820e-02 -8.807833
2019-11-12 13:41:33,011 train 700 2.622788e-02 -8.182376
2019-11-12 13:41:39,035 train 750 2.582126e-02 -7.636643
2019-11-12 13:41:45,066 train 800 2.548502e-02 -7.165432
2019-11-12 13:41:51,104 train 850 2.516229e-02 -6.746398
2019-11-12 13:41:53,622 training loss; R2: 2.506690e-02 -6.632454
2019-11-12 13:41:53,909 valid 000 2.032876e-02 0.131818
2019-11-12 13:41:55,598 valid 050 1.768371e-02 -1.013398
2019-11-12 13:41:57,223 validation loss; R2: 1.771128e-02 -0.510404
2019-11-12 13:41:57,236 epoch 1 lr 1.000000e-03
2019-11-12 13:41:57,715 train 000 1.964293e-02 0.135308
2019-11-12 13:42:03,486 train 050 1.949540e-02 -0.060802
2019-11-12 13:42:09,252 train 100 1.941524e-02 -0.030862
2019-11-12 13:42:15,021 train 150 1.930872e-02 -0.007019
2019-11-12 13:42:20,793 train 200 1.929365e-02 -0.009307
2019-11-12 13:42:26,558 train 250 1.923665e-02 -0.005389
2019-11-12 13:42:32,320 train 300 1.915524e-02 -0.010525
2019-11-12 13:42:38,082 train 350 1.903910e-02 -0.005960
2019-11-12 13:42:43,845 train 400 1.892376e-02 0.003359
2019-11-12 13:42:49,608 train 450 1.881891e-02 0.013704
2019-11-12 13:42:55,383 train 500 1.872494e-02 0.019735
2019-11-12 13:43:01,157 train 550 1.866379e-02 0.027101
2019-11-12 13:43:06,898 train 600 1.857221e-02 0.032868
2019-11-12 13:43:12,647 train 650 1.843940e-02 0.033095
2019-11-12 13:43:18,383 train 700 1.832649e-02 0.035072
2019-11-12 13:43:24,111 train 750 1.824077e-02 0.040201
2019-11-12 13:43:29,838 train 800 1.816131e-02 0.040326
2019-11-12 13:43:35,571 train 850 1.809000e-02 0.038766
2019-11-12 13:43:37,284 training loss; R2: 1.808063e-02 0.040808
2019-11-12 13:43:37,566 valid 000 1.715813e-02 0.228883
2019-11-12 13:43:39,275 valid 050 1.605750e-02 0.109006
2019-11-12 13:43:40,838 validation loss; R2: 1.606106e-02 0.091542
2019-11-12 13:43:40,851 epoch 2 lr 1.000000e-03
2019-11-12 13:43:41,186 train 000 1.841211e-02 0.167264
2019-11-12 13:43:47,096 train 050 1.686665e-02 0.089322
2019-11-12 13:43:53,030 train 100 1.665422e-02 0.117423
2019-11-12 13:43:59,031 train 150 1.652106e-02 0.107965
2019-11-12 13:44:04,810 train 200 1.646270e-02 0.115533
2019-11-12 13:44:10,669 train 250 1.656146e-02 0.116027
2019-11-12 13:44:16,744 train 300 1.656999e-02 0.120239
2019-11-12 13:44:22,808 train 350 1.644843e-02 0.123499
2019-11-12 13:44:28,862 train 400 1.641629e-02 0.122703
2019-11-12 13:44:34,933 train 450 1.635774e-02 0.128852
2019-11-12 13:44:40,995 train 500 1.630596e-02 0.128897
2019-11-12 13:44:47,050 train 550 1.625766e-02 0.130012
2019-11-12 13:44:53,109 train 600 1.620246e-02 0.135173
2019-11-12 13:44:59,168 train 650 1.616107e-02 0.073949
2019-11-12 13:45:05,227 train 700 1.611001e-02 0.078656
2019-11-12 13:45:11,292 train 750 1.605758e-02 0.086951
2019-11-12 13:45:17,346 train 800 1.599686e-02 0.086685
2019-11-12 13:45:23,397 train 850 1.596224e-02 0.092642
2019-11-12 13:45:25,202 training loss; R2: 1.594013e-02 0.093844
2019-11-12 13:45:25,496 valid 000 1.647407e-02 0.304780
2019-11-12 13:45:27,229 valid 050 1.372094e-02 0.295270
2019-11-12 13:45:28,783 validation loss; R2: 1.356573e-02 0.296884
2019-11-12 13:45:28,796 epoch 3 lr 1.000000e-03
2019-11-12 13:45:29,155 train 000 1.471574e-02 0.246140
2019-11-12 13:45:35,111 train 050 1.578334e-02 0.065007
2019-11-12 13:45:41,156 train 100 1.521543e-02 0.133612
2019-11-12 13:45:47,190 train 150 1.515703e-02 0.136690
2019-11-12 13:45:53,218 train 200 1.508223e-02 0.139127
2019-11-12 13:45:59,232 train 250 1.500925e-02 0.129062
2019-11-12 13:46:05,241 train 300 1.497651e-02 0.128242
2019-11-12 13:46:11,275 train 350 1.498278e-02 0.073207
2019-11-12 13:46:17,303 train 400 1.491184e-02 0.091129
2019-11-12 13:46:23,314 train 450 1.484604e-02 0.106098
2019-11-12 13:46:29,327 train 500 1.483324e-02 0.110824
2019-11-12 13:46:35,343 train 550 1.480064e-02 0.114212
2019-11-12 13:46:41,369 train 600 1.475031e-02 0.123819
2019-11-12 13:46:47,381 train 650 1.468875e-02 0.130672
2019-11-12 13:46:53,397 train 700 1.465511e-02 0.136915
2019-11-12 13:46:59,403 train 750 1.464610e-02 0.134533
2019-11-12 13:47:05,414 train 800 1.460587e-02 0.139742
2019-11-12 13:47:11,426 train 850 1.457478e-02 0.142683
2019-11-12 13:47:13,228 training loss; R2: 1.457343e-02 0.143696
2019-11-12 13:47:13,518 valid 000 1.172173e-02 0.303201
2019-11-12 13:47:15,243 valid 050 1.251916e-02 0.295284
2019-11-12 13:47:16,799 validation loss; R2: 1.270780e-02 0.298887
2019-11-12 13:47:16,818 epoch 4 lr 1.000000e-03
2019-11-12 13:47:17,141 train 000 1.529483e-02 0.256403
2019-11-12 13:47:23,197 train 050 1.363544e-02 0.181245
2019-11-12 13:47:29,264 train 100 1.367797e-02 0.177976
2019-11-12 13:47:35,327 train 150 1.372459e-02 0.176014
2019-11-12 13:47:41,103 train 200 1.381334e-02 0.178743
2019-11-12 13:47:46,874 train 250 1.391117e-02 0.182725
2019-11-12 13:47:52,643 train 300 1.387581e-02 0.192074
2019-11-12 13:47:58,410 train 350 1.380434e-02 0.199587
2019-11-12 13:48:04,186 train 400 1.381381e-02 0.203598
2019-11-12 13:48:09,955 train 450 1.383064e-02 0.199296
2019-11-12 13:48:15,714 train 500 1.379530e-02 0.195540
2019-11-12 13:48:21,467 train 550 1.377514e-02 0.077402
2019-11-12 13:48:27,230 train 600 1.376519e-02 0.091843
2019-11-12 13:48:32,985 train 650 1.376190e-02 0.100924
2019-11-12 13:48:38,745 train 700 1.376573e-02 0.109783
2019-11-12 13:48:44,501 train 750 1.373332e-02 0.118627
2019-11-12 13:48:50,258 train 800 1.370296e-02 0.125873
2019-11-12 13:48:56,021 train 850 1.368126e-02 0.128324
2019-11-12 13:48:57,738 training loss; R2: 1.366881e-02 0.130729
2019-11-12 13:48:58,006 valid 000 9.653895e-03 0.307209
2019-11-12 13:48:59,798 valid 050 1.249522e-02 0.298986
2019-11-12 13:49:01,318 validation loss; R2: 1.237198e-02 0.257767
2019-11-12 13:49:01,337 epoch 5 lr 1.000000e-03
2019-11-12 13:49:01,682 train 000 1.408601e-02 0.257251
2019-11-12 13:49:07,653 train 050 1.346411e-02 0.233002
2019-11-12 13:49:13,679 train 100 1.324510e-02 0.222706
2019-11-12 13:49:19,702 train 150 1.323170e-02 0.232140
2019-11-12 13:49:25,716 train 200 1.319516e-02 0.210959
2019-11-12 13:49:31,735 train 250 1.314951e-02 0.203275
2019-11-12 13:49:37,750 train 300 1.315675e-02 0.198181
2019-11-12 13:49:43,755 train 350 1.316585e-02 0.208906
2019-11-12 13:49:49,551 train 400 1.317981e-02 0.212039
2019-11-12 13:49:55,284 train 450 1.316905e-02 0.215970
2019-11-12 13:50:01,021 train 500 1.318486e-02 0.218341
2019-11-12 13:50:06,747 train 550 1.317527e-02 0.211012
2019-11-12 13:50:12,543 train 600 1.312829e-02 0.203164
2019-11-12 13:50:18,554 train 650 1.309498e-02 0.206858
2019-11-12 13:50:24,569 train 700 1.309878e-02 0.209998
2019-11-12 13:50:30,573 train 750 1.306880e-02 0.214166
2019-11-12 13:50:36,575 train 800 1.302560e-02 0.217270
2019-11-12 13:50:42,583 train 850 1.301470e-02 0.220427
2019-11-12 13:50:44,382 training loss; R2: 1.300054e-02 0.221204
2019-11-12 13:50:44,674 valid 000 1.019161e-02 0.279455
2019-11-12 13:50:46,394 valid 050 1.143610e-02 0.131998
2019-11-12 13:50:47,952 validation loss; R2: 1.153953e-02 0.162561
2019-11-12 13:50:47,971 epoch 6 lr 1.000000e-03
2019-11-12 13:50:48,339 train 000 1.249467e-02 0.255006
2019-11-12 13:50:54,411 train 050 1.282197e-02 0.243094
2019-11-12 13:51:00,434 train 100 1.271444e-02 0.254039
2019-11-12 13:51:06,455 train 150 1.285904e-02 0.257962
2019-11-12 13:51:12,499 train 200 1.286513e-02 0.261125
2019-11-12 13:51:18,510 train 250 1.282558e-02 0.265481
2019-11-12 13:51:24,514 train 300 1.280342e-02 0.256975
2019-11-12 13:51:30,554 train 350 1.278132e-02 0.253893
2019-11-12 13:51:36,575 train 400 1.278392e-02 0.254797
2019-11-12 13:51:42,607 train 450 1.272524e-02 0.257518
2019-11-12 13:51:48,627 train 500 1.268851e-02 0.253228
2019-11-12 13:51:54,638 train 550 1.266272e-02 0.257218
2019-11-12 13:52:00,658 train 600 1.267064e-02 0.249259
2019-11-12 13:52:06,677 train 650 1.263956e-02 0.251430
2019-11-12 13:52:12,685 train 700 1.261918e-02 0.253744
2019-11-12 13:52:18,700 train 750 1.260101e-02 0.090706
2019-11-12 13:52:24,712 train 800 1.258781e-02 0.088857
2019-11-12 13:52:30,724 train 850 1.258117e-02 0.099362
2019-11-12 13:52:32,523 training loss; R2: 1.256887e-02 0.101897
2019-11-12 13:52:32,814 valid 000 1.007487e-02 0.402674
2019-11-12 13:52:34,531 valid 050 1.103730e-02 0.365693
2019-11-12 13:52:36,102 validation loss; R2: 1.123978e-02 0.363154
2019-11-12 13:52:36,116 epoch 7 lr 1.000000e-03
2019-11-12 13:52:36,484 train 000 1.055679e-02 0.296407
2019-11-12 13:52:42,635 train 050 1.224751e-02 0.274014
2019-11-12 13:52:48,699 train 100 1.226675e-02 0.275124
2019-11-12 13:52:54,753 train 150 1.234322e-02 0.278014
2019-11-12 13:53:00,797 train 200 1.233574e-02 0.278400
2019-11-12 13:53:06,859 train 250 1.229229e-02 0.274136
2019-11-12 13:53:12,906 train 300 1.229359e-02 0.275512
2019-11-12 13:53:18,919 train 350 1.226539e-02 0.279535
2019-11-12 13:53:24,929 train 400 1.222168e-02 0.276180
2019-11-12 13:53:30,949 train 450 1.216322e-02 0.276284
2019-11-12 13:53:36,957 train 500 1.219062e-02 0.276802
2019-11-12 13:53:42,976 train 550 1.219515e-02 0.278544
2019-11-12 13:53:48,751 train 600 1.214454e-02 0.281549
2019-11-12 13:53:54,482 train 650 1.213257e-02 0.283547
2019-11-12 13:54:00,214 train 700 1.213610e-02 0.284753
2019-11-12 13:54:05,952 train 750 1.213927e-02 0.283007
2019-11-12 13:54:11,687 train 800 1.214195e-02 0.284061
2019-11-12 13:54:17,418 train 850 1.214326e-02 0.284856
2019-11-12 13:54:19,131 training loss; R2: 1.214390e-02 0.284246
2019-11-12 13:54:19,404 valid 000 8.436481e-03 0.389248
2019-11-12 13:54:21,128 valid 050 1.041778e-02 0.371398
2019-11-12 13:54:22,681 validation loss; R2: 1.045149e-02 0.371712
2019-11-12 13:54:22,700 epoch 8 lr 1.000000e-03
2019-11-12 13:54:23,044 train 000 1.280195e-02 0.305884
2019-11-12 13:54:28,815 train 050 1.245160e-02 0.299517
2019-11-12 13:54:34,876 train 100 1.206998e-02 0.284851
2019-11-12 13:54:40,922 train 150 1.211454e-02 0.269515
2019-11-12 13:54:46,801 train 200 1.208938e-02 0.273777
2019-11-12 13:54:52,565 train 250 1.207265e-02 0.271274
2019-11-12 13:54:58,328 train 300 1.204297e-02 0.272768
2019-11-12 13:55:04,095 train 350 1.200808e-02 0.276001
2019-11-12 13:55:09,846 train 400 1.199261e-02 0.280101
2019-11-12 13:55:15,604 train 450 1.198345e-02 0.281938
2019-11-12 13:55:21,366 train 500 1.194691e-02 0.284818
2019-11-12 13:55:27,118 train 550 1.193180e-02 0.274058
2019-11-12 13:55:32,874 train 600 1.195279e-02 0.264827
2019-11-12 13:55:38,626 train 650 1.194802e-02 0.266993
2019-11-12 13:55:44,394 train 700 1.194208e-02 0.269207
2019-11-12 13:55:50,167 train 750 1.192829e-02 0.270584
2019-11-12 13:55:55,924 train 800 1.192425e-02 0.268727
2019-11-12 13:56:01,679 train 850 1.190151e-02 0.269674
2019-11-12 13:56:03,406 training loss; R2: 1.189131e-02 0.270041
2019-11-12 13:56:03,681 valid 000 7.430573e-03 0.464585
2019-11-12 13:56:05,401 valid 050 1.012836e-02 0.410929
2019-11-12 13:56:06,973 validation loss; R2: 1.005480e-02 0.406135
2019-11-12 13:56:06,986 epoch 9 lr 1.000000e-03
2019-11-12 13:56:07,320 train 000 1.341459e-02 0.345007
2019-11-12 13:56:13,309 train 050 1.128807e-02 0.262834
2019-11-12 13:56:19,215 train 100 1.164136e-02 0.281439
2019-11-12 13:56:25,321 train 150 1.163131e-02 0.281520
2019-11-12 13:56:31,129 train 200 1.168552e-02 0.283425
2019-11-12 13:56:36,901 train 250 1.171783e-02 0.286775
2019-11-12 13:56:42,670 train 300 1.168936e-02 0.222856
2019-11-12 13:56:48,437 train 350 1.170409e-02 0.236321
2019-11-12 13:56:54,194 train 400 1.171030e-02 0.089117
2019-11-12 13:56:59,955 train 450 1.168538e-02 0.114636
2019-11-12 13:57:05,728 train 500 1.167944e-02 0.133112
2019-11-12 13:57:11,485 train 550 1.166544e-02 0.146892
2019-11-12 13:57:17,253 train 600 1.166352e-02 0.154545
2019-11-12 13:57:23,019 train 650 1.165572e-02 0.167332
2019-11-12 13:57:28,788 train 700 1.163053e-02 0.177265
2019-11-12 13:57:34,555 train 750 1.163071e-02 0.185667
2019-11-12 13:57:40,323 train 800 1.163004e-02 0.187494
2019-11-12 13:57:46,096 train 850 1.162448e-02 0.194000
2019-11-12 13:57:47,820 training loss; R2: 1.161552e-02 0.194550
2019-11-12 13:57:48,100 valid 000 1.102632e-02 0.449656
2019-11-12 13:57:49,815 valid 050 9.940396e-03 0.420141
2019-11-12 13:57:51,382 validation loss; R2: 9.833381e-03 0.406591
2019-11-12 13:57:51,395 epoch 10 lr 1.000000e-03
2019-11-12 13:57:51,751 train 000 1.344300e-02 0.294875
2019-11-12 13:57:57,774 train 050 1.154925e-02 0.308286
2019-11-12 13:58:03,571 train 100 1.149697e-02 0.299912
2019-11-12 13:58:09,365 train 150 1.150154e-02 0.307243
2019-11-12 13:58:15,160 train 200 1.149360e-02 0.311407
2019-11-12 13:58:21,151 train 250 1.146559e-02 0.303486
2019-11-12 13:58:27,211 train 300 1.144018e-02 0.294445
2019-11-12 13:58:33,262 train 350 1.145514e-02 -0.114128
2019-11-12 13:58:39,049 train 400 1.142568e-02 -0.102408
2019-11-12 13:58:44,840 train 450 1.141656e-02 -0.057054
2019-11-12 13:58:50,627 train 500 1.142770e-02 -0.021040
2019-11-12 13:58:56,419 train 550 1.145083e-02 0.005328
2019-11-12 13:59:02,209 train 600 1.144214e-02 0.030824
2019-11-12 13:59:08,006 train 650 1.146034e-02 0.047488
2019-11-12 13:59:13,809 train 700 1.145063e-02 0.066843
2019-11-12 13:59:19,617 train 750 1.142776e-02 0.081356
2019-11-12 13:59:25,419 train 800 1.141889e-02 0.094899
2019-11-12 13:59:31,219 train 850 1.140475e-02 0.108647
2019-11-12 13:59:32,954 training loss; R2: 1.140203e-02 0.112674
2019-11-12 13:59:33,238 valid 000 1.053003e-02 0.492211
2019-11-12 13:59:34,959 valid 050 1.004267e-02 0.414786
2019-11-12 13:59:36,519 validation loss; R2: 9.999063e-03 0.415307
2019-11-12 13:59:36,538 epoch 11 lr 1.000000e-03
2019-11-12 13:59:36,877 train 000 1.037838e-02 0.374201
2019-11-12 13:59:42,682 train 050 1.140720e-02 0.317329
2019-11-12 13:59:48,821 train 100 1.129217e-02 0.316204
2019-11-12 13:59:55,068 train 150 1.121505e-02 0.312006
2019-11-12 14:00:01,228 train 200 1.122487e-02 0.318408
2019-11-12 14:00:07,210 train 250 1.120241e-02 0.312564
2019-11-12 14:00:13,448 train 300 1.122006e-02 0.310699
2019-11-12 14:00:19,663 train 350 1.123611e-02 0.311427
2019-11-12 14:00:25,623 train 400 1.123328e-02 0.308736
2019-11-12 14:00:31,573 train 450 1.125870e-02 0.308401
2019-11-12 14:00:37,530 train 500 1.128581e-02 0.298601
2019-11-12 14:00:43,482 train 550 1.129116e-02 0.298541
2019-11-12 14:00:49,434 train 600 1.128057e-02 0.299015
2019-11-12 14:00:55,385 train 650 1.126436e-02 0.300902
2019-11-12 14:01:01,335 train 700 1.126061e-02 0.301974
2019-11-12 14:01:07,278 train 750 1.125063e-02 0.305161
2019-11-12 14:01:13,226 train 800 1.123994e-02 0.305071
2019-11-12 14:01:19,171 train 850 1.124482e-02 -0.608979
2019-11-12 14:01:20,950 training loss; R2: 1.124567e-02 -0.592996
2019-11-12 14:01:21,235 valid 000 8.831742e-03 0.337388
2019-11-12 14:01:22,912 valid 050 9.521132e-03 0.064265
2019-11-12 14:01:24,429 validation loss; R2: 9.545472e-03 0.213487
2019-11-12 14:01:24,449 epoch 12 lr 1.000000e-03
2019-11-12 14:01:24,816 train 000 1.051377e-02 0.347928
2019-11-12 14:01:30,839 train 050 1.133676e-02 0.231152
2019-11-12 14:01:36,672 train 100 1.119884e-02 0.262699
2019-11-12 14:01:42,399 train 150 1.117379e-02 0.274005
2019-11-12 14:01:48,129 train 200 1.117241e-02 0.280701
2019-11-12 14:01:53,856 train 250 1.113167e-02 0.273112
2019-11-12 14:01:59,581 train 300 1.110469e-02 0.283989
2019-11-12 14:02:05,300 train 350 1.111536e-02 0.289452
2019-11-12 14:02:11,017 train 400 1.114378e-02 0.288532
2019-11-12 14:02:16,731 train 450 1.113344e-02 0.290349
2019-11-12 14:02:22,457 train 500 1.115802e-02 0.291673
2019-11-12 14:02:28,176 train 550 1.116193e-02 0.293740
2019-11-12 14:02:33,890 train 600 1.117901e-02 0.280625
2019-11-12 14:02:39,607 train 650 1.118532e-02 0.284881
2019-11-12 14:02:45,328 train 700 1.117137e-02 0.287239
2019-11-12 14:02:51,046 train 750 1.115564e-02 0.289579
2019-11-12 14:02:56,765 train 800 1.113528e-02 0.291885
2019-11-12 14:03:02,482 train 850 1.111065e-02 0.282356
2019-11-12 14:03:04,192 training loss; R2: 1.111797e-02 0.265434
2019-11-12 14:03:04,484 valid 000 1.187329e-02 0.395203
2019-11-12 14:03:06,160 valid 050 9.547271e-03 0.371717
2019-11-12 14:03:07,707 validation loss; R2: 9.429306e-03 0.357113
2019-11-12 14:03:07,730 epoch 13 lr 1.000000e-03
2019-11-12 14:03:08,171 train 000 9.681324e-03 0.350868
2019-11-12 14:03:14,294 train 050 1.058353e-02 0.337911
2019-11-12 14:03:20,447 train 100 1.084504e-02 0.322884
2019-11-12 14:03:26,569 train 150 1.083859e-02 0.323524
2019-11-12 14:03:32,758 train 200 1.090048e-02 0.322817
2019-11-12 14:03:38,888 train 250 1.092186e-02 0.312982
2019-11-12 14:03:44,913 train 300 1.095993e-02 0.315303
2019-11-12 14:03:51,069 train 350 1.096155e-02 0.312227
2019-11-12 14:03:57,185 train 400 1.097239e-02 0.310599
2019-11-12 14:04:03,309 train 450 1.098899e-02 0.311103
2019-11-12 14:04:09,485 train 500 1.097399e-02 0.311114
2019-11-12 14:04:15,612 train 550 1.098523e-02 0.308330
2019-11-12 14:04:21,735 train 600 1.098071e-02 0.305294
2019-11-12 14:04:27,882 train 650 1.097280e-02 0.306574
2019-11-12 14:04:33,953 train 700 1.096103e-02 0.308351
2019-11-12 14:04:39,894 train 750 1.096485e-02 0.303488
2019-11-12 14:04:45,830 train 800 1.096018e-02 0.305414
2019-11-12 14:04:51,850 train 850 1.097279e-02 0.306668
2019-11-12 14:04:53,645 training loss; R2: 1.097791e-02 0.306546
2019-11-12 14:04:53,936 valid 000 1.330035e-02 0.359471
2019-11-12 14:04:55,594 valid 050 1.092083e-02 0.313106
2019-11-12 14:04:57,118 validation loss; R2: 1.090412e-02 0.254976
2019-11-12 14:04:57,141 epoch 14 lr 1.000000e-03
2019-11-12 14:04:57,558 train 000 1.111925e-02 0.425888
2019-11-12 14:05:03,700 train 050 1.112502e-02 0.327326
2019-11-12 14:05:09,774 train 100 1.110813e-02 0.303135
2019-11-12 14:05:15,800 train 150 1.106777e-02 0.306331
2019-11-12 14:05:21,868 train 200 1.103370e-02 0.214402
2019-11-12 14:05:28,029 train 250 1.103585e-02 0.240839
2019-11-12 14:05:34,153 train 300 1.101641e-02 0.253003
2019-11-12 14:05:40,279 train 350 1.102775e-02 0.257879
2019-11-12 14:05:46,384 train 400 1.101130e-02 0.260937
2019-11-12 14:05:52,475 train 450 1.101147e-02 0.268154
2019-11-12 14:05:58,657 train 500 1.096126e-02 0.274308
2019-11-12 14:06:04,820 train 550 1.095674e-02 0.275529
2019-11-12 14:06:10,970 train 600 1.095387e-02 0.278753
2019-11-12 14:06:17,098 train 650 1.094451e-02 0.283156
2019-11-12 14:06:23,219 train 700 1.093816e-02 0.287011
2019-11-12 14:06:29,377 train 750 1.092096e-02 0.288133
2019-11-12 14:06:35,435 train 800 1.091289e-02 0.291737
2019-11-12 14:06:41,502 train 850 1.091161e-02 0.294048
2019-11-12 14:06:43,251 training loss; R2: 1.090715e-02 0.294936
2019-11-12 14:06:43,516 valid 000 1.017554e-02 0.451041
2019-11-12 14:06:45,219 valid 050 9.553314e-03 0.424315
2019-11-12 14:06:46,757 validation loss; R2: 9.757642e-03 0.413272
2019-11-12 14:06:46,774 epoch 15 lr 1.000000e-03
2019-11-12 14:06:47,131 train 000 9.069078e-03 0.441673
2019-11-12 14:06:53,185 train 050 1.090572e-02 0.332277
2019-11-12 14:06:59,281 train 100 1.083901e-02 0.207194
2019-11-12 14:07:05,367 train 150 1.088588e-02 0.223816
2019-11-12 14:07:11,441 train 200 1.082468e-02 0.244493
2019-11-12 14:07:17,505 train 250 1.090275e-02 0.246494
2019-11-12 14:07:23,548 train 300 1.085858e-02 0.154447
2019-11-12 14:07:29,603 train 350 1.083586e-02 0.181274
2019-11-12 14:07:35,665 train 400 1.081731e-02 0.189024
2019-11-12 14:07:41,720 train 450 1.080451e-02 0.203314
2019-11-12 14:07:47,776 train 500 1.078618e-02 0.216082
2019-11-12 14:07:53,794 train 550 1.079871e-02 0.227391
2019-11-12 14:07:59,802 train 600 1.078512e-02 0.237665
2019-11-12 14:08:05,816 train 650 1.080835e-02 0.243485
2019-11-12 14:08:11,831 train 700 1.082227e-02 0.248818
2019-11-12 14:08:17,720 train 750 1.082584e-02 0.252437
2019-11-12 14:08:23,574 train 800 1.083803e-02 0.257524
2019-11-12 14:08:29,603 train 850 1.082648e-02 0.263591
2019-11-12 14:08:31,403 training loss; R2: 1.082037e-02 0.265144
2019-11-12 14:08:31,723 valid 000 9.751875e-03 0.390093
2019-11-12 14:08:33,389 valid 050 9.843224e-03 0.358070
2019-11-12 14:08:34,907 validation loss; R2: 9.673397e-03 0.361306
2019-11-12 14:08:34,921 epoch 16 lr 1.000000e-03
2019-11-12 14:08:35,285 train 000 1.017631e-02 0.364527
2019-11-12 14:08:41,282 train 050 1.061257e-02 0.339379
2019-11-12 14:08:47,385 train 100 1.058800e-02 0.335904
2019-11-12 14:08:53,268 train 150 1.067759e-02 0.319150
2019-11-12 14:08:59,203 train 200 1.064094e-02 0.327305
2019-11-12 14:09:05,040 train 250 1.062053e-02 0.327234
2019-11-12 14:09:10,959 train 300 1.063875e-02 0.324547
2019-11-12 14:09:17,075 train 350 1.061208e-02 0.329154
2019-11-12 14:09:23,162 train 400 1.062863e-02 0.326580
2019-11-12 14:09:29,259 train 450 1.064462e-02 0.329287
2019-11-12 14:09:35,121 train 500 1.067186e-02 0.322918
2019-11-12 14:09:41,266 train 550 1.067731e-02 0.325143
2019-11-12 14:09:47,568 train 600 1.067198e-02 0.325533
2019-11-12 14:09:53,732 train 650 1.067391e-02 0.325002
2019-11-12 14:09:59,755 train 700 1.068877e-02 0.324933
2019-11-12 14:10:06,055 train 750 1.068419e-02 0.323011
2019-11-12 14:10:12,427 train 800 1.067817e-02 0.322582
2019-11-12 14:10:18,481 train 850 1.069106e-02 0.323013
2019-11-12 14:10:20,267 training loss; R2: 1.069099e-02 0.323455
2019-11-12 14:10:20,546 valid 000 1.009704e-02 0.468332
2019-11-12 14:10:22,220 valid 050 1.026625e-02 0.302529
2019-11-12 14:10:23,741 validation loss; R2: 1.026950e-02 0.320223
2019-11-12 14:10:23,761 epoch 17 lr 1.000000e-03
2019-11-12 14:10:24,110 train 000 1.093601e-02 0.161294
2019-11-12 14:10:30,213 train 050 1.065285e-02 0.324192
2019-11-12 14:10:36,435 train 100 1.070418e-02 0.326564
2019-11-12 14:10:42,632 train 150 1.065596e-02 0.330238
2019-11-12 14:10:48,679 train 200 1.062940e-02 0.298765
2019-11-12 14:10:54,669 train 250 1.063704e-02 0.304439
2019-11-12 14:11:00,802 train 300 1.058283e-02 0.303460
2019-11-12 14:11:06,890 train 350 1.060985e-02 0.275024
2019-11-12 14:11:13,005 train 400 1.063104e-02 0.284110
2019-11-12 14:11:19,117 train 450 1.058501e-02 0.287880
2019-11-12 14:11:25,221 train 500 1.058702e-02 0.290822
2019-11-12 14:11:31,193 train 550 1.058356e-02 0.295176
2019-11-12 14:11:37,108 train 600 1.057092e-02 0.296421
2019-11-12 14:11:43,053 train 650 1.058263e-02 0.297762
2019-11-12 14:11:48,991 train 700 1.057322e-02 0.301307
2019-11-12 14:11:55,067 train 750 1.056718e-02 0.298407
2019-11-12 14:12:01,189 train 800 1.056515e-02 0.300643
2019-11-12 14:12:07,331 train 850 1.055534e-02 0.303164
2019-11-12 14:12:09,160 training loss; R2: 1.055652e-02 0.303639
2019-11-12 14:12:09,440 valid 000 9.696313e-03 0.383919
2019-11-12 14:12:11,146 valid 050 9.966263e-03 0.417136
2019-11-12 14:12:12,682 validation loss; R2: 9.802019e-03 0.411993
2019-11-12 14:12:12,705 epoch 18 lr 1.000000e-03
2019-11-12 14:12:13,064 train 000 1.239966e-02 0.351717
2019-11-12 14:12:19,049 train 050 1.050021e-02 0.316206
2019-11-12 14:12:25,111 train 100 1.052791e-02 0.318204
2019-11-12 14:12:31,179 train 150 1.067048e-02 0.315549
2019-11-12 14:12:37,230 train 200 1.063190e-02 0.323887
2019-11-12 14:12:43,235 train 250 1.061033e-02 0.229365
2019-11-12 14:12:49,005 train 300 1.054318e-02 0.246189
2019-11-12 14:12:54,944 train 350 1.052247e-02 0.259449
2019-11-12 14:13:00,995 train 400 1.052759e-02 0.269620
2019-11-12 14:13:07,067 train 450 1.057266e-02 0.276502
2019-11-12 14:13:13,115 train 500 1.058851e-02 0.280671
2019-11-12 14:13:19,162 train 550 1.057293e-02 0.281018
2019-11-12 14:13:25,209 train 600 1.055986e-02 0.285708
2019-11-12 14:13:31,269 train 650 1.055037e-02 0.289790
2019-11-12 14:13:37,320 train 700 1.057042e-02 0.291859
2019-11-12 14:13:43,370 train 750 1.055293e-02 0.296971
2019-11-12 14:13:49,427 train 800 1.055785e-02 0.299367
2019-11-12 14:13:55,478 train 850 1.053894e-02 0.300171
2019-11-12 14:13:57,286 training loss; R2: 1.053387e-02 0.301179
2019-11-12 14:13:57,587 valid 000 9.296656e-03 0.420107
2019-11-12 14:13:59,298 valid 050 1.114066e-02 0.393300
2019-11-12 14:14:00,849 validation loss; R2: 1.102454e-02 0.396817
2019-11-12 14:14:00,865 epoch 19 lr 1.000000e-03
2019-11-12 14:14:01,234 train 000 9.686457e-03 0.411027
2019-11-12 14:14:07,282 train 050 1.064860e-02 0.346654
2019-11-12 14:14:13,353 train 100 1.052486e-02 0.345291
2019-11-12 14:14:19,432 train 150 1.049623e-02 0.343857
2019-11-12 14:14:25,500 train 200 1.050998e-02 0.345961
2019-11-12 14:14:31,569 train 250 1.051067e-02 0.338871
2019-11-12 14:14:37,595 train 300 1.049894e-02 0.337575
2019-11-12 14:14:43,603 train 350 1.051231e-02 0.333241
2019-11-12 14:14:49,613 train 400 1.051942e-02 0.329544
2019-11-12 14:14:55,627 train 450 1.049360e-02 0.329627
2019-11-12 14:15:01,651 train 500 1.051550e-02 0.322318
2019-11-12 14:15:07,666 train 550 1.053506e-02 0.315300
2019-11-12 14:15:13,725 train 600 1.051859e-02 0.316380
2019-11-12 14:15:19,757 train 650 1.052424e-02 0.317158
2019-11-12 14:15:25,767 train 700 1.051948e-02 0.319716
2019-11-12 14:15:31,782 train 750 1.051762e-02 0.307573
2019-11-12 14:15:37,811 train 800 1.050369e-02 0.309388
2019-11-12 14:15:43,830 train 850 1.049276e-02 0.311646
2019-11-12 14:15:45,630 training loss; R2: 1.048929e-02 0.312205
2019-11-12 14:15:45,937 valid 000 2.581519e-02 0.002669
2019-11-12 14:15:47,639 valid 050 2.619768e-02 -0.041355
2019-11-12 14:15:49,202 validation loss; R2: 2.664714e-02 -0.047564
