2019-11-12 20:18:58,875 gpu device = 1
2019-11-12 20:18:58,876 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name='galaxy-zoo-cleaned', gpu=1, grad_clip=5, gz_dtree=False, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-galaxy-zoo-EXP-20191112-201858', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-11-12 20:19:10,480 param size = 0.326485MB
2019-11-12 20:19:10,484 epoch 0 lr 1.000000e-03
2019-11-12 20:19:12,689 train 000 1.109516e+00 -366.088052
2019-11-12 20:19:20,394 train 050 6.995123e-02 -17.500349
2019-11-12 20:19:28,152 train 100 5.060784e-02 -9.584905
2019-11-12 20:19:36,044 train 150 4.295675e-02 -6.679908
2019-11-12 20:19:43,969 train 200 3.862738e-02 -6.638165
2019-11-12 20:19:51,890 train 250 3.591826e-02 -5.409050
2019-11-12 20:19:59,810 train 300 3.395563e-02 -4.582074
2019-11-12 20:20:07,732 train 350 3.257697e-02 -3.970153
2019-11-12 20:20:15,645 train 400 3.145677e-02 -3.510185
2019-11-12 20:20:23,312 train 450 3.050701e-02 -3.154457
2019-11-12 20:20:31,033 train 500 2.969953e-02 -2.871458
2019-11-12 20:20:38,711 train 550 2.898957e-02 -2.628806
2019-11-12 20:20:46,348 train 600 2.837707e-02 -2.427520
2019-11-12 20:20:54,113 train 650 2.781864e-02 -2.250294
2019-11-12 20:21:01,766 train 700 2.733369e-02 -2.100258
2019-11-12 20:21:09,590 train 750 2.683860e-02 -1.974952
2019-11-12 20:21:17,249 train 800 2.642734e-02 -1.869298
2019-11-12 20:21:24,979 train 850 2.603954e-02 -1.772314
2019-11-12 20:21:28,062 training loss; R2: 2.592090e-02 -1.742264
2019-11-12 20:21:28,331 valid 000 2.571852e-02 -0.134783
2019-11-12 20:21:30,067 valid 050 2.740072e-02 -0.244644
2019-11-12 20:21:31,695 validation loss; R2: 2.731770e-02 -0.339807
2019-11-12 20:21:31,718 epoch 1 lr 1.000000e-03
2019-11-12 20:21:32,309 train 000 1.761765e-02 0.126355
2019-11-12 20:21:40,135 train 050 1.973894e-02 -0.012795
2019-11-12 20:21:47,854 train 100 1.950508e-02 -0.038614
2019-11-12 20:21:55,645 train 150 1.939989e-02 -0.054704
2019-11-12 20:22:03,287 train 200 1.926818e-02 -0.039743
2019-11-12 20:22:11,154 train 250 1.908074e-02 -0.024004
2019-11-12 20:22:18,926 train 300 1.889760e-02 -0.015320
2019-11-12 20:22:26,635 train 350 1.889685e-02 -0.009363
2019-11-12 20:22:34,444 train 400 1.880129e-02 -0.005031
2019-11-12 20:22:42,112 train 450 1.866358e-02 -0.008579
2019-11-12 20:22:49,780 train 500 1.858908e-02 -0.000573
2019-11-12 20:22:57,569 train 550 1.847140e-02 0.004540
2019-11-12 20:23:05,405 train 600 1.845198e-02 0.007305
2019-11-12 20:23:13,230 train 650 1.838973e-02 0.011334
2019-11-12 20:23:20,958 train 700 1.832772e-02 0.016060
2019-11-12 20:23:28,934 train 750 1.830095e-02 0.019255
2019-11-12 20:23:36,806 train 800 1.826041e-02 0.017507
2019-11-12 20:23:44,565 train 850 1.821234e-02 0.022921
2019-11-12 20:23:46,895 training loss; R2: 1.819206e-02 0.017115
2019-11-12 20:23:47,164 valid 000 2.183197e-02 -0.097896
2019-11-12 20:23:48,852 valid 050 1.867959e-02 0.103979
2019-11-12 20:23:50,379 validation loss; R2: 1.869633e-02 0.098611
2019-11-12 20:23:50,397 epoch 2 lr 1.000000e-03
2019-11-12 20:23:50,767 train 000 1.893090e-02 0.150167
2019-11-12 20:23:58,553 train 050 1.666492e-02 0.078718
2019-11-12 20:24:06,367 train 100 1.669317e-02 0.102102
2019-11-12 20:24:14,156 train 150 1.675987e-02 0.102667
2019-11-12 20:24:22,118 train 200 1.683593e-02 0.101528
2019-11-12 20:24:30,036 train 250 1.684826e-02 0.099222
2019-11-12 20:24:37,982 train 300 1.677071e-02 0.100453
2019-11-12 20:24:45,749 train 350 1.677442e-02 -0.097361
2019-11-12 20:24:53,633 train 400 1.672792e-02 -0.068257
2019-11-12 20:25:01,460 train 450 1.668239e-02 -0.061824
2019-11-12 20:25:09,143 train 500 1.662756e-02 -0.038740
2019-11-12 20:25:17,090 train 550 1.661601e-02 -1.097512
2019-11-12 20:25:25,020 train 600 1.657429e-02 -0.997457
2019-11-12 20:25:32,970 train 650 1.655143e-02 -0.910073
2019-11-12 20:25:40,910 train 700 1.651155e-02 -0.836235
2019-11-12 20:25:48,721 train 750 1.650126e-02 -0.771206
2019-11-12 20:25:56,631 train 800 1.647651e-02 -0.715611
2019-11-12 20:26:04,478 train 850 1.639494e-02 -0.663773
2019-11-12 20:26:06,847 training loss; R2: 1.638136e-02 -0.650452
2019-11-12 20:26:07,120 valid 000 1.355288e-02 0.274039
2019-11-12 20:26:08,829 valid 050 1.431449e-02 0.238766
2019-11-12 20:26:10,332 validation loss; R2: 1.436357e-02 0.240563
2019-11-12 20:26:10,350 epoch 3 lr 1.000000e-03
2019-11-12 20:26:10,774 train 000 1.568086e-02 0.164113
2019-11-12 20:26:18,587 train 050 1.544473e-02 0.161010
2019-11-12 20:26:26,523 train 100 1.542220e-02 0.168873
2019-11-12 20:26:34,390 train 150 1.550730e-02 0.165930
2019-11-12 20:26:42,332 train 200 1.547753e-02 0.158945
2019-11-12 20:26:50,260 train 250 1.544865e-02 0.167315
2019-11-12 20:26:58,044 train 300 1.540090e-02 0.145637
2019-11-12 20:27:05,851 train 350 1.529531e-02 0.151248
2019-11-12 20:27:13,521 train 400 1.527492e-02 0.154143
2019-11-12 20:27:21,286 train 450 1.525232e-02 0.153879
2019-11-12 20:27:29,001 train 500 1.524498e-02 0.154735
2019-11-12 20:27:36,685 train 550 1.523325e-02 0.158211
2019-11-12 20:27:44,538 train 600 1.516349e-02 0.159000
2019-11-12 20:27:52,486 train 650 1.511998e-02 0.158003
2019-11-12 20:28:00,444 train 700 1.510080e-02 0.158829
2019-11-12 20:28:08,164 train 750 1.507136e-02 0.163308
2019-11-12 20:28:15,980 train 800 1.504464e-02 0.163926
2019-11-12 20:28:23,685 train 850 1.501290e-02 0.163435
2019-11-12 20:28:25,956 training loss; R2: 1.499555e-02 0.162041
2019-11-12 20:28:26,240 valid 000 2.898876e-02 -0.142461
2019-11-12 20:28:27,949 valid 050 2.950818e-02 -0.213233
2019-11-12 20:28:29,498 validation loss; R2: 2.981355e-02 -0.229037
2019-11-12 20:28:29,522 epoch 4 lr 1.000000e-03
2019-11-12 20:28:29,916 train 000 1.485108e-02 0.190635
2019-11-12 20:28:37,557 train 050 1.419925e-02 0.180329
2019-11-12 20:28:45,404 train 100 1.428881e-02 0.151733
2019-11-12 20:28:53,127 train 150 1.435010e-02 0.148667
2019-11-12 20:29:01,096 train 200 1.436714e-02 0.157212
2019-11-12 20:29:09,048 train 250 1.432819e-02 0.144153
2019-11-12 20:29:17,000 train 300 1.433654e-02 0.155187
2019-11-12 20:29:24,859 train 350 1.433406e-02 0.152458
2019-11-12 20:29:32,689 train 400 1.434912e-02 0.156944
2019-11-12 20:29:40,643 train 450 1.430446e-02 0.163063
2019-11-12 20:29:48,374 train 500 1.425999e-02 0.169836
2019-11-12 20:29:56,063 train 550 1.428017e-02 0.174305
2019-11-12 20:30:03,970 train 600 1.425369e-02 0.177156
2019-11-12 20:30:11,876 train 650 1.423186e-02 0.180988
2019-11-12 20:30:19,833 train 700 1.419417e-02 0.184522
2019-11-12 20:30:27,633 train 750 1.417525e-02 0.183495
2019-11-12 20:30:35,453 train 800 1.412574e-02 0.186395
2019-11-12 20:30:43,200 train 850 1.409219e-02 0.188920
2019-11-12 20:30:45,480 training loss; R2: 1.408203e-02 0.189172
2019-11-12 20:30:45,746 valid 000 4.886428e-02 -2.731611
2019-11-12 20:30:47,445 valid 050 5.079502e-02 -4.378655
2019-11-12 20:30:48,955 validation loss; R2: 5.073934e-02 -3.506954
2019-11-12 20:30:48,978 epoch 5 lr 1.000000e-03
2019-11-12 20:30:49,406 train 000 1.431451e-02 0.265452
2019-11-12 20:30:57,038 train 050 1.385928e-02 0.184536
2019-11-12 20:31:04,788 train 100 1.376293e-02 0.209236
2019-11-12 20:31:12,755 train 150 1.375315e-02 0.214978
2019-11-12 20:31:20,520 train 200 1.359373e-02 0.220625
2019-11-12 20:31:28,230 train 250 1.362090e-02 0.217464
2019-11-12 20:31:36,188 train 300 1.363654e-02 0.211079
2019-11-12 20:31:44,142 train 350 1.359033e-02 0.217362
2019-11-12 20:31:52,063 train 400 1.350586e-02 0.220730
2019-11-12 20:31:59,661 train 450 1.350940e-02 0.224323
2019-11-12 20:32:07,273 train 500 1.348391e-02 0.225120
2019-11-12 20:32:14,963 train 550 1.347462e-02 0.224934
2019-11-12 20:32:22,532 train 600 1.344548e-02 0.220786
2019-11-12 20:32:30,094 train 650 1.346932e-02 0.201928
2019-11-12 20:32:37,655 train 700 1.342678e-02 0.205561
2019-11-12 20:32:45,217 train 750 1.342045e-02 0.208625
2019-11-12 20:32:52,777 train 800 1.339668e-02 0.212198
2019-11-12 20:33:00,338 train 850 1.338088e-02 0.215121
2019-11-12 20:33:02,599 training loss; R2: 1.337155e-02 0.211792
2019-11-12 20:33:02,897 valid 000 1.431002e-01 -5.479801
2019-11-12 20:33:04,620 valid 050 1.411723e-01 -7.034343
2019-11-12 20:33:06,144 validation loss; R2: 1.406327e-01 -7.106707
2019-11-12 20:33:06,166 epoch 6 lr 1.000000e-03
2019-11-12 20:33:06,565 train 000 1.159545e-02 0.344906
2019-11-12 20:33:14,129 train 050 1.293117e-02 0.248138
2019-11-12 20:33:21,693 train 100 1.295680e-02 0.243621
2019-11-12 20:33:29,257 train 150 1.289761e-02 0.256109
2019-11-12 20:33:36,821 train 200 1.291991e-02 0.252481
2019-11-12 20:33:44,385 train 250 1.301536e-02 0.251368
2019-11-12 20:33:51,952 train 300 1.297370e-02 0.250290
2019-11-12 20:33:59,510 train 350 1.296774e-02 0.253055
2019-11-12 20:34:07,073 train 400 1.292065e-02 0.256621
2019-11-12 20:34:14,636 train 450 1.293294e-02 0.256952
2019-11-12 20:34:22,201 train 500 1.293077e-02 0.259066
2019-11-12 20:34:29,768 train 550 1.295067e-02 0.258043
2019-11-12 20:34:37,365 train 600 1.293088e-02 0.258422
2019-11-12 20:34:44,927 train 650 1.294936e-02 0.240731
2019-11-12 20:34:52,510 train 700 1.291559e-02 0.241959
2019-11-12 20:35:00,072 train 750 1.288872e-02 0.243624
2019-11-12 20:35:07,634 train 800 1.287776e-02 0.242826
2019-11-12 20:35:15,195 train 850 1.287693e-02 0.242426
2019-11-12 20:35:17,458 training loss; R2: 1.287176e-02 0.242530
2019-11-12 20:35:17,745 valid 000 3.678285e-02 -0.863958
2019-11-12 20:35:19,476 valid 050 3.432264e-02 -0.818189
2019-11-12 20:35:21,038 validation loss; R2: 3.426801e-02 -0.784422
2019-11-12 20:35:21,065 epoch 7 lr 1.000000e-03
2019-11-12 20:35:21,483 train 000 1.000485e-02 0.382772
2019-11-12 20:35:29,293 train 050 1.287733e-02 0.287698
2019-11-12 20:35:36,975 train 100 1.265448e-02 0.277930
2019-11-12 20:35:44,634 train 150 1.266996e-02 0.268549
2019-11-12 20:35:52,304 train 200 1.263549e-02 0.264812
2019-11-12 20:35:59,916 train 250 1.257447e-02 0.233916
2019-11-12 20:36:07,523 train 300 1.260938e-02 0.242127
2019-11-12 20:36:15,124 train 350 1.256697e-02 0.250061
2019-11-12 20:36:22,840 train 400 1.250554e-02 0.253035
2019-11-12 20:36:30,498 train 450 1.252329e-02 0.253238
2019-11-12 20:36:38,099 train 500 1.254269e-02 0.250154
2019-11-12 20:36:45,741 train 550 1.252181e-02 0.245325
2019-11-12 20:36:53,355 train 600 1.252808e-02 0.247657
2019-11-12 20:37:01,137 train 650 1.252150e-02 0.247597
2019-11-12 20:37:08,748 train 700 1.251280e-02 0.250140
2019-11-12 20:37:16,386 train 750 1.251791e-02 0.245550
2019-11-12 20:37:24,040 train 800 1.252289e-02 0.235666
2019-11-12 20:37:31,890 train 850 1.252323e-02 0.237303
2019-11-12 20:37:34,203 training loss; R2: 1.251527e-02 0.237361
2019-11-12 20:37:34,500 valid 000 4.111421e-02 -0.720266
2019-11-12 20:37:36,240 valid 050 4.234356e-02 -0.970444
2019-11-12 20:37:37,799 validation loss; R2: 4.285147e-02 -1.153700
2019-11-12 20:37:37,820 epoch 8 lr 1.000000e-03
2019-11-12 20:37:38,223 train 000 1.223335e-02 0.285713
2019-11-12 20:37:45,923 train 050 1.220142e-02 0.246753
2019-11-12 20:37:53,622 train 100 1.218581e-02 0.245668
2019-11-12 20:38:01,262 train 150 1.224896e-02 0.256103
2019-11-12 20:38:08,902 train 200 1.222249e-02 0.261945
2019-11-12 20:38:16,562 train 250 1.221329e-02 0.265677
2019-11-12 20:38:24,241 train 300 1.222249e-02 0.268999
2019-11-12 20:38:32,000 train 350 1.220593e-02 0.272902
2019-11-12 20:38:39,706 train 400 1.220588e-02 0.269389
2019-11-12 20:38:47,412 train 450 1.221651e-02 0.271153
2019-11-12 20:38:55,117 train 500 1.221773e-02 0.269683
2019-11-12 20:39:02,811 train 550 1.223452e-02 0.264782
2019-11-12 20:39:10,522 train 600 1.223175e-02 0.262224
2019-11-12 20:39:18,233 train 650 1.218262e-02 0.256825
2019-11-12 20:39:25,969 train 700 1.217240e-02 0.259916
2019-11-12 20:39:33,719 train 750 1.218867e-02 0.260847
2019-11-12 20:39:41,404 train 800 1.218196e-02 0.258892
2019-11-12 20:39:49,187 train 850 1.216384e-02 0.260565
2019-11-12 20:39:51,562 training loss; R2: 1.216553e-02 0.261331
2019-11-12 20:39:51,848 valid 000 6.766744e-01 -39.691225
2019-11-12 20:39:53,512 valid 050 6.943197e-01 -52.967230
2019-11-12 20:39:55,036 validation loss; R2: 6.925752e-01 -56.067914
2019-11-12 20:39:55,055 epoch 9 lr 1.000000e-03
2019-11-12 20:39:55,487 train 000 1.078544e-02 0.163526
2019-11-12 20:40:03,105 train 050 1.185992e-02 0.287967
2019-11-12 20:40:10,721 train 100 1.193938e-02 0.269100
2019-11-12 20:40:18,340 train 150 1.193388e-02 0.255857
2019-11-12 20:40:25,956 train 200 1.198522e-02 0.266569
2019-11-12 20:40:33,581 train 250 1.193344e-02 0.268669
2019-11-12 20:40:41,212 train 300 1.188895e-02 0.272662
2019-11-12 20:40:48,909 train 350 1.190084e-02 0.272194
2019-11-12 20:40:56,551 train 400 1.188692e-02 0.264057
2019-11-12 20:41:04,267 train 450 1.190229e-02 0.267214
2019-11-12 20:41:11,998 train 500 1.191868e-02 0.267678
2019-11-12 20:41:19,666 train 550 1.191151e-02 0.271102
2019-11-12 20:41:27,296 train 600 1.189633e-02 0.270962
2019-11-12 20:41:34,940 train 650 1.189075e-02 0.273060
2019-11-12 20:41:42,628 train 700 1.187250e-02 0.274326
2019-11-12 20:41:50,295 train 750 1.187205e-02 0.275355
2019-11-12 20:41:57,930 train 800 1.185282e-02 0.274316
2019-11-12 20:42:05,549 train 850 1.185475e-02 0.276703
2019-11-12 20:42:07,827 training loss; R2: 1.185330e-02 0.277155
2019-11-12 20:42:08,126 valid 000 3.091318e+00 -232.175439
2019-11-12 20:42:09,850 valid 050 3.107276e+00 -262.906904
2019-11-12 20:42:11,480 validation loss; R2: 3.111584e+00 -286.896625
2019-11-12 20:42:11,503 epoch 10 lr 1.000000e-03
2019-11-12 20:42:11,950 train 000 1.209155e-02 0.366911
2019-11-12 20:42:19,739 train 050 1.191982e-02 0.307593
2019-11-12 20:42:27,482 train 100 1.169098e-02 0.308907
2019-11-12 20:42:35,241 train 150 1.169991e-02 0.289732
2019-11-12 20:42:43,032 train 200 1.169140e-02 0.293646
2019-11-12 20:42:50,895 train 250 1.166223e-02 0.292993
2019-11-12 20:42:58,638 train 300 1.162850e-02 0.255588
2019-11-12 20:43:06,386 train 350 1.163908e-02 0.263364
2019-11-12 20:43:14,106 train 400 1.160392e-02 0.267938
2019-11-12 20:43:21,808 train 450 1.159752e-02 0.269444
2019-11-12 20:43:29,484 train 500 1.161164e-02 0.257708
2019-11-12 20:43:37,193 train 550 1.164375e-02 0.256015
2019-11-12 20:43:44,939 train 600 1.164370e-02 0.252717
2019-11-12 20:43:52,598 train 650 1.166088e-02 0.257305
2019-11-12 20:44:00,347 train 700 1.165901e-02 0.259729
2019-11-12 20:44:08,114 train 750 1.165888e-02 0.262214
2019-11-12 20:44:15,845 train 800 1.165413e-02 0.258826
2019-11-12 20:44:23,532 train 850 1.165565e-02 0.260252
2019-11-12 20:44:25,850 training loss; R2: 1.165727e-02 0.261410
2019-11-12 20:44:26,158 valid 000 6.269405e+00 -359.355774
2019-11-12 20:44:27,909 valid 050 6.266338e+00 -613.838653
2019-11-12 20:44:29,468 validation loss; R2: 6.270774e+00 -624.638501
2019-11-12 20:44:29,492 epoch 11 lr 1.000000e-03
2019-11-12 20:44:29,927 train 000 1.192868e-02 0.292918
2019-11-12 20:44:37,655 train 050 1.154403e-02 0.286535
2019-11-12 20:44:45,405 train 100 1.149608e-02 -1.805924
2019-11-12 20:44:53,123 train 150 1.145904e-02 -1.170099
2019-11-12 20:45:00,924 train 200 1.150752e-02 -0.812335
2019-11-12 20:45:08,633 train 250 1.147754e-02 -0.589184
2019-11-12 20:45:16,307 train 300 1.148680e-02 -0.442745
2019-11-12 20:45:24,016 train 350 1.150170e-02 -0.335508
2019-11-12 20:45:31,780 train 400 1.147363e-02 -0.261189
2019-11-12 20:45:39,463 train 450 1.152711e-02 -0.201810
2019-11-12 20:45:47,127 train 500 1.151403e-02 -0.152110
2019-11-12 20:45:54,897 train 550 1.151729e-02 -0.110735
2019-11-12 20:46:02,628 train 600 1.150864e-02 -0.075524
2019-11-12 20:46:10,369 train 650 1.150633e-02 -0.046290
2019-11-12 20:46:18,170 train 700 1.149150e-02 -0.023532
2019-11-12 20:46:25,802 train 750 1.149033e-02 -0.002509
2019-11-12 20:46:33,497 train 800 1.149741e-02 0.017499
2019-11-12 20:46:41,223 train 850 1.149205e-02 0.033322
2019-11-12 20:46:43,499 training loss; R2: 1.149178e-02 0.038583
2019-11-12 20:46:43,798 valid 000 6.388273e+00 -1087.880700
2019-11-12 20:46:45,537 valid 050 6.334465e+00 -1479.292891
2019-11-12 20:46:47,100 validation loss; R2: 6.333680e+00 -1137.253395
2019-11-12 20:46:47,121 epoch 12 lr 1.000000e-03
2019-11-12 20:46:47,527 train 000 1.029281e-02 0.317230
2019-11-12 20:46:55,260 train 050 1.132513e-02 0.300560
2019-11-12 20:47:02,914 train 100 1.130506e-02 0.306321
2019-11-12 20:47:10,599 train 150 1.130720e-02 0.307994
2019-11-12 20:47:18,240 train 200 1.123900e-02 0.291487
2019-11-12 20:47:25,892 train 250 1.129627e-02 0.282020
2019-11-12 20:47:33,582 train 300 1.126830e-02 0.284613
2019-11-12 20:47:41,368 train 350 1.125118e-02 0.290051
2019-11-12 20:47:49,010 train 400 1.126321e-02 0.289250
2019-11-12 20:47:56,714 train 450 1.124886e-02 0.288243
2019-11-12 20:48:04,395 train 500 1.122117e-02 0.289651
2019-11-12 20:48:12,048 train 550 1.123639e-02 0.293360
2019-11-12 20:48:19,766 train 600 1.123765e-02 0.295596
2019-11-12 20:48:27,487 train 650 1.122505e-02 0.297260
2019-11-12 20:48:35,144 train 700 1.124462e-02 0.298848
2019-11-12 20:48:42,776 train 750 1.124754e-02 0.296066
2019-11-12 20:48:50,406 train 800 1.126706e-02 0.297614
2019-11-12 20:48:58,029 train 850 1.126851e-02 0.292427
2019-11-12 20:49:00,312 training loss; R2: 1.127582e-02 0.292037
2019-11-12 20:49:00,603 valid 000 5.177032e-01 -49.189756
2019-11-12 20:49:02,325 valid 050 5.039862e-01 -43.751073
2019-11-12 20:49:03,864 validation loss; R2: 5.048307e-01 -45.360147
2019-11-12 20:49:03,893 epoch 13 lr 1.000000e-03
2019-11-12 20:49:04,308 train 000 1.188501e-02 -0.001375
2019-11-12 20:49:12,086 train 050 1.169933e-02 0.304228
2019-11-12 20:49:19,793 train 100 1.136937e-02 0.266241
2019-11-12 20:49:27,481 train 150 1.130043e-02 0.286711
2019-11-12 20:49:35,142 train 200 1.124087e-02 0.274614
2019-11-12 20:49:42,805 train 250 1.122124e-02 0.278879
2019-11-12 20:49:50,578 train 300 1.120738e-02 0.284542
2019-11-12 20:49:58,308 train 350 1.119029e-02 0.285239
2019-11-12 20:50:06,027 train 400 1.120308e-02 0.289704
2019-11-12 20:50:13,684 train 450 1.122149e-02 0.292013
2019-11-12 20:50:21,387 train 500 1.120541e-02 0.291857
2019-11-12 20:50:29,107 train 550 1.116736e-02 0.295114
2019-11-12 20:50:36,763 train 600 1.116745e-02 0.296827
2019-11-12 20:50:44,501 train 650 1.114641e-02 0.269953
2019-11-12 20:50:52,180 train 700 1.116070e-02 0.272023
2019-11-12 20:51:00,036 train 750 1.116568e-02 0.274555
2019-11-12 20:51:07,927 train 800 1.117756e-02 0.277754
2019-11-12 20:51:15,640 train 850 1.117246e-02 0.278376
2019-11-12 20:51:18,051 training loss; R2: 1.117250e-02 0.279326
2019-11-12 20:51:18,371 valid 000 7.794624e-02 -5.183508
2019-11-12 20:51:20,147 valid 050 7.747663e-02 -8.283048
2019-11-12 20:51:21,750 validation loss; R2: 7.752146e-02 -8.388603
2019-11-12 20:51:21,768 epoch 14 lr 1.000000e-03
2019-11-12 20:51:22,194 train 000 1.314510e-02 0.365272
2019-11-12 20:51:29,905 train 050 1.118672e-02 0.305109
2019-11-12 20:51:37,596 train 100 1.098662e-02 0.309777
2019-11-12 20:51:45,319 train 150 1.100219e-02 0.318399
2019-11-12 20:51:53,084 train 200 1.100759e-02 0.306296
2019-11-12 20:52:00,745 train 250 1.095942e-02 0.306361
2019-11-12 20:52:08,437 train 300 1.099625e-02 0.302775
2019-11-12 20:52:16,136 train 350 1.098501e-02 0.296816
2019-11-12 20:52:23,785 train 400 1.100577e-02 0.296960
2019-11-12 20:52:31,459 train 450 1.104300e-02 0.296594
2019-11-12 20:52:39,181 train 500 1.103871e-02 0.299211
2019-11-12 20:52:46,876 train 550 1.102134e-02 0.301894
2019-11-12 20:52:54,546 train 600 1.103340e-02 0.302890
2019-11-12 20:53:02,220 train 650 1.103983e-02 0.303269
2019-11-12 20:53:09,885 train 700 1.103830e-02 0.302319
2019-11-12 20:53:17,660 train 750 1.101804e-02 0.298153
2019-11-12 20:53:25,346 train 800 1.102928e-02 0.299193
2019-11-12 20:53:33,007 train 850 1.102847e-02 0.301261
2019-11-12 20:53:35,288 training loss; R2: 1.102829e-02 0.297540
2019-11-12 20:53:35,580 valid 000 3.477749e+00 -288.448489
2019-11-12 20:53:37,303 valid 050 3.449498e+00 -388.155017
2019-11-12 20:53:38,858 validation loss; R2: 3.447600e+00 -575.298763
2019-11-12 20:53:38,877 epoch 15 lr 1.000000e-03
2019-11-12 20:53:39,273 train 000 1.313606e-02 0.301170
2019-11-12 20:53:47,023 train 050 1.117361e-02 0.318022
2019-11-12 20:53:54,717 train 100 1.090204e-02 0.166250
2019-11-12 20:54:02,413 train 150 1.089660e-02 0.218059
2019-11-12 20:54:10,070 train 200 1.086022e-02 0.237374
2019-11-12 20:54:17,838 train 250 1.086508e-02 0.247537
2019-11-12 20:54:25,618 train 300 1.093914e-02 0.255237
2019-11-12 20:54:33,319 train 350 1.093635e-02 0.259697
2019-11-12 20:54:40,950 train 400 1.096116e-02 0.263331
2019-11-12 20:54:48,622 train 450 1.096563e-02 0.269865
2019-11-12 20:54:56,563 train 500 1.098677e-02 0.273474
2019-11-12 20:55:04,356 train 550 1.095300e-02 0.266479
2019-11-12 20:55:12,116 train 600 1.095731e-02 0.266919
2019-11-12 20:55:19,782 train 650 1.094028e-02 0.269549
2019-11-12 20:55:27,513 train 700 1.095571e-02 0.273383
2019-11-12 20:55:35,217 train 750 1.094017e-02 0.261717
2019-11-12 20:55:43,136 train 800 1.094007e-02 0.263492
2019-11-12 20:55:51,009 train 850 1.093355e-02 0.267355
2019-11-12 20:55:53,380 training loss; R2: 1.093718e-02 0.268819
2019-11-12 20:55:53,696 valid 000 2.099696e-01 -30.951081
2019-11-12 20:55:55,429 valid 050 2.070272e-01 -37.756258
2019-11-12 20:55:56,987 validation loss; R2: 2.073437e-01 -34.667374
2019-11-12 20:55:57,017 epoch 16 lr 1.000000e-03
2019-11-12 20:55:57,471 train 000 1.617493e-02 0.232481
2019-11-12 20:56:05,063 train 050 1.083998e-02 0.343929
2019-11-12 20:56:12,638 train 100 1.084265e-02 0.336208
2019-11-12 20:56:20,262 train 150 1.088112e-02 0.311147
2019-11-12 20:56:27,927 train 200 1.089256e-02 0.307450
2019-11-12 20:56:35,841 train 250 1.088525e-02 0.294645
2019-11-12 20:56:43,755 train 300 1.088488e-02 0.282704
2019-11-12 20:56:51,656 train 350 1.088428e-02 0.286387
2019-11-12 20:56:59,562 train 400 1.088482e-02 0.280175
2019-11-12 20:57:07,313 train 450 1.092114e-02 0.276821
2019-11-12 20:57:14,900 train 500 1.091453e-02 0.283541
2019-11-12 20:57:22,476 train 550 1.088588e-02 0.286431
2019-11-12 20:57:30,056 train 600 1.089889e-02 0.286885
2019-11-12 20:57:37,634 train 650 1.088429e-02 0.287840
2019-11-12 20:57:45,211 train 700 1.089081e-02 0.284578
2019-11-12 20:57:52,787 train 750 1.090663e-02 0.281304
2019-11-12 20:58:00,366 train 800 1.090099e-02 0.283518
2019-11-12 20:58:07,942 train 850 1.089868e-02 0.278847
2019-11-12 20:58:10,207 training loss; R2: 1.089961e-02 0.280323
2019-11-12 20:58:10,494 valid 000 2.009361e+01 -4659.997196
2019-11-12 20:58:12,185 valid 050 2.015392e+01 -11583.154649
2019-11-12 20:58:13,710 validation loss; R2: 2.016031e+01 -10001.616010
2019-11-12 20:58:13,728 epoch 17 lr 1.000000e-03
2019-11-12 20:58:14,128 train 000 1.036312e-02 0.434798
2019-11-12 20:58:21,797 train 050 1.093193e-02 0.236145
2019-11-12 20:58:29,417 train 100 1.095246e-02 0.274743
2019-11-12 20:58:37,077 train 150 1.089305e-02 0.286772
2019-11-12 20:58:44,724 train 200 1.092217e-02 0.292613
2019-11-12 20:58:52,546 train 250 1.089921e-02 0.297610
2019-11-12 20:59:00,510 train 300 1.088933e-02 0.302686
2019-11-12 20:59:08,366 train 350 1.086690e-02 0.303244
2019-11-12 20:59:16,179 train 400 1.087661e-02 0.299985
2019-11-12 20:59:24,164 train 450 1.084503e-02 0.297397
2019-11-12 20:59:31,891 train 500 1.083571e-02 0.298847
2019-11-12 20:59:39,615 train 550 1.082739e-02 0.302663
2019-11-12 20:59:47,258 train 600 1.082379e-02 0.302665
2019-11-12 20:59:54,887 train 650 1.082309e-02 0.303355
2019-11-12 21:00:02,506 train 700 1.081496e-02 0.304471
2019-11-12 21:00:10,159 train 750 1.081070e-02 0.298061
2019-11-12 21:00:17,807 train 800 1.081371e-02 0.297671
2019-11-12 21:00:25,456 train 850 1.079968e-02 0.297732
2019-11-12 21:00:27,742 training loss; R2: 1.079695e-02 0.297376
2019-11-12 21:00:28,045 valid 000 9.329595e+00 -508.884231
2019-11-12 21:00:29,749 valid 050 9.336883e+00 -860.549525
2019-11-12 21:00:31,269 validation loss; R2: 9.324997e+00 -845.966243
2019-11-12 21:00:31,291 epoch 18 lr 1.000000e-03
2019-11-12 21:00:31,709 train 000 1.106734e-02 0.288296
2019-11-12 21:00:39,595 train 050 1.056184e-02 -0.654231
2019-11-12 21:00:47,589 train 100 1.050833e-02 -0.157845
2019-11-12 21:00:55,527 train 150 1.058948e-02 -0.007367
2019-11-12 21:01:03,172 train 200 1.062828e-02 0.055918
2019-11-12 21:01:10,797 train 250 1.062878e-02 0.094273
2019-11-12 21:01:18,458 train 300 1.064391e-02 0.126784
2019-11-12 21:01:26,079 train 350 1.066778e-02 0.151752
2019-11-12 21:01:33,698 train 400 1.067440e-02 0.170479
2019-11-12 21:01:41,318 train 450 1.067437e-02 0.182722
2019-11-12 21:01:48,943 train 500 1.064460e-02 0.188988
2019-11-12 21:01:56,566 train 550 1.066630e-02 0.196051
2019-11-12 21:02:04,185 train 600 1.065957e-02 0.204666
2019-11-12 21:02:11,820 train 650 1.064460e-02 0.214571
2019-11-12 21:02:19,455 train 700 1.066388e-02 0.223213
2019-11-12 21:02:27,079 train 750 1.066958e-02 0.228393
2019-11-12 21:02:34,706 train 800 1.066070e-02 0.223062
2019-11-12 21:02:42,322 train 850 1.066785e-02 0.158433
2019-11-12 21:02:44,605 training loss; R2: 1.067013e-02 0.161253
2019-11-12 21:02:44,898 valid 000 1.545244e-01 -6.837363
2019-11-12 21:02:46,574 valid 050 1.572100e-01 -8.075641
2019-11-12 21:02:48,101 validation loss; R2: 1.556972e-01 -7.930769
2019-11-12 21:02:48,119 epoch 19 lr 1.000000e-03
2019-11-12 21:02:48,536 train 000 8.771482e-03 0.475007
2019-11-12 21:02:56,298 train 050 1.049635e-02 0.357139
2019-11-12 21:03:04,014 train 100 1.049523e-02 0.342391
2019-11-12 21:03:11,730 train 150 1.058036e-02 0.326127
2019-11-12 21:03:19,569 train 200 1.063779e-02 0.318828
2019-11-12 21:03:27,439 train 250 1.061345e-02 0.322869
2019-11-12 21:03:35,484 train 300 1.054586e-02 0.320262
2019-11-12 21:03:43,179 train 350 1.055429e-02 0.314048
2019-11-12 21:03:50,880 train 400 1.051368e-02 0.310189
2019-11-12 21:03:58,968 train 450 1.053680e-02 0.304586
2019-11-12 21:04:06,948 train 500 1.053915e-02 0.309894
2019-11-12 21:04:14,665 train 550 1.051084e-02 0.316228
2019-11-12 21:04:22,457 train 600 1.050202e-02 0.303645
2019-11-12 21:04:30,195 train 650 1.050328e-02 0.300192
2019-11-12 21:04:38,216 train 700 1.050872e-02 0.296668
2019-11-12 21:04:46,150 train 750 1.052851e-02 0.300967
2019-11-12 21:04:54,240 train 800 1.052052e-02 0.303814
2019-11-12 21:05:02,246 train 850 1.050880e-02 0.304190
2019-11-12 21:05:04,538 training loss; R2: 1.050914e-02 0.305061
2019-11-12 21:05:04,830 valid 000 3.981366e-01 -29.626108
2019-11-12 21:05:06,541 valid 050 4.074099e-01 -33.097689
2019-11-12 21:05:08,080 validation loss; R2: 4.080989e-01 -31.961122
