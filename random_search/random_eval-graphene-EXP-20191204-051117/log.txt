2019-12-04 05:11:17,581 gpu device = 1
2019-12-04 05:11:17,581 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=96, cutout=False, cutout_length=16, data='../data', dataset='graphene', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name=None, gpu=1, grad_clip=5, gz_dtree=False, init_channels=36, layers=8, learning_rate=0.002, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-graphene-EXP-20191204-051117', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-12-04 05:11:20,785 param size = 1.009405MB
2019-12-04 05:11:20,788 epoch 0 lr 2.000000e-03
2019-12-04 05:11:23,384 train 000 3.924889e-01 -4.335817
2019-12-04 05:11:33,829 train 050 9.956827e-02 -2.968943
2019-12-04 05:11:44,217 train 100 6.279848e-02 -1.423869
2019-12-04 05:11:54,606 train 150 4.961747e-02 -0.852155
2019-12-04 05:12:04,998 train 200 4.213278e-02 -0.555374
2019-12-04 05:12:10,685 training loss; R2: 3.975348e-02 -0.453943
2019-12-04 05:12:10,820 valid 000 1.316328e-02 0.671647
2019-12-04 05:12:12,225 validation loss; R2: 1.269691e-02 0.596947
2019-12-04 05:12:12,253 epoch 1 lr 2.000000e-03
2019-12-04 05:12:12,631 train 000 1.772846e-02 0.562659
2019-12-04 05:12:22,750 train 050 1.541195e-02 0.493046
2019-12-04 05:12:32,873 train 100 1.471645e-02 0.494291
2019-12-04 05:12:42,990 train 150 1.397435e-02 0.536035
2019-12-04 05:12:53,116 train 200 1.327450e-02 0.562086
2019-12-04 05:12:57,664 training loss; R2: 1.313204e-02 0.567969
2019-12-04 05:12:57,789 valid 000 4.227143e-03 0.788885
2019-12-04 05:12:58,885 validation loss; R2: 5.761852e-03 0.812444
2019-12-04 05:12:58,904 epoch 2 lr 2.000000e-03
2019-12-04 05:12:59,187 train 000 8.445222e-03 0.504622
2019-12-04 05:13:09,318 train 050 9.031041e-03 0.707753
2019-12-04 05:13:19,445 train 100 9.165192e-03 0.697613
2019-12-04 05:13:29,573 train 150 9.422356e-03 0.686525
2019-12-04 05:13:39,702 train 200 9.568607e-03 0.679131
2019-12-04 05:13:44,257 training loss; R2: 9.363257e-03 0.686028
2019-12-04 05:13:44,381 valid 000 5.196406e-03 0.844012
2019-12-04 05:13:45,477 validation loss; R2: 4.570720e-03 0.850927
2019-12-04 05:13:45,496 epoch 3 lr 2.000000e-03
2019-12-04 05:13:45,780 train 000 6.975021e-03 0.823588
2019-12-04 05:13:55,913 train 050 7.572334e-03 0.738277
2019-12-04 05:14:06,042 train 100 8.492377e-03 0.720745
2019-12-04 05:14:16,171 train 150 8.144002e-03 0.727111
2019-12-04 05:14:26,301 train 200 7.955894e-03 0.726515
2019-12-04 05:14:30,856 training loss; R2: 7.803566e-03 0.728809
2019-12-04 05:14:30,982 valid 000 2.448506e-03 0.791721
2019-12-04 05:14:32,078 validation loss; R2: 3.927044e-03 0.867442
2019-12-04 05:14:32,097 epoch 4 lr 2.000000e-03
2019-12-04 05:14:32,381 train 000 4.558664e-03 0.886314
2019-12-04 05:14:42,518 train 050 7.414558e-03 0.769219
2019-12-04 05:14:52,651 train 100 7.616073e-03 0.753645
2019-12-04 05:15:02,781 train 150 7.090508e-03 0.764623
2019-12-04 05:15:13,112 train 200 6.923556e-03 0.771355
2019-12-04 05:15:17,784 training loss; R2: 6.913261e-03 0.772937
2019-12-04 05:15:17,909 valid 000 8.441656e-03 0.765854
2019-12-04 05:15:19,007 validation loss; R2: 7.581116e-03 0.749951
2019-12-04 05:15:19,026 epoch 5 lr 2.000000e-03
2019-12-04 05:15:19,317 train 000 2.210728e-02 0.746193
2019-12-04 05:15:29,697 train 050 6.624177e-03 0.780629
2019-12-04 05:15:40,082 train 100 6.456029e-03 0.788206
2019-12-04 05:15:50,466 train 150 6.261476e-03 0.788119
2019-12-04 05:16:00,849 train 200 6.074229e-03 0.797224
2019-12-04 05:16:05,519 training loss; R2: 6.183697e-03 0.792658
2019-12-04 05:16:05,644 valid 000 2.175435e-03 0.921770
2019-12-04 05:16:06,741 validation loss; R2: 3.793704e-03 0.865806
2019-12-04 05:16:06,761 epoch 6 lr 2.000000e-03
2019-12-04 05:16:07,052 train 000 4.855179e-03 0.756257
2019-12-04 05:16:17,437 train 050 5.952042e-03 0.797273
2019-12-04 05:16:27,820 train 100 5.533475e-03 0.812199
2019-12-04 05:16:38,201 train 150 5.696064e-03 0.806192
2019-12-04 05:16:48,579 train 200 5.641695e-03 0.810889
2019-12-04 05:16:53,249 training loss; R2: 5.661741e-03 0.809802
2019-12-04 05:16:53,375 valid 000 8.045626e-03 0.564183
2019-12-04 05:16:54,472 validation loss; R2: 6.623543e-03 0.770687
2019-12-04 05:16:54,497 epoch 7 lr 2.000000e-03
2019-12-04 05:16:54,789 train 000 4.319729e-03 0.845110
2019-12-04 05:17:05,171 train 050 4.936050e-03 0.822914
2019-12-04 05:17:15,550 train 100 5.539907e-03 0.805355
2019-12-04 05:17:25,923 train 150 5.569083e-03 0.810372
2019-12-04 05:17:36,300 train 200 5.398609e-03 0.814762
2019-12-04 05:17:40,969 training loss; R2: 5.396805e-03 0.812869
2019-12-04 05:17:41,101 valid 000 2.093937e-03 0.898093
2019-12-04 05:17:42,199 validation loss; R2: 4.042830e-03 0.863610
2019-12-04 05:17:42,218 epoch 8 lr 2.000000e-03
2019-12-04 05:17:42,508 train 000 1.115913e-02 0.786466
2019-12-04 05:17:52,885 train 050 6.235391e-03 0.804650
2019-12-04 05:18:03,261 train 100 5.285602e-03 0.821217
2019-12-04 05:18:13,642 train 150 5.544619e-03 0.820044
2019-12-04 05:18:24,019 train 200 5.602942e-03 0.813480
2019-12-04 05:18:28,688 training loss; R2: 5.505179e-03 0.814845
2019-12-04 05:18:28,813 valid 000 3.041408e-03 0.739359
2019-12-04 05:18:29,911 validation loss; R2: 3.843434e-03 0.866375
2019-12-04 05:18:29,930 epoch 9 lr 2.000000e-03
2019-12-04 05:18:30,220 train 000 4.604902e-03 0.827889
2019-12-04 05:18:40,599 train 050 4.739919e-03 0.836828
2019-12-04 05:18:50,976 train 100 4.755462e-03 0.837951
2019-12-04 05:19:01,354 train 150 4.881258e-03 0.832930
2019-12-04 05:19:11,732 train 200 4.728962e-03 0.836008
2019-12-04 05:19:16,401 training loss; R2: 4.785506e-03 0.835667
2019-12-04 05:19:16,526 valid 000 1.664391e-03 0.909971
2019-12-04 05:19:17,623 validation loss; R2: 3.328234e-03 0.889349
2019-12-04 05:19:17,642 epoch 10 lr 2.000000e-03
2019-12-04 05:19:17,932 train 000 4.746465e-03 0.900986
2019-12-04 05:19:28,311 train 050 4.949295e-03 0.831365
2019-12-04 05:19:38,473 train 100 4.701253e-03 0.836509
2019-12-04 05:19:48,586 train 150 4.973482e-03 0.830231
2019-12-04 05:19:58,694 train 200 4.964761e-03 0.833363
2019-12-04 05:20:03,245 training loss; R2: 4.922131e-03 0.834572
2019-12-04 05:20:03,374 valid 000 1.749874e-03 0.925621
2019-12-04 05:20:04,470 validation loss; R2: 2.816367e-03 0.906048
2019-12-04 05:20:04,488 epoch 11 lr 2.000000e-03
2019-12-04 05:20:04,770 train 000 3.714705e-03 0.881890
2019-12-04 05:20:14,889 train 050 4.443906e-03 0.853051
2019-12-04 05:20:25,013 train 100 4.536145e-03 0.847541
2019-12-04 05:20:35,128 train 150 4.658573e-03 0.840672
2019-12-04 05:20:45,248 train 200 4.669773e-03 0.837133
2019-12-04 05:20:49,798 training loss; R2: 4.638796e-03 0.838104
2019-12-04 05:20:49,923 valid 000 4.452525e-03 0.595943
2019-12-04 05:20:51,019 validation loss; R2: 5.514429e-03 0.810324
2019-12-04 05:20:51,037 epoch 12 lr 2.000000e-03
2019-12-04 05:20:51,335 train 000 3.169949e-03 0.915564
2019-12-04 05:21:01,447 train 050 4.713076e-03 0.839545
2019-12-04 05:21:11,558 train 100 4.443317e-03 0.844495
2019-12-04 05:21:21,666 train 150 4.353235e-03 0.849325
2019-12-04 05:21:31,773 train 200 4.373777e-03 0.849775
2019-12-04 05:21:36,318 training loss; R2: 4.423167e-03 0.847920
2019-12-04 05:21:36,447 valid 000 4.743897e-02 -0.198841
2019-12-04 05:21:37,544 validation loss; R2: 4.809264e-02 -0.667690
2019-12-04 05:21:37,562 epoch 13 lr 2.000000e-03
2019-12-04 05:21:37,852 train 000 4.336863e-03 0.912694
2019-12-04 05:21:47,965 train 050 4.396057e-03 0.843679
2019-12-04 05:21:58,075 train 100 4.249279e-03 0.844505
2019-12-04 05:22:08,180 train 150 4.212726e-03 0.852057
2019-12-04 05:22:18,300 train 200 4.258948e-03 0.852914
2019-12-04 05:22:22,849 training loss; R2: 4.305790e-03 0.851736
2019-12-04 05:22:22,976 valid 000 8.672485e-03 0.726882
2019-12-04 05:22:24,071 validation loss; R2: 1.009238e-02 0.673922
2019-12-04 05:22:24,089 epoch 14 lr 2.000000e-03
2019-12-04 05:22:24,377 train 000 4.009618e-03 0.719269
2019-12-04 05:22:34,478 train 050 4.738268e-03 0.832585
2019-12-04 05:22:44,575 train 100 4.529630e-03 0.844075
2019-12-04 05:22:54,672 train 150 4.433565e-03 0.846847
2019-12-04 05:23:04,774 train 200 4.431087e-03 0.847104
2019-12-04 05:23:09,319 training loss; R2: 4.516213e-03 0.846668
2019-12-04 05:23:09,449 valid 000 1.496579e-01 -4.138052
2019-12-04 05:23:10,544 validation loss; R2: 1.522410e-01 -4.860350
2019-12-04 05:23:10,562 epoch 15 lr 2.000000e-03
2019-12-04 05:23:10,846 train 000 3.816281e-03 0.857650
2019-12-04 05:23:20,949 train 050 4.412881e-03 0.855592
2019-12-04 05:23:31,054 train 100 4.128985e-03 0.857574
2019-12-04 05:23:41,163 train 150 4.188429e-03 0.856990
2019-12-04 05:23:51,268 train 200 4.167189e-03 0.858313
2019-12-04 05:23:55,811 training loss; R2: 4.138714e-03 0.859246
2019-12-04 05:23:55,939 valid 000 1.055567e-01 -2.417034
2019-12-04 05:23:57,033 validation loss; R2: 9.896092e-02 -2.569924
2019-12-04 05:23:57,051 epoch 16 lr 2.000000e-03
2019-12-04 05:23:57,333 train 000 5.095380e-03 0.906415
2019-12-04 05:24:07,430 train 050 3.691353e-03 0.855823
2019-12-04 05:24:17,520 train 100 4.003066e-03 0.846258
2019-12-04 05:24:27,605 train 150 4.214898e-03 0.844711
2019-12-04 05:24:37,692 train 200 4.162692e-03 0.849500
2019-12-04 05:24:42,225 training loss; R2: 4.155709e-03 0.851906
2019-12-04 05:24:42,352 valid 000 7.663277e-01 -36.492816
2019-12-04 05:24:43,447 validation loss; R2: 7.538347e-01 -27.043430
2019-12-04 05:24:43,465 epoch 17 lr 2.000000e-03
2019-12-04 05:24:43,746 train 000 4.072307e-03 0.842703
2019-12-04 05:24:53,833 train 050 4.182589e-03 0.858223
2019-12-04 05:25:03,913 train 100 4.363372e-03 0.856260
2019-12-04 05:25:13,989 train 150 4.200839e-03 0.859905
2019-12-04 05:25:24,066 train 200 4.050118e-03 0.861439
2019-12-04 05:25:28,596 training loss; R2: 4.038754e-03 0.862680
2019-12-04 05:25:28,723 valid 000 4.517741e-01 -6.996736
2019-12-04 05:25:29,818 validation loss; R2: 3.747496e-01 -12.786599
2019-12-04 05:25:29,836 epoch 18 lr 2.000000e-03
2019-12-04 05:25:30,122 train 000 2.889054e-03 0.911154
2019-12-04 05:25:40,204 train 050 5.330519e-03 0.814988
2019-12-04 05:25:50,274 train 100 4.804444e-03 0.837496
2019-12-04 05:26:00,339 train 150 4.567589e-03 0.843448
2019-12-04 05:26:10,406 train 200 4.508247e-03 0.844734
2019-12-04 05:26:14,935 training loss; R2: 4.454144e-03 0.846264
2019-12-04 05:26:15,062 valid 000 2.279447e+01 -413.280689
2019-12-04 05:26:16,156 validation loss; R2: 2.284811e+01 -874.715922
2019-12-04 05:26:16,174 epoch 19 lr 2.000000e-03
2019-12-04 05:26:16,458 train 000 2.354915e-03 0.951065
2019-12-04 05:26:26,519 train 050 3.974607e-03 0.871995
2019-12-04 05:26:36,579 train 100 3.856172e-03 0.866057
2019-12-04 05:26:46,637 train 150 3.804714e-03 0.866088
2019-12-04 05:26:56,689 train 200 3.987911e-03 0.861798
2019-12-04 05:27:01,207 training loss; R2: 4.001196e-03 0.862629
2019-12-04 05:27:01,341 valid 000 3.851805e+01 -1089.036843
2019-12-04 05:27:02,435 validation loss; R2: 3.864994e+01 -1482.865459
