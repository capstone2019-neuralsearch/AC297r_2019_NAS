2019-11-12 11:37:59,318 gpu device = 1
2019-11-12 11:37:59,318 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.2, epochs=10, fc1_size=1024, fc2_size=1024, folder_name='galaxy-zoo-cleaned', gpu=1, grad_clip=5, gz_dtree=False, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-galaxy-zoo-EXP-20191112-113759', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-11-12 11:38:10,769 param size = 0.270645MB
2019-11-12 11:38:10,773 epoch 0 lr 1.000000e-03
2019-11-12 11:38:12,976 train 000 1.879417e-01 -60.730840
2019-11-12 11:38:19,956 train 050 4.690223e-02 -3.832575
2019-11-12 11:38:26,891 train 100 3.684449e-02 -3.721504
2019-11-12 11:38:33,705 train 150 3.299405e-02 -21.088833
2019-11-12 11:38:40,498 train 200 3.095446e-02 -15.887654
2019-11-12 11:38:47,312 train 250 2.969977e-02 -12.751064
2019-11-12 11:38:54,255 train 300 2.865574e-02 -10.653229
2019-11-12 11:39:01,025 train 350 2.783153e-02 -9.151776
2019-11-12 11:39:07,842 train 400 2.714462e-02 -8.022161
2019-11-12 11:39:14,647 train 450 2.655344e-02 -7.194277
2019-11-12 11:39:21,597 train 500 2.604779e-02 -6.477991
2019-11-12 11:39:28,536 train 550 2.562989e-02 -5.891059
2019-11-12 11:39:35,301 train 600 2.525301e-02 -5.397446
2019-11-12 11:39:42,025 train 650 2.491546e-02 -4.979868
2019-11-12 11:39:49,031 train 700 2.460466e-02 -4.620972
2019-11-12 11:39:55,976 train 750 2.431217e-02 -4.310889
2019-11-12 11:40:02,665 train 800 2.407342e-02 -4.038850
2019-11-12 11:40:09,465 train 850 2.381990e-02 -3.797643
2019-11-12 11:40:12,268 training loss; R2: 2.373120e-02 -3.730407
2019-11-12 11:40:12,521 valid 000 1.929216e-02 0.202572
2019-11-12 11:40:14,285 valid 050 1.868713e-02 0.104814
2019-11-12 11:40:15,923 validation loss; R2: 1.841740e-02 0.109481
2019-11-12 11:40:15,945 epoch 1 lr 1.000000e-03
2019-11-12 11:40:16,530 train 000 1.757433e-02 0.208700
2019-11-12 11:40:23,359 train 050 1.968910e-02 0.056151
2019-11-12 11:40:30,241 train 100 1.975622e-02 0.059720
2019-11-12 11:40:36,996 train 150 1.956244e-02 0.039317
2019-11-12 11:40:43,874 train 200 1.945400e-02 0.055911
2019-11-12 11:40:50,754 train 250 1.932228e-02 0.067518
2019-11-12 11:40:57,454 train 300 1.923809e-02 0.073537
2019-11-12 11:41:04,243 train 350 1.924563e-02 0.067738
2019-11-12 11:41:11,032 train 400 1.917956e-02 0.071243
2019-11-12 11:41:17,840 train 450 1.915959e-02 0.076924
2019-11-12 11:41:24,526 train 500 1.906497e-02 0.081183
2019-11-12 11:41:31,374 train 550 1.893110e-02 0.084133
2019-11-12 11:41:38,111 train 600 1.886702e-02 0.085423
2019-11-12 11:41:44,799 train 650 1.884809e-02 0.088353
2019-11-12 11:41:51,548 train 700 1.877871e-02 0.090541
2019-11-12 11:41:58,128 train 750 1.870229e-02 0.091021
2019-11-12 11:42:04,669 train 800 1.861757e-02 0.093016
2019-11-12 11:42:11,215 train 850 1.854506e-02 0.096106
2019-11-12 11:42:13,224 training loss; R2: 1.851586e-02 0.097032
2019-11-12 11:42:13,494 valid 000 1.693546e-02 0.189360
2019-11-12 11:42:15,165 valid 050 1.487457e-02 0.155632
2019-11-12 11:42:16,708 validation loss; R2: 1.521104e-02 0.175746
2019-11-12 11:42:16,734 epoch 2 lr 1.000000e-03
2019-11-12 11:42:17,075 train 000 1.640048e-02 0.210860
2019-11-12 11:42:23,943 train 050 1.709762e-02 0.048767
2019-11-12 11:42:30,650 train 100 1.705056e-02 0.105672
2019-11-12 11:42:37,215 train 150 1.705054e-02 0.120626
2019-11-12 11:42:43,785 train 200 1.702061e-02 0.126086
2019-11-12 11:42:50,477 train 250 1.694532e-02 0.127360
2019-11-12 11:42:57,091 train 300 1.677220e-02 0.134683
2019-11-12 11:43:03,779 train 350 1.667009e-02 0.142421
2019-11-12 11:43:10,329 train 400 1.661401e-02 0.145382
2019-11-12 11:43:16,869 train 450 1.655875e-02 0.146447
2019-11-12 11:43:23,500 train 500 1.654727e-02 0.147656
2019-11-12 11:43:30,052 train 550 1.648873e-02 0.149793
2019-11-12 11:43:36,680 train 600 1.643969e-02 0.149610
2019-11-12 11:43:43,221 train 650 1.636527e-02 0.152262
2019-11-12 11:43:49,773 train 700 1.631213e-02 0.155703
2019-11-12 11:43:56,429 train 750 1.625371e-02 0.158653
2019-11-12 11:44:02,960 train 800 1.619989e-02 0.117891
2019-11-12 11:44:09,500 train 850 1.615871e-02 0.117880
2019-11-12 11:44:11,494 training loss; R2: 1.614061e-02 0.119157
2019-11-12 11:44:11,770 valid 000 1.199404e-02 0.298642
2019-11-12 11:44:13,516 valid 050 1.300358e-02 0.288135
2019-11-12 11:44:15,086 validation loss; R2: 1.297618e-02 0.267632
2019-11-12 11:44:15,112 epoch 3 lr 1.000000e-03
2019-11-12 11:44:15,511 train 000 1.228274e-02 0.322420
2019-11-12 11:44:22,439 train 050 1.498356e-02 0.213300
2019-11-12 11:44:29,049 train 100 1.495402e-02 0.220112
2019-11-12 11:44:35,648 train 150 1.497130e-02 0.211647
2019-11-12 11:44:42,236 train 200 1.494440e-02 0.212306
2019-11-12 11:44:48,828 train 250 1.485363e-02 0.058682
2019-11-12 11:44:55,424 train 300 1.483937e-02 0.081950
2019-11-12 11:45:02,012 train 350 1.484890e-02 0.103384
2019-11-12 11:45:08,617 train 400 1.480550e-02 0.118318
2019-11-12 11:45:15,340 train 450 1.478025e-02 0.123563
2019-11-12 11:45:21,948 train 500 1.474977e-02 0.126826
2019-11-12 11:45:28,533 train 550 1.470352e-02 0.132477
2019-11-12 11:45:35,280 train 600 1.464689e-02 0.137823
2019-11-12 11:45:41,864 train 650 1.460593e-02 0.145144
2019-11-12 11:45:48,540 train 700 1.457486e-02 0.150679
2019-11-12 11:45:55,260 train 750 1.454533e-02 0.154780
2019-11-12 11:46:02,005 train 800 1.451556e-02 0.144993
2019-11-12 11:46:08,591 train 850 1.448057e-02 0.151329
2019-11-12 11:46:10,603 training loss; R2: 1.446486e-02 0.153380
2019-11-12 11:46:10,877 valid 000 1.094465e-02 0.312829
2019-11-12 11:46:12,614 valid 050 1.285324e-02 0.223518
2019-11-12 11:46:14,193 validation loss; R2: 1.271450e-02 0.224051
2019-11-12 11:46:14,210 epoch 4 lr 1.000000e-03
2019-11-12 11:46:14,569 train 000 1.357534e-02 -3.128917
2019-11-12 11:46:21,165 train 050 1.382704e-02 0.169828
2019-11-12 11:46:27,838 train 100 1.352752e-02 0.203319
2019-11-12 11:46:34,636 train 150 1.359443e-02 0.208230
2019-11-12 11:46:41,542 train 200 1.367839e-02 0.200415
2019-11-12 11:46:48,225 train 250 1.363235e-02 0.208094
2019-11-12 11:46:55,091 train 300 1.364994e-02 0.213752
2019-11-12 11:47:02,015 train 350 1.364991e-02 0.192322
2019-11-12 11:47:08,811 train 400 1.364254e-02 0.196898
2019-11-12 11:47:15,540 train 450 1.364494e-02 0.204725
2019-11-12 11:47:22,212 train 500 1.360398e-02 0.210774
2019-11-12 11:47:28,918 train 550 1.357210e-02 0.212279
2019-11-12 11:47:35,616 train 600 1.357491e-02 0.215255
2019-11-12 11:47:42,330 train 650 1.353638e-02 0.216802
2019-11-12 11:47:48,976 train 700 1.349849e-02 0.219974
2019-11-12 11:47:55,681 train 750 1.349139e-02 0.224050
2019-11-12 11:48:02,600 train 800 1.345804e-02 0.223933
2019-11-12 11:48:09,398 train 850 1.346571e-02 0.225125
2019-11-12 11:48:11,385 training loss; R2: 1.345381e-02 0.226109
2019-11-12 11:48:11,665 valid 000 1.242643e-02 0.358426
2019-11-12 11:48:13,396 valid 050 1.123208e-02 0.355477
2019-11-12 11:48:14,953 validation loss; R2: 1.138978e-02 0.341775
2019-11-12 11:48:14,975 epoch 5 lr 1.000000e-03
2019-11-12 11:48:15,384 train 000 1.255823e-02 0.171685
2019-11-12 11:48:22,355 train 050 1.308818e-02 0.266285
2019-11-12 11:48:29,098 train 100 1.325119e-02 0.258575
2019-11-12 11:48:35,766 train 150 1.311906e-02 0.256384
2019-11-12 11:48:42,471 train 200 1.308455e-02 0.256629
2019-11-12 11:48:49,221 train 250 1.307248e-02 0.250539
2019-11-12 11:48:56,044 train 300 1.308512e-02 0.252626
2019-11-12 11:49:02,687 train 350 1.303679e-02 0.255485
2019-11-12 11:49:09,559 train 400 1.299914e-02 0.228347
2019-11-12 11:49:16,430 train 450 1.297456e-02 0.228870
2019-11-12 11:49:23,265 train 500 1.296347e-02 0.231084
2019-11-12 11:49:30,153 train 550 1.295350e-02 0.234650
2019-11-12 11:49:37,099 train 600 1.295646e-02 0.235840
2019-11-12 11:49:43,906 train 650 1.296728e-02 0.236479
2019-11-12 11:49:50,621 train 700 1.295909e-02 0.238599
2019-11-12 11:49:57,308 train 750 1.295904e-02 0.216166
2019-11-12 11:50:03,952 train 800 1.295047e-02 0.218960
2019-11-12 11:50:10,621 train 850 1.294724e-02 0.221014
2019-11-12 11:50:12,716 training loss; R2: 1.294665e-02 0.221351
2019-11-12 11:50:13,015 valid 000 1.266784e-02 0.369018
2019-11-12 11:50:14,726 valid 050 1.137632e-02 0.360217
2019-11-12 11:50:16,281 validation loss; R2: 1.116847e-02 0.359363
2019-11-12 11:50:16,305 epoch 6 lr 1.000000e-03
2019-11-12 11:50:16,708 train 000 1.498881e-02 0.310967
2019-11-12 11:50:23,727 train 050 1.265581e-02 0.268731
2019-11-12 11:50:30,638 train 100 1.265642e-02 0.284477
2019-11-12 11:50:37,494 train 150 1.274590e-02 0.268821
2019-11-12 11:50:44,340 train 200 1.271985e-02 0.266646
2019-11-12 11:50:51,191 train 250 1.262182e-02 0.269309
2019-11-12 11:50:58,047 train 300 1.262259e-02 0.269000
2019-11-12 11:51:04,887 train 350 1.259908e-02 0.271438
2019-11-12 11:51:11,728 train 400 1.258213e-02 0.270751
2019-11-12 11:51:18,567 train 450 1.258334e-02 0.272690
2019-11-12 11:51:25,425 train 500 1.258357e-02 0.271280
2019-11-12 11:51:32,276 train 550 1.259275e-02 0.266440
2019-11-12 11:51:39,109 train 600 1.256422e-02 0.262906
2019-11-12 11:51:45,949 train 650 1.253794e-02 0.154212
2019-11-12 11:51:52,783 train 700 1.254427e-02 0.163527
2019-11-12 11:51:59,621 train 750 1.253344e-02 0.168609
2019-11-12 11:52:06,460 train 800 1.250535e-02 0.177144
2019-11-12 11:52:13,290 train 850 1.248927e-02 0.182977
2019-11-12 11:52:15,334 training loss; R2: 1.249314e-02 0.184735
2019-11-12 11:52:15,613 valid 000 3.825074e-02 -0.527276
2019-11-12 11:52:17,299 valid 050 3.732748e-02 -0.775283
2019-11-12 11:52:18,818 validation loss; R2: 3.730538e-02 -0.783395
2019-11-12 11:52:18,836 epoch 7 lr 1.000000e-03
2019-11-12 11:52:19,195 train 000 1.248601e-02 0.365858
2019-11-12 11:52:26,005 train 050 1.218123e-02 0.288006
2019-11-12 11:52:32,555 train 100 1.233272e-02 0.291938
2019-11-12 11:52:39,160 train 150 1.239283e-02 0.279641
2019-11-12 11:52:45,703 train 200 1.232348e-02 0.279395
2019-11-12 11:52:52,287 train 250 1.226785e-02 0.280654
2019-11-12 11:52:58,823 train 300 1.223489e-02 0.276901
2019-11-12 11:53:05,367 train 350 1.220794e-02 0.278865
2019-11-12 11:53:11,904 train 400 1.218881e-02 0.275153
2019-11-12 11:53:18,438 train 450 1.220716e-02 0.274895
2019-11-12 11:53:24,973 train 500 1.221551e-02 0.270694
2019-11-12 11:53:31,599 train 550 1.221718e-02 0.272603
2019-11-12 11:53:38,140 train 600 1.220193e-02 0.269307
2019-11-12 11:53:44,774 train 650 1.219640e-02 0.270454
2019-11-12 11:53:51,310 train 700 1.218210e-02 0.273326
2019-11-12 11:53:57,861 train 750 1.217959e-02 0.274058
2019-11-12 11:54:04,399 train 800 1.216943e-02 0.269470
2019-11-12 11:54:10,945 train 850 1.215737e-02 0.273058
2019-11-12 11:54:12,897 training loss; R2: 1.216205e-02 0.274345
2019-11-12 11:54:13,186 valid 000 2.879037e-01 -21.335548
2019-11-12 11:54:14,860 valid 050 3.057089e-01 -25.163743
2019-11-12 11:54:16,366 validation loss; R2: 3.044311e-01 -26.378331
2019-11-12 11:54:16,390 epoch 8 lr 1.000000e-03
2019-11-12 11:54:16,802 train 000 1.035613e-02 0.405736
2019-11-12 11:54:23,873 train 050 1.181442e-02 0.316591
2019-11-12 11:54:30,564 train 100 1.183068e-02 0.310790
2019-11-12 11:54:37,251 train 150 1.191794e-02 0.274391
2019-11-12 11:54:44,031 train 200 1.198266e-02 0.276299
2019-11-12 11:54:51,002 train 250 1.202817e-02 0.269675
2019-11-12 11:54:58,009 train 300 1.207060e-02 0.274152
2019-11-12 11:55:04,830 train 350 1.208073e-02 0.277380
2019-11-12 11:55:11,849 train 400 1.202511e-02 0.280149
2019-11-12 11:55:18,777 train 450 1.199795e-02 0.282568
2019-11-12 11:55:25,671 train 500 1.199764e-02 0.285527
2019-11-12 11:55:32,384 train 550 1.200331e-02 0.288045
2019-11-12 11:55:39,301 train 600 1.200608e-02 0.287884
2019-11-12 11:55:46,223 train 650 1.196561e-02 0.276590
2019-11-12 11:55:53,255 train 700 1.193829e-02 0.278835
2019-11-12 11:56:00,344 train 750 1.190956e-02 0.282349
2019-11-12 11:56:07,445 train 800 1.189833e-02 0.285015
2019-11-12 11:56:14,297 train 850 1.188881e-02 0.288142
2019-11-12 11:56:16,419 training loss; R2: 1.189209e-02 0.288041
2019-11-12 11:56:16,712 valid 000 3.958403e-01 -25.333483
2019-11-12 11:56:18,386 valid 050 3.973242e-01 -43.482943
2019-11-12 11:56:19,914 validation loss; R2: 3.976922e-01 -45.104708
2019-11-12 11:56:19,936 epoch 9 lr 1.000000e-03
2019-11-12 11:56:20,415 train 000 9.491510e-03 0.294571
2019-11-12 11:56:27,406 train 050 1.114883e-02 0.313160
2019-11-12 11:56:34,420 train 100 1.133133e-02 0.324926
2019-11-12 11:56:41,466 train 150 1.148971e-02 0.321584
2019-11-12 11:56:48,318 train 200 1.145800e-02 0.306106
2019-11-12 11:56:55,404 train 250 1.149934e-02 0.302011
2019-11-12 11:57:02,533 train 300 1.145936e-02 0.306471
2019-11-12 11:57:09,592 train 350 1.147389e-02 0.309790
2019-11-12 11:57:16,441 train 400 1.148752e-02 0.308204
2019-11-12 11:57:23,324 train 450 1.148531e-02 0.308954
2019-11-12 11:57:30,173 train 500 1.148020e-02 0.303396
2019-11-12 11:57:37,077 train 550 1.144944e-02 0.305943
2019-11-12 11:57:43,843 train 600 1.142680e-02 0.275485
2019-11-12 11:57:50,850 train 650 1.141303e-02 0.277825
2019-11-12 11:57:57,813 train 700 1.143829e-02 0.282356
2019-11-12 11:58:04,768 train 750 1.145382e-02 0.283151
2019-11-12 11:58:11,857 train 800 1.146871e-02 0.282271
2019-11-12 11:58:18,997 train 850 1.147600e-02 0.285280
2019-11-12 11:58:21,083 training loss; R2: 1.147920e-02 0.285849
2019-11-12 11:58:21,346 valid 000 4.821265e-01 -150.594049
2019-11-12 11:58:23,043 valid 050 4.862689e-01 -74.811204
2019-11-12 11:58:24,561 validation loss; R2: 4.852121e-01 -69.883667
