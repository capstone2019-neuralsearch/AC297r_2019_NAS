2019-11-12 12:26:36,795 gpu device = 1
2019-11-12 12:26:36,795 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name='galaxy-zoo-cleaned', gpu=1, grad_clip=5, gz_dtree=False, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-galaxy-zoo-EXP-20191112-122636', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-11-12 12:26:48,338 param size = 0.225557MB
2019-11-12 12:26:48,341 epoch 0 lr 1.000000e-03
2019-11-12 12:26:50,545 train 000 6.790664e-01 -221.568901
2019-11-12 12:26:56,592 train 050 5.691807e-02 -11.612847
2019-11-12 12:27:02,685 train 100 4.234775e-02 -6.317644
2019-11-12 12:27:08,777 train 150 3.663234e-02 -4.433520
2019-11-12 12:27:14,761 train 200 3.346010e-02 -3.395250
2019-11-12 12:27:20,826 train 250 3.127853e-02 -2.783410
2019-11-12 12:27:26,817 train 300 2.972599e-02 -2.366211
2019-11-12 12:27:32,758 train 350 2.852707e-02 -2.056543
2019-11-12 12:27:38,735 train 400 2.757243e-02 -1.810259
2019-11-12 12:27:44,827 train 450 2.670564e-02 -1.632733
2019-11-12 12:27:50,865 train 500 2.597455e-02 -1.473615
2019-11-12 12:27:56,839 train 550 2.530435e-02 -1.346302
2019-11-12 12:28:02,814 train 600 2.473089e-02 -1.233415
2019-11-12 12:28:08,825 train 650 2.422835e-02 -1.138773
2019-11-12 12:28:14,803 train 700 2.376489e-02 -1.060418
2019-11-12 12:28:20,752 train 750 2.335169e-02 -0.985166
2019-11-12 12:28:26,708 train 800 2.301323e-02 -0.924058
2019-11-12 12:28:32,663 train 850 2.265421e-02 -0.868342
2019-11-12 12:28:35,109 training loss; R2: 2.255957e-02 -0.858372
2019-11-12 12:28:35,384 valid 000 1.442476e-02 0.232166
2019-11-12 12:28:37,087 valid 050 1.538848e-02 0.092104
2019-11-12 12:28:38,720 validation loss; R2: 1.547933e-02 0.141297
2019-11-12 12:28:38,737 epoch 1 lr 1.000000e-03
2019-11-12 12:28:39,268 train 000 1.657235e-02 0.151423
2019-11-12 12:28:45,244 train 050 1.647458e-02 0.103319
2019-11-12 12:28:51,215 train 100 1.642204e-02 0.085918
2019-11-12 12:28:57,193 train 150 1.634946e-02 0.094471
2019-11-12 12:29:03,164 train 200 1.629105e-02 0.079659
2019-11-12 12:29:09,151 train 250 1.630467e-02 0.084885
2019-11-12 12:29:15,162 train 300 1.628914e-02 0.085705
2019-11-12 12:29:21,151 train 350 1.622777e-02 0.091939
2019-11-12 12:29:27,133 train 400 1.620872e-02 0.094821
2019-11-12 12:29:33,127 train 450 1.612672e-02 0.095181
2019-11-12 12:29:39,079 train 500 1.603386e-02 0.096771
2019-11-12 12:29:45,026 train 550 1.592549e-02 0.098590
2019-11-12 12:29:50,999 train 600 1.587692e-02 0.101889
2019-11-12 12:29:56,944 train 650 1.579139e-02 0.100496
2019-11-12 12:30:02,736 train 700 1.574856e-02 0.105973
2019-11-12 12:30:08,437 train 750 1.566515e-02 0.109926
2019-11-12 12:30:14,227 train 800 1.560949e-02 0.113864
2019-11-12 12:30:20,182 train 850 1.556755e-02 0.115097
2019-11-12 12:30:21,969 training loss; R2: 1.555800e-02 0.116211
2019-11-12 12:30:22,249 valid 000 1.781261e-02 0.154476
2019-11-12 12:30:23,965 valid 050 1.757704e-02 0.177250
2019-11-12 12:30:25,525 validation loss; R2: 1.741248e-02 0.152923
2019-11-12 12:30:25,544 epoch 2 lr 1.000000e-03
2019-11-12 12:30:25,905 train 000 1.490824e-02 0.265966
2019-11-12 12:30:31,757 train 050 1.452492e-02 0.157732
2019-11-12 12:30:37,756 train 100 1.442570e-02 0.167378
2019-11-12 12:30:43,743 train 150 1.433813e-02 0.188333
2019-11-12 12:30:49,732 train 200 1.427850e-02 0.173062
2019-11-12 12:30:55,722 train 250 1.425110e-02 0.180025
2019-11-12 12:31:01,726 train 300 1.427332e-02 0.187998
2019-11-12 12:31:07,707 train 350 1.421315e-02 0.187136
2019-11-12 12:31:13,675 train 400 1.419684e-02 0.181984
2019-11-12 12:31:19,642 train 450 1.414817e-02 0.185852
2019-11-12 12:31:25,515 train 500 1.409603e-02 0.169291
2019-11-12 12:31:31,220 train 550 1.406766e-02 0.167960
2019-11-12 12:31:36,933 train 600 1.405530e-02 0.168906
2019-11-12 12:31:42,656 train 650 1.400018e-02 0.173101
2019-11-12 12:31:48,395 train 700 1.396953e-02 0.174830
2019-11-12 12:31:54,137 train 750 1.393504e-02 0.179237
2019-11-12 12:31:59,873 train 800 1.389953e-02 0.182248
2019-11-12 12:32:05,611 train 850 1.385040e-02 0.136117
2019-11-12 12:32:07,323 training loss; R2: 1.383946e-02 0.137780
2019-11-12 12:32:07,584 valid 000 1.243155e-02 0.340423
2019-11-12 12:32:09,308 valid 050 1.229157e-02 0.062620
2019-11-12 12:32:10,865 validation loss; R2: 1.246762e-02 0.010702
2019-11-12 12:32:10,877 epoch 3 lr 1.000000e-03
2019-11-12 12:32:11,202 train 000 1.197828e-02 0.189679
2019-11-12 12:32:17,147 train 050 1.304051e-02 0.029920
2019-11-12 12:32:23,170 train 100 1.327507e-02 0.094328
2019-11-12 12:32:29,214 train 150 1.326581e-02 0.148613
2019-11-12 12:32:35,256 train 200 1.320029e-02 0.118567
2019-11-12 12:32:41,277 train 250 1.324647e-02 0.143530
2019-11-12 12:32:47,303 train 300 1.325096e-02 0.157253
2019-11-12 12:32:53,333 train 350 1.323741e-02 0.167033
2019-11-12 12:32:59,362 train 400 1.320680e-02 0.176686
2019-11-12 12:33:05,390 train 450 1.316890e-02 0.183309
2019-11-12 12:33:11,417 train 500 1.314520e-02 0.188216
2019-11-12 12:33:17,442 train 550 1.310126e-02 0.192628
2019-11-12 12:33:23,463 train 600 1.309392e-02 0.195767
2019-11-12 12:33:29,488 train 650 1.306887e-02 0.197491
2019-11-12 12:33:35,515 train 700 1.300886e-02 0.201080
2019-11-12 12:33:41,489 train 750 1.297698e-02 0.202593
2019-11-12 12:33:47,258 train 800 1.296191e-02 0.199007
2019-11-12 12:33:53,042 train 850 1.295032e-02 0.197741
2019-11-12 12:33:54,769 training loss; R2: 1.295543e-02 0.198734
2019-11-12 12:33:55,033 valid 000 1.119464e-02 0.242676
2019-11-12 12:33:56,769 valid 050 1.159513e-02 0.255540
2019-11-12 12:33:58,344 validation loss; R2: 1.150183e-02 0.273146
2019-11-12 12:33:58,366 epoch 4 lr 1.000000e-03
2019-11-12 12:33:58,735 train 000 1.290285e-02 0.323621
2019-11-12 12:34:04,804 train 050 1.261613e-02 0.264978
2019-11-12 12:34:10,863 train 100 1.258182e-02 0.275912
2019-11-12 12:34:16,928 train 150 1.252225e-02 0.253478
2019-11-12 12:34:22,970 train 200 1.250317e-02 0.246659
2019-11-12 12:34:29,018 train 250 1.246285e-02 0.253817
2019-11-12 12:34:35,053 train 300 1.244851e-02 0.257495
2019-11-12 12:34:41,091 train 350 1.245681e-02 0.250560
2019-11-12 12:34:47,268 train 400 1.245011e-02 0.251708
2019-11-12 12:34:53,298 train 450 1.241423e-02 0.254729
2019-11-12 12:34:59,311 train 500 1.239780e-02 0.255284
2019-11-12 12:35:05,334 train 550 1.238812e-02 0.252670
2019-11-12 12:35:11,374 train 600 1.236532e-02 0.255421
2019-11-12 12:35:17,422 train 650 1.236257e-02 0.256293
2019-11-12 12:35:23,468 train 700 1.235075e-02 0.256841
2019-11-12 12:35:29,500 train 750 1.231550e-02 0.252951
2019-11-12 12:35:35,531 train 800 1.231824e-02 0.252313
2019-11-12 12:35:41,558 train 850 1.230106e-02 0.252287
2019-11-12 12:35:43,370 training loss; R2: 1.230460e-02 0.252944
2019-11-12 12:35:43,655 valid 000 1.177899e-02 -0.010444
2019-11-12 12:35:45,402 valid 050 1.083815e-02 0.297502
2019-11-12 12:35:46,963 validation loss; R2: 1.104659e-02 0.260449
2019-11-12 12:35:46,976 epoch 5 lr 1.000000e-03
2019-11-12 12:35:47,309 train 000 1.185328e-02 0.248504
2019-11-12 12:35:53,324 train 050 1.212106e-02 0.264621
2019-11-12 12:35:59,370 train 100 1.209361e-02 0.149218
2019-11-12 12:36:05,441 train 150 1.211749e-02 0.181953
2019-11-12 12:36:11,496 train 200 1.207498e-02 0.203287
2019-11-12 12:36:17,580 train 250 1.203184e-02 0.223313
2019-11-12 12:36:23,623 train 300 1.202674e-02 0.226298
2019-11-12 12:36:29,677 train 350 1.200646e-02 0.230276
2019-11-12 12:36:35,711 train 400 1.199295e-02 0.231722
2019-11-12 12:36:41,758 train 450 1.195595e-02 0.239974
2019-11-12 12:36:47,800 train 500 1.191830e-02 0.243128
2019-11-12 12:36:53,841 train 550 1.190304e-02 0.246930
2019-11-12 12:36:59,884 train 600 1.189428e-02 0.252202
2019-11-12 12:37:05,932 train 650 1.188906e-02 0.255517
2019-11-12 12:37:11,977 train 700 1.188254e-02 0.250259
2019-11-12 12:37:18,018 train 750 1.186495e-02 0.253636
2019-11-12 12:37:24,068 train 800 1.186591e-02 0.253622
2019-11-12 12:37:30,112 train 850 1.186089e-02 0.255749
2019-11-12 12:37:31,930 training loss; R2: 1.185783e-02 0.256594
2019-11-12 12:37:32,213 valid 000 1.044024e-02 0.382877
2019-11-12 12:37:33,992 valid 050 1.154935e-02 0.303055
2019-11-12 12:37:35,534 validation loss; R2: 1.140724e-02 0.290652
2019-11-12 12:37:35,546 epoch 6 lr 1.000000e-03
2019-11-12 12:37:35,860 train 000 1.242382e-02 0.324051
2019-11-12 12:37:41,698 train 050 1.132926e-02 0.255943
2019-11-12 12:37:47,530 train 100 1.149571e-02 0.278559
2019-11-12 12:37:53,348 train 150 1.148101e-02 0.293140
2019-11-12 12:37:59,156 train 200 1.153270e-02 0.303716
2019-11-12 12:38:04,948 train 250 1.151934e-02 0.306058
2019-11-12 12:38:10,731 train 300 1.151210e-02 0.293719
2019-11-12 12:38:16,506 train 350 1.148671e-02 0.293762
2019-11-12 12:38:22,288 train 400 1.146755e-02 0.292410
2019-11-12 12:38:28,061 train 450 1.145478e-02 0.293059
2019-11-12 12:38:33,830 train 500 1.144358e-02 0.294474
2019-11-12 12:38:39,592 train 550 1.148305e-02 0.286178
2019-11-12 12:38:45,372 train 600 1.148604e-02 0.288643
2019-11-12 12:38:51,143 train 650 1.149455e-02 0.284067
2019-11-12 12:38:56,885 train 700 1.151041e-02 0.282491
2019-11-12 12:39:02,641 train 750 1.150219e-02 0.282537
2019-11-12 12:39:08,592 train 800 1.151180e-02 0.282256
2019-11-12 12:39:14,593 train 850 1.150971e-02 0.283562
2019-11-12 12:39:16,387 training loss; R2: 1.151105e-02 0.283161
2019-11-12 12:39:16,663 valid 000 1.062114e-02 0.413636
2019-11-12 12:39:18,410 valid 050 1.027790e-02 0.361544
2019-11-12 12:39:19,976 validation loss; R2: 1.045194e-02 0.360059
2019-11-12 12:39:19,989 epoch 7 lr 1.000000e-03
2019-11-12 12:39:20,333 train 000 1.096718e-02 0.271551
2019-11-12 12:39:26,352 train 050 1.122214e-02 0.246375
2019-11-12 12:39:32,497 train 100 1.117746e-02 0.266062
2019-11-12 12:39:38,651 train 150 1.122598e-02 0.281279
2019-11-12 12:39:44,507 train 200 1.123434e-02 0.273843
2019-11-12 12:39:50,329 train 250 1.128922e-02 0.278769
2019-11-12 12:39:56,130 train 300 1.122597e-02 0.284560
2019-11-12 12:40:01,943 train 350 1.118713e-02 0.286348
2019-11-12 12:40:07,747 train 400 1.119100e-02 0.282887
2019-11-12 12:40:13,544 train 450 1.121395e-02 0.283181
2019-11-12 12:40:19,350 train 500 1.120935e-02 0.279034
2019-11-12 12:40:25,148 train 550 1.116879e-02 0.280626
2019-11-12 12:40:30,943 train 600 1.118843e-02 0.252429
2019-11-12 12:40:36,741 train 650 1.118944e-02 0.257422
2019-11-12 12:40:42,526 train 700 1.116587e-02 0.261657
2019-11-12 12:40:48,321 train 750 1.115802e-02 0.262623
2019-11-12 12:40:54,115 train 800 1.116621e-02 0.264294
2019-11-12 12:40:59,901 train 850 1.115554e-02 0.267594
2019-11-12 12:41:01,630 training loss; R2: 1.115835e-02 0.267656
2019-11-12 12:41:01,892 valid 000 1.052571e-02 0.411169
2019-11-12 12:41:03,638 valid 050 1.064091e-02 0.369573
2019-11-12 12:41:05,211 validation loss; R2: 1.060718e-02 0.369919
2019-11-12 12:41:05,224 epoch 8 lr 1.000000e-03
2019-11-12 12:41:05,568 train 000 1.362547e-02 0.300192
2019-11-12 12:41:11,385 train 050 1.106602e-02 0.304653
2019-11-12 12:41:17,465 train 100 1.104489e-02 0.289447
2019-11-12 12:41:23,406 train 150 1.113331e-02 0.297436
2019-11-12 12:41:29,389 train 200 1.102113e-02 0.304896
2019-11-12 12:41:35,488 train 250 1.098121e-02 0.266209
2019-11-12 12:41:41,393 train 300 1.095212e-02 0.270205
2019-11-12 12:41:47,250 train 350 1.096049e-02 0.278203
2019-11-12 12:41:53,148 train 400 1.098631e-02 0.237527
2019-11-12 12:41:58,979 train 450 1.098123e-02 0.244316
2019-11-12 12:42:04,823 train 500 1.098487e-02 0.253440
2019-11-12 12:42:10,677 train 550 1.098680e-02 0.237998
2019-11-12 12:42:16,559 train 600 1.096844e-02 0.242586
2019-11-12 12:42:22,452 train 650 1.096840e-02 0.249190
2019-11-12 12:42:28,419 train 700 1.093279e-02 0.253682
2019-11-12 12:42:34,250 train 750 1.093027e-02 0.256805
2019-11-12 12:42:40,316 train 800 1.092861e-02 0.260084
2019-11-12 12:42:46,312 train 850 1.094049e-02 0.260732
2019-11-12 12:42:48,120 training loss; R2: 1.093853e-02 0.261566
2019-11-12 12:42:48,395 valid 000 8.755980e-03 0.465676
2019-11-12 12:42:50,129 valid 050 1.007363e-02 0.282365
2019-11-12 12:42:51,699 validation loss; R2: 9.908156e-03 0.323050
2019-11-12 12:42:51,720 epoch 9 lr 1.000000e-03
2019-11-12 12:42:52,108 train 000 1.174739e-02 0.290617
2019-11-12 12:42:58,260 train 050 1.091784e-02 0.284774
2019-11-12 12:43:04,578 train 100 1.090751e-02 0.305142
2019-11-12 12:43:10,719 train 150 1.091882e-02 0.292313
2019-11-12 12:43:16,664 train 200 1.089759e-02 0.297223
2019-11-12 12:43:22,600 train 250 1.087677e-02 0.305150
2019-11-12 12:43:28,563 train 300 1.085287e-02 0.232194
2019-11-12 12:43:34,497 train 350 1.083667e-02 0.249761
2019-11-12 12:43:40,432 train 400 1.086491e-02 0.258723
2019-11-12 12:43:46,378 train 450 1.083799e-02 0.262047
2019-11-12 12:43:52,324 train 500 1.079260e-02 0.265555
2019-11-12 12:43:58,269 train 550 1.082406e-02 0.266995
2019-11-12 12:44:04,271 train 600 1.080268e-02 0.271861
2019-11-12 12:44:10,273 train 650 1.078882e-02 0.276248
2019-11-12 12:44:16,288 train 700 1.077863e-02 0.280841
2019-11-12 12:44:22,298 train 750 1.076854e-02 0.283511
2019-11-12 12:44:28,294 train 800 1.076843e-02 0.283748
2019-11-12 12:44:34,298 train 850 1.076098e-02 0.280084
2019-11-12 12:44:36,085 training loss; R2: 1.076049e-02 0.280323
2019-11-12 12:44:36,388 valid 000 6.519764e-03 0.443597
2019-11-12 12:44:38,079 valid 050 1.033819e-02 0.330888
2019-11-12 12:44:39,618 validation loss; R2: 1.029759e-02 0.322207
2019-11-12 12:44:39,630 epoch 10 lr 1.000000e-03
2019-11-12 12:44:40,013 train 000 1.077741e-02 0.111020
2019-11-12 12:44:45,964 train 050 1.064326e-02 0.296120
2019-11-12 12:44:51,986 train 100 1.089196e-02 0.287681
2019-11-12 12:44:58,032 train 150 1.080347e-02 0.294846
2019-11-12 12:45:04,158 train 200 1.079756e-02 0.297394
2019-11-12 12:45:10,467 train 250 1.080124e-02 0.297739
2019-11-12 12:45:16,593 train 300 1.072716e-02 0.298787
2019-11-12 12:45:22,677 train 350 1.067719e-02 0.298877
2019-11-12 12:45:28,732 train 400 1.065191e-02 0.306549
2019-11-12 12:45:34,716 train 450 1.061847e-02 0.309053
2019-11-12 12:45:40,802 train 500 1.061022e-02 0.310243
2019-11-12 12:45:46,894 train 550 1.064430e-02 0.307291
2019-11-12 12:45:52,928 train 600 1.062652e-02 0.305928
2019-11-12 12:45:59,006 train 650 1.063620e-02 0.296004
2019-11-12 12:46:05,103 train 700 1.062296e-02 0.293231
2019-11-12 12:46:11,228 train 750 1.060818e-02 0.291186
2019-11-12 12:46:17,289 train 800 1.060401e-02 0.292326
2019-11-12 12:46:23,338 train 850 1.059855e-02 0.294636
2019-11-12 12:46:25,129 training loss; R2: 1.060166e-02 0.295639
2019-11-12 12:46:25,399 valid 000 1.447146e-02 -0.690132
2019-11-12 12:46:27,108 valid 050 1.497145e-02 0.085439
2019-11-12 12:46:28,656 validation loss; R2: 1.521201e-02 0.092199
2019-11-12 12:46:28,671 epoch 11 lr 1.000000e-03
2019-11-12 12:46:29,090 train 000 1.094581e-02 0.346449
2019-11-12 12:46:35,081 train 050 1.059121e-02 0.328050
2019-11-12 12:46:41,096 train 100 1.062881e-02 0.316367
2019-11-12 12:46:47,111 train 150 1.059708e-02 0.263712
2019-11-12 12:46:53,106 train 200 1.055267e-02 0.280246
2019-11-12 12:46:59,092 train 250 1.057628e-02 0.286881
2019-11-12 12:47:05,103 train 300 1.058988e-02 0.288551
2019-11-12 12:47:11,097 train 350 1.054752e-02 0.285889
2019-11-12 12:47:17,077 train 400 1.055370e-02 0.283187
2019-11-12 12:47:23,066 train 450 1.053958e-02 0.282328
2019-11-12 12:47:29,048 train 500 1.053615e-02 0.282796
2019-11-12 12:47:35,030 train 550 1.052844e-02 0.285227
2019-11-12 12:47:41,014 train 600 1.053790e-02 0.291175
2019-11-12 12:47:47,001 train 650 1.054780e-02 0.290990
2019-11-12 12:47:52,982 train 700 1.054257e-02 0.294548
2019-11-12 12:47:58,959 train 750 1.054141e-02 0.298011
2019-11-12 12:48:04,937 train 800 1.051614e-02 0.297442
2019-11-12 12:48:10,940 train 850 1.049264e-02 0.299816
2019-11-12 12:48:12,731 training loss; R2: 1.049088e-02 0.300848
2019-11-12 12:48:13,028 valid 000 8.753486e-02 -3.071630
2019-11-12 12:48:14,728 valid 050 8.790386e-02 -4.510047
2019-11-12 12:48:16,269 validation loss; R2: 8.758181e-02 -4.350219
2019-11-12 12:48:16,280 epoch 12 lr 1.000000e-03
2019-11-12 12:48:16,661 train 000 8.224746e-03 0.421877
2019-11-12 12:48:22,704 train 050 1.017493e-02 0.326841
2019-11-12 12:48:28,812 train 100 1.026899e-02 0.327549
2019-11-12 12:48:34,895 train 150 1.025684e-02 0.317646
2019-11-12 12:48:40,983 train 200 1.019647e-02 0.320526
2019-11-12 12:48:47,100 train 250 1.021704e-02 0.320204
2019-11-12 12:48:53,178 train 300 1.026451e-02 0.302192
2019-11-12 12:48:59,237 train 350 1.029176e-02 0.296810
2019-11-12 12:49:05,309 train 400 1.031477e-02 0.298485
2019-11-12 12:49:11,401 train 450 1.035302e-02 0.287755
2019-11-12 12:49:17,507 train 500 1.034755e-02 0.293431
2019-11-12 12:49:23,590 train 550 1.035023e-02 0.297833
2019-11-12 12:49:29,642 train 600 1.034981e-02 0.302891
2019-11-12 12:49:35,719 train 650 1.035130e-02 0.306295
2019-11-12 12:49:41,770 train 700 1.031568e-02 0.306167
2019-11-12 12:49:47,731 train 750 1.030600e-02 0.307820
2019-11-12 12:49:53,757 train 800 1.030421e-02 0.309225
2019-11-12 12:49:59,844 train 850 1.029814e-02 0.311593
2019-11-12 12:50:01,663 training loss; R2: 1.030375e-02 0.312748
2019-11-12 12:50:01,950 valid 000 7.693307e-02 -2.638195
2019-11-12 12:50:03,698 valid 050 8.463205e-02 -2.604940
2019-11-12 12:50:05,271 validation loss; R2: 8.382329e-02 -2.915068
2019-11-12 12:50:05,286 epoch 13 lr 1.000000e-03
2019-11-12 12:50:05,664 train 000 1.030135e-02 0.374229
2019-11-12 12:50:11,672 train 050 1.016449e-02 0.364834
2019-11-12 12:50:17,728 train 100 1.020344e-02 0.358032
2019-11-12 12:50:23,769 train 150 1.016902e-02 0.321851
2019-11-12 12:50:29,734 train 200 1.018569e-02 0.322200
2019-11-12 12:50:35,523 train 250 1.020702e-02 0.326864
2019-11-12 12:50:41,443 train 300 1.026970e-02 0.311956
2019-11-12 12:50:47,428 train 350 1.027335e-02 0.290510
2019-11-12 12:50:53,480 train 400 1.023389e-02 0.298112
2019-11-12 12:50:59,569 train 450 1.021270e-02 0.304507
2019-11-12 12:51:05,616 train 500 1.021759e-02 0.310387
2019-11-12 12:51:11,556 train 550 1.020425e-02 0.309177
2019-11-12 12:51:17,491 train 600 1.022616e-02 0.312573
2019-11-12 12:51:23,432 train 650 1.020792e-02 0.311847
2019-11-12 12:51:29,368 train 700 1.021333e-02 0.315613
2019-11-12 12:51:35,305 train 750 1.021312e-02 0.317667
2019-11-12 12:51:41,239 train 800 1.021024e-02 0.315968
2019-11-12 12:51:47,176 train 850 1.020752e-02 0.318244
2019-11-12 12:51:48,949 training loss; R2: 1.020346e-02 0.318917
2019-11-12 12:51:49,221 valid 000 2.370434e-02 -0.511789
2019-11-12 12:51:50,914 valid 050 2.515149e-02 -0.339401
2019-11-12 12:51:52,449 validation loss; R2: 2.535765e-02 -0.382182
2019-11-12 12:51:52,467 epoch 14 lr 1.000000e-03
2019-11-12 12:51:52,800 train 000 1.079015e-02 0.406031
2019-11-12 12:51:58,909 train 050 1.012848e-02 0.343692
2019-11-12 12:52:04,926 train 100 1.021462e-02 0.280692
2019-11-12 12:52:10,879 train 150 1.024101e-02 0.284906
2019-11-12 12:52:16,946 train 200 1.020620e-02 0.297863
2019-11-12 12:52:23,078 train 250 1.011478e-02 0.289876
2019-11-12 12:52:29,222 train 300 1.007708e-02 0.293218
2019-11-12 12:52:35,290 train 350 1.011402e-02 0.291352
2019-11-12 12:52:41,318 train 400 1.010083e-02 0.288300
2019-11-12 12:52:47,332 train 450 1.014324e-02 0.292833
2019-11-12 12:52:53,374 train 500 1.014385e-02 0.295255
2019-11-12 12:52:59,394 train 550 1.012300e-02 0.300530
2019-11-12 12:53:05,470 train 600 1.011172e-02 0.306309
2019-11-12 12:53:11,559 train 650 1.011951e-02 0.141657
2019-11-12 12:53:17,473 train 700 1.012814e-02 0.157233
2019-11-12 12:53:23,496 train 750 1.014388e-02 0.169269
2019-11-12 12:53:29,337 train 800 1.013934e-02 0.180588
2019-11-12 12:53:35,223 train 850 1.013710e-02 0.190542
2019-11-12 12:53:36,957 training loss; R2: 1.012577e-02 0.192030
2019-11-12 12:53:37,235 valid 000 8.068556e-02 -6.666388
2019-11-12 12:53:38,967 valid 050 7.910518e-02 -15.902198
2019-11-12 12:53:40,523 validation loss; R2: 7.865384e-02 -12.388646
2019-11-12 12:53:40,544 epoch 15 lr 1.000000e-03
2019-11-12 12:53:40,940 train 000 9.254792e-03 0.448962
2019-11-12 12:53:47,129 train 050 1.013627e-02 0.322582
2019-11-12 12:53:53,210 train 100 1.017096e-02 0.333028
2019-11-12 12:53:59,269 train 150 1.017100e-02 0.340415
2019-11-12 12:54:05,344 train 200 1.010940e-02 0.341334
2019-11-12 12:54:11,492 train 250 1.011460e-02 0.344731
2019-11-12 12:54:17,563 train 300 1.012014e-02 0.346038
2019-11-12 12:54:23,706 train 350 1.012044e-02 0.346617
2019-11-12 12:54:29,913 train 400 1.006593e-02 0.342236
2019-11-12 12:54:35,900 train 450 1.006658e-02 0.344043
2019-11-12 12:54:42,002 train 500 1.003605e-02 0.346963
2019-11-12 12:54:48,118 train 550 1.001012e-02 0.349859
2019-11-12 12:54:54,028 train 600 1.002580e-02 0.342498
2019-11-12 12:54:59,824 train 650 1.000683e-02 0.344280
2019-11-12 12:55:05,627 train 700 9.988092e-03 0.344104
2019-11-12 12:55:11,413 train 750 9.987254e-03 0.345388
2019-11-12 12:55:17,209 train 800 9.991995e-03 0.344486
2019-11-12 12:55:23,022 train 850 1.000182e-02 0.345571
2019-11-12 12:55:24,746 training loss; R2: 9.985976e-03 0.345865
2019-11-12 12:55:25,030 valid 000 6.369969e-02 -1.717152
2019-11-12 12:55:26,772 valid 050 6.605204e-02 -2.134399
2019-11-12 12:55:28,299 validation loss; R2: 6.651320e-02 -2.280680
2019-11-12 12:55:28,318 epoch 16 lr 1.000000e-03
2019-11-12 12:55:28,701 train 000 1.038912e-02 0.428998
2019-11-12 12:55:34,748 train 050 9.612799e-03 0.387841
2019-11-12 12:55:40,912 train 100 9.683566e-03 0.367865
2019-11-12 12:55:47,188 train 150 9.724186e-03 0.363586
2019-11-12 12:55:53,237 train 200 9.755650e-03 0.353816
2019-11-12 12:55:59,352 train 250 9.781397e-03 0.360416
2019-11-12 12:56:05,458 train 300 9.779561e-03 0.361508
2019-11-12 12:56:11,893 train 350 9.785665e-03 0.332172
2019-11-12 12:56:18,041 train 400 9.809684e-03 0.332155
2019-11-12 12:56:24,284 train 450 9.820505e-03 0.261667
2019-11-12 12:56:30,353 train 500 9.799728e-03 0.273572
2019-11-12 12:56:36,412 train 550 9.816423e-03 0.273643
2019-11-12 12:56:42,480 train 600 9.824852e-03 0.280294
2019-11-12 12:56:48,553 train 650 9.818176e-03 0.283184
2019-11-12 12:56:54,633 train 700 9.830386e-03 0.282295
2019-11-12 12:57:00,719 train 750 9.828395e-03 0.288783
2019-11-12 12:57:06,800 train 800 9.809137e-03 0.292784
2019-11-12 12:57:12,886 train 850 9.809935e-03 0.294390
2019-11-12 12:57:14,704 training loss; R2: 9.807185e-03 0.294859
2019-11-12 12:57:14,999 valid 000 2.342644e-01 -41.874113
2019-11-12 12:57:16,700 valid 050 2.323649e-01 -22.286543
2019-11-12 12:57:18,250 validation loss; R2: 2.293868e-01 -22.114560
2019-11-12 12:57:18,267 epoch 17 lr 1.000000e-03
2019-11-12 12:57:18,697 train 000 9.854412e-03 0.379776
2019-11-12 12:57:24,851 train 050 9.384666e-03 0.393901
2019-11-12 12:57:30,899 train 100 9.588417e-03 0.367556
2019-11-12 12:57:36,953 train 150 9.666170e-03 0.356358
2019-11-12 12:57:43,085 train 200 9.708991e-03 0.346265
2019-11-12 12:57:49,187 train 250 9.703645e-03 0.348537
2019-11-12 12:57:55,310 train 300 9.712777e-03 0.350489
2019-11-12 12:58:01,388 train 350 9.704420e-03 0.357178
2019-11-12 12:58:07,505 train 400 9.709965e-03 0.360641
2019-11-12 12:58:13,750 train 450 9.694740e-03 0.360408
2019-11-12 12:58:19,824 train 500 9.677342e-03 0.363138
2019-11-12 12:58:26,106 train 550 9.674518e-03 0.364377
2019-11-12 12:58:32,207 train 600 9.658151e-03 0.366017
2019-11-12 12:58:38,259 train 650 9.637505e-03 0.360730
2019-11-12 12:58:44,133 train 700 9.617868e-03 0.363436
2019-11-12 12:58:50,096 train 750 9.627372e-03 0.362647
2019-11-12 12:58:56,106 train 800 9.641351e-03 0.364027
2019-11-12 12:59:02,365 train 850 9.652082e-03 0.363257
2019-11-12 12:59:04,204 training loss; R2: 9.663333e-03 0.363616
2019-11-12 12:59:04,505 valid 000 1.133070e+00 -446.028729
2019-11-12 12:59:06,246 valid 050 1.114431e+00 -289.412586
2019-11-12 12:59:07,848 validation loss; R2: 1.110943e+00 -284.783050
2019-11-12 12:59:07,863 epoch 18 lr 1.000000e-03
2019-11-12 12:59:08,253 train 000 9.534255e-03 0.484917
2019-11-12 12:59:14,347 train 050 9.857031e-03 0.359481
2019-11-12 12:59:20,337 train 100 9.900472e-03 0.348251
2019-11-12 12:59:26,169 train 150 9.927348e-03 0.338503
2019-11-12 12:59:32,151 train 200 9.905889e-03 0.337583
2019-11-12 12:59:38,120 train 250 9.901430e-03 0.339581
2019-11-12 12:59:44,061 train 300 9.919566e-03 0.342438
2019-11-12 12:59:50,209 train 350 9.890752e-03 0.345027
2019-11-12 12:59:56,395 train 400 9.915655e-03 0.346438
2019-11-12 13:00:02,535 train 450 9.926047e-03 0.346980
2019-11-12 13:00:08,596 train 500 9.967508e-03 0.346047
2019-11-12 13:00:14,722 train 550 9.983820e-03 0.345156
2019-11-12 13:00:20,887 train 600 9.996494e-03 0.342020
2019-11-12 13:00:26,977 train 650 9.995127e-03 0.343407
2019-11-12 13:00:32,934 train 700 9.999759e-03 0.332766
2019-11-12 13:00:39,028 train 750 1.002255e-02 0.332352
2019-11-12 13:00:44,868 train 800 1.004556e-02 0.331156
2019-11-12 13:00:50,661 train 850 1.005472e-02 0.332305
2019-11-12 13:00:52,391 training loss; R2: 1.006246e-02 0.332284
2019-11-12 13:00:52,681 valid 000 2.733868e-01 -26.837065
2019-11-12 13:00:54,397 valid 050 2.784045e-01 -33.489232
2019-11-12 13:00:55,969 validation loss; R2: 2.794693e-01 -33.929761
2019-11-12 13:00:55,984 epoch 19 lr 1.000000e-03
2019-11-12 13:00:56,369 train 000 9.810271e-03 0.155667
2019-11-12 13:01:02,460 train 050 9.999049e-03 0.314353
2019-11-12 13:01:08,549 train 100 1.010339e-02 0.335264
2019-11-12 13:01:14,644 train 150 1.008974e-02 0.337250
2019-11-12 13:01:20,727 train 200 1.008603e-02 0.343285
2019-11-12 13:01:26,820 train 250 1.010065e-02 0.347001
2019-11-12 13:01:32,895 train 300 1.006460e-02 0.348803
2019-11-12 13:01:38,957 train 350 1.016278e-02 0.346142
2019-11-12 13:01:45,046 train 400 1.019170e-02 0.306565
2019-11-12 13:01:51,145 train 450 1.019842e-02 0.306144
2019-11-12 13:01:57,240 train 500 1.018575e-02 0.308522
2019-11-12 13:02:03,336 train 550 1.017189e-02 0.311059
2019-11-12 13:02:09,429 train 600 1.014755e-02 0.315291
2019-11-12 13:02:15,499 train 650 1.014778e-02 0.315697
2019-11-12 13:02:21,579 train 700 1.016097e-02 0.315762
2019-11-12 13:02:27,645 train 750 1.016325e-02 0.317011
2019-11-12 13:02:33,708 train 800 1.017069e-02 0.318976
2019-11-12 13:02:39,783 train 850 1.017501e-02 0.319395
2019-11-12 13:02:41,595 training loss; R2: 1.018234e-02 0.319635
2019-11-12 13:02:41,898 valid 000 3.596679e-01 -45.267982
2019-11-12 13:02:43,611 valid 050 3.696816e-01 -41.693153
2019-11-12 13:02:45,172 validation loss; R2: 3.696207e-01 -50.272844
