2019-11-13 20:46:59,908 gpu device = 1
2019-11-13 20:46:59,908 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name='galaxy-zoo-cleaned', gpu=1, grad_clip=5, gz_dtree=False, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-galaxy-zoo-EXP-20191113-204659', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-11-13 20:47:11,549 param size = 0.235253MB
2019-11-13 20:47:11,553 epoch 0 lr 1.000000e-03
2019-11-13 20:47:13,710 train 000 3.643775e-01 -97.216990
2019-11-13 20:47:20,015 train 050 4.816680e-02 -7.411008
2019-11-13 20:47:26,389 train 100 3.784460e-02 -4.207328
2019-11-13 20:47:32,632 train 150 3.386697e-02 -2.999206
2019-11-13 20:47:38,692 train 200 3.173406e-02 -2.346601
2019-11-13 20:47:44,834 train 250 3.022061e-02 -1.964624
2019-11-13 20:47:51,178 train 300 2.918113e-02 -1.675654
2019-11-13 20:47:57,285 train 350 2.831067e-02 -1.484877
2019-11-13 20:48:03,542 train 400 2.760642e-02 -1.326400
2019-11-13 20:48:09,765 train 450 2.703230e-02 -1.199536
2019-11-13 20:48:15,918 train 500 2.650852e-02 -1.104040
2019-11-13 20:48:22,095 train 550 2.606439e-02 -1.022364
2019-11-13 20:48:28,333 train 600 2.560718e-02 -0.941905
2019-11-13 20:48:34,481 train 650 2.517876e-02 -0.871946
2019-11-13 20:48:40,472 train 700 2.483166e-02 -0.813007
2019-11-13 20:48:46,529 train 750 2.453566e-02 -0.758481
2019-11-13 20:48:52,878 train 800 2.424080e-02 -0.710671
2019-11-13 20:48:59,117 train 850 2.400370e-02 -0.668812
2019-11-13 20:49:01,674 training loss; R2: 2.392760e-02 -0.657055
2019-11-13 20:49:01,976 valid 000 2.056393e-02 0.041842
2019-11-13 20:49:03,705 valid 050 2.351935e-02 -0.056205
2019-11-13 20:49:05,345 validation loss; R2: 2.334758e-02 -0.127119
2019-11-13 20:49:05,361 epoch 1 lr 1.000000e-03
2019-11-13 20:49:05,893 train 000 1.645382e-02 -0.009299
2019-11-13 20:49:11,827 train 050 1.951911e-02 -0.040179
2019-11-13 20:49:17,936 train 100 1.953614e-02 -0.026832
2019-11-13 20:49:24,141 train 150 1.957530e-02 -0.012977
2019-11-13 20:49:30,306 train 200 1.950898e-02 -0.007486
2019-11-13 20:49:36,479 train 250 1.948462e-02 0.005270
2019-11-13 20:49:42,652 train 300 1.941032e-02 0.015423
2019-11-13 20:49:48,846 train 350 1.935867e-02 0.019568
2019-11-13 20:49:55,028 train 400 1.925115e-02 0.029888
2019-11-13 20:50:01,217 train 450 1.915568e-02 0.035118
2019-11-13 20:50:07,397 train 500 1.909935e-02 0.031502
2019-11-13 20:50:13,579 train 550 1.906198e-02 0.038081
2019-11-13 20:50:19,762 train 600 1.894368e-02 0.041506
2019-11-13 20:50:25,941 train 650 1.888596e-02 0.043767
2019-11-13 20:50:32,132 train 700 1.879948e-02 0.049460
2019-11-13 20:50:38,308 train 750 1.870283e-02 0.053039
2019-11-13 20:50:44,496 train 800 1.861251e-02 0.053999
2019-11-13 20:50:50,669 train 850 1.857162e-02 -0.841811
2019-11-13 20:50:52,519 training loss; R2: 1.855553e-02 -0.825295
2019-11-13 20:50:52,887 valid 000 1.684450e-02 0.218257
2019-11-13 20:50:54,547 valid 050 1.599457e-02 0.112838
2019-11-13 20:50:56,086 validation loss; R2: 1.603914e-02 0.139831
2019-11-13 20:50:56,100 epoch 2 lr 1.000000e-03
2019-11-13 20:50:56,554 train 000 2.012698e-02 -0.029448
2019-11-13 20:51:02,430 train 050 1.773236e-02 0.097869
2019-11-13 20:51:08,304 train 100 1.745909e-02 0.080334
2019-11-13 20:51:14,184 train 150 1.753453e-02 0.079767
2019-11-13 20:51:20,170 train 200 1.749902e-02 0.091753
2019-11-13 20:51:26,346 train 250 1.743404e-02 0.099352
2019-11-13 20:51:32,525 train 300 1.734136e-02 0.107999
2019-11-13 20:51:38,670 train 350 1.719093e-02 0.113896
2019-11-13 20:51:44,557 train 400 1.709505e-02 0.114232
2019-11-13 20:51:50,433 train 450 1.708284e-02 0.117454
2019-11-13 20:51:56,308 train 500 1.703077e-02 0.117520
2019-11-13 20:52:02,231 train 550 1.696330e-02 0.119435
2019-11-13 20:52:08,364 train 600 1.687271e-02 0.117202
2019-11-13 20:52:14,238 train 650 1.684665e-02 0.120161
2019-11-13 20:52:20,114 train 700 1.680538e-02 0.120559
2019-11-13 20:52:26,069 train 750 1.678495e-02 0.119994
2019-11-13 20:52:31,972 train 800 1.673893e-02 0.122227
2019-11-13 20:52:38,052 train 850 1.669343e-02 0.119137
2019-11-13 20:52:39,902 training loss; R2: 1.667037e-02 0.120275
2019-11-13 20:52:40,268 valid 000 1.556661e-02 0.241686
2019-11-13 20:52:42,014 valid 050 1.449488e-02 0.249125
2019-11-13 20:52:43,604 validation loss; R2: 1.422901e-02 0.250532
2019-11-13 20:52:43,626 epoch 3 lr 1.000000e-03
2019-11-13 20:52:44,060 train 000 1.483980e-02 0.252845
2019-11-13 20:52:50,367 train 050 1.605519e-02 0.178159
2019-11-13 20:52:56,416 train 100 1.596507e-02 0.165120
2019-11-13 20:53:02,308 train 150 1.590053e-02 0.122929
2019-11-13 20:53:08,190 train 200 1.579427e-02 0.136644
2019-11-13 20:53:14,074 train 250 1.572283e-02 -0.014842
2019-11-13 20:53:19,953 train 300 1.572490e-02 -0.003329
2019-11-13 20:53:25,835 train 350 1.569287e-02 0.020852
2019-11-13 20:53:31,717 train 400 1.565382e-02 0.040675
2019-11-13 20:53:37,593 train 450 1.561021e-02 0.045117
2019-11-13 20:53:43,468 train 500 1.550562e-02 0.061024
2019-11-13 20:53:49,343 train 550 1.545435e-02 0.069616
2019-11-13 20:53:55,215 train 600 1.542038e-02 0.078379
2019-11-13 20:54:01,091 train 650 1.539209e-02 0.084906
2019-11-13 20:54:06,967 train 700 1.534033e-02 0.092547
2019-11-13 20:54:12,849 train 750 1.529769e-02 0.073221
2019-11-13 20:54:18,729 train 800 1.525212e-02 0.082252
2019-11-13 20:54:24,608 train 850 1.520687e-02 0.090296
2019-11-13 20:54:26,368 training loss; R2: 1.519182e-02 0.092631
2019-11-13 20:54:26,715 valid 000 1.263275e-02 0.270229
2019-11-13 20:54:28,476 valid 050 1.270517e-02 0.287100
2019-11-13 20:54:30,082 validation loss; R2: 1.282707e-02 0.283179
2019-11-13 20:54:30,100 epoch 4 lr 1.000000e-03
2019-11-13 20:54:30,522 train 000 1.494688e-02 0.227825
2019-11-13 20:54:36,812 train 050 1.462263e-02 0.215341
2019-11-13 20:54:42,956 train 100 1.461636e-02 0.218826
2019-11-13 20:54:48,872 train 150 1.463997e-02 0.205007
2019-11-13 20:54:54,791 train 200 1.455302e-02 0.206491
2019-11-13 20:55:00,695 train 250 1.458818e-02 0.205813
2019-11-13 20:55:06,604 train 300 1.458308e-02 0.201833
2019-11-13 20:55:12,511 train 350 1.452183e-02 0.202303
2019-11-13 20:55:18,419 train 400 1.451998e-02 0.204722
2019-11-13 20:55:24,554 train 450 1.450305e-02 0.178965
2019-11-13 20:55:30,520 train 500 1.447666e-02 0.181189
2019-11-13 20:55:36,422 train 550 1.443728e-02 0.178358
2019-11-13 20:55:42,328 train 600 1.439106e-02 0.183727
2019-11-13 20:55:48,251 train 650 1.436507e-02 0.187402
2019-11-13 20:55:54,163 train 700 1.434566e-02 0.188796
2019-11-13 20:56:00,067 train 750 1.432177e-02 0.185729
2019-11-13 20:56:05,951 train 800 1.428781e-02 0.187289
2019-11-13 20:56:11,925 train 850 1.427486e-02 0.189913
2019-11-13 20:56:13,775 training loss; R2: 1.426218e-02 0.190823
2019-11-13 20:56:14,133 valid 000 1.276252e-02 0.295404
2019-11-13 20:56:15,878 valid 050 1.226485e-02 0.262457
2019-11-13 20:56:17,477 validation loss; R2: 1.216213e-02 0.268446
2019-11-13 20:56:17,491 epoch 5 lr 1.000000e-03
2019-11-13 20:56:17,911 train 000 1.294910e-02 0.354648
2019-11-13 20:56:24,138 train 050 1.385579e-02 0.216134
2019-11-13 20:56:30,071 train 100 1.382800e-02 0.226573
2019-11-13 20:56:36,161 train 150 1.387911e-02 0.233412
2019-11-13 20:56:42,359 train 200 1.386289e-02 0.186810
2019-11-13 20:56:48,274 train 250 1.380682e-02 0.199478
2019-11-13 20:56:54,284 train 300 1.375434e-02 0.209944
2019-11-13 20:57:00,192 train 350 1.372038e-02 0.213232
2019-11-13 20:57:06,109 train 400 1.371898e-02 0.216808
2019-11-13 20:57:12,034 train 450 1.369962e-02 0.214838
2019-11-13 20:57:17,944 train 500 1.369498e-02 0.218114
2019-11-13 20:57:23,846 train 550 1.366386e-02 0.220052
2019-11-13 20:57:29,762 train 600 1.366226e-02 0.222153
2019-11-13 20:57:35,668 train 650 1.366720e-02 0.215536
2019-11-13 20:57:41,581 train 700 1.364215e-02 0.216977
2019-11-13 20:57:47,481 train 750 1.362730e-02 0.217159
2019-11-13 20:57:53,404 train 800 1.361812e-02 0.218531
2019-11-13 20:57:59,307 train 850 1.358599e-02 0.219998
2019-11-13 20:58:01,073 training loss; R2: 1.359102e-02 0.220507
2019-11-13 20:58:01,421 valid 000 1.390269e-02 0.168160
2019-11-13 20:58:03,156 valid 050 1.205696e-02 0.090611
2019-11-13 20:58:04,748 validation loss; R2: 1.211107e-02 0.185602
2019-11-13 20:58:04,774 epoch 6 lr 1.000000e-03
2019-11-13 20:58:05,235 train 000 1.256892e-02 0.295320
2019-11-13 20:58:11,415 train 050 1.329330e-02 0.242193
2019-11-13 20:58:17,521 train 100 1.324327e-02 0.240782
2019-11-13 20:58:23,466 train 150 1.308994e-02 0.246559
2019-11-13 20:58:29,709 train 200 1.309829e-02 0.245840
2019-11-13 20:58:35,945 train 250 1.317145e-02 0.248660
2019-11-13 20:58:42,172 train 300 1.315763e-02 0.246453
2019-11-13 20:58:48,394 train 350 1.316841e-02 -1.488289
2019-11-13 20:58:54,635 train 400 1.316207e-02 -1.271260
2019-11-13 20:59:00,876 train 450 1.315720e-02 -1.101512
2019-11-13 20:59:07,055 train 500 1.314823e-02 -0.971570
2019-11-13 20:59:12,986 train 550 1.313371e-02 -0.858942
2019-11-13 20:59:18,921 train 600 1.314077e-02 -0.767109
2019-11-13 20:59:24,855 train 650 1.314873e-02 -0.688478
2019-11-13 20:59:30,785 train 700 1.316518e-02 -0.620808
2019-11-13 20:59:37,027 train 750 1.314166e-02 -0.561786
2019-11-13 20:59:43,320 train 800 1.312764e-02 -0.511598
2019-11-13 20:59:49,572 train 850 1.314110e-02 -0.466859
2019-11-13 20:59:51,450 training loss; R2: 1.312967e-02 -0.454013
2019-11-13 20:59:51,813 valid 000 1.198188e-02 0.367182
2019-11-13 20:59:53,525 valid 050 1.163060e-02 0.344069
2019-11-13 20:59:55,126 validation loss; R2: 1.168083e-02 0.329774
2019-11-13 20:59:55,143 epoch 7 lr 1.000000e-03
2019-11-13 20:59:55,624 train 000 1.193361e-02 0.354671
2019-11-13 21:00:02,042 train 050 1.302920e-02 0.261909
2019-11-13 21:00:08,413 train 100 1.303920e-02 0.255139
2019-11-13 21:00:14,965 train 150 1.305650e-02 0.255484
2019-11-13 21:00:21,115 train 200 1.300353e-02 0.257579
2019-11-13 21:00:27,265 train 250 1.298248e-02 0.238442
2019-11-13 21:00:33,390 train 300 1.297252e-02 0.231763
2019-11-13 21:00:39,530 train 350 1.295266e-02 0.231804
2019-11-13 21:00:45,682 train 400 1.296866e-02 0.236391
2019-11-13 21:00:51,822 train 450 1.294053e-02 0.237855
2019-11-13 21:00:57,970 train 500 1.293439e-02 0.231680
2019-11-13 21:01:04,121 train 550 1.292683e-02 0.229743
2019-11-13 21:01:10,243 train 600 1.292334e-02 0.230702
2019-11-13 21:01:16,364 train 650 1.290351e-02 0.233169
2019-11-13 21:01:22,483 train 700 1.288388e-02 0.232164
2019-11-13 21:01:28,603 train 750 1.286955e-02 0.233873
2019-11-13 21:01:34,738 train 800 1.285353e-02 0.229645
2019-11-13 21:01:40,864 train 850 1.284913e-02 0.230339
2019-11-13 21:01:42,693 training loss; R2: 1.284877e-02 0.229723
2019-11-13 21:01:43,025 valid 000 1.192929e-02 0.354145
2019-11-13 21:01:44,706 valid 050 1.206285e-02 0.335957
2019-11-13 21:01:46,249 validation loss; R2: 1.213501e-02 0.328413
2019-11-13 21:01:46,264 epoch 8 lr 1.000000e-03
2019-11-13 21:01:46,686 train 000 1.435398e-02 0.223326
2019-11-13 21:01:53,009 train 050 1.251475e-02 0.242845
2019-11-13 21:01:59,401 train 100 1.260830e-02 0.250897
2019-11-13 21:02:05,713 train 150 1.260842e-02 0.254488
2019-11-13 21:02:12,016 train 200 1.262798e-02 0.261487
2019-11-13 21:02:18,325 train 250 1.263052e-02 0.264908
2019-11-13 21:02:24,640 train 300 1.264829e-02 0.264709
2019-11-13 21:02:30,960 train 350 1.268299e-02 0.264198
2019-11-13 21:02:37,284 train 400 1.268565e-02 0.261301
2019-11-13 21:02:43,594 train 450 1.266194e-02 0.253135
2019-11-13 21:02:49,814 train 500 1.262869e-02 0.254961
2019-11-13 21:02:56,067 train 550 1.262652e-02 0.253051
2019-11-13 21:03:02,332 train 600 1.261232e-02 0.252020
2019-11-13 21:03:08,570 train 650 1.258638e-02 0.252894
2019-11-13 21:03:14,793 train 700 1.256078e-02 -1.579461
2019-11-13 21:03:21,199 train 750 1.255179e-02 -1.456673
2019-11-13 21:03:27,568 train 800 1.253189e-02 -1.349722
2019-11-13 21:03:33,568 train 850 1.253818e-02 -1.269793
2019-11-13 21:03:35,346 training loss; R2: 1.254367e-02 -1.242940
2019-11-13 21:03:35,650 valid 000 1.105825e-02 0.326803
2019-11-13 21:03:37,372 valid 050 1.213942e-02 0.077766
2019-11-13 21:03:38,923 validation loss; R2: 1.202012e-02 -0.029386
2019-11-13 21:03:38,941 epoch 9 lr 1.000000e-03
2019-11-13 21:03:39,362 train 000 1.342435e-02 0.218048
2019-11-13 21:03:45,636 train 050 1.249946e-02 0.199534
2019-11-13 21:03:51,890 train 100 1.246215e-02 0.233091
2019-11-13 21:03:57,939 train 150 1.239389e-02 0.241142
2019-11-13 21:04:03,925 train 200 1.238610e-02 0.246986
2019-11-13 21:04:09,888 train 250 1.236669e-02 0.258957
2019-11-13 21:04:15,911 train 300 1.238431e-02 0.262321
2019-11-13 21:04:21,916 train 350 1.239222e-02 0.238850
2019-11-13 21:04:28,160 train 400 1.240074e-02 0.244627
2019-11-13 21:04:34,410 train 450 1.240336e-02 0.246833
2019-11-13 21:04:40,623 train 500 1.236474e-02 0.247357
2019-11-13 21:04:46,850 train 550 1.235062e-02 0.252520
2019-11-13 21:04:53,069 train 600 1.233986e-02 0.251734
2019-11-13 21:04:59,148 train 650 1.235132e-02 0.255469
2019-11-13 21:05:05,027 train 700 1.235675e-02 0.256314
2019-11-13 21:05:10,922 train 750 1.237169e-02 0.256635
2019-11-13 21:05:16,814 train 800 1.235267e-02 0.258019
2019-11-13 21:05:22,699 train 850 1.233179e-02 0.257497
2019-11-13 21:05:24,457 training loss; R2: 1.233236e-02 0.257823
2019-11-13 21:05:24,821 valid 000 9.946400e-03 0.429629
2019-11-13 21:05:26,523 valid 050 1.129642e-02 0.356267
2019-11-13 21:05:28,083 validation loss; R2: 1.128662e-02 0.345231
2019-11-13 21:05:28,097 epoch 10 lr 1.000000e-03
2019-11-13 21:05:28,521 train 000 1.209314e-02 0.332835
2019-11-13 21:05:34,393 train 050 1.239170e-02 0.296231
2019-11-13 21:05:40,321 train 100 1.224010e-02 0.292359
2019-11-13 21:05:46,518 train 150 1.214010e-02 0.279516
2019-11-13 21:05:52,698 train 200 1.209959e-02 0.282345
2019-11-13 21:05:58,877 train 250 1.214768e-02 0.289042
2019-11-13 21:06:04,904 train 300 1.217821e-02 0.284623
2019-11-13 21:06:10,781 train 350 1.213731e-02 0.282220
2019-11-13 21:06:16,661 train 400 1.211257e-02 0.282140
2019-11-13 21:06:22,549 train 450 1.211504e-02 0.281842
2019-11-13 21:06:28,428 train 500 1.214458e-02 0.280958
2019-11-13 21:06:34,306 train 550 1.213190e-02 0.284566
2019-11-13 21:06:40,172 train 600 1.213064e-02 0.283629
2019-11-13 21:06:46,037 train 650 1.212290e-02 0.284884
2019-11-13 21:06:51,909 train 700 1.210257e-02 0.284156
2019-11-13 21:06:57,782 train 750 1.209175e-02 0.282760
2019-11-13 21:07:03,661 train 800 1.209690e-02 0.283271
2019-11-13 21:07:09,535 train 850 1.208738e-02 0.285477
2019-11-13 21:07:11,292 training loss; R2: 1.208289e-02 0.285727
2019-11-13 21:07:11,626 valid 000 1.152853e-02 0.362877
2019-11-13 21:07:13,364 valid 050 1.075655e-02 0.353982
2019-11-13 21:07:14,937 validation loss; R2: 1.084916e-02 0.354773
2019-11-13 21:07:14,961 epoch 11 lr 1.000000e-03
2019-11-13 21:07:15,411 train 000 1.222269e-02 0.012567
2019-11-13 21:07:21,712 train 050 1.216246e-02 0.167325
2019-11-13 21:07:27,984 train 100 1.220392e-02 0.226681
2019-11-13 21:07:33,970 train 150 1.208244e-02 0.248278
2019-11-13 21:07:39,923 train 200 1.207700e-02 0.238918
2019-11-13 21:07:45,929 train 250 1.204521e-02 0.253597
2019-11-13 21:07:51,868 train 300 1.199929e-02 0.257446
2019-11-13 21:07:58,082 train 350 1.203433e-02 0.260821
2019-11-13 21:08:04,162 train 400 1.197530e-02 0.266202
2019-11-13 21:08:10,105 train 450 1.194279e-02 0.271098
2019-11-13 21:08:16,102 train 500 1.194977e-02 0.271505
2019-11-13 21:08:22,126 train 550 1.195307e-02 0.268431
2019-11-13 21:08:28,375 train 600 1.193584e-02 0.270255
2019-11-13 21:08:34,512 train 650 1.191832e-02 0.272359
2019-11-13 21:08:40,764 train 700 1.192338e-02 0.275345
2019-11-13 21:08:46,714 train 750 1.191213e-02 0.271275
2019-11-13 21:08:52,642 train 800 1.192773e-02 0.270140
2019-11-13 21:08:58,588 train 850 1.192581e-02 0.266666
2019-11-13 21:09:00,365 training loss; R2: 1.192944e-02 0.266127
2019-11-13 21:09:00,699 valid 000 1.085526e-02 0.277094
2019-11-13 21:09:02,441 valid 050 1.169924e-02 0.318523
2019-11-13 21:09:04,014 validation loss; R2: 1.159994e-02 0.320493
2019-11-13 21:09:04,036 epoch 12 lr 1.000000e-03
2019-11-13 21:09:04,437 train 000 1.386662e-02 0.238625
2019-11-13 21:09:10,725 train 050 1.181008e-02 0.313067
2019-11-13 21:09:17,050 train 100 1.176757e-02 0.280651
2019-11-13 21:09:23,398 train 150 1.175722e-02 0.294750
2019-11-13 21:09:29,704 train 200 1.172035e-02 0.296781
2019-11-13 21:09:35,805 train 250 1.172939e-02 0.299091
2019-11-13 21:09:41,831 train 300 1.174695e-02 0.300356
2019-11-13 21:09:47,839 train 350 1.179952e-02 0.298860
2019-11-13 21:09:53,954 train 400 1.179265e-02 0.300672
2019-11-13 21:10:00,221 train 450 1.179318e-02 0.299965
2019-11-13 21:10:06,535 train 500 1.179693e-02 0.296252
2019-11-13 21:10:12,813 train 550 1.179393e-02 0.292609
2019-11-13 21:10:19,139 train 600 1.180009e-02 0.292649
2019-11-13 21:10:25,456 train 650 1.179372e-02 0.292111
2019-11-13 21:10:31,799 train 700 1.177660e-02 0.290506
2019-11-13 21:10:38,124 train 750 1.175675e-02 0.289874
2019-11-13 21:10:44,652 train 800 1.175724e-02 0.290433
2019-11-13 21:10:50,816 train 850 1.176880e-02 0.286749
2019-11-13 21:10:52,663 training loss; R2: 1.176785e-02 0.286990
2019-11-13 21:10:52,996 valid 000 1.110516e-02 0.307121
2019-11-13 21:10:54,689 valid 050 1.043770e-02 0.263899
2019-11-13 21:10:56,251 validation loss; R2: 1.036201e-02 0.199905
2019-11-13 21:10:56,265 epoch 13 lr 1.000000e-03
2019-11-13 21:10:56,662 train 000 1.195084e-02 0.252014
2019-11-13 21:11:02,928 train 050 1.172634e-02 0.311043
2019-11-13 21:11:09,151 train 100 1.168090e-02 0.297036
2019-11-13 21:11:15,073 train 150 1.159822e-02 0.283036
2019-11-13 21:11:21,019 train 200 1.159352e-02 0.258801
2019-11-13 21:11:26,947 train 250 1.155364e-02 0.266396
2019-11-13 21:11:32,874 train 300 1.155038e-02 0.269434
2019-11-13 21:11:38,788 train 350 1.157524e-02 0.256941
2019-11-13 21:11:44,955 train 400 1.157725e-02 0.261996
2019-11-13 21:11:50,992 train 450 1.159073e-02 0.261416
2019-11-13 21:11:56,917 train 500 1.156551e-02 -2.260203
2019-11-13 21:12:02,824 train 550 1.158056e-02 -2.029192
2019-11-13 21:12:08,729 train 600 1.155581e-02 -1.834455
2019-11-13 21:12:14,634 train 650 1.156284e-02 -1.671255
2019-11-13 21:12:20,536 train 700 1.158114e-02 -1.532091
2019-11-13 21:12:26,445 train 750 1.160253e-02 -1.410882
2019-11-13 21:12:32,363 train 800 1.159148e-02 -1.305605
2019-11-13 21:12:38,634 train 850 1.159987e-02 -1.211490
2019-11-13 21:12:40,498 training loss; R2: 1.159720e-02 -1.185021
2019-11-13 21:12:40,853 valid 000 1.036764e-02 0.368174
2019-11-13 21:12:42,549 valid 050 1.046508e-02 0.369818
2019-11-13 21:12:44,103 validation loss; R2: 1.049647e-02 0.374619
2019-11-13 21:12:44,118 epoch 14 lr 1.000000e-03
2019-11-13 21:12:44,580 train 000 1.328382e-02 0.330253
2019-11-13 21:12:50,677 train 050 1.174069e-02 0.317851
2019-11-13 21:12:56,986 train 100 1.160080e-02 0.319050
2019-11-13 21:13:03,291 train 150 1.158205e-02 0.318605
2019-11-13 21:13:09,590 train 200 1.158902e-02 0.315632
2019-11-13 21:13:15,977 train 250 1.158119e-02 0.312948
2019-11-13 21:13:22,310 train 300 1.158441e-02 0.315341
2019-11-13 21:13:28,604 train 350 1.156445e-02 0.314820
2019-11-13 21:13:34,931 train 400 1.154834e-02 0.315592
2019-11-13 21:13:41,254 train 450 1.153591e-02 0.311950
2019-11-13 21:13:47,545 train 500 1.151478e-02 0.310563
2019-11-13 21:13:53,826 train 550 1.152212e-02 0.289217
2019-11-13 21:13:59,951 train 600 1.153626e-02 0.276711
2019-11-13 21:14:06,036 train 650 1.151507e-02 0.279014
2019-11-13 21:14:12,071 train 700 1.150796e-02 0.281873
2019-11-13 21:14:18,172 train 750 1.151354e-02 0.281664
2019-11-13 21:14:24,200 train 800 1.149060e-02 0.283899
2019-11-13 21:14:30,480 train 850 1.149142e-02 0.253233
2019-11-13 21:14:32,360 training loss; R2: 1.148983e-02 0.253800
2019-11-13 21:14:32,707 valid 000 4.450849e-02 -2.570215
2019-11-13 21:14:34,456 valid 050 4.309125e-02 -6.638263
2019-11-13 21:14:36,032 validation loss; R2: 4.281959e-02 -5.527441
2019-11-13 21:14:36,046 epoch 15 lr 1.000000e-03
2019-11-13 21:14:36,470 train 000 1.097498e-02 0.435625
2019-11-13 21:14:42,521 train 050 1.139828e-02 -0.949827
2019-11-13 21:14:48,577 train 100 1.145427e-02 -0.358005
2019-11-13 21:14:55,017 train 150 1.151395e-02 -0.156024
2019-11-13 21:15:01,197 train 200 1.151969e-02 -0.037568
2019-11-13 21:15:07,271 train 250 1.149065e-02 0.032795
2019-11-13 21:15:13,516 train 300 1.141586e-02 0.075901
2019-11-13 21:15:19,559 train 350 1.138665e-02 0.112358
2019-11-13 21:15:25,624 train 400 1.138688e-02 0.132930
2019-11-13 21:15:31,693 train 450 1.139336e-02 0.150833
2019-11-13 21:15:38,105 train 500 1.141520e-02 0.165520
2019-11-13 21:15:44,243 train 550 1.138880e-02 0.180556
2019-11-13 21:15:50,379 train 600 1.141793e-02 0.188147
2019-11-13 21:15:56,669 train 650 1.142592e-02 0.191368
2019-11-13 21:16:02,795 train 700 1.139852e-02 0.200919
2019-11-13 21:16:08,921 train 750 1.139772e-02 0.204049
2019-11-13 21:16:15,039 train 800 1.137982e-02 0.209582
2019-11-13 21:16:21,173 train 850 1.138311e-02 0.214619
2019-11-13 21:16:23,003 training loss; R2: 1.138701e-02 0.216174
2019-11-13 21:16:23,325 valid 000 2.204445e-02 -0.040261
2019-11-13 21:16:25,028 valid 050 2.131791e-02 -0.031570
2019-11-13 21:16:26,568 validation loss; R2: 2.122892e-02 -0.055238
2019-11-13 21:16:26,588 epoch 16 lr 1.000000e-03
2019-11-13 21:16:27,015 train 000 1.060295e-02 0.424203
2019-11-13 21:16:33,296 train 050 1.155194e-02 0.208732
2019-11-13 21:16:39,425 train 100 1.151519e-02 0.251066
2019-11-13 21:16:45,700 train 150 1.144739e-02 0.266737
2019-11-13 21:16:52,005 train 200 1.138702e-02 0.276563
2019-11-13 21:16:58,345 train 250 1.137846e-02 0.281740
2019-11-13 21:17:04,627 train 300 1.136433e-02 0.290234
2019-11-13 21:17:10,936 train 350 1.134324e-02 0.285232
2019-11-13 21:17:17,225 train 400 1.136274e-02 0.283608
2019-11-13 21:17:23,319 train 450 1.134041e-02 0.287689
2019-11-13 21:17:29,353 train 500 1.132745e-02 0.291819
2019-11-13 21:17:35,452 train 550 1.133326e-02 0.291347
2019-11-13 21:17:41,576 train 600 1.132990e-02 0.293738
2019-11-13 21:17:47,574 train 650 1.134723e-02 0.296160
2019-11-13 21:17:53,771 train 700 1.138004e-02 0.290170
2019-11-13 21:18:00,101 train 750 1.136959e-02 0.273409
2019-11-13 21:18:06,493 train 800 1.136197e-02 0.276846
2019-11-13 21:18:12,709 train 850 1.135948e-02 0.276342
2019-11-13 21:18:14,570 training loss; R2: 1.135827e-02 0.277225
2019-11-13 21:18:14,913 valid 000 2.068029e-02 -0.331219
2019-11-13 21:18:16,612 valid 050 2.246976e-02 -0.579721
2019-11-13 21:18:18,152 validation loss; R2: 2.255147e-02 -0.602148
2019-11-13 21:18:18,177 epoch 17 lr 1.000000e-03
2019-11-13 21:18:18,603 train 000 1.113086e-02 0.407370
2019-11-13 21:18:24,713 train 050 1.142051e-02 0.308207
2019-11-13 21:18:30,734 train 100 1.128252e-02 0.287981
2019-11-13 21:18:36,950 train 150 1.134798e-02 0.300953
2019-11-13 21:18:43,172 train 200 1.130085e-02 0.301132
2019-11-13 21:18:49,387 train 250 1.123174e-02 0.263695
2019-11-13 21:18:55,595 train 300 1.125123e-02 0.272249
2019-11-13 21:19:01,796 train 350 1.124356e-02 0.272636
2019-11-13 21:19:08,008 train 400 1.122066e-02 0.280306
2019-11-13 21:19:14,217 train 450 1.123611e-02 0.275075
2019-11-13 21:19:20,424 train 500 1.120756e-02 0.279125
2019-11-13 21:19:26,643 train 550 1.121467e-02 0.282782
2019-11-13 21:19:32,854 train 600 1.122137e-02 0.286420
2019-11-13 21:19:39,067 train 650 1.122606e-02 0.290112
2019-11-13 21:19:45,278 train 700 1.126602e-02 0.292233
2019-11-13 21:19:51,419 train 750 1.128115e-02 0.293503
2019-11-13 21:19:57,326 train 800 1.129435e-02 0.294584
2019-11-13 21:20:03,214 train 850 1.128359e-02 0.293238
2019-11-13 21:20:04,981 training loss; R2: 1.127708e-02 0.294061
2019-11-13 21:20:05,339 valid 000 1.483538e-02 0.344005
2019-11-13 21:20:07,039 valid 050 1.236217e-02 -0.024836
2019-11-13 21:20:08,588 validation loss; R2: 1.233344e-02 -0.616824
2019-11-13 21:20:08,609 epoch 18 lr 1.000000e-03
2019-11-13 21:20:09,008 train 000 1.271659e-02 0.351325
2019-11-13 21:20:14,921 train 050 1.131800e-02 0.318797
2019-11-13 21:20:20,851 train 100 1.135405e-02 0.207178
2019-11-13 21:20:26,843 train 150 1.135673e-02 0.244153
2019-11-13 21:20:32,950 train 200 1.139620e-02 0.247228
2019-11-13 21:20:38,909 train 250 1.139933e-02 0.260960
2019-11-13 21:20:44,901 train 300 1.135726e-02 0.268542
2019-11-13 21:20:50,850 train 350 1.129809e-02 0.275627
2019-11-13 21:20:56,788 train 400 1.128990e-02 0.279529
2019-11-13 21:21:02,797 train 450 1.126323e-02 0.279330
2019-11-13 21:21:09,072 train 500 1.128977e-02 0.284307
2019-11-13 21:21:15,162 train 550 1.128244e-02 0.286466
2019-11-13 21:21:21,189 train 600 1.128494e-02 0.286843
2019-11-13 21:21:27,292 train 650 1.126795e-02 0.288636
2019-11-13 21:21:33,212 train 700 1.125278e-02 0.290221
2019-11-13 21:21:39,141 train 750 1.123940e-02 0.292621
2019-11-13 21:21:45,068 train 800 1.123117e-02 0.293761
2019-11-13 21:21:51,037 train 850 1.121132e-02 0.293053
2019-11-13 21:21:52,815 training loss; R2: 1.120320e-02 0.293833
2019-11-13 21:21:53,145 valid 000 9.751013e-03 0.402623
2019-11-13 21:21:54,888 valid 050 9.907449e-03 0.388727
2019-11-13 21:21:56,466 validation loss; R2: 9.849780e-03 0.368685
2019-11-13 21:21:56,488 epoch 19 lr 1.000000e-03
2019-11-13 21:21:56,885 train 000 1.357855e-02 0.361114
2019-11-13 21:22:02,846 train 050 1.108237e-02 0.338218
2019-11-13 21:22:08,813 train 100 1.091079e-02 0.347012
2019-11-13 21:22:15,106 train 150 1.098369e-02 0.320520
2019-11-13 21:22:21,419 train 200 1.100050e-02 0.318497
2019-11-13 21:22:27,605 train 250 1.100541e-02 0.319504
2019-11-13 21:22:33,675 train 300 1.102215e-02 0.323087
2019-11-13 21:22:39,759 train 350 1.100456e-02 0.264460
2019-11-13 21:22:45,789 train 400 1.107092e-02 0.271007
2019-11-13 21:22:51,980 train 450 1.107713e-02 -1.260921
2019-11-13 21:22:58,200 train 500 1.108669e-02 -1.104256
2019-11-13 21:23:04,391 train 550 1.110381e-02 -0.975477
2019-11-13 21:23:10,555 train 600 1.110263e-02 -0.867259
2019-11-13 21:23:16,443 train 650 1.111462e-02 -0.776970
2019-11-13 21:23:22,329 train 700 1.112882e-02 -0.715854
2019-11-13 21:23:28,227 train 750 1.112726e-02 -0.648711
2019-11-13 21:23:34,299 train 800 1.112493e-02 -0.589672
2019-11-13 21:23:40,501 train 850 1.113403e-02 -0.536163
2019-11-13 21:23:42,349 training loss; R2: 1.114668e-02 -0.585854
2019-11-13 21:23:42,692 valid 000 9.495583e-03 0.325195
2019-11-13 21:23:44,391 valid 050 9.816699e-03 0.405223
2019-11-13 21:23:45,937 validation loss; R2: 9.863108e-03 0.400919
