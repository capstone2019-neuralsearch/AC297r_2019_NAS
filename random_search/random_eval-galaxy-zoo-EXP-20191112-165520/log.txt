2019-11-12 16:55:20,946 gpu device = 1
2019-11-12 16:55:20,946 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.2, epochs=20, fc1_size=1024, fc2_size=1024, folder_name='galaxy-zoo-cleaned', gpu=1, grad_clip=5, gz_dtree=False, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', random=True, report_freq=50, save='random_eval-galaxy-zoo-EXP-20191112-165520', seed=0, train_portion=0.9, use_xarray=True, weight_decay=1e-06)
2019-11-12 16:55:32,539 param size = 0.246549MB
2019-11-12 16:55:32,543 epoch 0 lr 1.000000e-03
2019-11-12 16:55:34,710 train 000 9.980445e-01 -652.469670
2019-11-12 16:55:41,177 train 050 7.079728e-02 -29.570986
2019-11-12 16:55:47,424 train 100 5.061567e-02 -15.705771
2019-11-12 16:55:53,684 train 150 4.287977e-02 -10.721284
2019-11-12 16:55:59,897 train 200 3.880246e-02 -8.179938
2019-11-12 16:56:06,093 train 250 3.604873e-02 -6.633986
2019-11-12 16:56:12,283 train 300 3.417296e-02 -5.580356
2019-11-12 16:56:18,464 train 350 3.275966e-02 -4.817696
2019-11-12 16:56:24,322 train 400 3.162662e-02 -4.240262
2019-11-12 16:56:30,423 train 450 3.060568e-02 -3.794105
2019-11-12 16:56:36,608 train 500 2.981827e-02 -3.476776
2019-11-12 16:56:42,805 train 550 2.911422e-02 -3.175838
2019-11-12 16:56:48,988 train 600 2.843234e-02 -2.925563
2019-11-12 16:56:55,177 train 650 2.788899e-02 -2.708293
2019-11-12 16:57:01,062 train 700 2.740586e-02 -2.519335
2019-11-12 16:57:06,921 train 750 2.691487e-02 -2.358379
2019-11-12 16:57:12,787 train 800 2.648922e-02 -4.653936
2019-11-12 16:57:18,652 train 850 2.610601e-02 -4.384967
2019-11-12 16:57:21,137 training loss; R2: 2.599522e-02 -4.309116
2019-11-12 16:57:21,424 valid 000 2.282016e-02 0.122584
2019-11-12 16:57:23,118 valid 050 1.896193e-02 0.042448
2019-11-12 16:57:24,724 validation loss; R2: 1.881430e-02 0.072878
2019-11-12 16:57:24,740 epoch 1 lr 1.000000e-03
2019-11-12 16:57:25,256 train 000 1.958844e-02 0.060107
2019-11-12 16:57:31,144 train 050 2.020952e-02 -0.015640
2019-11-12 16:57:37,018 train 100 1.993566e-02 -0.020621
2019-11-12 16:57:42,896 train 150 1.977184e-02 -0.084292
2019-11-12 16:57:48,770 train 200 1.971741e-02 -0.061743
2019-11-12 16:57:54,644 train 250 1.952504e-02 -0.042432
2019-11-12 16:58:00,517 train 300 1.948629e-02 -0.036065
2019-11-12 16:58:06,393 train 350 1.940565e-02 -0.025556
2019-11-12 16:58:12,268 train 400 1.931993e-02 -0.013261
2019-11-12 16:58:18,145 train 450 1.928997e-02 -0.005616
2019-11-12 16:58:24,025 train 500 1.924261e-02 -0.001364
2019-11-12 16:58:29,906 train 550 1.913064e-02 0.003636
2019-11-12 16:58:35,786 train 600 1.904017e-02 -0.014954
2019-11-12 16:58:41,660 train 650 1.897105e-02 -0.008760
2019-11-12 16:58:47,543 train 700 1.888820e-02 -0.003627
2019-11-12 16:58:53,421 train 750 1.878208e-02 0.002587
2019-11-12 16:58:59,297 train 800 1.872770e-02 -0.000849
2019-11-12 16:59:05,172 train 850 1.864713e-02 0.001994
2019-11-12 16:59:06,928 training loss; R2: 1.861468e-02 0.004823
2019-11-12 16:59:07,199 valid 000 1.490818e-02 0.307428
2019-11-12 16:59:08,902 valid 050 1.639487e-02 0.212816
2019-11-12 16:59:10,458 validation loss; R2: 1.623633e-02 0.208967
2019-11-12 16:59:10,481 epoch 2 lr 1.000000e-03
2019-11-12 16:59:10,841 train 000 1.791789e-02 0.157746
2019-11-12 16:59:17,115 train 050 1.726082e-02 0.014867
2019-11-12 16:59:23,182 train 100 1.716167e-02 0.051303
2019-11-12 16:59:29,087 train 150 1.713769e-02 0.072736
2019-11-12 16:59:34,997 train 200 1.711287e-02 0.088601
2019-11-12 16:59:41,157 train 250 1.710785e-02 0.089406
2019-11-12 16:59:47,393 train 300 1.702298e-02 0.086346
2019-11-12 16:59:53,638 train 350 1.695838e-02 0.087735
2019-11-12 16:59:59,877 train 400 1.681794e-02 0.029892
2019-11-12 17:00:06,091 train 450 1.673743e-02 0.032395
2019-11-12 17:00:12,282 train 500 1.667714e-02 0.037358
2019-11-12 17:00:18,475 train 550 1.658804e-02 0.038193
2019-11-12 17:00:24,670 train 600 1.651847e-02 0.045414
2019-11-12 17:00:30,862 train 650 1.645548e-02 0.052483
2019-11-12 17:00:37,060 train 700 1.639613e-02 0.058808
2019-11-12 17:00:43,252 train 750 1.636203e-02 0.062228
2019-11-12 17:00:49,238 train 800 1.629712e-02 0.068119
2019-11-12 17:00:55,104 train 850 1.626409e-02 0.071131
2019-11-12 17:00:56,858 training loss; R2: 1.623143e-02 0.073020
2019-11-12 17:00:57,133 valid 000 1.341907e-02 0.200742
2019-11-12 17:00:58,837 valid 050 1.516614e-02 0.208643
2019-11-12 17:01:00,386 validation loss; R2: 1.515413e-02 0.204061
2019-11-12 17:01:00,402 epoch 3 lr 1.000000e-03
2019-11-12 17:01:00,762 train 000 1.708384e-02 0.227671
2019-11-12 17:01:06,869 train 050 1.501254e-02 0.169019
2019-11-12 17:01:12,799 train 100 1.517019e-02 0.154722
2019-11-12 17:01:18,801 train 150 1.515850e-02 0.162072
2019-11-12 17:01:24,718 train 200 1.502003e-02 -0.652350
2019-11-12 17:01:30,617 train 250 1.501339e-02 -0.487171
2019-11-12 17:01:36,519 train 300 1.500758e-02 -0.387646
2019-11-12 17:01:42,431 train 350 1.497356e-02 -0.309441
2019-11-12 17:01:48,345 train 400 1.490630e-02 -0.247703
2019-11-12 17:01:54,248 train 450 1.488214e-02 -0.199840
2019-11-12 17:02:00,152 train 500 1.483975e-02 -0.161724
2019-11-12 17:02:06,056 train 550 1.480521e-02 -0.127791
2019-11-12 17:02:11,958 train 600 1.479866e-02 -0.110150
2019-11-12 17:02:17,865 train 650 1.474590e-02 -0.086512
2019-11-12 17:02:23,765 train 700 1.470764e-02 -0.066931
2019-11-12 17:02:29,674 train 750 1.466708e-02 -0.105766
2019-11-12 17:02:35,583 train 800 1.461663e-02 -0.087686
2019-11-12 17:02:41,484 train 850 1.460208e-02 -0.339495
2019-11-12 17:02:43,246 training loss; R2: 1.459019e-02 -0.329569
2019-11-12 17:02:43,524 valid 000 1.268975e-02 0.290765
2019-11-12 17:02:45,233 valid 050 1.362832e-02 0.252433
2019-11-12 17:02:46,776 validation loss; R2: 1.353954e-02 0.231023
2019-11-12 17:02:46,796 epoch 4 lr 1.000000e-03
2019-11-12 17:02:47,184 train 000 1.829628e-02 0.107717
2019-11-12 17:02:53,436 train 050 1.401492e-02 0.199957
2019-11-12 17:02:59,510 train 100 1.391924e-02 0.207117
2019-11-12 17:03:05,679 train 150 1.369738e-02 0.204593
2019-11-12 17:03:11,920 train 200 1.369839e-02 0.203784
2019-11-12 17:03:18,159 train 250 1.360028e-02 0.181593
2019-11-12 17:03:24,351 train 300 1.362940e-02 0.008360
2019-11-12 17:03:30,262 train 350 1.362436e-02 0.040975
2019-11-12 17:03:36,165 train 400 1.365627e-02 0.058962
2019-11-12 17:03:42,070 train 450 1.362635e-02 0.076463
2019-11-12 17:03:47,968 train 500 1.364812e-02 0.092421
2019-11-12 17:03:53,869 train 550 1.361309e-02 0.104758
2019-11-12 17:03:59,888 train 600 1.356162e-02 0.111723
2019-11-12 17:04:06,112 train 650 1.352166e-02 0.119812
2019-11-12 17:04:12,304 train 700 1.351002e-02 0.125860
2019-11-12 17:04:18,497 train 750 1.348162e-02 0.132554
2019-11-12 17:04:24,694 train 800 1.345399e-02 0.138553
2019-11-12 17:04:30,890 train 850 1.343504e-02 0.143483
2019-11-12 17:04:32,743 training loss; R2: 1.342237e-02 0.145413
2019-11-12 17:04:33,042 valid 000 1.145502e-02 0.318111
2019-11-12 17:04:34,752 valid 050 1.220501e-02 0.288097
2019-11-12 17:04:36,321 validation loss; R2: 1.247615e-02 0.282807
2019-11-12 17:04:36,343 epoch 5 lr 1.000000e-03
2019-11-12 17:04:36,731 train 000 1.245117e-02 0.044785
2019-11-12 17:04:42,975 train 050 1.316387e-02 0.251502
2019-11-12 17:04:48,930 train 100 1.300023e-02 0.247797
2019-11-12 17:04:54,831 train 150 1.282435e-02 0.201194
2019-11-12 17:05:00,748 train 200 1.284482e-02 0.213598
2019-11-12 17:05:06,664 train 250 1.287060e-02 0.219865
2019-11-12 17:05:12,573 train 300 1.284244e-02 0.223888
2019-11-12 17:05:18,499 train 350 1.281638e-02 0.225185
2019-11-12 17:05:24,640 train 400 1.281403e-02 0.227980
2019-11-12 17:05:30,546 train 450 1.279107e-02 0.228629
2019-11-12 17:05:36,701 train 500 1.275297e-02 0.232231
2019-11-12 17:05:42,794 train 550 1.272219e-02 0.233996
2019-11-12 17:05:48,704 train 600 1.270560e-02 0.108737
2019-11-12 17:05:54,620 train 650 1.270502e-02 0.117675
2019-11-12 17:06:00,526 train 700 1.270443e-02 0.127783
2019-11-12 17:06:06,433 train 750 1.267487e-02 0.138292
2019-11-12 17:06:12,338 train 800 1.265750e-02 0.141618
2019-11-12 17:06:18,245 train 850 1.265385e-02 0.146980
2019-11-12 17:06:20,008 training loss; R2: 1.265060e-02 0.147593
2019-11-12 17:06:20,289 valid 000 1.010874e-02 0.398514
2019-11-12 17:06:21,995 valid 050 1.112022e-02 0.236106
2019-11-12 17:06:23,574 validation loss; R2: 1.115014e-02 0.249919
2019-11-12 17:06:23,596 epoch 6 lr 1.000000e-03
2019-11-12 17:06:23,953 train 000 1.168249e-02 0.311269
2019-11-12 17:06:30,028 train 050 1.229689e-02 0.237736
2019-11-12 17:06:36,297 train 100 1.237719e-02 0.245798
2019-11-12 17:06:42,592 train 150 1.236847e-02 0.250806
2019-11-12 17:06:48,837 train 200 1.234202e-02 0.263728
2019-11-12 17:06:55,020 train 250 1.236828e-02 0.267638
2019-11-12 17:07:00,924 train 300 1.234021e-02 0.260881
2019-11-12 17:07:06,819 train 350 1.231993e-02 0.259346
2019-11-12 17:07:12,723 train 400 1.228739e-02 0.261845
2019-11-12 17:07:18,620 train 450 1.229873e-02 0.250543
2019-11-12 17:07:24,518 train 500 1.223196e-02 0.257393
2019-11-12 17:07:30,428 train 550 1.218278e-02 0.256463
2019-11-12 17:07:36,325 train 600 1.218319e-02 0.256803
2019-11-12 17:07:42,223 train 650 1.217757e-02 0.256687
2019-11-12 17:07:48,117 train 700 1.215987e-02 0.255395
2019-11-12 17:07:54,032 train 750 1.217553e-02 0.251699
2019-11-12 17:07:59,925 train 800 1.213017e-02 0.253562
2019-11-12 17:08:05,821 train 850 1.210147e-02 0.254290
2019-11-12 17:08:07,578 training loss; R2: 1.210596e-02 0.254805
2019-11-12 17:08:07,858 valid 000 1.053900e-02 0.354196
2019-11-12 17:08:09,586 valid 050 1.120607e-02 0.322516
2019-11-12 17:08:11,139 validation loss; R2: 1.136112e-02 0.312436
2019-11-12 17:08:11,155 epoch 7 lr 1.000000e-03
2019-11-12 17:08:11,521 train 000 1.523411e-02 -0.004717
2019-11-12 17:08:17,809 train 050 1.226678e-02 0.282797
2019-11-12 17:08:24,156 train 100 1.198485e-02 0.288112
2019-11-12 17:08:30,098 train 150 1.185488e-02 0.274641
2019-11-12 17:08:36,048 train 200 1.189646e-02 0.271385
2019-11-12 17:08:41,996 train 250 1.184374e-02 0.278266
2019-11-12 17:08:47,939 train 300 1.181010e-02 0.269571
2019-11-12 17:08:53,873 train 350 1.176665e-02 0.275676
2019-11-12 17:09:00,081 train 400 1.176064e-02 0.280828
2019-11-12 17:09:06,377 train 450 1.178344e-02 0.261819
2019-11-12 17:09:12,646 train 500 1.179903e-02 0.258076
2019-11-12 17:09:18,738 train 550 1.181347e-02 0.260657
2019-11-12 17:09:24,680 train 600 1.179800e-02 0.261261
2019-11-12 17:09:30,624 train 650 1.178486e-02 0.261438
2019-11-12 17:09:36,570 train 700 1.177467e-02 0.261876
2019-11-12 17:09:42,511 train 750 1.179268e-02 0.263105
2019-11-12 17:09:48,449 train 800 1.177783e-02 0.265505
2019-11-12 17:09:54,387 train 850 1.176605e-02 0.265242
2019-11-12 17:09:56,159 training loss; R2: 1.175079e-02 0.266611
2019-11-12 17:09:56,429 valid 000 9.933884e-03 0.411958
2019-11-12 17:09:58,146 valid 050 1.087838e-02 0.347695
2019-11-12 17:09:59,696 validation loss; R2: 1.068881e-02 0.335169
2019-11-12 17:09:59,716 epoch 8 lr 1.000000e-03
2019-11-12 17:10:00,079 train 000 1.046780e-02 0.354066
2019-11-12 17:10:06,268 train 050 1.126824e-02 -0.379566
2019-11-12 17:10:12,424 train 100 1.135311e-02 -0.035514
2019-11-12 17:10:18,566 train 150 1.132027e-02 0.078104
2019-11-12 17:10:24,703 train 200 1.137446e-02 0.134570
2019-11-12 17:10:30,849 train 250 1.136210e-02 0.171617
2019-11-12 17:10:36,981 train 300 1.138668e-02 0.188917
2019-11-12 17:10:43,102 train 350 1.138231e-02 0.202600
2019-11-12 17:10:49,238 train 400 1.139027e-02 0.215651
2019-11-12 17:10:55,373 train 450 1.136967e-02 0.228025
2019-11-12 17:11:01,496 train 500 1.136549e-02 0.231985
2019-11-12 17:11:07,632 train 550 1.137321e-02 0.239447
2019-11-12 17:11:13,760 train 600 1.140391e-02 0.243044
2019-11-12 17:11:19,887 train 650 1.138218e-02 0.247329
2019-11-12 17:11:26,019 train 700 1.137810e-02 0.247981
2019-11-12 17:11:32,143 train 750 1.138122e-02 0.252387
2019-11-12 17:11:38,280 train 800 1.136640e-02 0.255647
2019-11-12 17:11:44,413 train 850 1.136729e-02 0.259628
2019-11-12 17:11:46,246 training loss; R2: 1.136796e-02 0.260326
2019-11-12 17:11:46,523 valid 000 3.813049e-02 -1.297044
2019-11-12 17:11:48,193 valid 050 3.541453e-02 -1.475546
2019-11-12 17:11:49,717 validation loss; R2: 3.526012e-02 -1.524491
2019-11-12 17:11:49,738 epoch 9 lr 1.000000e-03
2019-11-12 17:11:50,128 train 000 9.278862e-03 0.407613
2019-11-12 17:11:56,301 train 050 1.132415e-02 0.308387
2019-11-12 17:12:02,674 train 100 1.132257e-02 0.306485
2019-11-12 17:12:08,890 train 150 1.124092e-02 0.310435
2019-11-12 17:12:15,055 train 200 1.123140e-02 0.303196
2019-11-12 17:12:21,044 train 250 1.121066e-02 0.305531
2019-11-12 17:12:27,257 train 300 1.117760e-02 0.310373
2019-11-12 17:12:33,290 train 350 1.116928e-02 0.305226
2019-11-12 17:12:39,456 train 400 1.120288e-02 0.307436
2019-11-12 17:12:45,725 train 450 1.115685e-02 0.306115
2019-11-12 17:12:52,008 train 500 1.113900e-02 0.309004
2019-11-12 17:12:58,200 train 550 1.116007e-02 0.307404
2019-11-12 17:13:04,147 train 600 1.116935e-02 0.305488
2019-11-12 17:13:10,237 train 650 1.115713e-02 0.306647
2019-11-12 17:13:16,280 train 700 1.119873e-02 0.302894
2019-11-12 17:13:22,463 train 750 1.116356e-02 0.303238
2019-11-12 17:13:28,605 train 800 1.114996e-02 0.302066
2019-11-12 17:13:34,742 train 850 1.114537e-02 0.303827
2019-11-12 17:13:36,519 training loss; R2: 1.114031e-02 0.300659
2019-11-12 17:13:36,796 valid 000 6.824157e-02 -4.027908
2019-11-12 17:13:38,501 valid 050 7.554590e-02 -3.891736
2019-11-12 17:13:40,011 validation loss; R2: 7.490635e-02 -3.838900
2019-11-12 17:13:40,036 epoch 10 lr 1.000000e-03
2019-11-12 17:13:40,407 train 000 1.194546e-02 0.295576
2019-11-12 17:13:46,625 train 050 1.102352e-02 0.292578
2019-11-12 17:13:52,853 train 100 1.110279e-02 0.290635
2019-11-12 17:13:59,008 train 150 1.104073e-02 0.281138
2019-11-12 17:14:05,044 train 200 1.101977e-02 0.291022
2019-11-12 17:14:11,135 train 250 1.098386e-02 0.296406
2019-11-12 17:14:17,277 train 300 1.093284e-02 0.300651
2019-11-12 17:14:23,226 train 350 1.088934e-02 0.302155
2019-11-12 17:14:29,147 train 400 1.089763e-02 0.295564
2019-11-12 17:14:35,067 train 450 1.091044e-02 0.295500
2019-11-12 17:14:40,976 train 500 1.091486e-02 0.300192
2019-11-12 17:14:47,089 train 550 1.090038e-02 0.293302
2019-11-12 17:14:53,013 train 600 1.088941e-02 0.298387
2019-11-12 17:14:58,949 train 650 1.087723e-02 0.300618
2019-11-12 17:15:04,883 train 700 1.087825e-02 0.302765
2019-11-12 17:15:10,793 train 750 1.088845e-02 0.301507
2019-11-12 17:15:16,722 train 800 1.088439e-02 0.303061
2019-11-12 17:15:22,658 train 850 1.087598e-02 0.303901
2019-11-12 17:15:24,432 training loss; R2: 1.087766e-02 0.304356
2019-11-12 17:15:24,726 valid 000 7.775972e-02 -2.201829
2019-11-12 17:15:26,392 valid 050 7.962543e-02 -3.232670
2019-11-12 17:15:27,941 validation loss; R2: 7.953572e-02 -3.345061
2019-11-12 17:15:27,962 epoch 11 lr 1.000000e-03
2019-11-12 17:15:28,329 train 000 1.027899e-02 0.435292
2019-11-12 17:15:34,427 train 050 1.079533e-02 0.314362
2019-11-12 17:15:40,388 train 100 1.078265e-02 0.323748
2019-11-12 17:15:46,287 train 150 1.073998e-02 0.324594
2019-11-12 17:15:52,196 train 200 1.077155e-02 0.326299
2019-11-12 17:15:58,086 train 250 1.076484e-02 0.328598
2019-11-12 17:16:03,973 train 300 1.076882e-02 0.324725
2019-11-12 17:16:09,883 train 350 1.078062e-02 0.325491
2019-11-12 17:16:15,828 train 400 1.076110e-02 0.278361
2019-11-12 17:16:21,748 train 450 1.073982e-02 0.286077
2019-11-12 17:16:27,651 train 500 1.072527e-02 0.273417
2019-11-12 17:16:33,562 train 550 1.071986e-02 0.278118
2019-11-12 17:16:39,471 train 600 1.070485e-02 0.283079
2019-11-12 17:16:45,401 train 650 1.069013e-02 0.281805
2019-11-12 17:16:51,311 train 700 1.069385e-02 0.281117
2019-11-12 17:16:57,217 train 750 1.068945e-02 0.284456
2019-11-12 17:17:03,114 train 800 1.070265e-02 0.287341
2019-11-12 17:17:09,032 train 850 1.070723e-02 0.278441
2019-11-12 17:17:10,803 training loss; R2: 1.071992e-02 0.279347
2019-11-12 17:17:11,090 valid 000 5.986415e-02 -3.165195
2019-11-12 17:17:12,825 valid 050 5.559338e-02 -4.446949
2019-11-12 17:17:14,374 validation loss; R2: 5.560135e-02 -4.283522
2019-11-12 17:17:14,389 epoch 12 lr 1.000000e-03
2019-11-12 17:17:14,766 train 000 1.163299e-02 0.197067
2019-11-12 17:17:21,158 train 050 1.073336e-02 0.316079
2019-11-12 17:17:27,588 train 100 1.047703e-02 0.338898
2019-11-12 17:17:34,088 train 150 1.071558e-02 0.327984
2019-11-12 17:17:40,375 train 200 1.068041e-02 0.329559
2019-11-12 17:17:46,840 train 250 1.066800e-02 0.330199
2019-11-12 17:17:53,262 train 300 1.066659e-02 0.327389
2019-11-12 17:17:59,544 train 350 1.061655e-02 0.318577
2019-11-12 17:18:05,648 train 400 1.054895e-02 0.309195
2019-11-12 17:18:11,680 train 450 1.053152e-02 0.312093
2019-11-12 17:18:17,760 train 500 1.059102e-02 0.313368
2019-11-12 17:18:23,892 train 550 1.062724e-02 0.311859
2019-11-12 17:18:30,019 train 600 1.064205e-02 0.311485
2019-11-12 17:18:36,074 train 650 1.068280e-02 0.311880
2019-11-12 17:18:42,282 train 700 1.068868e-02 0.313555
2019-11-12 17:18:48,388 train 750 1.066427e-02 0.312401
2019-11-12 17:18:54,486 train 800 1.062699e-02 0.316266
2019-11-12 17:19:00,637 train 850 1.061049e-02 0.318756
2019-11-12 17:19:02,460 training loss; R2: 1.061037e-02 0.302094
2019-11-12 17:19:02,749 valid 000 4.244536e-02 -1.422252
2019-11-12 17:19:04,477 valid 050 4.327258e-02 -2.321029
2019-11-12 17:19:06,025 validation loss; R2: 4.372739e-02 -2.369522
2019-11-12 17:19:06,040 epoch 13 lr 1.000000e-03
2019-11-12 17:19:06,418 train 000 9.526848e-03 0.413550
2019-11-12 17:19:12,549 train 050 1.089527e-02 0.295389
2019-11-12 17:19:18,767 train 100 1.061533e-02 0.328223
2019-11-12 17:19:24,975 train 150 1.057893e-02 0.339145
2019-11-12 17:19:31,172 train 200 1.061596e-02 0.338684
2019-11-12 17:19:37,326 train 250 1.054830e-02 0.338243
2019-11-12 17:19:43,706 train 300 1.053785e-02 0.341421
2019-11-12 17:19:50,033 train 350 1.058163e-02 0.336314
2019-11-12 17:19:56,475 train 400 1.060335e-02 0.337046
2019-11-12 17:20:02,602 train 450 1.063152e-02 0.180111
2019-11-12 17:20:08,729 train 500 1.063467e-02 0.194589
2019-11-12 17:20:14,870 train 550 1.065717e-02 0.205801
2019-11-12 17:20:21,056 train 600 1.068813e-02 0.212245
2019-11-12 17:20:27,222 train 650 1.069023e-02 0.222056
2019-11-12 17:20:33,351 train 700 1.069158e-02 0.176933
2019-11-12 17:20:39,515 train 750 1.067818e-02 0.184441
2019-11-12 17:20:45,706 train 800 1.067415e-02 0.192484
2019-11-12 17:20:51,906 train 850 1.067171e-02 0.198486
2019-11-12 17:20:53,759 training loss; R2: 1.067231e-02 0.199306
2019-11-12 17:20:54,051 valid 000 2.103441e+00 -111.512083
2019-11-12 17:20:55,699 valid 050 2.122389e+00 -171.587529
2019-11-12 17:20:57,234 validation loss; R2: 2.116839e+00 -166.756158
2019-11-12 17:20:57,250 epoch 14 lr 1.000000e-03
2019-11-12 17:20:57,630 train 000 1.035104e-02 0.381507
2019-11-12 17:21:04,064 train 050 1.033787e-02 0.303305
2019-11-12 17:21:10,327 train 100 1.017876e-02 0.322839
2019-11-12 17:21:16,737 train 150 1.014834e-02 0.320167
2019-11-12 17:21:23,133 train 200 1.009623e-02 0.331387
2019-11-12 17:21:29,299 train 250 1.012920e-02 0.335792
2019-11-12 17:21:35,360 train 300 1.011251e-02 0.345770
2019-11-12 17:21:41,406 train 350 1.007936e-02 0.345704
2019-11-12 17:21:47,573 train 400 1.019036e-02 -16.526240
2019-11-12 17:21:53,824 train 450 1.026331e-02 -14.667886
2019-11-12 17:22:00,045 train 500 1.036051e-02 -13.171415
2019-11-12 17:22:06,299 train 550 1.041050e-02 -11.944471
2019-11-12 17:22:12,523 train 600 1.044933e-02 -10.926596
2019-11-12 17:22:18,562 train 650 1.046282e-02 -10.062627
2019-11-12 17:22:24,440 train 700 1.049339e-02 -9.339730
2019-11-12 17:22:30,316 train 750 1.053041e-02 -8.697645
2019-11-12 17:22:36,193 train 800 1.055320e-02 -8.133086
2019-11-12 17:22:42,067 train 850 1.058478e-02 -7.640750
2019-11-12 17:22:43,824 training loss; R2: 1.058589e-02 -7.503879
2019-11-12 17:22:44,123 valid 000 2.659511e-01 -36.290352
2019-11-12 17:22:45,803 valid 050 2.702473e-01 -41.579540
2019-11-12 17:22:47,316 validation loss; R2: 2.707262e-01 -41.300647
2019-11-12 17:22:47,334 epoch 15 lr 1.000000e-03
2019-11-12 17:22:47,780 train 000 1.157573e-02 0.387616
2019-11-12 17:22:54,109 train 050 1.048793e-02 0.284882
2019-11-12 17:23:00,455 train 100 1.040537e-02 0.283607
2019-11-12 17:23:06,779 train 150 1.051468e-02 0.289236
2019-11-12 17:23:13,098 train 200 1.053202e-02 0.298499
2019-11-12 17:23:19,409 train 250 1.055335e-02 0.298934
2019-11-12 17:23:25,712 train 300 1.055991e-02 0.299912
2019-11-12 17:23:32,002 train 350 1.057283e-02 0.302239
2019-11-12 17:23:38,355 train 400 1.059023e-02 0.307873
2019-11-12 17:23:44,772 train 450 1.059781e-02 0.308425
2019-11-12 17:23:51,098 train 500 1.060910e-02 0.308786
2019-11-12 17:23:57,182 train 550 1.061744e-02 0.299193
2019-11-12 17:24:03,309 train 600 1.062821e-02 0.282911
2019-11-12 17:24:09,470 train 650 1.065024e-02 0.284502
2019-11-12 17:24:15,676 train 700 1.068073e-02 0.284781
2019-11-12 17:24:21,808 train 750 1.068142e-02 0.288670
2019-11-12 17:24:27,853 train 800 1.067056e-02 0.288268
2019-11-12 17:24:33,883 train 850 1.065194e-02 0.290601
2019-11-12 17:24:35,674 training loss; R2: 1.065090e-02 0.290860
2019-11-12 17:24:35,967 valid 000 4.983461e+00 -199.411359
2019-11-12 17:24:37,670 valid 050 4.971757e+00 -317.179802
2019-11-12 17:24:39,221 validation loss; R2: 4.970295e+00 -300.878680
2019-11-12 17:24:39,235 epoch 16 lr 1.000000e-03
2019-11-12 17:24:39,598 train 000 1.035289e-02 0.340003
2019-11-12 17:24:45,838 train 050 1.032114e-02 0.291437
2019-11-12 17:24:52,152 train 100 1.027471e-02 0.306049
2019-11-12 17:24:58,507 train 150 1.040035e-02 0.301201
2019-11-12 17:25:04,673 train 200 1.046010e-02 0.303581
2019-11-12 17:25:11,133 train 250 1.047612e-02 0.310122
2019-11-12 17:25:17,323 train 300 1.040449e-02 0.320635
2019-11-12 17:25:23,537 train 350 1.038155e-02 0.322460
2019-11-12 17:25:29,652 train 400 1.038385e-02 0.326128
2019-11-12 17:25:35,537 train 450 1.038740e-02 0.324287
2019-11-12 17:25:41,424 train 500 1.036729e-02 0.322943
2019-11-12 17:25:47,335 train 550 1.039498e-02 0.319478
2019-11-12 17:25:53,222 train 600 1.040507e-02 0.315390
2019-11-12 17:25:59,104 train 650 1.040721e-02 0.316697
2019-11-12 17:26:04,990 train 700 1.041863e-02 0.316209
2019-11-12 17:26:11,204 train 750 1.043342e-02 0.315893
2019-11-12 17:26:17,437 train 800 1.043572e-02 0.314546
2019-11-12 17:26:23,672 train 850 1.045357e-02 0.316672
2019-11-12 17:26:25,532 training loss; R2: 1.045894e-02 0.317485
2019-11-12 17:26:25,843 valid 000 3.834376e+00 -305.619395
2019-11-12 17:26:27,512 valid 050 3.830274e+00 -413.539672
2019-11-12 17:26:29,051 validation loss; R2: 3.831611e+00 -412.471720
2019-11-12 17:26:29,066 epoch 17 lr 1.000000e-03
2019-11-12 17:26:29,425 train 000 1.137195e-02 0.354714
2019-11-12 17:26:35,673 train 050 1.071879e-02 0.310260
2019-11-12 17:26:41,906 train 100 1.053634e-02 0.305153
2019-11-12 17:26:48,223 train 150 1.037774e-02 0.309592
2019-11-12 17:26:54,507 train 200 1.048141e-02 0.302751
2019-11-12 17:27:00,866 train 250 1.049224e-02 0.296697
2019-11-12 17:27:07,125 train 300 1.048297e-02 0.293277
2019-11-12 17:27:13,408 train 350 1.045521e-02 0.298166
2019-11-12 17:27:19,727 train 400 1.043667e-02 0.301099
2019-11-12 17:27:26,072 train 450 1.041461e-02 0.307544
2019-11-12 17:27:32,352 train 500 1.038804e-02 0.291884
2019-11-12 17:27:38,665 train 550 1.038570e-02 0.289760
2019-11-12 17:27:44,970 train 600 1.035775e-02 0.294822
2019-11-12 17:27:51,309 train 650 1.037827e-02 0.295749
2019-11-12 17:27:57,674 train 700 1.037028e-02 0.277909
2019-11-12 17:28:04,042 train 750 1.035492e-02 0.281814
2019-11-12 17:28:10,304 train 800 1.036226e-02 0.265002
2019-11-12 17:28:16,496 train 850 1.035684e-02 0.265778
2019-11-12 17:28:18,280 training loss; R2: 1.034858e-02 0.266735
2019-11-12 17:28:18,559 valid 000 3.223590e-01 -23.937595
2019-11-12 17:28:20,228 valid 050 3.223522e-01 -29.723220
2019-11-12 17:28:21,739 validation loss; R2: 3.221934e-01 -30.077014
2019-11-12 17:28:21,759 epoch 18 lr 1.000000e-03
2019-11-12 17:28:22,141 train 000 8.962956e-03 0.449454
2019-11-12 17:28:28,162 train 050 1.025591e-02 0.325137
2019-11-12 17:28:34,432 train 100 1.041182e-02 0.328585
2019-11-12 17:28:40,729 train 150 1.032905e-02 0.326164
2019-11-12 17:28:47,037 train 200 1.030789e-02 0.313443
2019-11-12 17:28:53,272 train 250 1.022975e-02 0.321057
2019-11-12 17:28:59,527 train 300 1.022882e-02 0.306308
2019-11-12 17:29:05,743 train 350 1.025196e-02 0.302705
2019-11-12 17:29:11,972 train 400 1.019178e-02 0.306886
2019-11-12 17:29:18,271 train 450 1.019826e-02 0.304565
2019-11-12 17:29:24,581 train 500 1.021266e-02 0.307437
2019-11-12 17:29:30,857 train 550 1.023564e-02 0.145386
2019-11-12 17:29:37,119 train 600 1.022726e-02 0.159336
2019-11-12 17:29:43,355 train 650 1.023862e-02 -0.536398
2019-11-12 17:29:49,265 train 700 1.024972e-02 -0.473166
2019-11-12 17:29:55,164 train 750 1.026199e-02 -0.419645
2019-11-12 17:30:01,065 train 800 1.026158e-02 -0.373425
2019-11-12 17:30:07,034 train 850 1.025911e-02 -0.332662
2019-11-12 17:30:08,797 training loss; R2: 1.025065e-02 -0.321429
2019-11-12 17:30:09,088 valid 000 1.471661e-01 -28.304469
2019-11-12 17:30:10,782 valid 050 1.491945e-01 -17.219115
2019-11-12 17:30:12,356 validation loss; R2: 1.493464e-01 -17.757787
2019-11-12 17:30:12,374 epoch 19 lr 1.000000e-03
2019-11-12 17:30:12,797 train 000 1.066535e-02 0.303311
2019-11-12 17:30:19,146 train 050 1.026112e-02 0.254447
2019-11-12 17:30:25,614 train 100 1.016961e-02 0.305709
2019-11-12 17:30:31,928 train 150 1.017276e-02 0.300572
2019-11-12 17:30:38,271 train 200 1.017929e-02 0.308052
2019-11-12 17:30:44,612 train 250 1.017560e-02 0.304357
2019-11-12 17:30:50,942 train 300 1.017037e-02 0.289685
2019-11-12 17:30:57,275 train 350 1.014134e-02 0.292462
2019-11-12 17:31:03,585 train 400 1.019840e-02 0.293566
2019-11-12 17:31:09,941 train 450 1.019470e-02 0.283620
2019-11-12 17:31:16,160 train 500 1.019373e-02 0.283825
2019-11-12 17:31:22,348 train 550 1.017198e-02 0.281093
2019-11-12 17:31:28,562 train 600 1.017286e-02 0.285624
2019-11-12 17:31:34,603 train 650 1.017360e-02 0.281642
2019-11-12 17:31:40,709 train 700 1.019416e-02 0.284520
2019-11-12 17:31:46,900 train 750 1.019719e-02 0.287146
2019-11-12 17:31:53,266 train 800 1.018339e-02 0.288815
2019-11-12 17:31:59,594 train 850 1.018545e-02 0.282500
2019-11-12 17:32:01,411 training loss; R2: 1.018218e-02 0.283087
2019-11-12 17:32:01,693 valid 000 1.247969e+00 -82.799676
2019-11-12 17:32:03,403 valid 050 1.254952e+00 -90.726881
2019-11-12 17:32:04,952 validation loss; R2: 1.253393e+00 -90.329600
