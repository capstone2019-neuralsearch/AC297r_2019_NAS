2019-11-04 16:36:33,119 gpu device = 3
2019-11-04 16:36:33,120 args = Namespace(arch_learning_rate=0.01, arch_weight_decay=1e-06, batch_size=20, cutout=False, cutout_length=16, data='../data', dataset='GalaxyZoo', drop_path_prob=0.5, epochs=50, fc1_size=1024, fc2_size=1024, gpu=3, grad_clip=5, gz_regression=False, init_channels=16, layers=8, learning_rate=0.001, learning_rate_min=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', primitives='Default', report_freq=50, save='search-GZ-09-20191104-163633', seed=2, train_portion=0.5, unrolled=True, weight_decay=1e-06)
2019-11-04 16:36:36,719 param size = 1.937557MB
2019-11-04 16:36:36,731 epoch 0 lr 1.000000e-03
2019-11-04 16:36:36,732 genotype = Genotype(normal=[('sep_conv_5x5', 1), ('dil_conv_5x5', 0), ('max_pool_2x2', 1), ('max_pool_2x2', 0), ('skip_connect', 0), ('max_pool_2x2', 2), ('max_pool_2x2', 2), ('avg_pool_3x3', 1)], normal_concat=range(2, 6), reduce=[('dil_conv_3x3', 1), ('max_pool_2x2', 0), ('dil_conv_5x5', 1), ('avg_pool_3x3', 2), ('avg_pool_3x3', 1), ('max_pool_3x3', 0), ('max_pool_2x2', 2), ('dil_conv_3x3', 1)], reduce_concat=range(2, 6))
2019-11-04 16:36:36,735 
alphas_normal = Variable containing:
 0.1112  0.1111  0.1110  0.1110  0.1112  0.1112  0.1111  0.1109  0.1113
 0.1110  0.1111  0.1111  0.1112  0.1112  0.1111  0.1113  0.1108  0.1112
 0.1110  0.1112  0.1112  0.1112  0.1111  0.1110  0.1110  0.1112  0.1110
 0.1110  0.1112  0.1114  0.1111  0.1111  0.1112  0.1110  0.1111  0.1109
 0.1112  0.1111  0.1110  0.1111  0.1112  0.1111  0.1110  0.1111  0.1112
 0.1111  0.1113  0.1110  0.1111  0.1109  0.1111  0.1112  0.1111  0.1111
 0.1114  0.1111  0.1110  0.1111  0.1113  0.1112  0.1112  0.1109  0.1110
 0.1111  0.1111  0.1113  0.1111  0.1112  0.1111  0.1111  0.1110  0.1111
 0.1111  0.1111  0.1112  0.1111  0.1111  0.1110  0.1111  0.1111  0.1112
 0.1112  0.1111  0.1111  0.1109  0.1111  0.1110  0.1112  0.1112  0.1112
 0.1111  0.1111  0.1111  0.1111  0.1113  0.1111  0.1110  0.1111  0.1111
 0.1112  0.1112  0.1114  0.1110  0.1110  0.1111  0.1111  0.1109  0.1111
 0.1111  0.1109  0.1110  0.1111  0.1112  0.1113  0.1111  0.1111  0.1112
 0.1113  0.1111  0.1111  0.1111  0.1110  0.1113  0.1112  0.1111  0.1110
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-04 16:36:36,737 
alphas_reduce = Variable containing:
 0.1111  0.1111  0.1113  0.1112  0.1110  0.1111  0.1112  0.1110  0.1110
 0.1111  0.1111  0.1110  0.1111  0.1110  0.1111  0.1110  0.1113  0.1111
 0.1110  0.1111  0.1111  0.1111  0.1109  0.1113  0.1112  0.1111  0.1112
 0.1108  0.1110  0.1111  0.1111  0.1111  0.1113  0.1111  0.1111  0.1114
 0.1110  0.1111  0.1111  0.1111  0.1113  0.1112  0.1110  0.1111  0.1110
 0.1110  0.1112  0.1113  0.1113  0.1111  0.1111  0.1111  0.1110  0.1110
 0.1112  0.1110  0.1112  0.1110  0.1113  0.1111  0.1111  0.1110  0.1111
 0.1111  0.1111  0.1111  0.1112  0.1111  0.1111  0.1110  0.1112  0.1110
 0.1110  0.1111  0.1112  0.1110  0.1111  0.1111  0.1110  0.1111  0.1113
 0.1111  0.1111  0.1111  0.1110  0.1110  0.1112  0.1110  0.1113  0.1112
 0.1113  0.1112  0.1110  0.1112  0.1109  0.1110  0.1112  0.1113  0.1109
 0.1110  0.1112  0.1113  0.1110  0.1111  0.1111  0.1110  0.1112  0.1112
 0.1113  0.1111  0.1110  0.1111  0.1111  0.1111  0.1112  0.1110  0.1111
 0.1112  0.1111  0.1111  0.1111  0.1110  0.1110  0.1112  0.1112  0.1110
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-04 16:36:46,071 train 000 1.521161e-01 -86.934052
2019-11-04 16:39:45,254 train 050 6.737880e-02 -11.821353
2019-11-04 16:42:35,638 train 100 5.837817e-02 -6.595710
2019-11-04 16:45:37,541 train 150 5.323303e-02 -4.801005
2019-11-04 16:48:29,717 train 200 4.882120e-02 -4.085602
2019-11-04 16:51:22,220 train 250 4.550093e-02 -4.355929
2019-11-04 16:54:14,004 train 300 4.283719e-02 -3.843428
2019-11-04 16:57:03,605 train 350 4.036285e-02 -3.426445
2019-11-04 17:00:01,681 train 400 3.837884e-02 -3.075333
2019-11-04 17:03:00,858 train 450 3.654648e-02 -2.876464
2019-11-04 17:06:02,428 train 500 3.487232e-02 -2.626070
2019-11-04 17:08:53,220 train 550 3.352949e-02 -2.435413
2019-11-04 17:11:54,884 train 600 3.222893e-02 -2.266166
2019-11-04 17:14:54,536 train 650 3.108640e-02 -2.112934
2019-11-04 17:17:52,236 train 700 3.008857e-02 -1.980779
2019-11-04 17:20:48,035 train 750 2.912175e-02 -1.880536
2019-11-04 17:23:40,585 train 800 2.831511e-02 -1.788581
2019-11-04 17:26:35,068 train 850 2.754011e-02 -1.721301
2019-11-04 17:29:32,705 train 900 2.683308e-02 -2.074716
2019-11-04 17:32:32,418 train 950 2.622276e-02 -1.979482
2019-11-04 17:35:27,205 train 1000 2.561573e-02 -1.890665
2019-11-04 17:38:22,706 train 1050 2.507977e-02 -1.847751
2019-11-04 17:41:18,688 train 1100 2.456518e-02 -1.782424
2019-11-04 17:44:13,520 train 1150 2.410540e-02 -1.763845
2019-11-04 17:47:08,122 train 1200 2.363588e-02 -1.700211
2019-11-04 17:50:06,452 train 1250 2.322563e-02 -1.659625
2019-11-04 17:53:04,676 train 1300 2.284610e-02 -1.599272
2019-11-04 17:56:03,018 train 1350 2.247703e-02 -2.033519
2019-11-04 17:58:58,872 train 1400 2.216511e-02 -1.989924
2019-11-04 18:01:53,438 train 1450 2.185902e-02 -1.925480
2019-11-04 18:04:49,159 train 1500 2.153626e-02 -1.867873
2019-11-04 18:07:08,928 training loss; R2: 2.131640e-02 -1.878561
2019-11-04 18:07:09,424 valid 000 1.014554e-02 -0.877727
2019-11-04 18:07:18,835 valid 050 1.289449e-02 -0.169948
2019-11-04 18:07:28,203 valid 100 1.239397e-02 -0.240510
2019-11-04 18:07:37,772 valid 150 1.262758e-02 -0.283268
2019-11-04 18:07:47,197 valid 200 1.271761e-02 -0.242815
2019-11-04 18:07:56,288 valid 250 1.259997e-02 -0.200428
2019-11-04 18:08:05,244 valid 300 1.252412e-02 -0.202925
2019-11-04 18:08:14,232 valid 350 1.248824e-02 -0.203192
2019-11-04 18:08:23,174 valid 400 1.251394e-02 -0.194130
2019-11-04 18:08:32,143 valid 450 1.247133e-02 -0.258031
2019-11-04 18:08:41,082 valid 500 1.248476e-02 -0.271667
2019-11-04 18:08:50,049 valid 550 1.245866e-02 -0.278012
2019-11-04 18:08:58,987 valid 600 1.246597e-02 -0.265340
2019-11-04 18:09:07,931 valid 650 1.245812e-02 -0.325560
2019-11-04 18:09:16,873 valid 700 1.242057e-02 -0.313011
2019-11-04 18:09:25,842 valid 750 1.242778e-02 -0.300179
2019-11-04 18:09:34,793 valid 800 1.240205e-02 -0.286811
2019-11-04 18:09:43,749 valid 850 1.239093e-02 -0.284605
2019-11-04 18:09:52,712 valid 900 1.238355e-02 -0.282145
2019-11-04 18:10:01,673 valid 950 1.239506e-02 -0.269054
2019-11-04 18:10:10,629 valid 1000 1.239606e-02 -0.304936
2019-11-04 18:10:19,622 valid 1050 1.239230e-02 -0.306533
2019-11-04 18:10:28,599 valid 1100 1.241880e-02 -0.298213
2019-11-04 18:10:37,562 valid 1150 1.242508e-02 -0.296849
2019-11-04 18:10:46,549 valid 1200 1.242719e-02 -0.288953
2019-11-04 18:10:55,545 valid 1250 1.241832e-02 -0.294683
2019-11-04 18:11:04,555 valid 1300 1.242350e-02 -0.286936
2019-11-04 18:11:13,542 valid 1350 1.241176e-02 -0.293111
2019-11-04 18:11:22,495 valid 1400 1.243036e-02 -0.291631
2019-11-04 18:11:31,451 valid 1450 1.243627e-02 -0.282044
2019-11-04 18:11:40,420 valid 1500 1.244478e-02 -0.276983
2019-11-04 18:11:47,416 validation loss; R2: 1.246725e-02 -0.275927
2019-11-04 18:11:47,543 epoch 1 lr 1.000000e-03
2019-11-04 18:11:47,544 genotype = Genotype(normal=[('dil_conv_5x5', 0), ('dil_conv_5x5', 1), ('dil_conv_3x3', 2), ('max_pool_2x2', 0), ('dil_conv_5x5', 0), ('dil_conv_5x5', 1), ('max_pool_3x3', 1), ('max_pool_3x3', 2)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 0), ('skip_connect', 1), ('dil_conv_5x5', 2), ('max_pool_3x3', 0), ('sep_conv_5x5', 2), ('max_pool_3x3', 1), ('max_pool_2x2', 2), ('max_pool_3x3', 1)], reduce_concat=range(2, 6))
2019-11-04 18:11:47,546 
alphas_normal = Variable containing:
 0.1272  0.0937  0.1262  0.1126  0.0388  0.1082  0.0481  0.1438  0.2014
 0.3070  0.0851  0.0891  0.0708  0.0298  0.0733  0.0787  0.0979  0.1683
 0.1124  0.0986  0.1661  0.1526  0.0551  0.1099  0.1051  0.0743  0.1258
 0.2454  0.0900  0.1531  0.1253  0.0431  0.0727  0.0766  0.0699  0.1238
 0.1971  0.1334  0.1072  0.0965  0.0524  0.0531  0.0776  0.1668  0.1159
 0.1150  0.0724  0.1641  0.1445  0.0546  0.0641  0.0913  0.1004  0.1936
 0.2009  0.0677  0.1130  0.1078  0.0463  0.0788  0.1079  0.0967  0.1808
 0.2297  0.1127  0.1301  0.1089  0.0643  0.0736  0.1056  0.0752  0.0999
 0.2476  0.0813  0.1292  0.0999  0.0611  0.0793  0.0872  0.1312  0.0831
 0.2588  0.0538  0.1216  0.1309  0.0473  0.0765  0.1152  0.0991  0.0969
 0.1652  0.1028  0.1395  0.1704  0.0570  0.1562  0.0917  0.0595  0.0578
 0.1755  0.0997  0.1373  0.1441  0.0652  0.1257  0.0703  0.0945  0.0876
 0.2131  0.0904  0.1185  0.1209  0.0693  0.0932  0.0833  0.1015  0.1099
 0.1733  0.1045  0.1364  0.1210  0.0772  0.0665  0.1198  0.0738  0.1275
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-04 18:11:47,547 
alphas_reduce = Variable containing:
 0.0861  0.1336  0.1330  0.1690  0.0848  0.0925  0.0901  0.0682  0.1427
 0.1335  0.1521  0.1308  0.1491  0.0765  0.0729  0.1000  0.0957  0.0893
 0.1053  0.0708  0.1533  0.2468  0.0949  0.0664  0.0703  0.0746  0.1176
 0.1239  0.0860  0.1449  0.1866  0.0898  0.0511  0.1291  0.1069  0.0817
 0.0673  0.0769  0.0919  0.1091  0.0362  0.0778  0.1590  0.1049  0.2770
 0.1346  0.1654  0.1346  0.1870  0.0693  0.0739  0.0717  0.0799  0.0836
 0.1166  0.0865  0.1363  0.2316  0.0794  0.0715  0.0976  0.1013  0.0793
 0.0635  0.0539  0.0771  0.0903  0.0287  0.0746  0.2741  0.1274  0.2103
 0.1552  0.0841  0.1033  0.1104  0.0464  0.0720  0.0985  0.1298  0.2004
 0.1300  0.1135  0.1321  0.1548  0.0684  0.0922  0.0825  0.1156  0.1108
 0.1233  0.1328  0.1284  0.1819  0.0660  0.0717  0.1086  0.0708  0.1166
 0.0931  0.0730  0.1992  0.1435  0.0367  0.0731  0.1207  0.1314  0.1293
 0.1212  0.0931  0.1679  0.1327  0.0533  0.0790  0.1037  0.1045  0.1448
 0.0922  0.0914  0.1166  0.1041  0.0485  0.1118  0.1308  0.1281  0.1763
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-04 18:11:52,351 train 000 1.433955e-02 0.026667
2019-11-04 18:14:43,140 train 050 1.258154e-02 -0.105537
2019-11-04 18:17:36,662 train 100 1.263776e-02 -0.364445
2019-11-04 18:20:30,801 train 150 1.269313e-02 -0.483585
2019-11-04 18:23:23,296 train 200 1.288880e-02 -0.500264
2019-11-04 18:26:14,242 train 250 1.278552e-02 -0.433387
2019-11-04 18:29:07,368 train 300 1.279869e-02 -0.388320
2019-11-04 18:31:58,594 train 350 1.276744e-02 -0.343532
2019-11-04 18:34:57,430 train 400 1.276303e-02 -0.476294
2019-11-04 18:37:48,134 train 450 1.279056e-02 -3.109346
2019-11-04 18:40:39,000 train 500 1.279952e-02 -2.816573
2019-11-04 18:43:28,825 train 550 1.275363e-02 -2.582276
2019-11-04 18:46:17,476 train 600 1.274186e-02 -2.377662
2019-11-04 18:49:08,069 train 650 1.268963e-02 -2.203806
2019-11-04 18:52:02,410 train 700 1.266331e-02 -2.057627
2019-11-04 18:55:06,544 train 750 1.263460e-02 -1.933406
2019-11-04 18:58:12,187 train 800 1.259691e-02 -1.853975
2019-11-04 19:01:20,565 train 850 1.256140e-02 -1.767037
2019-11-04 19:04:24,455 train 900 1.250719e-02 -1.675072
2019-11-04 19:07:30,823 train 950 1.246502e-02 -1.594409
2019-11-04 19:10:35,357 train 1000 1.246589e-02 -1.549691
2019-11-04 19:13:37,773 train 1050 1.244653e-02 -1.481959
2019-11-04 19:16:42,677 train 1100 1.240976e-02 -1.417132
2019-11-04 19:19:41,627 train 1150 1.239631e-02 -1.365325
2019-11-04 19:22:40,216 train 1200 1.237548e-02 -1.312137
2019-11-04 19:25:39,048 train 1250 1.235905e-02 -1.262628
2019-11-04 19:28:40,338 train 1300 1.233983e-02 -1.217399
2019-11-04 19:31:41,391 train 1350 1.231781e-02 -1.202208
2019-11-04 19:34:40,189 train 1400 1.229255e-02 -1.165591
2019-11-04 19:37:47,829 train 1450 1.228823e-02 -1.141160
2019-11-04 19:40:46,578 train 1500 1.226607e-02 -1.110162
2019-11-04 19:43:06,223 training loss; R2: 1.224309e-02 -1.083752
2019-11-04 19:43:06,756 valid 000 9.241144e-03 0.295470
2019-11-04 19:43:16,559 valid 050 1.131425e-02 -0.135918
2019-11-04 19:43:26,039 valid 100 1.137078e-02 -0.074958
2019-11-04 19:43:35,477 valid 150 1.150550e-02 -0.106989
2019-11-04 19:43:44,940 valid 200 1.154402e-02 -0.069598
2019-11-04 19:43:54,453 valid 250 1.168843e-02 -0.078037
2019-11-04 19:44:03,856 valid 300 1.169942e-02 -0.090139
2019-11-04 19:44:13,333 valid 350 1.169573e-02 -0.074552
2019-11-04 19:44:22,869 valid 400 1.169471e-02 -0.060611
2019-11-04 19:44:32,395 valid 450 1.168176e-02 -0.061369
2019-11-04 19:44:41,921 valid 500 1.169015e-02 -0.057701
2019-11-04 19:44:51,407 valid 550 1.172360e-02 -0.046815
2019-11-04 19:45:00,907 valid 600 1.174701e-02 -0.050489
2019-11-04 19:45:10,404 valid 650 1.175269e-02 -0.046651
2019-11-04 19:45:19,964 valid 700 1.175715e-02 -0.065893
2019-11-04 19:45:29,462 valid 750 1.177933e-02 -0.067817
2019-11-04 19:45:38,968 valid 800 1.176910e-02 -0.060292
2019-11-04 19:45:48,461 valid 850 1.181074e-02 -0.090799
2019-11-04 19:45:57,892 valid 900 1.182865e-02 -0.085568
2019-11-04 19:46:07,313 valid 950 1.183588e-02 -0.078223
2019-11-04 19:46:16,986 valid 1000 1.177669e-02 -0.074660
2019-11-04 19:46:26,650 valid 1050 1.177696e-02 -0.803969
2019-11-04 19:46:36,316 valid 1100 1.179341e-02 -0.774089
2019-11-04 19:46:45,984 valid 1150 1.180319e-02 -0.761290
2019-11-04 19:46:55,658 valid 1200 1.178902e-02 -0.729407
2019-11-04 19:47:05,324 valid 1250 1.178840e-02 -0.719856
2019-11-04 19:47:15,010 valid 1300 1.177397e-02 -0.691345
2019-11-04 19:47:24,694 valid 1350 1.175641e-02 -0.673706
2019-11-04 19:47:34,420 valid 1400 1.176537e-02 -0.649348
2019-11-04 19:47:44,136 valid 1450 1.176451e-02 -0.634364
2019-11-04 19:47:53,856 valid 1500 1.175975e-02 -0.614955
2019-11-04 19:48:01,399 validation loss; R2: 1.176347e-02 -0.601079
2019-11-04 19:48:01,541 epoch 2 lr 1.000000e-03
2019-11-04 19:48:01,542 genotype = Genotype(normal=[('dil_conv_5x5', 0), ('dil_conv_5x5', 1), ('max_pool_2x2', 0), ('max_pool_2x2', 1), ('dil_conv_5x5', 0), ('max_pool_2x2', 2), ('max_pool_3x3', 0), ('max_pool_3x3', 1)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 0), ('skip_connect', 1), ('max_pool_3x3', 0), ('dil_conv_5x5', 2), ('sep_conv_5x5', 2), ('max_pool_3x3', 1), ('dil_conv_5x5', 4), ('max_pool_2x2', 3)], reduce_concat=range(2, 6))
2019-11-04 19:48:01,544 
alphas_normal = Variable containing:
 0.1171  0.1170  0.1407  0.0979  0.0313  0.0919  0.0248  0.1555  0.2238
 0.5984  0.0427  0.0424  0.0345  0.0150  0.0385  0.0376  0.0602  0.1308
 0.1669  0.0935  0.1908  0.1321  0.0388  0.0785  0.0894  0.0675  0.1427
 0.4089  0.0678  0.1881  0.1072  0.0278  0.0320  0.0390  0.0384  0.0909
 0.4002  0.0970  0.0728  0.0535  0.0262  0.0388  0.0514  0.0974  0.1627
 0.1096  0.0630  0.1731  0.1654  0.0407  0.0428  0.0712  0.0598  0.2744
 0.3881  0.0429  0.0851  0.1030  0.0268  0.0930  0.0726  0.0591  0.1294
 0.3893  0.1204  0.1345  0.0972  0.0356  0.0393  0.0742  0.0528  0.0568
 0.4966  0.0544  0.0828  0.0748  0.0314  0.0626  0.0715  0.0679  0.0580
 0.3033  0.0498  0.1653  0.1858  0.0364  0.0462  0.0625  0.0608  0.0900
 0.3743  0.0579  0.1048  0.1703  0.0322  0.1010  0.0573  0.0543  0.0478
 0.4228  0.0704  0.1129  0.1113  0.0314  0.0752  0.0519  0.0696  0.0545
 0.6309  0.0469  0.0678  0.0681  0.0288  0.0360  0.0264  0.0341  0.0609
 0.3468  0.0904  0.1279  0.0910  0.0489  0.0462  0.0970  0.0450  0.1069
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-04 19:48:01,546 
alphas_reduce = Variable containing:
 0.0761  0.1326  0.1868  0.3007  0.0823  0.0442  0.0471  0.0490  0.0811
 0.1064  0.2306  0.1605  0.1632  0.0848  0.0656  0.0571  0.0748  0.0569
 0.0977  0.0509  0.1776  0.3959  0.0762  0.0363  0.0374  0.0567  0.0713
 0.1777  0.0810  0.1530  0.1811  0.0925  0.0345  0.1600  0.0755  0.0447
 0.0623  0.0467  0.0466  0.0689  0.0266  0.0578  0.1975  0.0991  0.3945
 0.1678  0.1242  0.1560  0.2200  0.0693  0.0504  0.0769  0.0795  0.0557
 0.1468  0.0980  0.1663  0.3198  0.0598  0.0393  0.0642  0.0651  0.0407
 0.0451  0.0378  0.0451  0.0571  0.0274  0.0483  0.4333  0.0862  0.2198
 0.1332  0.0820  0.1087  0.1079  0.0511  0.1036  0.1032  0.1322  0.1782
 0.1446  0.1112  0.1694  0.1982  0.0762  0.0548  0.0602  0.1155  0.0698
 0.1527  0.1504  0.1726  0.2066  0.0715  0.0447  0.0679  0.0543  0.0793
 0.0873  0.0693  0.1384  0.1443  0.0396  0.0538  0.1349  0.1731  0.1594
 0.0952  0.0819  0.2071  0.1530  0.0521  0.0678  0.0668  0.1050  0.1710
 0.0669  0.0931  0.0914  0.0865  0.0436  0.0661  0.1411  0.1120  0.2992
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-04 19:48:06,378 train 000 1.158109e-02 0.201669
2019-11-04 19:51:04,169 train 050 1.150520e-02 -0.373498
2019-11-04 19:54:09,633 train 100 1.188149e-02 -0.304242
2019-11-04 19:57:08,818 train 150 1.191348e-02 -0.660786
2019-11-04 20:00:07,153 train 200 1.196045e-02 -1.285156
2019-11-04 20:03:05,434 train 250 1.195336e-02 -1.077938
2019-11-04 20:06:01,431 train 300 1.195107e-02 -0.962393
2019-11-04 20:09:00,652 train 350 1.199491e-02 -0.855175
2019-11-04 20:12:02,796 train 400 1.196345e-02 -0.750910
2019-11-04 20:15:01,612 train 450 1.193116e-02 -0.681283
2019-11-04 20:18:00,290 train 500 1.187588e-02 -0.617614
2019-11-04 20:20:57,111 train 550 1.184620e-02 -0.573317
2019-11-04 20:23:55,992 train 600 1.184218e-02 -0.553722
2019-11-04 20:26:55,988 train 650 1.181256e-02 -0.600967
2019-11-04 20:29:54,656 train 700 1.178031e-02 -0.582029
2019-11-04 20:32:50,185 train 750 1.174672e-02 -0.547363
2019-11-04 20:35:44,864 train 800 1.167546e-02 -0.563695
2019-11-04 20:38:42,621 train 850 1.168100e-02 -0.535128
2019-11-04 20:41:38,880 train 900 1.168280e-02 -0.566771
2019-11-04 20:44:32,656 train 950 1.164845e-02 -0.535473
2019-11-04 20:47:26,974 train 1000 1.163069e-02 -0.521306
2019-11-04 20:50:22,397 train 1050 1.161219e-02 -0.508049
2019-11-04 20:53:20,727 train 1100 1.159506e-02 -0.488017
2019-11-04 20:56:20,086 train 1150 1.157799e-02 -0.473475
2019-11-04 20:59:19,818 train 1200 1.156496e-02 -0.462708
2019-11-04 21:02:15,958 train 1250 1.154263e-02 -0.454875
2019-11-04 21:05:19,374 train 1300 1.153800e-02 -0.437665
2019-11-04 21:08:19,113 train 1350 1.151986e-02 -0.427659
2019-11-04 21:11:19,273 train 1400 1.153032e-02 -0.647715
2019-11-04 21:14:19,282 train 1450 1.150442e-02 -0.630017
2019-11-04 21:17:24,154 train 1500 1.148463e-02 -0.763307
2019-11-04 21:19:50,726 training loss; R2: 1.149524e-02 -0.746797
2019-11-04 21:19:51,250 valid 000 7.002425e-03 -0.608962
2019-11-04 21:20:01,196 valid 050 1.092421e-02 -1.904666
2019-11-04 21:20:11,140 valid 100 1.117716e-02 -1.099709
2019-11-04 21:20:21,072 valid 150 1.095556e-02 -0.783854
2019-11-04 21:20:31,004 valid 200 1.087959e-02 -0.563410
2019-11-04 21:20:40,931 valid 250 1.095847e-02 -0.697210
2019-11-04 21:20:50,876 valid 300 1.087021e-02 -0.624985
2019-11-04 21:21:00,833 valid 350 1.081540e-02 -0.671954
2019-11-04 21:21:10,772 valid 400 1.084568e-02 -0.595856
2019-11-04 21:21:20,695 valid 450 1.087045e-02 -0.565478
2019-11-04 21:21:30,604 valid 500 1.082342e-02 -0.512240
2019-11-04 21:21:40,578 valid 550 1.081161e-02 -0.477457
2019-11-04 21:21:50,654 valid 600 1.083377e-02 -0.441938
2019-11-04 21:22:00,628 valid 650 1.088235e-02 -0.438311
2019-11-04 21:22:10,461 valid 700 1.086420e-02 -0.429363
2019-11-04 21:22:20,221 valid 750 1.090871e-02 -0.410648
2019-11-04 21:22:30,134 valid 800 1.091680e-02 -0.386630
2019-11-04 21:22:40,071 valid 850 1.095365e-02 -0.367237
2019-11-04 21:22:50,010 valid 900 1.097014e-02 -0.355683
2019-11-04 21:22:59,903 valid 950 1.094101e-02 -0.342601
2019-11-04 21:23:09,854 valid 1000 1.095818e-02 -0.339020
2019-11-04 21:23:19,809 valid 1050 1.095644e-02 -0.324603
2019-11-04 21:23:29,831 valid 1100 1.096769e-02 -0.314427
2019-11-04 21:23:39,855 valid 1150 1.096025e-02 -0.305261
2019-11-04 21:23:49,958 valid 1200 1.096288e-02 -0.300236
2019-11-04 21:24:00,085 valid 1250 1.093926e-02 -0.291229
2019-11-04 21:24:10,165 valid 1300 1.093296e-02 -0.289105
2019-11-04 21:24:20,239 valid 1350 1.095122e-02 -0.282165
2019-11-04 21:24:30,313 valid 1400 1.095426e-02 -0.353655
2019-11-04 21:24:40,396 valid 1450 1.095119e-02 -0.341822
2019-11-04 21:24:50,731 valid 1500 1.094913e-02 -0.336198
2019-11-04 21:24:58,780 validation loss; R2: 1.095442e-02 -0.334103
2019-11-04 21:24:58,925 epoch 3 lr 1.000000e-03
2019-11-04 21:24:58,926 genotype = Genotype(normal=[('dil_conv_5x5', 0), ('dil_conv_5x5', 1), ('max_pool_2x2', 1), ('max_pool_2x2', 0), ('dil_conv_5x5', 0), ('dil_conv_5x5', 1), ('max_pool_3x3', 1), ('max_pool_3x3', 0)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 0), ('max_pool_3x3', 1), ('max_pool_3x3', 0), ('dil_conv_5x5', 2), ('dil_conv_5x5', 2), ('max_pool_3x3', 1), ('dil_conv_5x5', 4), ('max_pool_3x3', 2)], reduce_concat=range(2, 6))
2019-11-04 21:24:58,928 
alphas_normal = Variable containing:
 0.1012  0.0891  0.1286  0.0827  0.0209  0.1227  0.0288  0.1996  0.2264
 0.6010  0.0440  0.0395  0.0294  0.0173  0.0337  0.0409  0.0535  0.1407
 0.1639  0.0774  0.1803  0.1437  0.0281  0.1084  0.0902  0.0740  0.1338
 0.3011  0.0668  0.2365  0.1347  0.0320  0.0262  0.0486  0.0369  0.1172
 0.4207  0.0867  0.0686  0.0495  0.0263  0.0278  0.0568  0.1022  0.1614
 0.0988  0.0508  0.1660  0.1930  0.0317  0.0426  0.0694  0.0616  0.2860
 0.3716  0.0499  0.0978  0.1097  0.0309  0.0749  0.0795  0.0405  0.1452
 0.3961  0.1174  0.1333  0.0794  0.0349  0.0558  0.0876  0.0345  0.0609
 0.4850  0.0512  0.0778  0.0622  0.0293  0.0839  0.0715  0.0626  0.0766
 0.3186  0.0357  0.1397  0.2062  0.0259  0.0355  0.1016  0.0396  0.0970
 0.2523  0.0546  0.1030  0.2230  0.0366  0.1892  0.0448  0.0475  0.0490
 0.3828  0.0619  0.1104  0.1218  0.0310  0.1001  0.0483  0.0667  0.0769
 0.6117  0.0475  0.0610  0.0609  0.0278  0.0310  0.0373  0.0326  0.0903
 0.3493  0.0726  0.1041  0.0775  0.0371  0.0350  0.1212  0.0414  0.1617
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-04 21:24:58,930 
alphas_reduce = Variable containing:
 0.0716  0.0714  0.1306  0.4628  0.0441  0.0392  0.0455  0.0422  0.0926
 0.1444  0.1530  0.1544  0.1663  0.0442  0.0717  0.0744  0.1157  0.0758
 0.0820  0.0360  0.1223  0.5607  0.0450  0.0269  0.0269  0.0383  0.0619
 0.1657  0.0619  0.1556  0.1971  0.0566  0.0396  0.1992  0.0737  0.0507
 0.0757  0.0463  0.0530  0.0815  0.0285  0.0559  0.1777  0.0622  0.4192
 0.1621  0.0885  0.0964  0.2848  0.0442  0.0565  0.1196  0.0820  0.0658
 0.2189  0.0788  0.1083  0.3025  0.0452  0.0459  0.0821  0.0772  0.0411
 0.0560  0.0342  0.0505  0.0884  0.0266  0.0421  0.2677  0.1235  0.3109
 0.0709  0.0408  0.0711  0.1195  0.0341  0.0706  0.1437  0.1701  0.2791
 0.1440  0.1267  0.1098  0.2103  0.0499  0.0769  0.0755  0.1162  0.0907
 0.2063  0.1702  0.1159  0.1734  0.0527  0.0496  0.0771  0.0594  0.0956
 0.0659  0.0424  0.1235  0.2930  0.0349  0.0702  0.1096  0.1380  0.1226
 0.0709  0.0450  0.1781  0.2693  0.0392  0.0545  0.0485  0.1057  0.1888
 0.0555  0.0736  0.1004  0.1259  0.0495  0.0876  0.1151  0.0914  0.3009
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-04 21:25:03,684 train 000 1.754011e-02 -0.584088
2019-11-04 21:28:19,037 train 050 1.091477e-02 -0.741713
2019-11-04 21:31:30,384 train 100 1.090367e-02 -0.353405
2019-11-04 21:34:39,658 train 150 1.098805e-02 -0.316454
2019-11-04 21:37:48,900 train 200 1.108352e-02 -0.243001
2019-11-04 21:40:59,388 train 250 1.112945e-02 -0.217968
2019-11-04 21:44:10,010 train 300 1.124822e-02 -0.187006
2019-11-04 21:47:18,957 train 350 1.119510e-02 -0.162939
2019-11-04 21:50:29,373 train 400 1.117257e-02 -0.176885
2019-11-04 21:53:39,562 train 450 1.119446e-02 -0.165109
2019-11-04 21:56:49,084 train 500 1.116820e-02 -0.559359
2019-11-04 21:59:58,661 train 550 1.112823e-02 -0.539833
2019-11-04 22:03:08,728 train 600 1.110527e-02 -0.586172
2019-11-04 22:06:17,296 train 650 1.111027e-02 -0.547420
2019-11-04 22:09:27,251 train 700 1.110227e-02 -0.553011
2019-11-04 22:12:37,732 train 750 1.112940e-02 -0.555244
2019-11-04 22:15:46,863 train 800 1.108538e-02 -0.633102
2019-11-04 22:18:57,756 train 850 1.109548e-02 -0.643581
2019-11-04 22:22:06,768 train 900 1.107085e-02 -0.607091
2019-11-04 22:25:15,805 train 950 1.104596e-02 -0.659611
2019-11-04 22:28:24,596 train 1000 1.101938e-02 -0.630652
2019-11-04 22:31:31,720 train 1050 1.101174e-02 -0.682292
2019-11-04 22:34:41,040 train 1100 1.101608e-02 -0.671680
2019-11-04 22:37:50,702 train 1150 1.101743e-02 -0.662874
2019-11-04 22:40:59,986 train 1200 1.099604e-02 -0.662286
2019-11-04 22:44:09,684 train 1250 1.100897e-02 -0.643624
2019-11-04 22:47:21,350 train 1300 1.101842e-02 -0.624053
2019-11-04 22:50:30,864 train 1350 1.101446e-02 -0.605759
2019-11-04 22:53:41,049 train 1400 1.101747e-02 -0.588760
2019-11-04 22:56:52,667 train 1450 1.101233e-02 -0.570937
2019-11-04 23:00:04,018 train 1500 1.100620e-02 -0.563210
2019-11-04 23:02:32,513 training loss; R2: 1.099837e-02 -0.548806
2019-11-04 23:02:33,016 valid 000 1.555758e-02 0.238349
2019-11-04 23:02:43,619 valid 050 1.124359e-02 -0.005642
2019-11-04 23:02:55,992 valid 100 1.122749e-02 -0.084892
2019-11-04 23:03:08,263 valid 150 1.108867e-02 -0.121878
2019-11-04 23:03:20,512 valid 200 1.119578e-02 -0.180380
2019-11-04 23:03:32,732 valid 250 1.111429e-02 -0.167357
2019-11-04 23:03:44,914 valid 300 1.119945e-02 -0.133112
2019-11-04 23:03:57,044 valid 350 1.120925e-02 -0.142455
2019-11-04 23:04:09,178 valid 400 1.117158e-02 -0.151956
2019-11-04 23:04:21,323 valid 450 1.121218e-02 -0.144594
2019-11-04 23:04:33,400 valid 500 1.125298e-02 -0.186616
2019-11-04 23:04:45,557 valid 550 1.123689e-02 -0.178868
2019-11-04 23:04:57,783 valid 600 1.127151e-02 -0.178017
2019-11-04 23:05:09,942 valid 650 1.127944e-02 -0.179655
2019-11-04 23:05:22,118 valid 700 1.131257e-02 -0.167996
2019-11-04 23:05:34,304 valid 750 1.130685e-02 -0.163915
2019-11-04 23:05:46,484 valid 800 1.128116e-02 -0.180892
2019-11-04 23:05:56,552 valid 850 1.128557e-02 -0.187279
2019-11-04 23:06:06,542 valid 900 1.130105e-02 -0.216703
2019-11-04 23:06:16,526 valid 950 1.129658e-02 -0.245677
2019-11-04 23:06:26,564 valid 1000 1.128724e-02 -0.235480
2019-11-04 23:06:36,578 valid 1050 1.129722e-02 -0.375067
2019-11-04 23:06:46,602 valid 1100 1.130982e-02 -0.367507
2019-11-04 23:06:57,745 valid 1150 1.131183e-02 -0.356527
2019-11-04 23:07:09,734 valid 1200 1.131986e-02 -0.353178
2019-11-04 23:07:20,633 valid 1250 1.132057e-02 -0.351811
2019-11-04 23:07:30,768 valid 1300 1.129702e-02 -0.340629
2019-11-04 23:07:40,896 valid 1350 1.129668e-02 -0.334672
2019-11-04 23:07:50,933 valid 1400 1.129835e-02 -0.429591
2019-11-04 23:08:00,930 valid 1450 1.129862e-02 -0.428050
2019-11-04 23:08:10,967 valid 1500 1.129606e-02 -0.474215
2019-11-04 23:08:18,821 validation loss; R2: 1.130310e-02 -0.470016
2019-11-04 23:08:18,969 epoch 4 lr 1.000000e-03
2019-11-04 23:08:18,970 genotype = Genotype(normal=[('dil_conv_3x3', 0), ('dil_conv_5x5', 1), ('dil_conv_5x5', 2), ('max_pool_2x2', 1), ('dil_conv_5x5', 0), ('dil_conv_5x5', 1), ('max_pool_3x3', 1), ('max_pool_3x3', 0)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 0), ('max_pool_3x3', 1), ('max_pool_3x3', 0), ('dil_conv_5x5', 2), ('dil_conv_5x5', 2), ('max_pool_3x3', 1), ('dil_conv_5x5', 4), ('max_pool_3x3', 3)], reduce_concat=range(2, 6))
2019-11-04 23:08:18,972 
alphas_normal = Variable containing:
 0.1263  0.0822  0.1268  0.0735  0.0188  0.1186  0.0209  0.2187  0.2142
 0.6103  0.0389  0.0344  0.0257  0.0150  0.0360  0.0352  0.0798  0.1246
 0.2602  0.0728  0.1768  0.1467  0.0260  0.0751  0.0508  0.0392  0.1525
 0.3317  0.0700  0.1982  0.1288  0.0288  0.0397  0.0388  0.0303  0.1338
 0.4698  0.0730  0.0445  0.0336  0.0234  0.0221  0.0420  0.0909  0.2008
 0.0993  0.0532  0.1800  0.2134  0.0283  0.0572  0.0418  0.0523  0.2746
 0.3733  0.0611  0.0979  0.0932  0.0297  0.0865  0.0758  0.0378  0.1447
 0.4563  0.1155  0.0965  0.0585  0.0283  0.0672  0.0961  0.0286  0.0529
 0.4854  0.0622  0.0716  0.0590  0.0290  0.0910  0.0435  0.0518  0.1064
 0.3692  0.0359  0.1363  0.2142  0.0220  0.0212  0.0700  0.0382  0.0928
 0.2934  0.0380  0.0734  0.2564  0.0223  0.1960  0.0304  0.0367  0.0534
 0.4640  0.0676  0.0792  0.0836  0.0252  0.1198  0.0324  0.0638  0.0645
 0.6757  0.0456  0.0462  0.0481  0.0220  0.0208  0.0294  0.0242  0.0880
 0.3707  0.0628  0.0793  0.0580  0.0269  0.0333  0.1291  0.0386  0.2015
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-04 23:08:18,974 
alphas_reduce = Variable containing:
 0.0685  0.0926  0.1204  0.3962  0.0403  0.0458  0.0422  0.0764  0.1176
 0.1562  0.1192  0.1564  0.1796  0.0347  0.0984  0.0625  0.1289  0.0641
 0.0744  0.0380  0.1122  0.5475  0.0484  0.0249  0.0261  0.0548  0.0736
 0.1980  0.0387  0.1958  0.2208  0.0545  0.0381  0.1532  0.0556  0.0454
 0.0552  0.0400  0.0483  0.0656  0.0250  0.0309  0.1602  0.0547  0.5200
 0.1742  0.1344  0.1011  0.2208  0.0554  0.0475  0.1110  0.0706  0.0849
 0.2086  0.0698  0.1141  0.3197  0.0573  0.0666  0.0714  0.0522  0.0403
 0.0569  0.0382  0.0434  0.0682  0.0256  0.0431  0.2430  0.0888  0.3928
 0.0625  0.0481  0.0647  0.1091  0.0372  0.0644  0.1599  0.1896  0.2646
 0.1453  0.0985  0.1379  0.2022  0.0750  0.0668  0.0772  0.1240  0.0731
 0.1998  0.1074  0.1330  0.1693  0.0672  0.0662  0.0808  0.0543  0.1222
 0.0752  0.0517  0.0865  0.2153  0.0329  0.0588  0.1411  0.2120  0.1264
 0.0825  0.0732  0.1440  0.2564  0.0510  0.0743  0.0495  0.0904  0.1786
 0.0778  0.0640  0.0703  0.0805  0.0476  0.1217  0.1518  0.1089  0.2774
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-04 23:08:23,703 train 000 8.659117e-03 -0.224615
2019-11-04 23:11:30,004 train 050 1.039384e-02 0.042295
2019-11-04 23:14:36,944 train 100 1.039457e-02 -0.626976
2019-11-04 23:17:45,209 train 150 1.054946e-02 -0.466830
2019-11-04 23:20:57,092 train 200 1.065196e-02 -0.404629
2019-11-04 23:24:09,525 train 250 1.061688e-02 -0.432507
2019-11-04 23:27:20,855 train 300 1.057520e-02 -0.605430
2019-11-04 23:30:32,736 train 350 1.052546e-02 -0.517909
2019-11-04 23:33:44,990 train 400 1.053522e-02 -0.460966
2019-11-04 23:36:52,163 train 450 1.053660e-02 -0.423875
2019-11-04 23:39:58,023 train 500 1.058116e-02 -0.397367
2019-11-04 23:43:05,760 train 550 1.057233e-02 -0.410401
2019-11-04 23:46:15,813 train 600 1.061745e-02 -0.377079
2019-11-04 23:49:22,747 train 650 1.064561e-02 -0.397574
2019-11-04 23:52:37,021 train 700 1.067684e-02 -0.378257
2019-11-04 23:55:47,543 train 750 1.069878e-02 -0.366134
2019-11-04 23:58:57,694 train 800 1.070399e-02 -0.342155
2019-11-05 00:02:11,601 train 850 1.071622e-02 -0.339897
2019-11-05 00:05:21,773 train 900 1.072462e-02 -0.320273
2019-11-05 00:08:29,801 train 950 1.072200e-02 -0.305694
2019-11-05 00:11:38,518 train 1000 1.071269e-02 -0.295538
2019-11-05 00:14:46,932 train 1050 1.069539e-02 -0.318997
2019-11-05 00:17:54,659 train 1100 1.068068e-02 -0.312275
2019-11-05 00:21:03,757 train 1150 1.066082e-02 -0.301471
2019-11-05 00:24:12,823 train 1200 1.065240e-02 -0.291111
2019-11-05 00:27:21,577 train 1250 1.067523e-02 -0.277193
2019-11-05 00:30:30,498 train 1300 1.068524e-02 -0.288004
2019-11-05 00:33:39,806 train 1350 1.067769e-02 -0.478898
2019-11-05 00:36:49,576 train 1400 1.067410e-02 -0.460016
2019-11-05 00:39:58,950 train 1450 1.067328e-02 -0.476962
2019-11-05 00:43:08,700 train 1500 1.068723e-02 -0.460022
2019-11-05 00:45:39,451 training loss; R2: 1.067894e-02 -1.010574
2019-11-05 00:45:39,983 valid 000 8.658295e-03 -0.237079
2019-11-05 00:45:50,306 valid 050 1.009400e-02 0.128646
2019-11-05 00:46:00,567 valid 100 1.020556e-02 0.039892
2019-11-05 00:46:10,816 valid 150 1.011332e-02 -0.025739
2019-11-05 00:46:21,058 valid 200 1.007268e-02 -0.050552
2019-11-05 00:46:31,281 valid 250 1.007088e-02 -0.040128
2019-11-05 00:46:41,497 valid 300 1.008549e-02 -0.025078
2019-11-05 00:46:51,682 valid 350 1.007035e-02 -0.039148
2019-11-05 00:47:01,834 valid 400 1.011296e-02 -0.029178
2019-11-05 00:47:12,020 valid 450 1.009022e-02 -0.016436
2019-11-05 00:47:22,205 valid 500 1.008525e-02 -0.052851
2019-11-05 00:47:32,331 valid 550 1.006049e-02 -0.121585
2019-11-05 00:47:42,413 valid 600 1.005867e-02 -0.113004
2019-11-05 00:47:52,533 valid 650 1.010108e-02 -0.095809
2019-11-05 00:48:02,632 valid 700 1.010794e-02 -0.651706
2019-11-05 00:48:12,767 valid 750 1.008881e-02 -0.598246
2019-11-05 00:48:23,039 valid 800 1.011248e-02 -0.561690
2019-11-05 00:48:33,181 valid 850 1.011133e-02 -0.548636
2019-11-05 00:48:43,288 valid 900 1.010283e-02 -0.517981
2019-11-05 00:48:53,411 valid 950 1.007289e-02 -0.489837
2019-11-05 00:49:03,513 valid 1000 1.009869e-02 -0.510208
2019-11-05 00:49:13,617 valid 1050 1.007560e-02 -0.480226
2019-11-05 00:49:23,736 valid 1100 1.007913e-02 -0.468075
2019-11-05 00:49:33,843 valid 1150 1.009217e-02 -0.455728
2019-11-05 00:49:43,925 valid 1200 1.009607e-02 -0.438234
2019-11-05 00:49:54,000 valid 1250 1.008084e-02 -0.416333
2019-11-05 00:50:03,954 valid 1300 1.008009e-02 -0.407160
2019-11-05 00:50:13,885 valid 1350 1.006917e-02 -0.408917
2019-11-05 00:50:24,091 valid 1400 1.006446e-02 -0.395309
2019-11-05 00:50:34,310 valid 1450 1.006729e-02 -0.379729
2019-11-05 00:50:44,431 valid 1500 1.007505e-02 -0.368205
2019-11-05 00:50:52,321 validation loss; R2: 1.007960e-02 -0.360173
2019-11-05 00:50:52,471 epoch 5 lr 1.000000e-03
2019-11-05 00:50:52,472 genotype = Genotype(normal=[('dil_conv_5x5', 0), ('dil_conv_5x5', 1), ('dil_conv_5x5', 2), ('max_pool_2x2', 0), ('dil_conv_5x5', 0), ('dil_conv_5x5', 1), ('max_pool_3x3', 1), ('dil_conv_5x5', 4)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 0), ('max_pool_3x3', 1), ('dil_conv_5x5', 2), ('max_pool_3x3', 0), ('dil_conv_5x5', 2), ('dil_conv_5x5', 3), ('max_pool_3x3', 3), ('dil_conv_5x5', 4)], reduce_concat=range(2, 6))
2019-11-05 00:50:52,473 
alphas_normal = Variable containing:
 0.1370  0.0947  0.1503  0.0808  0.0270  0.1085  0.0158  0.1681  0.2178
 0.7046  0.0269  0.0237  0.0167  0.0131  0.0242  0.0179  0.0525  0.1204
 0.3342  0.0633  0.1787  0.1293  0.0283  0.0551  0.0476  0.0370  0.1265
 0.4156  0.0421  0.1585  0.1078  0.0237  0.0449  0.0427  0.0281  0.1366
 0.4219  0.0499  0.0392  0.0317  0.0218  0.0302  0.0662  0.1017  0.2374
 0.1048  0.0489  0.1757  0.2215  0.0302  0.0399  0.0369  0.0362  0.3059
 0.4108  0.0501  0.0865  0.0871  0.0292  0.0767  0.0550  0.0601  0.1446
 0.5584  0.0800  0.0859  0.0646  0.0249  0.0466  0.0749  0.0266  0.0382
 0.4985  0.0538  0.0713  0.0613  0.0280  0.0964  0.0699  0.0392  0.0816
 0.5307  0.0231  0.0858  0.1524  0.0176  0.0182  0.0754  0.0351  0.0617
 0.3851  0.0350  0.0676  0.2150  0.0254  0.1286  0.0377  0.0477  0.0578
 0.4882  0.0452  0.0653  0.0860  0.0217  0.1538  0.0330  0.0556  0.0512
 0.6673  0.0374  0.0462  0.0504  0.0197  0.0443  0.0365  0.0259  0.0723
 0.3900  0.0451  0.0683  0.0584  0.0219  0.0574  0.1271  0.0325  0.1993
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-05 00:50:52,475 
alphas_reduce = Variable containing:
 0.0814  0.0624  0.1068  0.4639  0.0317  0.0363  0.0511  0.0403  0.1262
 0.1569  0.1128  0.1723  0.1806  0.0444  0.0856  0.0711  0.1038  0.0726
 0.1001  0.0409  0.0847  0.5546  0.0335  0.0265  0.0222  0.0895  0.0480
 0.2335  0.0366  0.1846  0.1935  0.0525  0.0334  0.1672  0.0543  0.0443
 0.0368  0.0272  0.0384  0.0557  0.0205  0.0305  0.1246  0.0489  0.6176
 0.2863  0.1291  0.0824  0.1781  0.0521  0.0402  0.0750  0.0759  0.0809
 0.2460  0.0583  0.1150  0.2841  0.0516  0.0777  0.0737  0.0521  0.0415
 0.0538  0.0321  0.0493  0.0731  0.0290  0.0327  0.2636  0.0881  0.3783
 0.0478  0.0360  0.0592  0.0853  0.0345  0.0522  0.1456  0.2448  0.2946
 0.2103  0.0927  0.0742  0.1249  0.0406  0.0918  0.0840  0.1709  0.1106
 0.1595  0.0995  0.1064  0.1238  0.0569  0.1719  0.0719  0.0761  0.1340
 0.0687  0.0366  0.0843  0.2184  0.0307  0.1113  0.1623  0.1786  0.1092
 0.0636  0.0376  0.1280  0.3238  0.0337  0.0633  0.0539  0.1041  0.1919
 0.0691  0.0570  0.1132  0.1314  0.0460  0.1066  0.1305  0.0846  0.2616
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-05 00:50:57,245 train 000 1.053977e-02 -0.139410
2019-11-05 00:54:07,164 train 050 1.013398e-02 -0.037012
2019-11-05 00:57:17,537 train 100 1.040650e-02 -0.005622
2019-11-05 01:00:27,974 train 150 1.047572e-02 -0.067567
2019-11-05 01:03:37,289 train 200 1.041683e-02 -2.215321
2019-11-05 01:06:48,060 train 250 1.041877e-02 -1.825447
2019-11-05 01:09:59,722 train 300 1.044109e-02 -1.532558
2019-11-05 01:13:09,042 train 350 1.043735e-02 -1.316267
2019-11-05 01:16:19,544 train 400 1.043780e-02 -1.236492
2019-11-05 01:19:30,442 train 450 1.042953e-02 -1.102755
2019-11-05 01:22:40,530 train 500 1.040564e-02 -1.003892
2019-11-05 01:25:50,641 train 550 1.041831e-02 -0.918755
2019-11-05 01:29:00,087 train 600 1.039489e-02 -0.939773
2019-11-05 01:32:10,096 train 650 1.040898e-02 -0.904784
2019-11-05 01:35:20,694 train 700 1.040371e-02 -0.878649
2019-11-05 01:38:30,090 train 750 1.041320e-02 -0.820511
2019-11-05 01:41:41,174 train 800 1.041745e-02 -0.805861
2019-11-05 01:44:51,620 train 850 1.038490e-02 -0.756142
2019-11-05 01:48:00,353 train 900 1.037705e-02 -0.737890
2019-11-05 01:51:08,566 train 950 1.036571e-02 -0.705874
2019-11-05 01:54:17,361 train 1000 1.034094e-02 -0.668521
2019-11-05 01:57:27,031 train 1050 1.031768e-02 -0.647324
2019-11-05 02:00:35,749 train 1100 1.031166e-02 -0.619589
2019-11-05 02:03:44,434 train 1150 1.031448e-02 -0.606609
2019-11-05 02:06:52,918 train 1200 1.030255e-02 -1.063867
2019-11-05 02:10:01,961 train 1250 1.030124e-02 -1.119269
2019-11-05 02:13:12,105 train 1300 1.029915e-02 -1.110910
2019-11-05 02:16:21,105 train 1350 1.030040e-02 -1.075095
2019-11-05 02:19:30,068 train 1400 1.029396e-02 -1.038459
2019-11-05 02:22:40,317 train 1450 1.028636e-02 -1.009083
2019-11-05 02:25:49,612 train 1500 1.029194e-02 -0.979328
2019-11-05 02:28:20,229 training loss; R2: 1.029312e-02 -0.960625
2019-11-05 02:28:20,779 valid 000 1.049533e-02 0.306483
2019-11-05 02:28:31,328 valid 050 9.238018e-03 -0.258804
2019-11-05 02:28:41,616 valid 100 9.235207e-03 -0.065221
2019-11-05 02:28:51,749 valid 150 9.440728e-03 -0.029610
2019-11-05 02:29:01,815 valid 200 9.405658e-03 -0.016987
2019-11-05 02:29:12,046 valid 250 9.322977e-03 -0.250842
2019-11-05 02:29:22,560 valid 300 9.338457e-03 -0.873024
2019-11-05 02:29:32,793 valid 350 9.341708e-03 -0.727997
2019-11-05 02:29:43,006 valid 400 9.353485e-03 -0.610730
2019-11-05 02:29:53,210 valid 450 9.405548e-03 -0.641438
2019-11-05 02:30:03,408 valid 500 9.385675e-03 -0.587915
2019-11-05 02:30:13,627 valid 550 9.396620e-03 -0.531168
2019-11-05 02:30:23,881 valid 600 9.393398e-03 -0.997801
2019-11-05 02:30:34,093 valid 650 9.380453e-03 -0.911230
2019-11-05 02:30:44,300 valid 700 9.352809e-03 -0.839748
2019-11-05 02:30:54,491 valid 750 9.379579e-03 -0.771054
2019-11-05 02:31:04,633 valid 800 9.382190e-03 -0.716786
2019-11-05 02:31:14,960 valid 850 9.356447e-03 -0.668293
2019-11-05 02:31:25,585 valid 900 9.374388e-03 -0.621327
2019-11-05 02:31:36,100 valid 950 9.361526e-03 -0.588014
2019-11-05 02:31:46,516 valid 1000 9.343780e-03 -0.555069
2019-11-05 02:31:56,856 valid 1050 9.338402e-03 -0.521137
2019-11-05 02:32:07,112 valid 1100 9.351844e-03 -0.490451
2019-11-05 02:32:17,383 valid 1150 9.354553e-03 -0.483113
2019-11-05 02:32:27,620 valid 1200 9.360821e-03 -0.459440
2019-11-05 02:32:37,808 valid 1250 9.359304e-03 -0.433014
2019-11-05 02:32:48,054 valid 1300 9.340174e-03 -0.410789
2019-11-05 02:32:58,398 valid 1350 9.339340e-03 -0.392590
2019-11-05 02:33:08,716 valid 1400 9.335735e-03 -0.417784
2019-11-05 02:33:18,948 valid 1450 9.348987e-03 -0.405428
2019-11-05 02:33:29,176 valid 1500 9.357113e-03 -0.387856
2019-11-05 02:33:37,116 validation loss; R2: 9.347954e-03 -0.381091
2019-11-05 02:33:37,268 epoch 6 lr 1.000000e-03
2019-11-05 02:33:37,269 genotype = Genotype(normal=[('dil_conv_5x5', 0), ('dil_conv_5x5', 1), ('dil_conv_5x5', 2), ('max_pool_2x2', 0), ('dil_conv_5x5', 0), ('dil_conv_5x5', 3), ('dil_conv_5x5', 4), ('max_pool_3x3', 1)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 0), ('skip_connect', 1), ('max_pool_3x3', 0), ('dil_conv_5x5', 2), ('dil_conv_5x5', 2), ('dil_conv_5x5', 3), ('dil_conv_5x5', 4), ('max_pool_3x3', 2)], reduce_concat=range(2, 6))
2019-11-05 02:33:37,271 
alphas_normal = Variable containing:
 0.1109  0.1050  0.1582  0.0786  0.0316  0.0959  0.0171  0.1915  0.2114
 0.7164  0.0223  0.0192  0.0145  0.0139  0.0254  0.0161  0.0585  0.1138
 0.3430  0.0612  0.1826  0.1232  0.0344  0.0791  0.0489  0.0358  0.0919
 0.4210  0.0374  0.1343  0.0979  0.0249  0.0473  0.0523  0.0284  0.1565
 0.4195  0.0476  0.0358  0.0268  0.0295  0.0283  0.0589  0.0934  0.2602
 0.1008  0.0390  0.1642  0.2015  0.0247  0.0410  0.0288  0.0425  0.3575
 0.5933  0.0315  0.0532  0.0636  0.0229  0.0774  0.0377  0.0340  0.0863
 0.6288  0.0645  0.0637  0.0405  0.0226  0.0453  0.0633  0.0338  0.0375
 0.5789  0.0487  0.0567  0.0484  0.0276  0.0642  0.0596  0.0271  0.0887
 0.6211  0.0234  0.0712  0.1231  0.0171  0.0162  0.0563  0.0291  0.0426
 0.4032  0.0252  0.0424  0.1985  0.0231  0.1350  0.0518  0.0641  0.0567
 0.4184  0.0575  0.0820  0.0910  0.0319  0.1562  0.0533  0.0624  0.0472
 0.7348  0.0323  0.0331  0.0392  0.0191  0.0341  0.0311  0.0199  0.0564
 0.3880  0.0454  0.0605  0.0526  0.0274  0.0540  0.1371  0.0264  0.2086
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-05 02:33:37,272 
alphas_reduce = Variable containing:
 0.0665  0.0639  0.1196  0.4906  0.0498  0.0236  0.0369  0.0439  0.1052
 0.2026  0.1493  0.1361  0.1462  0.0417  0.0738  0.0583  0.1165  0.0754
 0.0550  0.0277  0.0702  0.6729  0.0411  0.0226  0.0191  0.0603  0.0312
 0.3056  0.0440  0.1303  0.1828  0.0527  0.0367  0.1582  0.0515  0.0382
 0.0529  0.0329  0.0407  0.0623  0.0286  0.0277  0.1565  0.0325  0.5659
 0.1941  0.1285  0.0863  0.1833  0.0666  0.0580  0.1121  0.0927  0.0785
 0.3767  0.0566  0.0873  0.2031  0.0588  0.0568  0.0575  0.0578  0.0453
 0.0652  0.0376  0.0566  0.0834  0.0350  0.0474  0.2587  0.0820  0.3342
 0.0529  0.0364  0.0510  0.0636  0.0390  0.0452  0.1762  0.2026  0.3331
 0.1109  0.0680  0.0907  0.1322  0.0635  0.0929  0.0587  0.1899  0.1933
 0.1299  0.0906  0.1120  0.1650  0.0735  0.1659  0.0662  0.0628  0.1341
 0.0734  0.0554  0.0965  0.2656  0.0420  0.0863  0.1081  0.1679  0.1048
 0.0656  0.0542  0.0979  0.2596  0.0494  0.0452  0.0736  0.1043  0.2502
 0.0664  0.0568  0.0838  0.1127  0.0479  0.0928  0.1163  0.0711  0.3522
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-05 02:33:42,080 train 000 1.140265e-02 0.355831
2019-11-05 02:36:51,017 train 050 1.054324e-02 -0.059948
2019-11-05 02:40:00,850 train 100 1.042463e-02 -0.069060
2019-11-05 02:43:09,800 train 150 1.051985e-02 -0.091517
2019-11-05 02:46:19,613 train 200 1.049304e-02 -0.078827
2019-11-05 02:49:28,642 train 250 1.039904e-02 -0.081055
2019-11-05 02:52:38,284 train 300 1.034251e-02 -0.175095
2019-11-05 02:55:47,435 train 350 1.029917e-02 -0.163485
2019-11-05 02:58:55,692 train 400 1.024738e-02 -0.162332
2019-11-05 03:02:05,719 train 450 1.026554e-02 -0.149842
2019-11-05 03:05:14,944 train 500 1.025403e-02 -0.163990
2019-11-05 03:08:22,579 train 550 1.019683e-02 -0.154909
2019-11-05 03:11:31,980 train 600 1.021056e-02 -0.157606
2019-11-05 03:14:43,500 train 650 1.020641e-02 -0.152257
2019-11-05 03:17:54,481 train 700 1.019720e-02 -0.178878
2019-11-05 03:21:05,082 train 750 1.017062e-02 -0.171576
2019-11-05 03:24:14,438 train 800 1.018800e-02 -0.162715
2019-11-05 03:27:23,571 train 850 1.016322e-02 -0.160659
2019-11-05 03:30:33,553 train 900 1.014950e-02 -0.147900
2019-11-05 03:33:44,382 train 950 1.017358e-02 -0.703339
2019-11-05 03:36:54,904 train 1000 1.015877e-02 -0.680374
2019-11-05 03:40:04,630 train 1050 1.014743e-02 -0.654149
2019-11-05 03:43:14,126 train 1100 1.013503e-02 -0.625737
2019-11-05 03:46:24,829 train 1150 1.011948e-02 -0.595541
2019-11-05 03:49:33,217 train 1200 1.010398e-02 -0.609705
2019-11-05 03:52:43,381 train 1250 1.010106e-02 -0.592198
2019-11-05 03:55:52,405 train 1300 1.009254e-02 -0.572284
2019-11-05 03:59:01,302 train 1350 1.008678e-02 -0.557196
2019-11-05 04:02:10,931 train 1400 1.007286e-02 -0.650874
2019-11-05 04:05:25,547 train 1450 1.007991e-02 -0.633432
2019-11-05 04:08:30,879 train 1500 1.006977e-02 -0.608689
2019-11-05 04:11:03,779 training loss; R2: 1.006842e-02 -0.593286
2019-11-05 04:11:04,303 valid 000 8.029029e-03 0.320457
2019-11-05 04:11:15,814 valid 050 1.045018e-02 -3.420491
2019-11-05 04:11:28,018 valid 100 1.008145e-02 -1.688937
2019-11-05 04:11:40,158 valid 150 1.002907e-02 -1.094877
2019-11-05 04:11:52,383 valid 200 1.001934e-02 -0.836989
2019-11-05 04:12:04,590 valid 250 1.007280e-02 -0.725731
2019-11-05 04:12:16,780 valid 300 1.001872e-02 -0.606030
2019-11-05 04:12:28,962 valid 350 1.002748e-02 -0.509708
2019-11-05 04:12:41,134 valid 400 1.007616e-02 -0.438849
2019-11-05 04:12:53,315 valid 450 1.008971e-02 -0.393327
2019-11-05 04:13:05,465 valid 500 1.010100e-02 -0.624997
2019-11-05 04:13:17,702 valid 550 1.016444e-02 -0.568447
2019-11-05 04:13:29,921 valid 600 1.015585e-02 -0.512150
2019-11-05 04:13:42,086 valid 650 1.015430e-02 -0.507734
2019-11-05 04:13:52,885 valid 700 1.016175e-02 -0.469669
2019-11-05 04:14:02,957 valid 750 1.014524e-02 -0.437909
2019-11-05 04:14:12,995 valid 800 1.014531e-02 -0.415056
2019-11-05 04:14:23,025 valid 850 1.016337e-02 -0.389219
2019-11-05 04:14:33,094 valid 900 1.017123e-02 -0.587538
2019-11-05 04:14:43,171 valid 950 1.014619e-02 -0.618189
2019-11-05 04:14:53,094 valid 1000 1.014360e-02 -0.884631
2019-11-05 04:15:03,138 valid 1050 1.014694e-02 -0.842952
2019-11-05 04:15:13,054 valid 1100 1.014034e-02 -0.802850
2019-11-05 04:15:23,103 valid 1150 1.013073e-02 -0.765182
2019-11-05 04:15:33,114 valid 1200 1.013689e-02 -0.756342
2019-11-05 04:15:43,036 valid 1250 1.011264e-02 -0.723172
2019-11-05 04:15:53,031 valid 1300 1.012244e-02 -0.690657
2019-11-05 04:16:03,007 valid 1350 1.012201e-02 -0.677384
2019-11-05 04:16:12,953 valid 1400 1.011977e-02 -0.660315
2019-11-05 04:16:22,915 valid 1450 1.011349e-02 -0.695218
2019-11-05 04:16:32,894 valid 1500 1.011358e-02 -0.668840
2019-11-05 04:16:40,645 validation loss; R2: 1.011712e-02 -0.651193
2019-11-05 04:16:40,785 epoch 7 lr 1.000000e-03
2019-11-05 04:16:40,786 genotype = Genotype(normal=[('dil_conv_5x5', 0), ('dil_conv_5x5', 1), ('dil_conv_5x5', 2), ('dil_conv_5x5', 1), ('dil_conv_5x5', 0), ('sep_conv_5x5', 2), ('dil_conv_5x5', 4), ('max_pool_3x3', 1)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 0), ('skip_connect', 1), ('max_pool_3x3', 0), ('dil_conv_5x5', 2), ('dil_conv_3x3', 3), ('sep_conv_5x5', 2), ('dil_conv_5x5', 4), ('max_pool_3x3', 2)], reduce_concat=range(2, 6))
2019-11-05 04:16:40,788 
alphas_normal = Variable containing:
 0.1015  0.1042  0.1421  0.0553  0.0342  0.0879  0.0178  0.1878  0.2691
 0.7488  0.0208  0.0189  0.0162  0.0158  0.0164  0.0087  0.0520  0.1024
 0.3789  0.0598  0.1547  0.0829  0.0356  0.0676  0.0584  0.0558  0.1063
 0.4004  0.0365  0.1244  0.1000  0.0282  0.0340  0.0735  0.0254  0.1776
 0.5051  0.0562  0.0305  0.0199  0.0307  0.0220  0.0390  0.0556  0.2410
 0.0791  0.0456  0.1820  0.2335  0.0334  0.0613  0.0256  0.0386  0.3010
 0.5665  0.0397  0.0679  0.0794  0.0385  0.0752  0.0357  0.0342  0.0628
 0.6796  0.0501  0.0535  0.0341  0.0206  0.0365  0.0828  0.0150  0.0278
 0.6364  0.0458  0.0494  0.0430  0.0258  0.0512  0.0614  0.0192  0.0678
 0.5573  0.0294  0.0793  0.1163  0.0243  0.0264  0.0804  0.0372  0.0494
 0.4884  0.0361  0.0412  0.1588  0.0388  0.0969  0.0648  0.0342  0.0407
 0.5093  0.0579  0.0772  0.0688  0.0320  0.1186  0.0487  0.0533  0.0341
 0.7396  0.0456  0.0421  0.0401  0.0291  0.0299  0.0165  0.0153  0.0418
 0.4494  0.0392  0.0448  0.0370  0.0214  0.0614  0.1402  0.0273  0.1792
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-05 04:16:40,790 
alphas_reduce = Variable containing:
 0.0525  0.0770  0.0894  0.4783  0.0511  0.0226  0.0746  0.0318  0.1228
 0.2370  0.1575  0.1250  0.1260  0.0399  0.0693  0.0570  0.1254  0.0628
 0.0408  0.0228  0.0402  0.7715  0.0309  0.0167  0.0174  0.0382  0.0214
 0.4860  0.0355  0.0904  0.1409  0.0436  0.0295  0.1046  0.0460  0.0235
 0.0335  0.0244  0.0307  0.0445  0.0203  0.0205  0.1479  0.0235  0.6546
 0.2534  0.1131  0.0963  0.1326  0.1063  0.0550  0.1073  0.0684  0.0677
 0.4542  0.0479  0.0937  0.1385  0.0592  0.0724  0.0520  0.0433  0.0387
 0.0910  0.0405  0.0561  0.0567  0.0358  0.0552  0.2970  0.0818  0.2860
 0.0405  0.0302  0.0390  0.0405  0.0320  0.0440  0.2011  0.3101  0.2627
 0.0642  0.0477  0.0573  0.0912  0.0523  0.0677  0.0806  0.2652  0.2739
 0.1942  0.1008  0.0900  0.1334  0.0663  0.1859  0.0551  0.0585  0.1159
 0.0894  0.0529  0.0944  0.3029  0.0457  0.0704  0.1127  0.1622  0.0696
 0.0842  0.0574  0.0918  0.2679  0.0619  0.0609  0.0673  0.1192  0.1894
 0.0858  0.0678  0.0848  0.1391  0.0620  0.0564  0.1043  0.0655  0.3343
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-05 04:16:45,555 train 000 6.937363e-03 0.230430
2019-11-05 04:19:58,181 train 050 9.570336e-03 -0.013680
2019-11-05 04:23:11,423 train 100 9.973994e-03 -0.069559
2019-11-05 04:26:21,118 train 150 1.001348e-02 -0.087815
2019-11-05 04:29:28,344 train 200 9.968971e-03 -0.044905
2019-11-05 04:32:38,497 train 250 9.942451e-03 -0.060886
2019-11-05 04:35:46,438 train 300 9.941901e-03 -0.090073
2019-11-05 04:38:53,575 train 350 9.979950e-03 -0.215498
2019-11-05 04:42:03,698 train 400 1.001502e-02 -0.230879
2019-11-05 04:45:10,663 train 450 1.001488e-02 -0.201722
2019-11-05 04:48:30,609 train 500 9.985057e-03 -0.191326
2019-11-05 04:51:41,356 train 550 9.970668e-03 -0.193545
2019-11-05 04:54:52,490 train 600 1.001448e-02 -0.185717
2019-11-05 04:58:04,224 train 650 1.000015e-02 -0.190974
2019-11-05 05:01:17,838 train 700 1.000599e-02 -0.182939
2019-11-05 05:04:26,018 train 750 1.001655e-02 -0.179553
2019-11-05 05:07:35,471 train 800 1.001991e-02 -0.202876
2019-11-05 05:10:41,910 train 850 9.997500e-03 -0.191219
2019-11-05 05:13:47,290 train 900 1.001530e-02 -0.192689
2019-11-05 05:16:58,358 train 950 9.993750e-03 -0.178775
2019-11-05 05:20:10,535 train 1000 1.000531e-02 -0.190581
2019-11-05 05:23:24,013 train 1050 9.993557e-03 -0.324730
2019-11-05 05:26:36,808 train 1100 9.982848e-03 -0.320311
2019-11-05 05:29:48,649 train 1150 9.964390e-03 -0.322447
2019-11-05 05:33:04,849 train 1200 9.952360e-03 -0.322421
2019-11-05 05:36:14,260 train 1250 9.963024e-03 -0.312623
2019-11-05 05:39:22,516 train 1300 9.956251e-03 -0.300279
2019-11-05 05:42:31,930 train 1350 9.938702e-03 -0.296206
2019-11-05 05:45:43,177 train 1400 9.936032e-03 -0.422686
2019-11-05 05:48:52,182 train 1450 9.916037e-03 -0.414837
2019-11-05 05:52:00,689 train 1500 9.914599e-03 -0.400414
2019-11-05 05:54:29,890 training loss; R2: 9.925161e-03 -0.397515
2019-11-05 05:54:30,441 valid 000 9.478356e-03 0.260906
2019-11-05 05:54:40,633 valid 050 8.880900e-03 0.044936
2019-11-05 05:54:50,838 valid 100 9.089188e-03 -0.419318
2019-11-05 05:55:01,042 valid 150 9.220667e-03 -0.409448
2019-11-05 05:55:11,239 valid 200 9.202776e-03 -0.360889
2019-11-05 05:55:21,441 valid 250 9.264722e-03 -0.256821
2019-11-05 05:55:32,168 valid 300 9.226151e-03 -0.227708
2019-11-05 05:55:42,402 valid 350 9.229629e-03 -0.185498
2019-11-05 05:55:52,403 valid 400 9.234941e-03 -0.179074
2019-11-05 05:56:02,592 valid 450 9.256614e-03 -0.140668
2019-11-05 05:56:12,811 valid 500 9.269199e-03 -0.114193
2019-11-05 05:56:22,995 valid 550 9.259775e-03 -0.099465
2019-11-05 05:56:33,144 valid 600 9.253017e-03 -0.408506
2019-11-05 05:56:43,321 valid 650 9.252912e-03 -0.396393
2019-11-05 05:56:53,559 valid 700 9.269031e-03 -0.358607
2019-11-05 05:57:03,814 valid 750 9.308127e-03 -0.454861
2019-11-05 05:57:14,046 valid 800 9.303635e-03 -0.423881
2019-11-05 05:57:24,273 valid 850 9.309081e-03 -0.397390
2019-11-05 05:57:34,507 valid 900 9.306364e-03 -0.435933
2019-11-05 05:57:44,741 valid 950 9.288043e-03 -0.407650
2019-11-05 05:57:54,999 valid 1000 9.282063e-03 -0.408160
2019-11-05 05:58:05,245 valid 1050 9.294876e-03 -0.397173
2019-11-05 05:58:15,470 valid 1100 9.286654e-03 -0.381552
2019-11-05 05:58:25,739 valid 1150 9.273741e-03 -0.377329
2019-11-05 05:58:35,961 valid 1200 9.268036e-03 -0.359779
2019-11-05 05:58:46,202 valid 1250 9.284177e-03 -0.343854
2019-11-05 05:58:56,734 valid 1300 9.277271e-03 -0.333073
2019-11-05 05:59:06,999 valid 1350 9.275426e-03 -0.320123
2019-11-05 05:59:17,165 valid 1400 9.281509e-03 -0.311048
2019-11-05 05:59:27,428 valid 1450 9.280976e-03 -0.298380
2019-11-05 05:59:37,711 valid 1500 9.296976e-03 -0.283337
2019-11-05 05:59:45,700 validation loss; R2: 9.303760e-03 -0.301106
2019-11-05 05:59:45,850 epoch 8 lr 1.000000e-03
2019-11-05 05:59:45,852 genotype = Genotype(normal=[('dil_conv_3x3', 0), ('dil_conv_5x5', 1), ('dil_conv_5x5', 2), ('dil_conv_5x5', 1), ('dil_conv_5x5', 0), ('sep_conv_5x5', 2), ('dil_conv_5x5', 4), ('max_pool_3x3', 1)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 0), ('skip_connect', 1), ('max_pool_3x3', 0), ('dil_conv_5x5', 2), ('dil_conv_5x5', 2), ('dil_conv_5x5', 3), ('dil_conv_5x5', 4), ('dil_conv_5x5', 0)], reduce_concat=range(2, 6))
2019-11-05 05:59:45,853 
alphas_normal = Variable containing:
 0.0876  0.1382  0.1645  0.0536  0.0405  0.1075  0.0162  0.1975  0.1944
 0.7609  0.0187  0.0163  0.0133  0.0138  0.0167  0.0090  0.0580  0.0933
 0.3309  0.0890  0.1893  0.0834  0.0503  0.0419  0.0655  0.0451  0.1046
 0.4335  0.0289  0.0912  0.0818  0.0219  0.0444  0.0690  0.0215  0.2078
 0.4188  0.0660  0.0286  0.0201  0.0317  0.0255  0.0559  0.0594  0.2940
 0.0814  0.0579  0.1924  0.2151  0.0393  0.0372  0.0237  0.0358  0.3173
 0.6537  0.0332  0.0654  0.0646  0.0263  0.0436  0.0390  0.0276  0.0466
 0.6567  0.0638  0.0498  0.0284  0.0274  0.0337  0.1055  0.0179  0.0167
 0.6519  0.0622  0.0479  0.0357  0.0337  0.0478  0.0510  0.0152  0.0546
 0.6246  0.0317  0.0684  0.1018  0.0240  0.0217  0.0577  0.0293  0.0408
 0.5935  0.0250  0.0322  0.1362  0.0224  0.0700  0.0449  0.0296  0.0462
 0.5531  0.0691  0.0620  0.0651  0.0327  0.1249  0.0330  0.0305  0.0295
 0.7617  0.0395  0.0320  0.0339  0.0228  0.0443  0.0180  0.0164  0.0315
 0.4775  0.0399  0.0374  0.0331  0.0241  0.0560  0.1100  0.0324  0.1895
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-05 05:59:45,855 
alphas_reduce = Variable containing:
 0.0552  0.0585  0.0859  0.4975  0.0485  0.0274  0.0658  0.0396  0.1214
 0.2738  0.1719  0.1261  0.1106  0.0409  0.0754  0.0510  0.1074  0.0430
 0.0385  0.0214  0.0398  0.7680  0.0383  0.0206  0.0225  0.0288  0.0221
 0.5510  0.0321  0.0790  0.1118  0.0416  0.0310  0.0768  0.0498  0.0271
 0.0390  0.0278  0.0286  0.0390  0.0244  0.0221  0.0995  0.0234  0.6962
 0.2268  0.0859  0.0924  0.1667  0.1029  0.0837  0.1218  0.0555  0.0642
 0.4296  0.0359  0.1268  0.1329  0.0624  0.0679  0.0668  0.0369  0.0408
 0.0598  0.0415  0.0518  0.0557  0.0379  0.0488  0.2281  0.0591  0.4174
 0.0488  0.0440  0.0454  0.0489  0.0423  0.0351  0.1957  0.2243  0.3155
 0.0661  0.0490  0.0620  0.0801  0.0623  0.0639  0.1206  0.1904  0.3057
 0.1355  0.0971  0.0750  0.1037  0.0504  0.2741  0.0770  0.0637  0.1237
 0.1429  0.0613  0.0599  0.1997  0.0424  0.0787  0.1041  0.2401  0.0711
 0.1073  0.0557  0.0520  0.1716  0.0463  0.0534  0.0795  0.1534  0.2809
 0.0677  0.0506  0.0542  0.0838  0.0436  0.0641  0.1292  0.0629  0.4440
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-05 05:59:50,714 train 000 7.966726e-03 0.193177
2019-11-05 06:03:01,114 train 050 1.013068e-02 -0.036612
2019-11-05 06:06:12,785 train 100 9.813291e-03 -0.046362
2019-11-05 06:09:21,787 train 150 9.872556e-03 -0.090158
2019-11-05 06:12:30,463 train 200 9.892479e-03 -0.231097
2019-11-05 06:15:37,399 train 250 9.983133e-03 -0.418834
2019-11-05 06:18:44,259 train 300 9.923624e-03 -0.368513
2019-11-05 06:21:48,465 train 350 9.908876e-03 -0.346318
2019-11-05 06:24:53,175 train 400 9.828452e-03 -0.325508
2019-11-05 06:27:57,811 train 450 9.772399e-03 -0.288539
2019-11-05 06:30:58,041 train 500 9.767715e-03 -0.290549
2019-11-05 06:34:04,267 train 550 9.766199e-03 -0.266271
2019-11-05 06:37:14,989 train 600 9.745233e-03 -0.239830
2019-11-05 06:40:15,373 train 650 9.731501e-03 -0.263063
2019-11-05 06:43:16,337 train 700 9.728955e-03 -0.245030
2019-11-05 06:46:17,678 train 750 9.725254e-03 -0.246224
2019-11-05 06:49:18,884 train 800 9.754453e-03 -0.230826
2019-11-05 06:52:19,763 train 850 9.745109e-03 -0.212669
2019-11-05 06:55:22,269 train 900 9.730461e-03 -0.273286
2019-11-05 06:58:25,537 train 950 9.749100e-03 -0.268152
2019-11-05 07:01:29,480 train 1000 9.744293e-03 -0.260969
2019-11-05 07:04:36,160 train 1050 9.763188e-03 -0.256658
2019-11-05 07:07:39,737 train 1100 9.768367e-03 -0.240704
2019-11-05 07:10:44,987 train 1150 9.763724e-03 -0.235304
2019-11-05 07:13:50,782 train 1200 9.779383e-03 -0.232055
2019-11-05 07:16:56,118 train 1250 9.775242e-03 -0.223999
2019-11-05 07:19:56,440 train 1300 9.781327e-03 -0.222627
2019-11-05 07:22:55,651 train 1350 9.804544e-03 -0.221567
2019-11-05 07:25:55,927 train 1400 9.806859e-03 -1.554464
2019-11-05 07:28:56,869 train 1450 9.811904e-03 -1.504382
2019-11-05 07:31:58,989 train 1500 9.829248e-03 -1.453447
2019-11-05 07:34:20,637 training loss; R2: 9.827463e-03 -1.483684
2019-11-05 07:34:21,160 valid 000 9.706166e-03 -0.858174
2019-11-05 07:34:30,924 valid 050 9.587425e-03 -10.173171
2019-11-05 07:34:40,680 valid 100 9.134465e-03 -5.126834
2019-11-05 07:34:50,448 valid 150 9.056733e-03 -3.473908
2019-11-05 07:35:00,200 valid 200 9.124989e-03 -2.589222
2019-11-05 07:35:09,966 valid 250 9.073444e-03 -2.073883
2019-11-05 07:35:19,738 valid 300 9.046790e-03 -1.727180
2019-11-05 07:35:29,483 valid 350 9.089935e-03 -1.496686
2019-11-05 07:35:39,211 valid 400 9.066477e-03 -1.313600
2019-11-05 07:35:49,020 valid 450 9.044027e-03 -1.163270
2019-11-05 07:35:58,839 valid 500 9.040792e-03 -1.045514
2019-11-05 07:36:08,554 valid 550 9.041850e-03 -1.087777
2019-11-05 07:36:18,239 valid 600 9.067837e-03 -1.098520
2019-11-05 07:36:27,948 valid 650 9.088208e-03 -1.030524
2019-11-05 07:36:37,643 valid 700 9.086953e-03 -0.967301
2019-11-05 07:36:47,369 valid 750 9.066498e-03 -0.930019
2019-11-05 07:36:57,112 valid 800 9.040761e-03 -1.065308
2019-11-05 07:37:06,827 valid 850 9.050839e-03 -1.003503
2019-11-05 07:37:16,526 valid 900 9.075342e-03 -0.977260
2019-11-05 07:37:26,231 valid 950 9.071456e-03 -0.920021
2019-11-05 07:37:35,938 valid 1000 9.072988e-03 -0.877772
2019-11-05 07:37:45,607 valid 1050 9.065906e-03 -0.832021
2019-11-05 07:37:55,298 valid 1100 9.073349e-03 -0.792313
2019-11-05 07:38:04,991 valid 1150 9.067826e-03 -0.767851
2019-11-05 07:38:14,639 valid 1200 9.072775e-03 -0.761914
2019-11-05 07:38:24,273 valid 1250 9.072883e-03 -0.735272
2019-11-05 07:38:33,946 valid 1300 9.061681e-03 -0.711448
2019-11-05 07:38:43,663 valid 1350 9.061149e-03 -0.687918
2019-11-05 07:38:53,349 valid 1400 9.050739e-03 -0.665540
2019-11-05 07:39:02,993 valid 1450 9.041967e-03 -0.639848
2019-11-05 07:39:12,443 valid 1500 9.027589e-03 -0.629694
2019-11-05 07:39:19,781 validation loss; R2: 9.028585e-03 -0.613940
2019-11-05 07:39:19,915 epoch 9 lr 1.000000e-03
2019-11-05 07:39:19,916 genotype = Genotype(normal=[('dil_conv_5x5', 0), ('dil_conv_5x5', 1), ('dil_conv_5x5', 2), ('max_pool_2x2', 0), ('max_pool_3x3', 0), ('sep_conv_5x5', 2), ('sep_conv_5x5', 4), ('max_pool_3x3', 1)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 0), ('max_pool_2x2', 1), ('max_pool_3x3', 0), ('dil_conv_5x5', 2), ('dil_conv_5x5', 2), ('dil_conv_5x5', 3), ('dil_conv_5x5', 4), ('dil_conv_5x5', 0)], reduce_concat=range(2, 6))
2019-11-05 07:39:19,918 
alphas_normal = Variable containing:
 0.0869  0.1315  0.1583  0.0535  0.0421  0.1222  0.0170  0.1797  0.2087
 0.7325  0.0242  0.0210  0.0183  0.0182  0.0174  0.0090  0.0621  0.0974
 0.3437  0.0747  0.1925  0.0914  0.0535  0.0393  0.0381  0.0562  0.1105
 0.4042  0.0272  0.0894  0.1020  0.0227  0.0582  0.0944  0.0251  0.1769
 0.4199  0.0514  0.0339  0.0280  0.0300  0.0338  0.0494  0.0474  0.3062
 0.0564  0.0428  0.2012  0.2992  0.0314  0.0298  0.0177  0.0255  0.2960
 0.6366  0.0325  0.0650  0.0783  0.0252  0.0667  0.0324  0.0277  0.0355
 0.6431  0.0441  0.0575  0.0508  0.0270  0.0556  0.0820  0.0189  0.0209
 0.5766  0.0469  0.0665  0.0622  0.0317  0.0684  0.0733  0.0156  0.0587
 0.5884  0.0324  0.0741  0.1606  0.0273  0.0175  0.0406  0.0238  0.0355
 0.5019  0.0266  0.0371  0.1860  0.0239  0.0853  0.0551  0.0385  0.0455
 0.4887  0.0553  0.0664  0.1096  0.0310  0.1379  0.0395  0.0389  0.0326
 0.7533  0.0289  0.0388  0.0538  0.0212  0.0405  0.0173  0.0154  0.0307
 0.3310  0.0326  0.0464  0.0649  0.0240  0.0588  0.2139  0.0246  0.2038
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-05 07:39:19,920 
alphas_reduce = Variable containing:
 0.0474  0.0401  0.0886  0.5509  0.0443  0.0260  0.0687  0.0408  0.0932
 0.2813  0.1374  0.1581  0.1310  0.0540  0.0561  0.0505  0.0860  0.0457
 0.0374  0.0187  0.0459  0.7543  0.0448  0.0215  0.0265  0.0265  0.0246
 0.5342  0.0264  0.0917  0.1331  0.0560  0.0247  0.0555  0.0339  0.0444
 0.0463  0.0386  0.0359  0.0443  0.0325  0.0308  0.1005  0.0365  0.6346
 0.2260  0.0556  0.0798  0.1677  0.0947  0.1542  0.1095  0.0645  0.0480
 0.3942  0.0285  0.1155  0.1407  0.0625  0.1212  0.0571  0.0369  0.0434
 0.0531  0.0434  0.0498  0.0668  0.0389  0.0395  0.1526  0.0482  0.5077
 0.0515  0.0410  0.0381  0.0501  0.0460  0.0332  0.1798  0.2074  0.3528
 0.0823  0.0415  0.0615  0.0857  0.0709  0.0699  0.1208  0.1760  0.2913
 0.1607  0.0729  0.0700  0.0918  0.0622  0.2725  0.0589  0.0548  0.1562
 0.1018  0.0525  0.0633  0.2546  0.0469  0.0685  0.1187  0.2081  0.0856
 0.1106  0.0629  0.0604  0.1795  0.0754  0.0529  0.1273  0.0834  0.2476
 0.0647  0.0567  0.0583  0.0810  0.0642  0.0435  0.0973  0.0506  0.4837
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-05 07:39:24,875 train 000 1.073059e-02 0.164910
2019-11-05 07:42:27,371 train 050 1.029553e-02 -24.992787
2019-11-05 07:45:27,406 train 100 9.742993e-03 -12.651909
2019-11-05 07:48:36,781 train 150 9.786691e-03 -10.522072
2019-11-05 07:51:41,866 train 200 9.631783e-03 -7.962159
2019-11-05 07:54:48,095 train 250 9.631765e-03 -6.535414
2019-11-05 07:57:49,618 train 300 9.639120e-03 -5.461377
2019-11-05 08:00:53,185 train 350 9.654652e-03 -4.998495
2019-11-05 08:04:04,969 train 400 9.694375e-03 -4.376813
2019-11-05 08:07:09,097 train 450 9.665399e-03 -3.912957
2019-11-05 08:10:12,355 train 500 9.716365e-03 -3.635366
2019-11-05 08:13:13,182 train 550 9.707767e-03 -3.436921
2019-11-05 08:16:13,896 train 600 9.723438e-03 -3.183671
2019-11-05 08:19:13,676 train 650 9.699301e-03 -2.944114
2019-11-05 08:22:17,019 train 700 9.713958e-03 -2.734260
2019-11-05 08:25:21,237 train 750 9.704681e-03 -2.553583
2019-11-05 08:28:22,730 train 800 9.682787e-03 -2.394888
2019-11-05 08:31:23,187 train 850 9.694343e-03 -2.273034
2019-11-05 08:34:22,134 train 900 9.704853e-03 -2.156990
2019-11-05 08:37:21,601 train 950 9.712325e-03 -2.043326
2019-11-05 08:40:21,575 train 1000 9.691123e-03 -1.937742
2019-11-05 08:43:21,308 train 1050 9.689840e-03 -1.842287
2019-11-05 08:46:22,641 train 1100 9.696218e-03 -1.778994
2019-11-05 08:49:30,038 train 1150 9.704615e-03 -1.700192
2019-11-05 08:52:41,506 train 1200 9.705833e-03 -1.625191
2019-11-05 08:55:43,283 train 1250 9.700491e-03 -1.563958
2019-11-05 08:58:46,417 train 1300 9.679966e-03 -1.556218
2019-11-05 09:01:48,688 train 1350 9.692908e-03 -1.499174
2019-11-05 09:04:50,128 train 1400 9.716501e-03 -1.446133
2019-11-05 09:07:48,906 train 1450 9.689137e-03 -1.393995
2019-11-05 09:10:48,627 train 1500 9.685429e-03 -1.353354
2019-11-05 09:13:13,796 training loss; R2: 9.690327e-03 -1.319276
2019-11-05 09:13:14,315 valid 000 9.757998e-03 -0.188194
2019-11-05 09:13:24,153 valid 050 9.295587e-03 -1.504486
2019-11-05 09:13:33,959 valid 100 9.256624e-03 -0.758616
2019-11-05 09:13:43,784 valid 150 9.117004e-03 -0.504341
2019-11-05 09:13:53,597 valid 200 9.136247e-03 -0.486148
2019-11-05 09:14:03,365 valid 250 9.115216e-03 -0.820437
2019-11-05 09:14:13,063 valid 300 9.078057e-03 -0.691267
2019-11-05 09:14:22,853 valid 350 9.150592e-03 -0.594144
2019-11-05 09:14:32,667 valid 400 9.186866e-03 -0.502557
2019-11-05 09:14:42,505 valid 450 9.196508e-03 -0.448788
2019-11-05 09:14:52,303 valid 500 9.177119e-03 -0.403261
2019-11-05 09:15:02,148 valid 550 9.153021e-03 -0.361014
2019-11-05 09:15:11,959 valid 600 9.125759e-03 -0.319552
2019-11-05 09:15:21,775 valid 650 9.124197e-03 -0.303018
2019-11-05 09:15:31,887 valid 700 9.100555e-03 -0.278937
2019-11-05 09:15:41,607 valid 750 9.094218e-03 -0.253522
2019-11-05 09:15:51,292 valid 800 9.083496e-03 -0.261713
2019-11-05 09:16:00,978 valid 850 9.061936e-03 -0.244223
2019-11-05 09:16:10,613 valid 900 9.041111e-03 -0.219445
2019-11-05 09:16:20,271 valid 950 9.028288e-03 -0.219855
2019-11-05 09:16:29,957 valid 1000 9.014768e-03 -0.199935
2019-11-05 09:16:39,671 valid 1050 8.991028e-03 -0.189676
2019-11-05 09:16:49,400 valid 1100 8.978993e-03 -0.185155
2019-11-05 09:16:59,149 valid 1150 8.983002e-03 -0.172064
2019-11-05 09:17:08,854 valid 1200 8.993476e-03 -0.168827
2019-11-05 09:17:18,607 valid 1250 8.987514e-03 -0.187136
2019-11-05 09:17:27,785 valid 1300 8.980464e-03 -0.177863
2019-11-05 09:17:36,753 valid 1350 8.978559e-03 -0.249857
2019-11-05 09:17:45,744 valid 1400 8.990710e-03 -0.269328
2019-11-05 09:17:54,734 valid 1450 8.994551e-03 -0.261835
2019-11-05 09:18:03,857 valid 1500 8.989384e-03 -0.251077
2019-11-05 09:18:10,861 validation loss; R2: 8.993143e-03 -0.243955
2019-11-05 09:18:10,994 epoch 10 lr 1.000000e-03
2019-11-05 09:18:10,995 genotype = Genotype(normal=[('dil_conv_3x3', 0), ('dil_conv_5x5', 1), ('dil_conv_5x5', 2), ('max_pool_2x2', 0), ('dil_conv_5x5', 0), ('sep_conv_5x5', 2), ('sep_conv_5x5', 4), ('max_pool_3x3', 1)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 0), ('max_pool_3x3', 1), ('max_pool_3x3', 0), ('dil_conv_5x5', 2), ('dil_conv_5x5', 2), ('dil_conv_5x5', 3), ('dil_conv_5x5', 4), ('dil_conv_5x5', 0)], reduce_concat=range(2, 6))
2019-11-05 09:18:10,997 
alphas_normal = Variable containing:
 0.0680  0.1310  0.1691  0.0578  0.0367  0.1296  0.0154  0.2127  0.1797
 0.6964  0.0317  0.0291  0.0272  0.0222  0.0139  0.0090  0.0678  0.1026
 0.3618  0.0943  0.1948  0.0783  0.0407  0.0432  0.0327  0.0776  0.0767
 0.4138  0.0296  0.0940  0.1359  0.0221  0.0441  0.0593  0.0199  0.1813
 0.3695  0.0432  0.0282  0.0246  0.0220  0.0374  0.0320  0.0600  0.3831
 0.0518  0.0292  0.1883  0.2791  0.0192  0.0249  0.0166  0.0222  0.3686
 0.6855  0.0292  0.0531  0.0790  0.0211  0.0437  0.0248  0.0368  0.0268
 0.6786  0.0371  0.0449  0.0423  0.0202  0.0481  0.0890  0.0195  0.0203
 0.6010  0.0450  0.0670  0.0545  0.0257  0.0658  0.0576  0.0179  0.0654
 0.6411  0.0289  0.0704  0.1283  0.0218  0.0244  0.0428  0.0147  0.0276
 0.4611  0.0318  0.0431  0.1758  0.0265  0.0615  0.0676  0.0762  0.0564
 0.4617  0.0628  0.0812  0.1436  0.0316  0.1364  0.0371  0.0247  0.0209
 0.7648  0.0322  0.0415  0.0561  0.0218  0.0281  0.0225  0.0133  0.0197
 0.2947  0.0322  0.0435  0.0661  0.0212  0.0721  0.2592  0.0321  0.1789
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-05 09:18:10,998 
alphas_reduce = Variable containing:
 0.0541  0.0370  0.1020  0.5231  0.0505  0.0162  0.0833  0.0344  0.0994
 0.2231  0.1320  0.1573  0.1795  0.0612  0.0400  0.0765  0.0841  0.0464
 0.0238  0.0144  0.0312  0.8420  0.0270  0.0120  0.0132  0.0187  0.0176
 0.5460  0.0257  0.0865  0.1225  0.0501  0.0298  0.0637  0.0394  0.0364
 0.0566  0.0479  0.0463  0.0540  0.0368  0.0310  0.0874  0.0367  0.6033
 0.1576  0.0948  0.0821  0.1238  0.1421  0.0879  0.1135  0.1175  0.0806
 0.4680  0.0454  0.1002  0.1065  0.0614  0.0647  0.0377  0.0458  0.0702
 0.0884  0.0453  0.0413  0.0396  0.0410  0.0388  0.2156  0.0567  0.4333
 0.0698  0.0339  0.0323  0.0331  0.0404  0.0592  0.1916  0.2237  0.3159
 0.0656  0.0412  0.0598  0.0774  0.0559  0.0849  0.0661  0.1754  0.3737
 0.1411  0.0947  0.0796  0.1022  0.0622  0.2536  0.0406  0.0502  0.1758
 0.0934  0.0557  0.0708  0.2181  0.0423  0.0809  0.1614  0.2164  0.0610
 0.0910  0.0664  0.0739  0.1815  0.0640  0.1058  0.1356  0.0675  0.2143
 0.0930  0.0580  0.0610  0.0846  0.0490  0.0379  0.1226  0.0430  0.4509
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-05 09:18:16,234 train 000 1.018564e-02 -0.493218
2019-11-05 09:21:06,201 train 050 9.789787e-03 -1.967758
2019-11-05 09:23:55,829 train 100 9.706773e-03 -1.258631
2019-11-05 09:26:46,043 train 150 9.684016e-03 -0.943078
2019-11-05 09:29:38,165 train 200 9.718458e-03 -0.684308
2019-11-05 09:32:38,639 train 250 9.716765e-03 -0.549003
2019-11-05 09:35:27,721 train 300 9.701653e-03 -0.883537
2019-11-05 09:38:17,400 train 350 9.729654e-03 -0.778962
2019-11-05 09:41:08,710 train 400 9.661043e-03 -0.678573
2019-11-05 09:44:10,382 train 450 9.611967e-03 -0.633307
2019-11-05 09:47:16,021 train 500 9.620204e-03 -0.575550
2019-11-05 09:50:15,022 train 550 9.605711e-03 -0.529944
2019-11-05 09:53:17,969 train 600 9.665637e-03 -0.493567
2019-11-05 09:56:19,726 train 650 9.678890e-03 -0.467747
2019-11-05 09:59:20,287 train 700 9.663128e-03 -417.754436
2019-11-05 10:02:20,329 train 750 9.646426e-03 -390.059030
2019-11-05 10:05:32,874 train 800 9.611036e-03 -366.346781
2019-11-05 10:08:34,723 train 850 9.641660e-03 -344.833420
2019-11-05 10:11:34,913 train 900 9.637657e-03 -338.240522
2019-11-05 10:14:42,286 train 950 9.631328e-03 -320.459822
2019-11-05 10:17:45,737 train 1000 9.629836e-03 -304.447711
2019-11-05 10:20:46,043 train 1050 9.628167e-03 -289.963621
2019-11-05 10:23:45,681 train 1100 9.624907e-03 -276.800052
2019-11-05 10:26:46,078 train 1150 9.626598e-03 -264.790996
2019-11-05 10:29:44,469 train 1200 9.612079e-03 -253.763078
2019-11-05 10:32:42,969 train 1250 9.601986e-03 -243.626978
2019-11-05 10:35:42,403 train 1300 9.598706e-03 -234.275028
2019-11-05 10:38:48,110 train 1350 9.597944e-03 -225.604310
2019-11-05 10:41:50,136 train 1400 9.595298e-03 -217.564794
2019-11-05 10:44:56,666 train 1450 9.595394e-03 -210.071293
2019-11-05 10:48:03,691 train 1500 9.595390e-03 -203.068744
2019-11-05 10:50:26,825 training loss; R2: 9.596163e-03 -197.996176
2019-11-05 10:50:27,365 valid 000 1.129139e-02 0.235620
2019-11-05 10:50:37,146 valid 050 9.930952e-03 0.088337
2019-11-05 10:50:46,935 valid 100 9.347868e-03 0.092423
2019-11-05 10:50:56,785 valid 150 9.282122e-03 0.096246
2019-11-05 10:51:06,640 valid 200 9.284825e-03 0.096660
2019-11-05 10:51:16,483 valid 250 9.234244e-03 -0.062217
2019-11-05 10:51:26,336 valid 300 9.189070e-03 -0.018359
2019-11-05 10:51:36,170 valid 350 9.163576e-03 -0.017381
2019-11-05 10:51:46,004 valid 400 9.193547e-03 -0.368711
2019-11-05 10:51:55,856 valid 450 9.207103e-03 -0.311766
2019-11-05 10:52:05,674 valid 500 9.225880e-03 -0.272518
2019-11-05 10:52:15,512 valid 550 9.225781e-03 -0.240160
2019-11-05 10:52:25,349 valid 600 9.163440e-03 -0.219012
2019-11-05 10:52:35,229 valid 650 9.155033e-03 -0.206066
2019-11-05 10:52:45,042 valid 700 9.172278e-03 -0.191228
2019-11-05 10:52:54,860 valid 750 9.169042e-03 -0.192338
2019-11-05 10:53:04,677 valid 800 9.167354e-03 -0.174523
2019-11-05 10:53:14,492 valid 850 9.174948e-03 -0.161036
2019-11-05 10:53:24,277 valid 900 9.188972e-03 -0.149956
2019-11-05 10:53:34,083 valid 950 9.188146e-03 -0.137735
2019-11-05 10:53:43,891 valid 1000 9.171898e-03 -0.131211
2019-11-05 10:53:53,480 valid 1050 9.162197e-03 -0.121221
2019-11-05 10:54:02,898 valid 1100 9.155704e-03 -0.119518
2019-11-05 10:54:12,455 valid 1150 9.142365e-03 -0.111617
2019-11-05 10:54:22,227 valid 1200 9.130590e-03 -0.106928
2019-11-05 10:54:31,959 valid 1250 9.135249e-03 -0.106689
2019-11-05 10:54:41,717 valid 1300 9.125474e-03 -0.147994
2019-11-05 10:54:51,550 valid 1350 9.138719e-03 -0.140319
2019-11-05 10:55:01,408 valid 1400 9.129577e-03 -0.134029
2019-11-05 10:55:11,248 valid 1450 9.121691e-03 -0.126020
2019-11-05 10:55:21,090 valid 1500 9.126797e-03 -0.114237
2019-11-05 10:55:28,740 validation loss; R2: 9.128146e-03 -0.109753
2019-11-05 10:55:28,878 epoch 11 lr 1.000000e-03
2019-11-05 10:55:28,879 genotype = Genotype(normal=[('dil_conv_3x3', 0), ('dil_conv_3x3', 1), ('dil_conv_5x5', 2), ('max_pool_2x2', 0), ('dil_conv_5x5', 0), ('sep_conv_5x5', 2), ('dil_conv_5x5', 4), ('max_pool_3x3', 1)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 0), ('skip_connect', 1), ('max_pool_3x3', 0), ('dil_conv_5x5', 2), ('dil_conv_5x5', 3), ('dil_conv_5x5', 2), ('dil_conv_5x5', 4), ('dil_conv_5x5', 0)], reduce_concat=range(2, 6))
2019-11-05 10:55:28,881 
alphas_normal = Variable containing:
 0.0818  0.1443  0.1508  0.0480  0.0388  0.1611  0.0128  0.2073  0.1551
 0.7653  0.0210  0.0204  0.0160  0.0159  0.0146  0.0090  0.0715  0.0662
 0.2901  0.1135  0.2285  0.0812  0.0531  0.0435  0.0474  0.0778  0.0649
 0.3710  0.0309  0.0896  0.1219  0.0243  0.0517  0.0821  0.0221  0.2063
 0.3384  0.0513  0.0345  0.0290  0.0299  0.0324  0.0350  0.0464  0.4030
 0.0602  0.0369  0.2082  0.2852  0.0260  0.0264  0.0139  0.0235  0.3196
 0.6689  0.0298  0.0627  0.0838  0.0228  0.0511  0.0397  0.0248  0.0164
 0.6448  0.0488  0.0481  0.0447  0.0247  0.0429  0.1109  0.0186  0.0165
 0.6527  0.0386  0.0478  0.0424  0.0221  0.0752  0.0517  0.0134  0.0560
 0.6653  0.0211  0.0523  0.0821  0.0162  0.0349  0.0883  0.0172  0.0227
 0.4238  0.0354  0.0521  0.2058  0.0294  0.0550  0.0696  0.0673  0.0616
 0.3833  0.0619  0.1007  0.1631  0.0307  0.1785  0.0376  0.0237  0.0206
 0.8266  0.0181  0.0237  0.0326  0.0127  0.0369  0.0219  0.0153  0.0121
 0.3344  0.0336  0.0333  0.0443  0.0196  0.0665  0.1897  0.0476  0.2310
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-05 10:55:28,882 
alphas_reduce = Variable containing:
 0.0365  0.0211  0.0976  0.6234  0.0463  0.0151  0.0762  0.0183  0.0656
 0.2978  0.1662  0.1218  0.1569  0.0499  0.0399  0.0513  0.0742  0.0420
 0.0239  0.0116  0.0296  0.8578  0.0245  0.0125  0.0141  0.0156  0.0105
 0.6646  0.0210  0.0668  0.0978  0.0405  0.0212  0.0367  0.0249  0.0265
 0.0386  0.0298  0.0316  0.0415  0.0217  0.0238  0.0610  0.0243  0.7276
 0.1655  0.0572  0.1335  0.1474  0.1424  0.1176  0.0777  0.0872  0.0715
 0.4283  0.0390  0.1076  0.1003  0.0650  0.0682  0.0427  0.0448  0.1042
 0.1291  0.0599  0.0817  0.0719  0.0490  0.0396  0.1610  0.0348  0.3729
 0.0567  0.0364  0.0407  0.0351  0.0411  0.0614  0.1626  0.1299  0.4359
 0.0588  0.0351  0.0735  0.1024  0.0623  0.1009  0.0938  0.1329  0.3403
 0.1544  0.0821  0.0927  0.1013  0.0676  0.2708  0.0384  0.0632  0.1295
 0.0846  0.0792  0.1063  0.2265  0.0542  0.0916  0.1173  0.1956  0.0447
 0.0644  0.0607  0.0746  0.1358  0.0537  0.0679  0.2070  0.0750  0.2608
 0.0626  0.0481  0.0478  0.0618  0.0413  0.0331  0.1195  0.0373  0.5484
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-05 10:55:33,857 train 000 5.454695e-03 -0.093019
2019-11-05 10:58:37,856 train 050 9.251898e-03 -0.061481
2019-11-05 11:01:41,133 train 100 9.335331e-03 -0.065494
2019-11-05 11:04:43,035 train 150 9.478470e-03 -0.131145
2019-11-05 11:07:41,623 train 200 9.447270e-03 -0.091684
2019-11-05 11:10:41,192 train 250 9.440032e-03 -0.068868
2019-11-05 11:13:51,945 train 300 9.532976e-03 -2.443154
2019-11-05 11:16:53,568 train 350 9.582064e-03 -2.103912
2019-11-05 11:19:55,019 train 400 9.551346e-03 -1.844427
2019-11-05 11:22:55,929 train 450 9.567353e-03 -1.639868
2019-11-05 11:25:56,355 train 500 9.542408e-03 -1.461752
2019-11-05 11:28:56,294 train 550 9.508892e-03 -1.852507
2019-11-05 11:31:57,543 train 600 9.494892e-03 -2.179353
2019-11-05 11:35:00,079 train 650 9.499723e-03 -2.007169
2019-11-05 11:38:00,992 train 700 9.514495e-03 -1.870710
2019-11-05 11:41:01,124 train 750 9.512557e-03 -1.749899
2019-11-05 11:44:03,833 train 800 9.492829e-03 -1.645835
2019-11-05 11:47:03,469 train 850 9.490186e-03 -1.551282
2019-11-05 11:50:03,917 train 900 9.466495e-03 -1.461655
2019-11-05 11:53:10,066 train 950 9.461218e-03 -1.410039
2019-11-05 11:56:18,657 train 1000 9.460871e-03 -4.548393
2019-11-05 11:59:25,016 train 1050 9.469095e-03 -4.383443
2019-11-05 12:02:29,881 train 1100 9.460425e-03 -4.193363
2019-11-05 12:05:34,200 train 1150 9.476520e-03 -4.027410
2019-11-05 12:08:38,691 train 1200 9.474563e-03 -3.866803
2019-11-05 12:11:43,362 train 1250 9.484113e-03 -3.717717
2019-11-05 12:14:46,920 train 1300 9.469760e-03 -3.571427
2019-11-05 12:17:49,220 train 1350 9.484593e-03 -3.436891
2019-11-05 12:20:53,424 train 1400 9.492846e-03 -3.464986
2019-11-05 12:23:57,184 train 1450 9.487334e-03 -3.348219
2019-11-05 12:27:00,145 train 1500 9.473242e-03 -3.236098
2019-11-05 12:29:26,936 training loss; R2: 9.469892e-03 -3.156525
2019-11-05 12:29:27,478 valid 000 6.701112e-03 -0.009995
2019-11-05 12:29:37,505 valid 050 8.853311e-03 -25.468112
2019-11-05 12:29:47,509 valid 100 9.085400e-03 -12.848814
2019-11-05 12:29:57,474 valid 150 9.232056e-03 -8.549578
2019-11-05 12:30:07,316 valid 200 9.237143e-03 -6.412294
2019-11-05 12:30:16,977 valid 250 9.133474e-03 -5.134821
2019-11-05 12:30:26,666 valid 300 9.157476e-03 -4.276119
2019-11-05 12:30:36,497 valid 350 9.187959e-03 -3.653164
2019-11-05 12:30:46,354 valid 400 9.188200e-03 -3.195782
2019-11-05 12:30:56,223 valid 450 9.141179e-03 -2.965477
2019-11-05 12:31:06,133 valid 500 9.158520e-03 -2.690956
2019-11-05 12:31:16,005 valid 550 9.165651e-03 -2.449943
2019-11-05 12:31:25,886 valid 600 9.158359e-03 -2.245296
2019-11-05 12:31:35,789 valid 650 9.165666e-03 -2.506677
2019-11-05 12:31:45,692 valid 700 9.183585e-03 -2.317703
2019-11-05 12:31:55,556 valid 750 9.177092e-03 -2.157500
2019-11-05 12:32:05,433 valid 800 9.171644e-03 -2.020997
2019-11-05 12:32:15,313 valid 850 9.155162e-03 -1.933579
2019-11-05 12:32:25,109 valid 900 9.151051e-03 -1.823196
2019-11-05 12:32:34,979 valid 950 9.145512e-03 -1.732524
2019-11-05 12:32:44,867 valid 1000 9.143510e-03 -1.642750
2019-11-05 12:32:54,745 valid 1050 9.144120e-03 -1.592844
2019-11-05 12:33:04,669 valid 1100 9.143843e-03 -1.516911
2019-11-05 12:33:14,579 valid 1150 9.123354e-03 -1.567875
2019-11-05 12:33:24,467 valid 1200 9.103953e-03 -1.515102
2019-11-05 12:33:34,158 valid 1250 9.100649e-03 -1.454407
2019-11-05 12:33:43,856 valid 1300 9.118667e-03 -1.407196
2019-11-05 12:33:53,726 valid 1350 9.108172e-03 -1.375632
2019-11-05 12:34:03,612 valid 1400 9.114455e-03 -1.326358
2019-11-05 12:34:13,500 valid 1450 9.114354e-03 -1.299107
2019-11-05 12:34:23,377 valid 1500 9.124019e-03 -1.252129
2019-11-05 12:34:31,059 validation loss; R2: 9.118224e-03 -1.219482
2019-11-05 12:34:31,201 epoch 12 lr 1.000000e-03
2019-11-05 12:34:31,202 genotype = Genotype(normal=[('dil_conv_3x3', 0), ('dil_conv_3x3', 1), ('dil_conv_5x5', 2), ('dil_conv_5x5', 1), ('dil_conv_5x5', 0), ('sep_conv_5x5', 2), ('dil_conv_5x5', 4), ('sep_conv_3x3', 2)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 0), ('max_pool_3x3', 1), ('max_pool_3x3', 0), ('dil_conv_5x5', 2), ('dil_conv_5x5', 3), ('dil_conv_5x5', 2), ('dil_conv_5x5', 4), ('dil_conv_5x5', 0)], reduce_concat=range(2, 6))
2019-11-05 12:34:31,204 
alphas_normal = Variable containing:
 0.0672  0.1420  0.1573  0.0488  0.0322  0.1686  0.0294  0.2133  0.1411
 0.8096  0.0186  0.0167  0.0162  0.0143  0.0119  0.0092  0.0526  0.0510
 0.2989  0.1164  0.2132  0.0647  0.0420  0.0450  0.0344  0.0968  0.0886
 0.3717  0.0271  0.0700  0.1312  0.0222  0.0523  0.0724  0.0167  0.2365
 0.3462  0.0475  0.0295  0.0228  0.0251  0.0344  0.0326  0.0512  0.4106
 0.0370  0.0347  0.1818  0.2299  0.0228  0.0269  0.0199  0.0245  0.4224
 0.6917  0.0221  0.0412  0.0999  0.0181  0.0602  0.0266  0.0261  0.0142
 0.6950  0.0393  0.0302  0.0336  0.0192  0.0334  0.1155  0.0180  0.0158
 0.6402  0.0424  0.0546  0.0474  0.0265  0.0824  0.0503  0.0182  0.0380
 0.7267  0.0294  0.0458  0.0599  0.0198  0.0220  0.0559  0.0216  0.0189
 0.5249  0.0271  0.0337  0.1344  0.0242  0.0416  0.0432  0.0829  0.0880
 0.4868  0.0739  0.0911  0.1148  0.0328  0.1375  0.0354  0.0147  0.0131
 0.8437  0.0214  0.0257  0.0285  0.0163  0.0211  0.0219  0.0106  0.0107
 0.4160  0.0381  0.0344  0.0377  0.0222  0.0645  0.1465  0.0350  0.2055
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-05 12:34:31,206 
alphas_reduce = Variable containing:
 0.0377  0.0266  0.1217  0.5851  0.0516  0.0164  0.0639  0.0146  0.0824
 0.3259  0.1292  0.1216  0.1483  0.0659  0.0275  0.0500  0.0896  0.0420
 0.0344  0.0199  0.0547  0.7782  0.0364  0.0211  0.0181  0.0165  0.0207
 0.6781  0.0235  0.0544  0.0687  0.0345  0.0272  0.0452  0.0487  0.0197
 0.0364  0.0247  0.0268  0.0334  0.0224  0.0200  0.0895  0.0224  0.7242
 0.1633  0.0769  0.1299  0.1194  0.1460  0.0984  0.0903  0.1237  0.0522
 0.4457  0.0530  0.1048  0.0969  0.0699  0.0452  0.0421  0.0680  0.0745
 0.1627  0.0557  0.0752  0.0874  0.0597  0.0396  0.1906  0.0424  0.2866
 0.0451  0.0368  0.0383  0.0365  0.0434  0.1145  0.2106  0.1295  0.3453
 0.0427  0.0304  0.0490  0.0640  0.0423  0.0814  0.0722  0.0946  0.5233
 0.1292  0.1157  0.0831  0.0885  0.0588  0.2644  0.0374  0.0765  0.1465
 0.0982  0.0763  0.0858  0.2274  0.0552  0.0736  0.1325  0.1920  0.0588
 0.0670  0.0581  0.0563  0.1012  0.0530  0.0740  0.2146  0.0654  0.3105
 0.0624  0.0481  0.0445  0.0618  0.0382  0.0468  0.0811  0.0540  0.5631
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-05 12:34:36,228 train 000 9.759994e-03 0.346142
2019-11-05 12:37:41,041 train 050 9.058980e-03 -3.459779
2019-11-05 12:40:49,284 train 100 9.338639e-03 -1.821448
2019-11-05 12:43:56,662 train 150 9.320897e-03 -1.186896
2019-11-05 12:46:56,464 train 200 9.299410e-03 -0.882466
2019-11-05 12:49:57,727 train 250 9.386837e-03 -1.007932
2019-11-05 12:52:58,994 train 300 9.435410e-03 -0.833490
2019-11-05 12:55:56,283 train 350 9.448851e-03 -0.726553
2019-11-05 12:58:54,648 train 400 9.504012e-03 -0.655760
2019-11-05 13:02:09,432 train 450 9.527028e-03 -0.738286
2019-11-05 13:05:13,389 train 500 9.527286e-03 -0.841922
2019-11-05 13:08:22,263 train 550 9.484612e-03 -0.944963
2019-11-05 13:11:23,856 train 600 9.465009e-03 -0.886091
2019-11-05 13:14:37,089 train 650 9.425314e-03 -0.819172
2019-11-05 13:17:45,784 train 700 9.420303e-03 -0.776420
2019-11-05 13:20:46,911 train 750 9.438713e-03 -0.740294
2019-11-05 13:23:47,935 train 800 9.437118e-03 -0.696441
2019-11-05 13:26:51,644 train 850 9.432319e-03 -1.153146
2019-11-05 13:29:53,255 train 900 9.438097e-03 -10.826695
2019-11-05 13:32:55,319 train 950 9.421078e-03 -10.612663
2019-11-05 13:36:05,414 train 1000 9.435221e-03 -10.096116
2019-11-05 13:39:08,430 train 1050 9.437839e-03 -9.614939
2019-11-05 13:42:17,381 train 1100 9.426649e-03 -9.180214
2019-11-05 13:45:22,217 train 1150 9.426307e-03 -8.786807
2019-11-05 13:48:36,605 train 1200 9.421424e-03 -8.629012
2019-11-05 13:51:39,371 train 1250 9.406166e-03 -8.397149
2019-11-05 13:54:43,628 train 1300 9.405704e-03 -8.075548
2019-11-05 13:57:50,425 train 1350 9.405597e-03 -7.778434
2019-11-05 14:00:51,710 train 1400 9.418754e-03 -7.498737
2019-11-05 14:03:51,934 train 1450 9.416380e-03 -7.267227
2019-11-05 14:06:52,614 train 1500 9.429574e-03 -7.024177
2019-11-05 14:09:16,411 training loss; R2: 9.437370e-03 -6.849210
2019-11-05 14:09:16,984 valid 000 7.263903e-03 0.175944
2019-11-05 14:09:28,863 valid 050 8.953303e-03 -0.064210
2019-11-05 14:09:40,726 valid 100 9.260280e-03 -0.027211
2019-11-05 14:09:52,550 valid 150 9.202142e-03 -0.088120
2019-11-05 14:10:04,366 valid 200 9.175856e-03 -0.058231
2019-11-05 14:10:15,620 valid 250 9.107907e-03 -0.051840
2019-11-05 14:10:25,340 valid 300 9.084383e-03 -0.068747
2019-11-05 14:10:35,057 valid 350 9.068321e-03 -0.056535
2019-11-05 14:10:44,782 valid 400 9.121649e-03 -0.535410
2019-11-05 14:10:54,510 valid 450 9.141607e-03 -0.527445
2019-11-05 14:11:04,265 valid 500 9.163551e-03 -0.477215
2019-11-05 14:11:14,009 valid 550 9.147549e-03 -0.450980
2019-11-05 14:11:23,749 valid 600 9.143075e-03 -0.441394
2019-11-05 14:11:33,481 valid 650 9.140022e-03 -0.420014
2019-11-05 14:11:43,221 valid 700 9.168704e-03 -0.411111
2019-11-05 14:11:54,233 valid 750 9.176067e-03 -0.394629
2019-11-05 14:12:06,136 valid 800 9.144853e-03 -0.367661
2019-11-05 14:12:17,968 valid 850 9.156085e-03 -0.344666
2019-11-05 14:12:29,843 valid 900 9.128787e-03 -0.324278
2019-11-05 14:12:41,724 valid 950 9.130834e-03 -0.301805
2019-11-05 14:12:53,089 valid 1000 9.131929e-03 -0.284593
2019-11-05 14:13:02,854 valid 1050 9.150464e-03 -0.892776
2019-11-05 14:13:12,628 valid 1100 9.137056e-03 -0.859667
2019-11-05 14:13:22,380 valid 1150 9.131998e-03 -0.833355
2019-11-05 14:13:32,121 valid 1200 9.147688e-03 -0.805656
2019-11-05 14:13:41,868 valid 1250 9.150907e-03 -0.778750
2019-11-05 14:13:51,639 valid 1300 9.134105e-03 -0.764697
2019-11-05 14:14:01,470 valid 1350 9.134018e-03 -0.736367
2019-11-05 14:14:11,268 valid 1400 9.135445e-03 -0.711107
2019-11-05 14:14:20,994 valid 1450 9.141292e-03 -0.777593
2019-11-05 14:14:30,735 valid 1500 9.139039e-03 -0.971250
2019-11-05 14:14:38,297 validation loss; R2: 9.133591e-03 -0.952282
2019-11-05 14:14:38,447 epoch 13 lr 1.000000e-03
2019-11-05 14:14:38,448 genotype = Genotype(normal=[('dil_conv_5x5', 0), ('dil_conv_5x5', 1), ('dil_conv_5x5', 2), ('dil_conv_5x5', 1), ('dil_conv_5x5', 0), ('sep_conv_5x5', 2), ('dil_conv_5x5', 4), ('sep_conv_3x3', 2)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 0), ('skip_connect', 1), ('max_pool_3x3', 0), ('dil_conv_5x5', 2), ('dil_conv_5x5', 3), ('dil_conv_5x5', 2), ('dil_conv_5x5', 0), ('dil_conv_5x5', 4)], reduce_concat=range(2, 6))
2019-11-05 14:14:38,449 
alphas_normal = Variable containing:
 0.0609  0.1512  0.1609  0.0505  0.0324  0.1510  0.0348  0.1727  0.1857
 0.7937  0.0208  0.0185  0.0157  0.0159  0.0128  0.0116  0.0553  0.0557
 0.2773  0.1347  0.2103  0.0586  0.0548  0.0386  0.0540  0.0953  0.0763
 0.3418  0.0259  0.0585  0.1224  0.0224  0.0631  0.0833  0.0116  0.2710
 0.3551  0.0393  0.0259  0.0216  0.0243  0.0564  0.0408  0.0458  0.3909
 0.0303  0.0450  0.2072  0.2464  0.0379  0.0194  0.0166  0.0255  0.3716
 0.6942  0.0331  0.0484  0.0984  0.0243  0.0415  0.0163  0.0274  0.0165
 0.7229  0.0367  0.0290  0.0273  0.0237  0.0216  0.1097  0.0164  0.0126
 0.6128  0.0420  0.0650  0.0548  0.0303  0.1049  0.0399  0.0154  0.0350
 0.6440  0.0437  0.0621  0.0756  0.0279  0.0344  0.0579  0.0328  0.0216
 0.5172  0.0279  0.0447  0.1302  0.0237  0.0390  0.0502  0.0979  0.0692
 0.4737  0.0632  0.1097  0.1179  0.0312  0.1311  0.0465  0.0119  0.0149
 0.7903  0.0263  0.0337  0.0385  0.0219  0.0395  0.0300  0.0100  0.0097
 0.3938  0.0342  0.0427  0.0451  0.0243  0.0587  0.1599  0.0285  0.2126
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-05 14:14:38,451 
alphas_reduce = Variable containing:
 0.0319  0.0288  0.1234  0.5767  0.0421  0.0141  0.0792  0.0129  0.0908
 0.4464  0.1303  0.0883  0.1155  0.0396  0.0225  0.0521  0.0684  0.0370
 0.0220  0.0161  0.0358  0.8188  0.0257  0.0225  0.0219  0.0167  0.0204
 0.7188  0.0212  0.0408  0.0573  0.0311  0.0290  0.0319  0.0470  0.0230
 0.0549  0.0487  0.0568  0.0797  0.0432  0.0309  0.0726  0.0293  0.5839
 0.1607  0.0841  0.1192  0.1177  0.1385  0.1078  0.0647  0.1221  0.0852
 0.5098  0.0659  0.0778  0.0772  0.0629  0.0469  0.0385  0.0484  0.0727
 0.1992  0.0603  0.0926  0.1121  0.0627  0.0427  0.1451  0.0430  0.2423
 0.0575  0.0337  0.0377  0.0356  0.0384  0.1200  0.1748  0.1431  0.3593
 0.0377  0.0269  0.0400  0.0436  0.0399  0.0776  0.0674  0.0631  0.6039
 0.1247  0.0939  0.0677  0.0717  0.0509  0.3329  0.0426  0.0700  0.1456
 0.1124  0.0712  0.0742  0.2305  0.0550  0.0493  0.1419  0.2037  0.0618
 0.0765  0.0588  0.0571  0.1088  0.0546  0.1056  0.1499  0.0646  0.3243
 0.0647  0.0552  0.0515  0.0647  0.0466  0.0479  0.1079  0.0353  0.5263
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-05 14:14:43,407 train 000 1.032855e-02 0.037287
2019-11-05 14:17:47,823 train 050 9.666475e-03 -0.166184
2019-11-05 14:20:53,937 train 100 9.601258e-03 -0.353834
2019-11-05 14:23:57,851 train 150 9.621563e-03 -2.291831
2019-11-05 14:27:02,339 train 200 9.603634e-03 -1.783361
2019-11-05 14:30:05,780 train 250 9.505503e-03 -1.514756
2019-11-05 14:33:11,014 train 300 9.480187e-03 -1.280062
2019-11-05 14:36:20,811 train 350 9.461285e-03 -1.097004
2019-11-05 14:39:23,606 train 400 9.478876e-03 -0.975591
2019-11-05 14:42:32,688 train 450 9.515560e-03 -0.864698
2019-11-05 14:45:33,620 train 500 9.494977e-03 -0.783161
2019-11-05 14:48:34,100 train 550 9.501544e-03 -1.313876
2019-11-05 14:51:34,411 train 600 9.471871e-03 -1.200519
2019-11-05 14:54:34,850 train 650 9.460233e-03 -1.118672
2019-11-05 14:57:42,176 train 700 9.468612e-03 -1.053131
2019-11-05 15:00:44,322 train 750 9.460050e-03 -0.986475
2019-11-05 15:03:45,983 train 800 9.426434e-03 -0.918190
2019-11-05 15:06:49,929 train 850 9.417367e-03 -1.229224
2019-11-05 15:09:54,411 train 900 9.396567e-03 -1.179372
2019-11-05 15:12:54,506 train 950 9.420856e-03 -1.122885
2019-11-05 15:15:55,947 train 1000 9.415080e-03 -1.070769
2019-11-05 15:18:59,205 train 1050 9.427186e-03 -1.019476
2019-11-05 15:22:07,171 train 1100 9.429084e-03 -0.972194
2019-11-05 15:25:14,224 train 1150 9.405817e-03 -0.959387
2019-11-05 15:28:20,459 train 1200 9.401213e-03 -0.920762
2019-11-05 15:31:28,581 train 1250 9.383034e-03 -0.896857
2019-11-05 15:34:30,923 train 1300 9.379846e-03 -0.864567
2019-11-05 15:37:35,109 train 1350 9.367627e-03 -0.888284
2019-11-05 15:40:35,620 train 1400 9.351137e-03 -0.865265
2019-11-05 15:43:35,815 train 1450 9.357945e-03 -0.836337
2019-11-05 15:46:37,257 train 1500 9.357894e-03 -0.809726
2019-11-05 15:49:00,026 training loss; R2: 9.370744e-03 -0.820173
2019-11-05 15:49:00,547 valid 000 6.928281e-03 0.531367
2019-11-05 15:49:10,359 valid 050 8.500575e-03 0.253009
2019-11-05 15:49:20,146 valid 100 8.505865e-03 0.234486
2019-11-05 15:49:29,909 valid 150 8.502397e-03 -0.383617
2019-11-05 15:49:39,699 valid 200 8.567297e-03 -0.426101
2019-11-05 15:49:49,479 valid 250 8.574515e-03 -0.317238
2019-11-05 15:49:59,345 valid 300 8.583858e-03 -0.241756
2019-11-05 15:50:10,604 valid 350 8.574178e-03 -0.205902
2019-11-05 15:50:22,532 valid 400 8.575912e-03 -0.249613
2019-11-05 15:50:34,245 valid 450 8.570065e-03 -0.270023
2019-11-05 15:50:44,000 valid 500 8.566617e-03 -0.228612
2019-11-05 15:50:53,818 valid 550 8.539141e-03 -0.190506
2019-11-05 15:51:03,597 valid 600 8.532949e-03 -0.170309
2019-11-05 15:51:13,410 valid 650 8.542842e-03 -0.145436
2019-11-05 15:51:23,225 valid 700 8.545806e-03 -0.125817
2019-11-05 15:51:33,027 valid 750 8.556711e-03 -0.126197
2019-11-05 15:51:42,871 valid 800 8.600733e-03 -0.108129
2019-11-05 15:51:52,681 valid 850 8.603762e-03 -0.093789
2019-11-05 15:52:02,489 valid 900 8.607819e-03 -0.088247
2019-11-05 15:52:12,355 valid 950 8.617319e-03 -0.078777
2019-11-05 15:52:22,196 valid 1000 8.621168e-03 -0.071250
2019-11-05 15:52:31,959 valid 1050 8.619078e-03 -0.059281
2019-11-05 15:52:41,699 valid 1100 8.616995e-03 -0.216547
2019-11-05 15:52:51,433 valid 1150 8.611072e-03 -0.210320
2019-11-05 15:53:01,183 valid 1200 8.617014e-03 -0.196380
2019-11-05 15:53:10,963 valid 1250 8.605837e-03 -0.183217
2019-11-05 15:53:20,729 valid 1300 8.601616e-03 -0.167537
2019-11-05 15:53:30,492 valid 1350 8.605939e-03 -0.165846
2019-11-05 15:53:40,199 valid 1400 8.598595e-03 -0.155510
2019-11-05 15:53:49,954 valid 1450 8.607721e-03 -0.157877
2019-11-05 15:53:59,700 valid 1500 8.618086e-03 -0.194845
2019-11-05 15:54:07,279 validation loss; R2: 8.619633e-03 -0.185443
2019-11-05 15:54:07,440 epoch 14 lr 1.000000e-03
2019-11-05 15:54:07,440 genotype = Genotype(normal=[('dil_conv_3x3', 0), ('dil_conv_3x3', 1), ('dil_conv_5x5', 2), ('dil_conv_5x5', 1), ('dil_conv_5x5', 0), ('sep_conv_5x5', 2), ('dil_conv_5x5', 4), ('sep_conv_3x3', 2)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 0), ('skip_connect', 1), ('max_pool_3x3', 0), ('dil_conv_5x5', 2), ('dil_conv_5x5', 3), ('dil_conv_5x5', 2), ('dil_conv_5x5', 0), ('dil_conv_5x5', 4)], reduce_concat=range(2, 6))
2019-11-05 15:54:07,442 
alphas_normal = Variable containing:
 0.0495  0.1748  0.1574  0.0459  0.0363  0.1464  0.0188  0.2021  0.1687
 0.7855  0.0174  0.0153  0.0133  0.0138  0.0126  0.0120  0.0762  0.0539
 0.2681  0.1530  0.1942  0.0718  0.0514  0.0587  0.0443  0.0831  0.0754
 0.3114  0.0303  0.0531  0.1352  0.0250  0.0639  0.0910  0.0159  0.2742
 0.4174  0.0469  0.0303  0.0235  0.0275  0.0271  0.0342  0.0325  0.3607
 0.0370  0.0353  0.1784  0.2725  0.0273  0.0287  0.0157  0.0271  0.3780
 0.7373  0.0281  0.0386  0.0820  0.0194  0.0406  0.0180  0.0241  0.0118
 0.7069  0.0369  0.0310  0.0279  0.0204  0.0379  0.1092  0.0160  0.0138
 0.6389  0.0478  0.0559  0.0559  0.0267  0.0826  0.0360  0.0166  0.0397
 0.7344  0.0350  0.0505  0.0611  0.0222  0.0239  0.0308  0.0227  0.0193
 0.5687  0.0234  0.0372  0.1067  0.0203  0.0389  0.0513  0.1103  0.0431
 0.5267  0.0576  0.0985  0.1111  0.0302  0.1147  0.0352  0.0148  0.0111
 0.7972  0.0352  0.0350  0.0422  0.0229  0.0289  0.0187  0.0109  0.0089
 0.4033  0.0393  0.0441  0.0461  0.0238  0.0477  0.1445  0.0279  0.2232
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-05 15:54:07,444 
alphas_reduce = Variable containing:
 0.0354  0.0271  0.1210  0.5735  0.0475  0.0159  0.0722  0.0138  0.0936
 0.4116  0.1459  0.0908  0.1186  0.0384  0.0324  0.0635  0.0611  0.0379
 0.0244  0.0171  0.0326  0.8168  0.0276  0.0194  0.0244  0.0191  0.0186
 0.6664  0.0284  0.0480  0.0681  0.0348  0.0323  0.0452  0.0556  0.0213
 0.0455  0.0412  0.0412  0.0596  0.0351  0.0206  0.0951  0.0270  0.6346
 0.1403  0.1118  0.1347  0.1194  0.1384  0.0852  0.0687  0.1104  0.0910
 0.4907  0.0695  0.0962  0.0812  0.0612  0.0360  0.0377  0.0508  0.0768
 0.2044  0.0521  0.0760  0.0833  0.0563  0.0591  0.1633  0.0331  0.2724
 0.0396  0.0290  0.0297  0.0277  0.0296  0.1090  0.1530  0.1156  0.4669
 0.0517  0.0357  0.0522  0.0529  0.0543  0.1345  0.0462  0.0717  0.5008
 0.1304  0.1021  0.0656  0.0694  0.0508  0.3096  0.0516  0.0616  0.1588
 0.1621  0.0810  0.0803  0.2009  0.0604  0.0731  0.1001  0.1909  0.0512
 0.1335  0.0669  0.0725  0.0903  0.0677  0.0867  0.1247  0.0872  0.2705
 0.1148  0.0654  0.0606  0.0665  0.0535  0.0494  0.0810  0.0345  0.4742
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-05 15:54:12,564 train 000 7.586030e-03 0.157380
2019-11-05 15:57:31,465 train 050 9.330158e-03 -0.479273
2019-11-05 16:00:39,081 train 100 9.504272e-03 -0.283033
2019-11-05 16:03:40,249 train 150 9.326159e-03 -0.223806
2019-11-05 16:06:48,977 train 200 9.404808e-03 -0.206456
2019-11-05 16:09:51,052 train 250 9.392264e-03 -0.163344
2019-11-05 16:12:53,166 train 300 9.467408e-03 -0.122552
2019-11-05 16:15:53,802 train 350 9.445304e-03 -0.096211
2019-11-05 16:18:54,414 train 400 9.435480e-03 -0.097386
2019-11-05 16:22:00,837 train 450 9.487387e-03 -0.091304
2019-11-05 16:25:02,356 train 500 9.461969e-03 -0.091259
2019-11-05 16:28:04,859 train 550 9.424324e-03 -0.085162
2019-11-05 16:31:06,660 train 600 9.412448e-03 -0.090807
2019-11-05 16:34:08,790 train 650 9.385494e-03 -0.888261
2019-11-05 16:37:12,351 train 700 9.356711e-03 -0.890104
2019-11-05 16:40:12,589 train 750 9.357796e-03 -0.839235
2019-11-05 16:43:14,500 train 800 9.354502e-03 -0.797087
2019-11-05 16:46:15,260 train 850 9.343068e-03 -0.768997
2019-11-05 16:49:24,928 train 900 9.336925e-03 -0.729357
2019-11-05 16:52:27,259 train 950 9.327801e-03 -0.694922
2019-11-05 16:55:32,191 train 1000 9.309150e-03 -0.676675
2019-11-05 16:58:42,359 train 1050 9.296711e-03 -0.680534
2019-11-05 17:01:47,292 train 1100 9.301301e-03 -0.645939
2019-11-05 17:04:54,850 train 1150 9.306156e-03 -0.633035
2019-11-05 17:08:02,514 train 1200 9.305222e-03 -0.603962
2019-11-05 17:11:10,043 train 1250 9.311637e-03 -0.576030
2019-11-05 17:14:18,054 train 1300 9.312374e-03 -0.550016
2019-11-05 17:17:22,594 train 1350 9.310059e-03 -0.534327
2019-11-05 17:20:38,168 train 1400 9.303589e-03 -0.514021
2019-11-05 17:23:46,200 train 1450 9.308911e-03 -0.494271
2019-11-05 17:26:47,272 train 1500 9.316844e-03 -0.475695
2019-11-05 17:29:11,643 training loss; R2: 9.311181e-03 -0.474420
2019-11-05 17:29:12,225 valid 000 1.236922e-02 0.081223
2019-11-05 17:29:24,089 valid 050 8.387911e-03 0.124735
2019-11-05 17:29:35,977 valid 100 8.376143e-03 0.171388
2019-11-05 17:29:47,856 valid 150 8.507913e-03 -0.066590
2019-11-05 17:29:59,731 valid 200 8.509917e-03 0.022952
2019-11-05 17:30:11,621 valid 250 8.483196e-03 0.028123
2019-11-05 17:30:23,482 valid 300 8.521470e-03 0.054705
2019-11-05 17:30:35,391 valid 350 8.496160e-03 -0.105233
2019-11-05 17:30:47,300 valid 400 8.479201e-03 -0.072016
2019-11-05 17:30:59,185 valid 450 8.482990e-03 -0.035380
2019-11-05 17:31:11,056 valid 500 8.503083e-03 -0.008037
2019-11-05 17:31:22,917 valid 550 8.487716e-03 -0.026705
2019-11-05 17:31:34,741 valid 600 8.471571e-03 0.002179
2019-11-05 17:31:46,604 valid 650 8.461570e-03 -0.062071
2019-11-05 17:31:58,512 valid 700 8.459016e-03 -0.038929
2019-11-05 17:32:10,412 valid 750 8.479538e-03 -0.024212
2019-11-05 17:32:22,330 valid 800 8.488812e-03 -0.012932
2019-11-05 17:32:34,233 valid 850 8.480056e-03 -0.048175
2019-11-05 17:32:46,141 valid 900 8.462558e-03 -0.037282
2019-11-05 17:32:58,038 valid 950 8.467900e-03 -0.032216
2019-11-05 17:33:09,939 valid 1000 8.460646e-03 -0.020199
2019-11-05 17:33:21,877 valid 1050 8.472492e-03 -0.006350
2019-11-05 17:33:33,794 valid 1100 8.447387e-03 0.007444
2019-11-05 17:33:45,711 valid 1150 8.451828e-03 -0.152637
2019-11-05 17:33:57,615 valid 1200 8.463473e-03 -0.139509
2019-11-05 17:34:09,518 valid 1250 8.458832e-03 -0.128218
2019-11-05 17:34:21,053 valid 1300 8.438221e-03 -0.164151
2019-11-05 17:34:30,951 valid 1350 8.452803e-03 -0.150210
2019-11-05 17:34:40,830 valid 1400 8.452367e-03 -0.657987
2019-11-05 17:34:50,685 valid 1450 8.452388e-03 -0.627878
2019-11-05 17:35:00,538 valid 1500 8.440550e-03 -0.603856
2019-11-05 17:35:08,185 validation loss; R2: 8.436767e-03 -0.584481
2019-11-05 17:35:08,325 epoch 15 lr 1.000000e-03
2019-11-05 17:35:08,326 genotype = Genotype(normal=[('dil_conv_3x3', 0), ('dil_conv_3x3', 1), ('dil_conv_5x5', 2), ('dil_conv_5x5', 1), ('dil_conv_5x5', 0), ('sep_conv_5x5', 2), ('dil_conv_5x5', 4), ('sep_conv_3x3', 2)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 0), ('skip_connect', 1), ('max_pool_3x3', 0), ('dil_conv_5x5', 2), ('dil_conv_5x5', 3), ('dil_conv_5x5', 2), ('dil_conv_5x5', 0), ('dil_conv_5x5', 4)], reduce_concat=range(2, 6))
2019-11-05 17:35:08,328 
alphas_normal = Variable containing:
 0.0651  0.1603  0.1383  0.0437  0.0336  0.1510  0.0118  0.2016  0.1946
 0.8056  0.0141  0.0152  0.0122  0.0115  0.0090  0.0110  0.0880  0.0334
 0.2651  0.1458  0.1969  0.0785  0.0517  0.0353  0.0505  0.1104  0.0659
 0.2896  0.0258  0.0504  0.1382  0.0211  0.0559  0.1138  0.0152  0.2900
 0.3927  0.0371  0.0240  0.0197  0.0224  0.0223  0.0223  0.0276  0.4319
 0.0370  0.0326  0.1402  0.3081  0.0261  0.0355  0.0131  0.0337  0.3736
 0.7289  0.0249  0.0354  0.0838  0.0181  0.0421  0.0217  0.0325  0.0127
 0.7272  0.0362  0.0297  0.0356  0.0195  0.0301  0.0973  0.0126  0.0117
 0.6174  0.0620  0.0705  0.0659  0.0345  0.0483  0.0388  0.0181  0.0445
 0.7740  0.0244  0.0365  0.0467  0.0167  0.0231  0.0336  0.0202  0.0249
 0.6159  0.0228  0.0355  0.1121  0.0210  0.0309  0.0454  0.0778  0.0386
 0.6298  0.0377  0.0556  0.0843  0.0177  0.1292  0.0238  0.0137  0.0081
 0.8709  0.0172  0.0205  0.0258  0.0131  0.0190  0.0169  0.0081  0.0086
 0.4344  0.0343  0.0362  0.0404  0.0186  0.0354  0.1172  0.0365  0.2470
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-05 17:35:08,329 
alphas_reduce = Variable containing:
 0.0332  0.0245  0.0968  0.6318  0.0369  0.0201  0.0599  0.0128  0.0841
 0.4403  0.1290  0.0903  0.1241  0.0364  0.0392  0.0528  0.0470  0.0410
 0.0260  0.0172  0.0361  0.8198  0.0241  0.0145  0.0210  0.0169  0.0244
 0.7840  0.0179  0.0321  0.0464  0.0230  0.0209  0.0279  0.0344  0.0135
 0.0368  0.0307  0.0304  0.0426  0.0225  0.0183  0.0478  0.0407  0.7301
 0.1443  0.0970  0.1381  0.1370  0.1360  0.0842  0.0572  0.0979  0.1083
 0.5365  0.0665  0.0585  0.0578  0.0450  0.0611  0.0467  0.0712  0.0567
 0.2183  0.0515  0.0596  0.0716  0.0473  0.0609  0.1383  0.0332  0.3193
 0.0395  0.0310  0.0294  0.0287  0.0313  0.1103  0.1481  0.0873  0.4945
 0.0427  0.0320  0.0546  0.0605  0.0556  0.0862  0.0426  0.0519  0.5738
 0.1567  0.0829  0.0478  0.0495  0.0407  0.4288  0.0429  0.0671  0.0837
 0.1440  0.0812  0.0864  0.1829  0.0608  0.0658  0.1066  0.2200  0.0523
 0.1094  0.0728  0.0694  0.0871  0.0715  0.0680  0.1736  0.0554  0.2929
 0.0975  0.0642  0.0522  0.0607  0.0543  0.0434  0.0753  0.0336  0.5188
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-05 17:35:13,341 train 000 8.154093e-03 0.279175
2019-11-05 17:38:15,612 train 050 9.430255e-03 0.014829
2019-11-05 17:41:22,561 train 100 9.618501e-03 0.005377
2019-11-05 17:44:32,285 train 150 9.511851e-03 -0.057112
2019-11-05 17:47:34,601 train 200 9.406366e-03 -0.118805
2019-11-05 17:50:37,699 train 250 9.382721e-03 -0.129297
2019-11-05 17:53:39,606 train 300 9.393552e-03 -0.084795
2019-11-05 17:56:43,519 train 350 9.388407e-03 -0.063381
2019-11-05 17:59:47,869 train 400 9.392808e-03 -0.046093
2019-11-05 18:02:48,464 train 450 9.366052e-03 -0.040383
2019-11-05 18:05:54,599 train 500 9.344978e-03 -0.047642
2019-11-05 18:09:00,244 train 550 9.334274e-03 -0.047335
2019-11-05 18:12:01,322 train 600 9.320265e-03 -0.199200
2019-11-05 18:15:05,991 train 650 9.311135e-03 -0.188740
2019-11-05 18:18:10,698 train 700 9.304209e-03 -0.168517
2019-11-05 18:21:13,060 train 750 9.273652e-03 -0.163421
2019-11-05 18:24:16,516 train 800 9.302314e-03 -0.151519
2019-11-05 18:27:20,991 train 850 9.307669e-03 -0.149533
2019-11-05 18:30:31,419 train 900 9.275939e-03 -0.158960
2019-11-05 18:33:33,720 train 950 9.274036e-03 -0.146188
2019-11-05 18:36:40,035 train 1000 9.286240e-03 -0.168770
2019-11-05 18:39:53,446 train 1050 9.282926e-03 -0.176440
2019-11-05 18:43:00,721 train 1100 9.278335e-03 -0.171715
2019-11-05 18:46:02,817 train 1150 9.278171e-03 -0.169766
2019-11-05 18:49:04,403 train 1200 9.257122e-03 -0.162844
2019-11-05 18:52:19,521 train 1250 9.247657e-03 -0.155073
2019-11-05 18:55:21,411 train 1300 9.264775e-03 -0.162226
2019-11-05 18:58:22,310 train 1350 9.262014e-03 -0.155198
2019-11-05 19:01:23,305 train 1400 9.261799e-03 -0.167403
2019-11-05 19:04:31,650 train 1450 9.268129e-03 -0.248253
2019-11-05 19:07:35,022 train 1500 9.249559e-03 -0.240462
2019-11-05 19:10:09,813 training loss; R2: 9.254331e-03 -0.272686
2019-11-05 19:10:10,382 valid 000 7.460540e-03 0.135594
2019-11-05 19:10:22,270 valid 050 8.653415e-03 -0.491346
2019-11-05 19:10:34,163 valid 100 8.722089e-03 -0.336481
2019-11-05 19:10:46,113 valid 150 8.716564e-03 -0.223812
2019-11-05 19:10:58,009 valid 200 8.736152e-03 -0.154743
2019-11-05 19:11:09,872 valid 250 8.692152e-03 -0.150098
2019-11-05 19:11:21,735 valid 300 8.658913e-03 -0.133072
2019-11-05 19:11:33,567 valid 350 8.652627e-03 -0.122683
2019-11-05 19:11:45,427 valid 400 8.621747e-03 -0.091325
2019-11-05 19:11:57,285 valid 450 8.671624e-03 -0.157939
2019-11-05 19:12:09,137 valid 500 8.657178e-03 -0.138831
2019-11-05 19:12:20,100 valid 550 8.691480e-03 -0.125666
2019-11-05 19:12:29,881 valid 600 8.713321e-03 -0.104953
2019-11-05 19:12:40,813 valid 650 8.732469e-03 -0.134720
2019-11-05 19:12:52,656 valid 700 8.731444e-03 -0.129590
2019-11-05 19:13:04,514 valid 750 8.715900e-03 -0.125521
2019-11-05 19:13:16,395 valid 800 8.705534e-03 -0.112356
2019-11-05 19:13:27,691 valid 850 8.695995e-03 -0.103182
2019-11-05 19:13:37,463 valid 900 8.702528e-03 -0.139232
2019-11-05 19:13:47,229 valid 950 8.694525e-03 -0.128781
2019-11-05 19:13:57,003 valid 1000 8.688340e-03 -0.162564
2019-11-05 19:14:06,802 valid 1050 8.698072e-03 -0.150169
2019-11-05 19:14:16,592 valid 1100 8.713681e-03 -0.261080
2019-11-05 19:14:26,360 valid 1150 8.724695e-03 -0.245055
2019-11-05 19:14:36,160 valid 1200 8.713675e-03 -0.242458
2019-11-05 19:14:45,946 valid 1250 8.724045e-03 -0.232405
2019-11-05 19:14:55,725 valid 1300 8.719546e-03 -0.223523
2019-11-05 19:15:05,506 valid 1350 8.715294e-03 -0.235685
2019-11-05 19:15:15,307 valid 1400 8.723844e-03 -0.222937
2019-11-05 19:15:25,142 valid 1450 8.739747e-03 -0.217987
2019-11-05 19:15:35,130 valid 1500 8.741475e-03 -0.207172
2019-11-05 19:15:42,902 validation loss; R2: 8.745774e-03 -0.205939
2019-11-05 19:15:43,045 epoch 16 lr 1.000000e-03
2019-11-05 19:15:43,046 genotype = Genotype(normal=[('sep_conv_3x3', 0), ('dil_conv_3x3', 1), ('dil_conv_5x5', 2), ('max_pool_2x2', 0), ('dil_conv_5x5', 0), ('sep_conv_5x5', 2), ('dil_conv_5x5', 4), ('sep_conv_3x3', 2)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 0), ('skip_connect', 1), ('max_pool_3x3', 0), ('dil_conv_5x5', 2), ('dil_conv_5x5', 3), ('dil_conv_5x5', 2), ('dil_conv_5x5', 0), ('sep_conv_3x3', 1)], reduce_concat=range(2, 6))
2019-11-05 19:15:43,047 
alphas_normal = Variable containing:
 0.0645  0.1580  0.1377  0.0411  0.0320  0.2058  0.0150  0.1994  0.1464
 0.7825  0.0218  0.0207  0.0178  0.0162  0.0181  0.0140  0.0647  0.0441
 0.2709  0.1604  0.2281  0.0808  0.0566  0.0303  0.0462  0.0847  0.0420
 0.4449  0.0278  0.0503  0.1269  0.0205  0.0512  0.0712  0.0158  0.1913
 0.3063  0.0373  0.0245  0.0227  0.0235  0.0519  0.0265  0.0432  0.4641
 0.0856  0.0327  0.1508  0.2965  0.0231  0.0283  0.0187  0.0210  0.3434
 0.6805  0.0234  0.0437  0.1090  0.0167  0.0555  0.0323  0.0271  0.0118
 0.6393  0.0464  0.0471  0.0413  0.0195  0.0545  0.1211  0.0180  0.0129
 0.6423  0.0365  0.0561  0.0531  0.0186  0.0492  0.0625  0.0341  0.0476
 0.7623  0.0331  0.0381  0.0491  0.0223  0.0316  0.0311  0.0130  0.0194
 0.5724  0.0290  0.0398  0.1276  0.0252  0.0579  0.0489  0.0756  0.0236
 0.4779  0.0549  0.0718  0.1384  0.0251  0.1656  0.0340  0.0217  0.0106
 0.8623  0.0187  0.0225  0.0307  0.0134  0.0232  0.0129  0.0091  0.0072
 0.3896  0.0339  0.0373  0.0537  0.0226  0.0564  0.1407  0.0387  0.2271
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-05 19:15:43,049 
alphas_reduce = Variable containing:
 0.0644  0.0294  0.1051  0.5428  0.0379  0.0281  0.0616  0.0310  0.0997
 0.3764  0.1422  0.1099  0.1371  0.0338  0.0370  0.0623  0.0685  0.0329
 0.0290  0.0127  0.0289  0.8369  0.0220  0.0158  0.0196  0.0165  0.0186
 0.6475  0.0183  0.0696  0.0861  0.0431  0.0337  0.0421  0.0389  0.0207
 0.0555  0.0412  0.0447  0.0523  0.0282  0.0209  0.0788  0.0298  0.6486
 0.1637  0.0687  0.0932  0.1102  0.1126  0.1444  0.0624  0.1244  0.1202
 0.5585  0.0362  0.0929  0.0723  0.0663  0.0364  0.0389  0.0415  0.0569
 0.2411  0.0548  0.0489  0.0527  0.0457  0.0958  0.0990  0.0348  0.3272
 0.0335  0.0309  0.0277  0.0281  0.0323  0.0946  0.1495  0.0769  0.5266
 0.0499  0.0233  0.0518  0.0529  0.0452  0.0824  0.0496  0.0602  0.5847
 0.0735  0.0482  0.0823  0.0791  0.0538  0.4737  0.0396  0.0379  0.1120
 0.1259  0.0718  0.0987  0.2051  0.0505  0.0620  0.1556  0.1872  0.0433
 0.0640  0.0749  0.0726  0.1306  0.0651  0.0758  0.1605  0.0652  0.2913
 0.0727  0.0958  0.0844  0.0991  0.0740  0.0386  0.0794  0.0367  0.4194
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-05 19:15:48,028 train 000 1.142493e-02 -0.433136
2019-11-05 19:18:52,174 train 050 9.385092e-03 0.145338
2019-11-05 19:21:55,523 train 100 9.378704e-03 -0.605133
2019-11-05 19:24:57,379 train 150 9.309693e-03 -0.457358
2019-11-05 19:27:59,440 train 200 9.285712e-03 -0.391543
2019-11-05 19:31:08,102 train 250 9.281364e-03 -0.300612
2019-11-05 19:34:10,349 train 300 9.263044e-03 -0.259711
2019-11-05 19:37:14,302 train 350 9.251520e-03 -0.243662
2019-11-05 19:40:15,455 train 400 9.259278e-03 -0.230953
2019-11-05 19:43:15,879 train 450 9.275618e-03 -0.280954
2019-11-05 19:46:19,032 train 500 9.265168e-03 -0.249689
2019-11-05 19:49:25,912 train 550 9.272598e-03 -0.214250
2019-11-05 19:52:28,563 train 600 9.244785e-03 -0.260689
2019-11-05 19:55:37,871 train 650 9.245092e-03 -0.245483
2019-11-05 19:58:38,735 train 700 9.248991e-03 -0.224306
2019-11-05 20:01:51,260 train 750 9.240080e-03 -0.222970
2019-11-05 20:04:51,866 train 800 9.248336e-03 -0.837366
2019-11-05 20:07:55,614 train 850 9.237332e-03 -0.798687
2019-11-05 20:10:59,872 train 900 9.219437e-03 -0.752676
2019-11-05 20:14:00,225 train 950 9.217520e-03 -0.717402
2019-11-05 20:17:07,013 train 1000 9.209939e-03 -0.783416
2019-11-05 20:20:06,987 train 1050 9.220669e-03 -0.754107
2019-11-05 20:23:07,330 train 1100 9.231818e-03 -0.720273
2019-11-05 20:26:07,420 train 1150 9.236478e-03 -0.691123
2019-11-05 20:29:10,041 train 1200 9.232089e-03 -0.664032
2019-11-05 20:32:17,610 train 1250 9.234395e-03 -0.635837
2019-11-05 20:35:18,738 train 1300 9.227773e-03 -0.625411
2019-11-05 20:38:19,565 train 1350 9.216803e-03 -0.606485
2019-11-05 20:41:23,084 train 1400 9.211396e-03 -0.589590
2019-11-05 20:44:24,533 train 1450 9.196046e-03 -0.568652
2019-11-05 20:47:25,042 train 1500 9.192429e-03 -0.548653
2019-11-05 20:49:47,684 training loss; R2: 9.190495e-03 -0.539614
2019-11-05 20:49:48,204 valid 000 6.712754e-03 0.262554
2019-11-05 20:49:57,965 valid 050 9.273238e-03 0.060658
2019-11-05 20:50:07,711 valid 100 9.157178e-03 0.039518
2019-11-05 20:50:17,404 valid 150 9.149124e-03 0.019952
2019-11-05 20:50:27,138 valid 200 9.162183e-03 -1.218156
2019-11-05 20:50:36,874 valid 250 9.118136e-03 -0.959854
2019-11-05 20:50:46,737 valid 300 9.134779e-03 -0.839350
2019-11-05 20:50:56,613 valid 350 9.189360e-03 -0.705165
2019-11-05 20:51:06,354 valid 400 9.220851e-03 -0.613758
2019-11-05 20:51:16,102 valid 450 9.233174e-03 -0.555049
2019-11-05 20:51:25,832 valid 500 9.236803e-03 -0.519876
2019-11-05 20:51:35,565 valid 550 9.254813e-03 -0.482277
2019-11-05 20:51:45,320 valid 600 9.260836e-03 -0.436513
2019-11-05 20:51:55,088 valid 650 9.291951e-03 -0.401248
2019-11-05 20:52:04,831 valid 700 9.300121e-03 -1.154928
2019-11-05 20:52:14,594 valid 750 9.342071e-03 -1.083630
2019-11-05 20:52:24,356 valid 800 9.339688e-03 -1.049622
2019-11-05 20:52:34,125 valid 850 9.345810e-03 -0.984469
2019-11-05 20:52:43,893 valid 900 9.364069e-03 -1.001939
2019-11-05 20:52:53,717 valid 950 9.352289e-03 -0.950588
2019-11-05 20:53:03,460 valid 1000 9.351927e-03 -2.075530
2019-11-05 20:53:13,220 valid 1050 9.366336e-03 -1.984522
2019-11-05 20:53:22,979 valid 1100 9.354212e-03 -1.896639
2019-11-05 20:53:34,268 valid 1150 9.356725e-03 -1.837820
2019-11-05 20:53:46,150 valid 1200 9.356673e-03 -1.877577
2019-11-05 20:53:58,041 valid 1250 9.355455e-03 -1.801196
2019-11-05 20:54:09,931 valid 1300 9.367485e-03 -1.762023
2019-11-05 20:54:21,837 valid 1350 9.341436e-03 -1.791872
2019-11-05 20:54:33,742 valid 1400 9.330600e-03 -1.737496
2019-11-05 20:54:45,634 valid 1450 9.328773e-03 -1.677347
2019-11-05 20:54:57,504 valid 1500 9.341812e-03 -1.620715
2019-11-05 20:55:06,727 validation loss; R2: 9.342625e-03 -1.579620
2019-11-05 20:55:06,870 epoch 17 lr 1.000000e-03
2019-11-05 20:55:06,871 genotype = Genotype(normal=[('sep_conv_3x3', 0), ('dil_conv_3x3', 1), ('dil_conv_5x5', 2), ('dil_conv_5x5', 1), ('max_pool_3x3', 0), ('max_pool_3x3', 1), ('dil_conv_5x5', 4), ('sep_conv_3x3', 2)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 0), ('max_pool_3x3', 1), ('max_pool_3x3', 0), ('dil_conv_5x5', 2), ('dil_conv_5x5', 3), ('dil_conv_5x5', 2), ('dil_conv_5x5', 0), ('sep_conv_3x3', 1)], reduce_concat=range(2, 6))
2019-11-05 20:55:06,873 
alphas_normal = Variable containing:
 0.0684  0.1545  0.1230  0.0379  0.0306  0.2344  0.0115  0.1964  0.1432
 0.7819  0.0219  0.0218  0.0184  0.0141  0.0151  0.0126  0.0728  0.0413
 0.4095  0.1068  0.1660  0.0856  0.0379  0.0235  0.0431  0.0789  0.0487
 0.3819  0.0279  0.0538  0.1484  0.0203  0.0484  0.0617  0.0168  0.2409
 0.3445  0.0440  0.0393  0.0371  0.0252  0.0745  0.0219  0.0295  0.3840
 0.0609  0.0336  0.1481  0.3373  0.0234  0.0360  0.0167  0.0247  0.3193
 0.6776  0.0245  0.0426  0.1282  0.0175  0.0442  0.0286  0.0261  0.0108
 0.5980  0.0433  0.0576  0.0578  0.0194  0.0642  0.1194  0.0231  0.0172
 0.6824  0.0290  0.0422  0.0586  0.0149  0.0533  0.0418  0.0285  0.0494
 0.8120  0.0217  0.0254  0.0355  0.0146  0.0302  0.0312  0.0128  0.0167
 0.6023  0.0229  0.0353  0.1078  0.0184  0.0530  0.0484  0.0862  0.0257
 0.5273  0.0428  0.0577  0.1085  0.0203  0.1748  0.0290  0.0219  0.0176
 0.8509  0.0171  0.0210  0.0296  0.0122  0.0306  0.0185  0.0083  0.0116
 0.3923  0.0369  0.0365  0.0483  0.0241  0.0525  0.1468  0.0355  0.2270
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-05 20:55:06,875 
alphas_reduce = Variable containing:
 0.0506  0.0243  0.0928  0.5598  0.0430  0.0263  0.0679  0.0264  0.1089
 0.4131  0.1079  0.1167  0.1485  0.0295  0.0321  0.0667  0.0493  0.0361
 0.0310  0.0133  0.0355  0.8104  0.0340  0.0178  0.0216  0.0170  0.0195
 0.7211  0.0184  0.0419  0.0647  0.0266  0.0387  0.0389  0.0301  0.0195
 0.0355  0.0273  0.0256  0.0307  0.0200  0.0206  0.0458  0.0273  0.7672
 0.1411  0.0657  0.0984  0.1451  0.1326  0.1120  0.0769  0.1263  0.1020
 0.5771  0.0338  0.0911  0.0803  0.0560  0.0441  0.0247  0.0390  0.0540
 0.2038  0.0565  0.0480  0.0561  0.0446  0.0787  0.0878  0.0347  0.3898
 0.0499  0.0333  0.0287  0.0307  0.0333  0.0668  0.1946  0.0595  0.5032
 0.0421  0.0245  0.0570  0.0548  0.0537  0.0647  0.0703  0.0571  0.5758
 0.1035  0.0474  0.0666  0.0741  0.0433  0.4110  0.0530  0.0510  0.1500
 0.0967  0.0668  0.1171  0.1845  0.0516  0.0689  0.1506  0.2247  0.0391
 0.0483  0.0658  0.0735  0.0975  0.0566  0.0880  0.1912  0.0419  0.3373
 0.1143  0.0852  0.0832  0.0905  0.0670  0.0495  0.0840  0.0449  0.3813
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-05 20:55:11,825 train 000 1.209002e-02 0.279903
2019-11-05 20:58:20,163 train 050 9.087577e-03 -0.302203
2019-11-05 21:01:25,249 train 100 9.056526e-03 -0.213737
2019-11-05 21:04:33,319 train 150 9.055167e-03 -0.143029
2019-11-05 21:07:37,171 train 200 9.076327e-03 -0.202313
2019-11-05 21:10:41,152 train 250 9.010780e-03 -0.146438
2019-11-05 21:13:48,200 train 300 9.088664e-03 -0.190492
2019-11-05 21:16:49,315 train 350 9.088189e-03 -0.151113
2019-11-05 21:19:50,221 train 400 9.040426e-03 -0.140315
2019-11-05 21:22:50,847 train 450 9.071524e-03 -0.124479
2019-11-05 21:25:50,530 train 500 9.106318e-03 -0.612160
2019-11-05 21:28:59,009 train 550 9.086336e-03 -0.723595
2019-11-05 21:32:07,218 train 600 9.100647e-03 -0.691976
2019-11-05 21:35:16,902 train 650 9.080075e-03 -0.651992
2019-11-05 21:38:23,319 train 700 9.072706e-03 -0.603007
2019-11-05 21:41:34,587 train 750 9.082434e-03 -0.570535
2019-11-05 21:44:36,322 train 800 9.129038e-03 -0.540769
2019-11-05 21:47:35,745 train 850 9.131121e-03 -0.517353
2019-11-05 21:50:41,012 train 900 9.153310e-03 -0.488244
2019-11-05 21:53:41,454 train 950 9.165762e-03 -0.458226
2019-11-05 21:56:41,681 train 1000 9.170869e-03 -0.437171
2019-11-05 21:59:46,586 train 1050 9.197829e-03 -0.445585
2019-11-05 22:02:49,522 train 1100 9.204207e-03 -0.440132
2019-11-05 22:05:53,352 train 1150 9.205556e-03 -0.608356
2019-11-05 22:08:55,562 train 1200 9.213571e-03 -1.177140
2019-11-05 22:12:02,627 train 1250 9.222153e-03 -1.127911
2019-11-05 22:15:10,894 train 1300 9.203031e-03 -1.086263
2019-11-05 22:18:12,894 train 1350 9.199618e-03 -1.049599
2019-11-05 22:21:12,443 train 1400 9.195482e-03 -1.014143
2019-11-05 22:24:12,063 train 1450 9.188925e-03 -1.185381
2019-11-05 22:27:12,181 train 1500 9.192868e-03 -1.143160
2019-11-05 22:29:39,366 training loss; R2: 9.197786e-03 -1.117344
2019-11-05 22:29:39,897 valid 000 7.860362e-03 0.278748
2019-11-05 22:29:49,705 valid 050 8.363060e-03 -0.038799
2019-11-05 22:29:59,496 valid 100 8.330164e-03 -0.058936
2019-11-05 22:30:09,310 valid 150 8.395556e-03 0.016494
2019-11-05 22:30:19,121 valid 200 8.356917e-03 0.018627
2019-11-05 22:30:28,921 valid 250 8.339226e-03 0.045971
2019-11-05 22:30:38,723 valid 300 8.336988e-03 0.063925
2019-11-05 22:30:48,535 valid 350 8.263319e-03 0.002914
2019-11-05 22:30:58,350 valid 400 8.262779e-03 0.014225
2019-11-05 22:31:08,145 valid 450 8.262540e-03 0.029595
2019-11-05 22:31:17,930 valid 500 8.255764e-03 0.019461
2019-11-05 22:31:27,706 valid 550 8.263569e-03 -0.067678
2019-11-05 22:31:37,566 valid 600 8.246275e-03 -0.054390
2019-11-05 22:31:47,389 valid 650 8.255038e-03 -0.041128
2019-11-05 22:31:57,200 valid 700 8.261413e-03 -0.199578
2019-11-05 22:32:07,320 valid 750 8.278820e-03 -0.204018
2019-11-05 22:32:19,220 valid 800 8.278025e-03 -0.186795
2019-11-05 22:32:31,137 valid 850 8.282185e-03 -0.172979
2019-11-05 22:32:43,047 valid 900 8.282674e-03 -0.157035
2019-11-05 22:32:54,959 valid 950 8.292976e-03 -0.138391
2019-11-05 22:33:06,851 valid 1000 8.286465e-03 -0.124069
2019-11-05 22:33:18,756 valid 1050 8.292115e-03 -0.111089
2019-11-05 22:33:30,665 valid 1100 8.287817e-03 -0.095590
2019-11-05 22:33:42,547 valid 1150 8.273171e-03 -0.086763
2019-11-05 22:33:54,418 valid 1200 8.280641e-03 -0.075872
2019-11-05 22:34:06,304 valid 1250 8.285883e-03 -0.071003
2019-11-05 22:34:18,186 valid 1300 8.277903e-03 -0.268306
2019-11-05 22:34:30,016 valid 1350 8.262566e-03 -0.275971
2019-11-05 22:34:41,582 valid 1400 8.259214e-03 -0.259976
2019-11-05 22:34:53,335 valid 1450 8.248480e-03 -0.263353
2019-11-05 22:35:05,227 valid 1500 8.239946e-03 -0.251232
2019-11-05 22:35:14,465 validation loss; R2: 8.238798e-03 -0.245088
2019-11-05 22:35:14,609 epoch 18 lr 1.000000e-03
2019-11-05 22:35:14,610 genotype = Genotype(normal=[('dil_conv_3x3', 0), ('dil_conv_3x3', 1), ('dil_conv_5x5', 2), ('dil_conv_5x5', 1), ('max_pool_3x3', 0), ('sep_conv_5x5', 2), ('dil_conv_5x5', 4), ('sep_conv_3x3', 2)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 0), ('max_pool_3x3', 1), ('max_pool_3x3', 0), ('dil_conv_5x5', 2), ('dil_conv_5x5', 3), ('dil_conv_5x5', 2), ('dil_conv_5x5', 0), ('dil_conv_5x5', 4)], reduce_concat=range(2, 6))
2019-11-05 22:35:14,612 
alphas_normal = Variable containing:
 0.0507  0.1503  0.1269  0.0458  0.0299  0.2169  0.0147  0.2228  0.1420
 0.8138  0.0156  0.0181  0.0138  0.0106  0.0118  0.0137  0.0721  0.0306
 0.2904  0.1360  0.1964  0.0983  0.0470  0.0404  0.0855  0.0729  0.0330
 0.4225  0.0219  0.0377  0.1182  0.0158  0.0397  0.0639  0.0237  0.2566
 0.4112  0.0476  0.0338  0.0281  0.0242  0.0281  0.0162  0.0267  0.3841
 0.0565  0.0243  0.1220  0.3608  0.0181  0.0489  0.0210  0.0259  0.3226
 0.7129  0.0175  0.0330  0.0958  0.0131  0.0490  0.0323  0.0321  0.0144
 0.5697  0.0346  0.0549  0.0602  0.0175  0.0696  0.1450  0.0242  0.0243
 0.6648  0.0311  0.0473  0.0582  0.0184  0.0641  0.0405  0.0253  0.0503
 0.6917  0.0422  0.0529  0.0699  0.0351  0.0344  0.0388  0.0146  0.0204
 0.6110  0.0245  0.0374  0.1038  0.0188  0.0524  0.0450  0.0862  0.0210
 0.4932  0.0445  0.0686  0.1236  0.0293  0.1869  0.0227  0.0169  0.0142
 0.8513  0.0217  0.0261  0.0308  0.0164  0.0215  0.0170  0.0072  0.0079
 0.4087  0.0412  0.0343  0.0444  0.0248  0.0392  0.1505  0.0480  0.2090
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-05 22:35:14,614 
alphas_reduce = Variable containing:
 0.0400  0.0188  0.1105  0.5952  0.0326  0.0332  0.0597  0.0261  0.0839
 0.3939  0.0847  0.1368  0.1644  0.0301  0.0504  0.0534  0.0448  0.0415
 0.0240  0.0149  0.0407  0.8075  0.0310  0.0199  0.0253  0.0136  0.0231
 0.7602  0.0187  0.0407  0.0596  0.0253  0.0254  0.0243  0.0302  0.0155
 0.0307  0.0291  0.0330  0.0389  0.0249  0.0266  0.0513  0.0329  0.7326
 0.1510  0.0554  0.1167  0.1394  0.1559  0.1229  0.0622  0.1197  0.0770
 0.5872  0.0342  0.0796  0.0741  0.0462  0.0387  0.0395  0.0562  0.0443
 0.2369  0.0579  0.0453  0.0498  0.0468  0.0739  0.1027  0.0475  0.3392
 0.0448  0.0317  0.0273  0.0306  0.0335  0.0595  0.2075  0.0416  0.5236
 0.0578  0.0289  0.0488  0.0528  0.0474  0.0885  0.0626  0.0494  0.5637
 0.1325  0.0466  0.0664  0.0762  0.0467  0.3958  0.0607  0.0740  0.1011
 0.1188  0.0706  0.0907  0.1725  0.0515  0.0698  0.1006  0.2741  0.0515
 0.0799  0.0664  0.0613  0.1018  0.0581  0.0463  0.1755  0.0410  0.3698
 0.0878  0.0657  0.0637  0.0878  0.0594  0.0590  0.0779  0.0379  0.4609
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-05 22:35:19,763 train 000 7.817468e-03 0.383134
2019-11-05 22:38:25,965 train 050 8.866479e-03 -0.074114
2019-11-05 22:41:28,702 train 100 8.874231e-03 -0.283806
2019-11-05 22:44:34,238 train 150 8.894042e-03 -0.291500
2019-11-05 22:47:48,147 train 200 8.968685e-03 -0.225459
2019-11-05 22:50:51,570 train 250 8.967611e-03 -0.207769
2019-11-05 22:53:58,377 train 300 9.067212e-03 -0.572610
2019-11-05 22:56:58,831 train 350 9.033580e-03 -0.496082
2019-11-05 22:59:59,293 train 400 9.117968e-03 -0.447995
2019-11-05 23:02:59,605 train 450 9.129948e-03 -1.972445
2019-11-05 23:06:02,057 train 500 9.137945e-03 -1.765814
2019-11-05 23:09:19,755 train 550 9.144133e-03 -5.200550
2019-11-05 23:12:26,932 train 600 9.137992e-03 -4.759691
2019-11-05 23:15:30,332 train 650 9.119159e-03 -4.389399
2019-11-05 23:18:31,433 train 700 9.137630e-03 -4.102366
2019-11-05 23:21:37,359 train 750 9.172256e-03 -3.833323
2019-11-05 23:24:41,860 train 800 9.149015e-03 -3.596771
2019-11-05 23:27:42,351 train 850 9.128811e-03 -3.379563
2019-11-05 23:30:45,311 train 900 9.148786e-03 -3.277239
2019-11-05 23:33:45,609 train 950 9.142738e-03 -3.104235
2019-11-05 23:36:45,247 train 1000 9.154448e-03 -2.987986
2019-11-05 23:39:53,255 train 1050 9.159313e-03 -2.891408
2019-11-05 23:42:57,285 train 1100 9.145675e-03 -2.754912
2019-11-05 23:46:03,261 train 1150 9.159299e-03 -2.645286
2019-11-05 23:49:04,393 train 1200 9.137314e-03 -2.535410
2019-11-05 23:52:05,189 train 1250 9.137180e-03 -2.437824
2019-11-05 23:55:08,966 train 1300 9.127147e-03 -2.345793
2019-11-05 23:58:14,828 train 1350 9.137332e-03 -2.262485
2019-11-06 00:01:30,115 train 1400 9.134153e-03 -2.179447
2019-11-06 00:04:36,482 train 1450 9.129214e-03 -2.103414
2019-11-06 00:07:36,139 train 1500 9.127928e-03 -2.031473
2019-11-06 00:10:02,339 training loss; R2: 9.120446e-03 -1.988697
2019-11-06 00:10:02,872 valid 000 7.341250e-03 0.390285
2019-11-06 00:10:12,714 valid 050 8.494961e-03 0.058007
2019-11-06 00:10:22,543 valid 100 8.575505e-03 -0.036260
2019-11-06 00:10:32,363 valid 150 8.500863e-03 0.017901
2019-11-06 00:10:42,191 valid 200 8.561222e-03 -0.002011
2019-11-06 00:10:52,028 valid 250 8.546564e-03 -0.004874
2019-11-06 00:11:01,873 valid 300 8.521703e-03 0.015978
2019-11-06 00:11:11,688 valid 350 8.536923e-03 -0.000186
2019-11-06 00:11:21,483 valid 400 8.574330e-03 0.013556
2019-11-06 00:11:31,275 valid 450 8.549530e-03 -0.076174
2019-11-06 00:11:41,064 valid 500 8.524432e-03 -0.061532
2019-11-06 00:11:50,851 valid 550 8.551650e-03 -0.054323
2019-11-06 00:12:00,629 valid 600 8.569715e-03 -0.043411
2019-11-06 00:12:10,394 valid 650 8.580455e-03 -0.056474
2019-11-06 00:12:20,160 valid 700 8.603552e-03 -0.047041
2019-11-06 00:12:29,924 valid 750 8.603091e-03 -0.039126
2019-11-06 00:12:39,665 valid 800 8.594287e-03 -0.039254
2019-11-06 00:12:49,461 valid 850 8.581056e-03 -0.039437
2019-11-06 00:12:59,231 valid 900 8.565660e-03 -0.033028
2019-11-06 00:13:09,013 valid 950 8.571339e-03 -0.023764
2019-11-06 00:13:18,787 valid 1000 8.575753e-03 -0.017324
2019-11-06 00:13:28,556 valid 1050 8.555588e-03 -0.015207
2019-11-06 00:13:38,326 valid 1100 8.531175e-03 -0.016294
2019-11-06 00:13:48,101 valid 1150 8.531198e-03 -0.018385
2019-11-06 00:13:57,859 valid 1200 8.530454e-03 -0.037661
2019-11-06 00:14:07,645 valid 1250 8.536496e-03 -0.044519
2019-11-06 00:14:17,421 valid 1300 8.521211e-03 -0.191327
2019-11-06 00:14:27,207 valid 1350 8.520276e-03 -0.183336
2019-11-06 00:14:37,015 valid 1400 8.517976e-03 -0.179481
2019-11-06 00:14:46,830 valid 1450 8.506930e-03 -0.171402
2019-11-06 00:14:56,594 valid 1500 8.508167e-03 -0.230919
2019-11-06 00:15:04,297 validation loss; R2: 8.520701e-03 -0.223698
2019-11-06 00:15:04,437 epoch 19 lr 1.000000e-03
2019-11-06 00:15:04,437 genotype = Genotype(normal=[('dil_conv_3x3', 0), ('dil_conv_3x3', 1), ('dil_conv_5x5', 2), ('dil_conv_5x5', 1), ('dil_conv_5x5', 0), ('sep_conv_5x5', 2), ('dil_conv_5x5', 4), ('sep_conv_3x3', 2)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 0), ('max_pool_3x3', 1), ('max_pool_3x3', 0), ('dil_conv_5x5', 2), ('dil_conv_5x5', 3), ('dil_conv_5x5', 2), ('dil_conv_5x5', 4), ('dil_conv_5x5', 0)], reduce_concat=range(2, 6))
2019-11-06 00:15:04,439 
alphas_normal = Variable containing:
 0.0782  0.1670  0.1336  0.0432  0.0366  0.2000  0.0126  0.2021  0.1268
 0.8136  0.0163  0.0172  0.0130  0.0115  0.0150  0.0182  0.0653  0.0299
 0.3102  0.1462  0.2018  0.0740  0.0657  0.0257  0.0639  0.0851  0.0274
 0.3997  0.0264  0.0488  0.1342  0.0216  0.0469  0.0747  0.0189  0.2287
 0.4034  0.0458  0.0272  0.0214  0.0236  0.0305  0.0171  0.0204  0.4105
 0.0376  0.0273  0.1284  0.3259  0.0221  0.0499  0.0187  0.0156  0.3745
 0.7600  0.0148  0.0248  0.0722  0.0121  0.0532  0.0224  0.0288  0.0118
 0.5669  0.0388  0.0442  0.0518  0.0189  0.0942  0.1196  0.0374  0.0282
 0.6571  0.0396  0.0431  0.0472  0.0215  0.0345  0.0380  0.0456  0.0734
 0.7263  0.0287  0.0385  0.0487  0.0251  0.0578  0.0483  0.0095  0.0171
 0.6018  0.0250  0.0342  0.0952  0.0203  0.0715  0.0339  0.0905  0.0275
 0.5062  0.0438  0.0643  0.1416  0.0230  0.1694  0.0228  0.0155  0.0134
 0.8443  0.0152  0.0165  0.0222  0.0110  0.0547  0.0127  0.0106  0.0128
 0.4100  0.0372  0.0312  0.0428  0.0218  0.0389  0.1361  0.0531  0.2288
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-06 00:15:04,441 
alphas_reduce = Variable containing:
 0.0417  0.0231  0.1002  0.6115  0.0306  0.0273  0.0464  0.0274  0.0918
 0.3808  0.0776  0.1588  0.1807  0.0341  0.0365  0.0628  0.0414  0.0274
 0.0301  0.0170  0.0395  0.7723  0.0407  0.0202  0.0329  0.0231  0.0241
 0.7729  0.0158  0.0388  0.0500  0.0312  0.0265  0.0258  0.0201  0.0189
 0.0430  0.0358  0.0341  0.0413  0.0335  0.0265  0.0389  0.0377  0.7091
 0.1947  0.0677  0.1100  0.1368  0.1312  0.1056  0.0655  0.1415  0.0471
 0.5117  0.0313  0.0857  0.0821  0.0601  0.0699  0.0408  0.0597  0.0586
 0.2243  0.0553  0.0485  0.0563  0.0446  0.0948  0.1088  0.0427  0.3248
 0.0492  0.0385  0.0346  0.0330  0.0374  0.0608  0.2246  0.0452  0.4767
 0.0497  0.0246  0.0585  0.0582  0.0604  0.1404  0.0549  0.0580  0.4952
 0.0837  0.0369  0.0774  0.0761  0.0544  0.4249  0.0499  0.0495  0.1471
 0.0866  0.0799  0.0861  0.1285  0.0558  0.0457  0.1222  0.3417  0.0534
 0.0668  0.0642  0.0519  0.0702  0.0541  0.0678  0.3500  0.0468  0.2282
 0.0797  0.0630  0.0545  0.0668  0.0507  0.0491  0.0667  0.0542  0.5154
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-06 00:15:09,412 train 000 9.524177e-03 -5.761418
2019-11-06 00:18:11,541 train 050 9.395825e-03 -0.142075
2019-11-06 00:21:12,595 train 100 9.083263e-03 -0.416848
2019-11-06 00:24:13,867 train 150 9.006778e-03 -9.717127
2019-11-06 00:27:19,126 train 200 8.998708e-03 -7.280799
2019-11-06 00:30:32,512 train 250 8.864021e-03 -5.844504
2019-11-06 00:33:32,169 train 300 8.845029e-03 -4.875656
2019-11-06 00:36:32,035 train 350 8.895558e-03 -4.171801
2019-11-06 00:39:34,137 train 400 8.869086e-03 -3.645853
2019-11-06 00:42:34,703 train 450 8.933298e-03 -3.267987
2019-11-06 00:45:36,246 train 500 9.014754e-03 -2.955545
2019-11-06 00:48:46,652 train 550 9.059331e-03 -2.709988
2019-11-06 00:51:48,875 train 600 9.038880e-03 -2.652390
2019-11-06 00:54:52,131 train 650 9.065229e-03 -2.451074
2019-11-06 00:58:09,364 train 700 9.090189e-03 -2.289065
2019-11-06 01:01:17,782 train 750 9.090543e-03 -2.143508
2019-11-06 01:04:18,459 train 800 9.091872e-03 -2.014037
2019-11-06 01:07:24,099 train 850 9.103819e-03 -1.894191
2019-11-06 01:10:26,435 train 900 9.085076e-03 -1.804106
2019-11-06 01:13:30,473 train 950 9.049746e-03 -1.712577
2019-11-06 01:16:40,843 train 1000 9.041661e-03 -1.622962
2019-11-06 01:19:45,262 train 1050 9.044584e-03 -1.548980
2019-11-06 01:22:47,173 train 1100 9.050892e-03 -1.474592
2019-11-06 01:25:48,167 train 1150 9.067429e-03 -1.415367
2019-11-06 01:29:00,740 train 1200 9.085042e-03 -1.357951
2019-11-06 01:32:11,838 train 1250 9.089684e-03 -1.320374
2019-11-06 01:35:13,809 train 1300 9.108635e-03 -1.299316
2019-11-06 01:38:17,099 train 1350 9.113305e-03 -1.261180
2019-11-06 01:41:20,998 train 1400 9.136283e-03 -1.219191
2019-11-06 01:44:26,265 train 1450 9.132235e-03 -1.180303
2019-11-06 01:47:41,139 train 1500 9.118902e-03 -1.144574
2019-11-06 01:50:05,533 training loss; R2: 9.113707e-03 -1.118865
2019-11-06 01:50:06,047 valid 000 1.242676e-02 0.340738
2019-11-06 01:50:15,797 valid 050 8.385017e-03 -0.261470
2019-11-06 01:50:25,864 valid 100 8.399457e-03 -0.120997
2019-11-06 01:50:35,771 valid 150 8.208142e-03 -0.023132
2019-11-06 01:50:45,821 valid 200 8.194733e-03 -0.899899
2019-11-06 01:50:57,566 valid 250 8.204113e-03 -0.721106
2019-11-06 01:51:09,513 valid 300 8.203869e-03 -0.640609
2019-11-06 01:51:21,414 valid 350 8.311586e-03 -0.707869
2019-11-06 01:51:33,270 valid 400 8.319243e-03 -0.615880
2019-11-06 01:51:45,157 valid 450 8.344492e-03 -0.531164
2019-11-06 01:51:57,020 valid 500 8.324923e-03 -0.458884
2019-11-06 01:52:08,889 valid 550 8.352940e-03 -0.563970
2019-11-06 01:52:20,759 valid 600 8.375677e-03 -0.512882
2019-11-06 01:52:32,649 valid 650 8.374193e-03 -0.484135
2019-11-06 01:52:44,542 valid 700 8.388961e-03 -0.447148
2019-11-06 01:52:54,356 valid 750 8.402708e-03 -0.406870
2019-11-06 01:53:04,099 valid 800 8.415907e-03 -0.383077
2019-11-06 01:53:13,839 valid 850 8.417031e-03 -0.388619
2019-11-06 01:53:23,600 valid 900 8.418306e-03 -0.363467
2019-11-06 01:53:33,354 valid 950 8.415699e-03 -0.349332
2019-11-06 01:53:43,359 valid 1000 8.431444e-03 -0.336156
2019-11-06 01:53:53,169 valid 1050 8.424750e-03 -0.311116
2019-11-06 01:54:02,989 valid 1100 8.428852e-03 -0.292659
2019-11-06 01:54:12,750 valid 1150 8.442890e-03 -0.271907
2019-11-06 01:54:22,508 valid 1200 8.436873e-03 -0.258306
2019-11-06 01:54:32,304 valid 1250 8.446399e-03 -0.242895
2019-11-06 01:54:42,091 valid 1300 8.442671e-03 -0.226326
2019-11-06 01:54:51,876 valid 1350 8.444887e-03 -0.230701
2019-11-06 01:55:01,748 valid 1400 8.434658e-03 -0.265559
2019-11-06 01:55:11,668 valid 1450 8.442524e-03 -0.253327
2019-11-06 01:55:21,459 valid 1500 8.436123e-03 -0.241496
2019-11-06 01:55:29,073 validation loss; R2: 8.440767e-03 -0.235761
2019-11-06 01:55:29,210 epoch 20 lr 1.000000e-03
2019-11-06 01:55:29,211 genotype = Genotype(normal=[('dil_conv_3x3', 0), ('dil_conv_3x3', 1), ('dil_conv_5x5', 2), ('dil_conv_5x5', 1), ('max_pool_3x3', 0), ('max_pool_3x3', 1), ('sep_conv_3x3', 2), ('dil_conv_5x5', 4)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 0), ('max_pool_3x3', 1), ('dil_conv_5x5', 2), ('max_pool_3x3', 0), ('dil_conv_5x5', 3), ('dil_conv_5x5', 2), ('dil_conv_5x5', 0), ('dil_conv_5x5', 4)], reduce_concat=range(2, 6))
2019-11-06 01:55:29,213 
alphas_normal = Variable containing:
 0.1235  0.1273  0.1022  0.0308  0.0269  0.2066  0.0152  0.2294  0.1381
 0.7837  0.0235  0.0219  0.0139  0.0168  0.0121  0.0322  0.0756  0.0203
 0.3695  0.1368  0.1736  0.0647  0.0566  0.0223  0.0808  0.0731  0.0227
 0.4043  0.0302  0.0485  0.1366  0.0259  0.0318  0.0628  0.0265  0.2333
 0.3881  0.0425  0.0295  0.0220  0.0245  0.0315  0.0167  0.0246  0.4205
 0.0413  0.0316  0.1255  0.3937  0.0236  0.0426  0.0132  0.0152  0.3134
 0.6477  0.0240  0.0459  0.1348  0.0201  0.0617  0.0220  0.0306  0.0133
 0.5934  0.0450  0.0429  0.0525  0.0198  0.0918  0.1086  0.0289  0.0170
 0.6846  0.0369  0.0464  0.0535  0.0204  0.0388  0.0335  0.0310  0.0549
 0.7626  0.0258  0.0357  0.0442  0.0211  0.0414  0.0318  0.0123  0.0251
 0.6399  0.0221  0.0353  0.0886  0.0187  0.0577  0.0318  0.0830  0.0227
 0.4564  0.0476  0.0773  0.1352  0.0247  0.2188  0.0165  0.0132  0.0104
 0.8235  0.0189  0.0218  0.0239  0.0143  0.0638  0.0141  0.0080  0.0119
 0.4350  0.0389  0.0340  0.0405  0.0224  0.0371  0.1301  0.0717  0.1902
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-06 01:55:29,214 
alphas_reduce = Variable containing:
 0.0381  0.0270  0.0954  0.5659  0.0447  0.0270  0.0658  0.0236  0.1125
 0.4189  0.0773  0.1405  0.1739  0.0338  0.0348  0.0568  0.0391  0.0250
 0.0395  0.0169  0.0630  0.7116  0.0459  0.0263  0.0467  0.0281  0.0221
 0.7873  0.0168  0.0358  0.0393  0.0288  0.0261  0.0254  0.0197  0.0208
 0.0353  0.0309  0.0298  0.0357  0.0256  0.0264  0.0347  0.0257  0.7559
 0.1640  0.0666  0.1099  0.1362  0.1456  0.1197  0.0705  0.1400  0.0474
 0.5054  0.0385  0.0914  0.0776  0.0628  0.0595  0.0504  0.0562  0.0583
 0.2437  0.0706  0.0549  0.0570  0.0478  0.0939  0.0815  0.0445  0.3060
 0.0334  0.0324  0.0238  0.0226  0.0307  0.0355  0.2522  0.0374  0.5320
 0.0511  0.0270  0.0681  0.0787  0.0638  0.1079  0.0649  0.0749  0.4636
 0.1337  0.0473  0.1102  0.0946  0.0666  0.3232  0.0402  0.0404  0.1436
 0.0582  0.0790  0.1328  0.1942  0.0608  0.0474  0.1276  0.2612  0.0388
 0.0668  0.0781  0.0794  0.0878  0.0723  0.0694  0.3262  0.0370  0.1829
 0.1132  0.0697  0.0724  0.0767  0.0648  0.0435  0.0563  0.0499  0.4534
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-06 01:55:34,094 train 000 1.098550e-02 0.457830
2019-11-06 01:58:39,954 train 050 8.785299e-03 0.025409
2019-11-06 02:01:42,794 train 100 8.965911e-03 -0.262957
2019-11-06 02:04:46,900 train 150 8.986820e-03 -0.245421
2019-11-06 02:07:46,855 train 200 8.986873e-03 -0.181007
2019-11-06 02:10:53,699 train 250 8.990215e-03 -0.219295
2019-11-06 02:13:53,941 train 300 8.968586e-03 -0.165893
2019-11-06 02:16:54,812 train 350 9.044597e-03 -0.239394
2019-11-06 02:20:00,803 train 400 8.969275e-03 -0.208367
2019-11-06 02:23:03,238 train 450 8.932396e-03 -0.173130
2019-11-06 02:26:05,258 train 500 8.930726e-03 -0.154464
2019-11-06 02:29:13,058 train 550 8.925710e-03 -0.140867
2019-11-06 02:32:13,706 train 600 8.935382e-03 -0.132423
2019-11-06 02:35:14,105 train 650 8.983579e-03 -0.127358
2019-11-06 02:38:15,448 train 700 8.979108e-03 -0.145559
2019-11-06 02:41:18,751 train 750 8.973105e-03 -0.227537
2019-11-06 02:44:20,236 train 800 8.977250e-03 -0.206316
2019-11-06 02:47:27,255 train 850 8.973988e-03 -0.190444
2019-11-06 02:50:36,501 train 900 8.969862e-03 -0.181936
2019-11-06 02:53:36,979 train 950 8.965473e-03 -0.190180
2019-11-06 02:56:37,105 train 1000 8.979364e-03 -0.196756
2019-11-06 02:59:38,566 train 1050 8.980033e-03 -0.182122
2019-11-06 03:02:42,931 train 1100 8.985573e-03 -0.299709
2019-11-06 03:05:44,181 train 1150 8.991230e-03 -0.287171
2019-11-06 03:08:43,799 train 1200 8.991260e-03 -0.269871
2019-11-06 03:11:44,280 train 1250 8.998104e-03 -0.294540
2019-11-06 03:14:44,357 train 1300 9.002768e-03 -0.339758
2019-11-06 03:17:44,880 train 1350 9.007044e-03 -0.327793
2019-11-06 03:20:45,158 train 1400 9.010723e-03 -0.313234
2019-11-06 03:23:51,195 train 1450 9.019703e-03 -0.305947
2019-11-06 03:26:58,080 train 1500 9.033546e-03 -0.292116
2019-11-06 03:29:22,641 training loss; R2: 9.037828e-03 -0.282097
2019-11-06 03:29:23,159 valid 000 1.072385e-02 -1.540588
2019-11-06 03:29:32,925 valid 050 8.607065e-03 0.081466
2019-11-06 03:29:42,683 valid 100 8.397909e-03 0.023905
2019-11-06 03:29:52,455 valid 150 8.473030e-03 -0.286207
2019-11-06 03:30:02,204 valid 200 8.559123e-03 -0.188194
2019-11-06 03:30:11,939 valid 250 8.642915e-03 -0.140243
2019-11-06 03:30:21,691 valid 300 8.627514e-03 -0.098494
2019-11-06 03:30:31,438 valid 350 8.630633e-03 -0.070266
2019-11-06 03:30:41,188 valid 400 8.663282e-03 -0.042057
2019-11-06 03:30:50,948 valid 450 8.637900e-03 -0.033562
2019-11-06 03:31:00,700 valid 500 8.674494e-03 -0.033642
2019-11-06 03:31:10,478 valid 550 8.693471e-03 -0.030463
2019-11-06 03:31:20,280 valid 600 8.686677e-03 -0.049517
2019-11-06 03:31:30,054 valid 650 8.674365e-03 -0.066243
2019-11-06 03:31:39,785 valid 700 8.651561e-03 -0.058526
2019-11-06 03:31:49,561 valid 750 8.643621e-03 -0.051496
2019-11-06 03:31:59,337 valid 800 8.651708e-03 -0.063777
2019-11-06 03:32:09,105 valid 850 8.661179e-03 -0.148031
2019-11-06 03:32:18,900 valid 900 8.668045e-03 -0.134309
2019-11-06 03:32:28,689 valid 950 8.654602e-03 -0.129517
2019-11-06 03:32:38,491 valid 1000 8.650526e-03 -0.118928
2019-11-06 03:32:48,267 valid 1050 8.656930e-03 -0.104987
2019-11-06 03:32:58,049 valid 1100 8.670900e-03 -0.098306
2019-11-06 03:33:07,830 valid 1150 8.659749e-03 -0.105818
2019-11-06 03:33:17,619 valid 1200 8.660452e-03 -0.097523
2019-11-06 03:33:27,422 valid 1250 8.658446e-03 -0.090106
2019-11-06 03:33:37,232 valid 1300 8.656123e-03 -0.136224
2019-11-06 03:33:47,009 valid 1350 8.650989e-03 -0.129792
2019-11-06 03:33:56,781 valid 1400 8.644133e-03 -0.121197
2019-11-06 03:34:06,951 valid 1450 8.627105e-03 -0.112948
2019-11-06 03:34:18,895 valid 1500 8.634191e-03 -0.223530
2019-11-06 03:34:28,176 validation loss; R2: 8.636053e-03 -0.220098
2019-11-06 03:34:28,318 epoch 21 lr 1.000000e-03
2019-11-06 03:34:28,319 genotype = Genotype(normal=[('dil_conv_3x3', 0), ('dil_conv_3x3', 1), ('dil_conv_5x5', 2), ('dil_conv_5x5', 1), ('max_pool_3x3', 0), ('sep_conv_5x5', 2), ('sep_conv_3x3', 2), ('dil_conv_5x5', 4)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 0), ('max_pool_3x3', 1), ('max_pool_3x3', 0), ('dil_conv_5x5', 2), ('dil_conv_5x5', 3), ('dil_conv_5x5', 2), ('dil_conv_5x5', 0), ('dil_conv_5x5', 4)], reduce_concat=range(2, 6))
2019-11-06 03:34:28,321 
alphas_normal = Variable containing:
 0.1038  0.1522  0.1201  0.0327  0.0324  0.1579  0.0232  0.2169  0.1608
 0.8356  0.0131  0.0130  0.0100  0.0102  0.0099  0.0264  0.0686  0.0133
 0.3997  0.1486  0.1348  0.0454  0.0532  0.0367  0.0554  0.0872  0.0390
 0.4602  0.0306  0.0376  0.0973  0.0251  0.0411  0.0480  0.0234  0.2367
 0.4191  0.0468  0.0297  0.0229  0.0297  0.0348  0.0164  0.0312  0.3694
 0.0332  0.0326  0.1263  0.3661  0.0258  0.0326  0.0178  0.0166  0.3490
 0.6928  0.0266  0.0357  0.0992  0.0204  0.0627  0.0181  0.0297  0.0148
 0.5875  0.0437  0.0507  0.0578  0.0212  0.0839  0.1140  0.0283  0.0128
 0.6933  0.0314  0.0361  0.0484  0.0177  0.0573  0.0336  0.0243  0.0580
 0.6694  0.0331  0.0440  0.0464  0.0250  0.0815  0.0651  0.0140  0.0214
 0.6342  0.0219  0.0310  0.0711  0.0184  0.0440  0.0369  0.1217  0.0208
 0.4469  0.0507  0.0752  0.1376  0.0261  0.2135  0.0257  0.0127  0.0117
 0.8063  0.0286  0.0239  0.0251  0.0176  0.0615  0.0189  0.0094  0.0087
 0.4523  0.0336  0.0281  0.0324  0.0181  0.0418  0.1383  0.0612  0.1944
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-06 03:34:28,323 
alphas_reduce = Variable containing:
 0.0361  0.0294  0.1007  0.5708  0.0399  0.0329  0.0482  0.0203  0.1217
 0.3834  0.0806  0.1520  0.1542  0.0404  0.0466  0.0664  0.0458  0.0304
 0.0273  0.0162  0.0580  0.7770  0.0369  0.0172  0.0314  0.0179  0.0179
 0.7652  0.0178  0.0353  0.0371  0.0316  0.0382  0.0287  0.0245  0.0216
 0.0391  0.0379  0.0363  0.0415  0.0313  0.0274  0.0383  0.0283  0.7199
 0.2280  0.0698  0.1254  0.1096  0.1597  0.0781  0.0556  0.1183  0.0555
 0.5028  0.0341  0.0836  0.0748  0.0623  0.0528  0.0720  0.0634  0.0541
 0.2853  0.0687  0.0513  0.0463  0.0476  0.0938  0.0797  0.0415  0.2858
 0.0337  0.0272  0.0225  0.0218  0.0279  0.0389  0.2602  0.0341  0.5337
 0.0549  0.0288  0.0499  0.0535  0.0451  0.1193  0.0617  0.0732  0.5137
 0.1443  0.0490  0.0716  0.0724  0.0484  0.4151  0.0395  0.0358  0.1238
 0.0712  0.0542  0.0873  0.1471  0.0397  0.0529  0.1064  0.4071  0.0341
 0.0929  0.0703  0.0860  0.0950  0.0678  0.0997  0.2902  0.0346  0.1636
 0.0870  0.0559  0.0576  0.0688  0.0512  0.0572  0.0984  0.0444  0.4796
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-06 03:34:33,301 train 000 8.654974e-03 0.330863
2019-11-06 03:37:37,261 train 050 8.749488e-03 -0.007298
2019-11-06 03:40:42,268 train 100 8.782645e-03 -0.241181
2019-11-06 03:43:43,064 train 150 8.990064e-03 -0.235090
2019-11-06 03:46:45,544 train 200 9.025959e-03 -0.170492
2019-11-06 03:49:56,662 train 250 9.060135e-03 -0.142003
2019-11-06 03:52:56,894 train 300 9.008008e-03 -0.206619
2019-11-06 03:55:58,208 train 350 9.020409e-03 -0.358961
2019-11-06 03:59:03,192 train 400 9.003582e-03 -0.323163
2019-11-06 04:02:07,111 train 450 8.998966e-03 -0.281623
2019-11-06 04:05:07,675 train 500 8.991290e-03 -0.279343
2019-11-06 04:08:11,143 train 550 9.037623e-03 -0.757633
2019-11-06 04:11:10,756 train 600 9.033162e-03 -0.688680
2019-11-06 04:14:12,665 train 650 9.036505e-03 -0.631540
2019-11-06 04:17:21,843 train 700 9.017354e-03 -0.592369
2019-11-06 04:20:29,709 train 750 9.041545e-03 -0.545090
2019-11-06 04:23:30,396 train 800 9.061728e-03 -0.515512
2019-11-06 04:26:31,670 train 850 9.073576e-03 -0.481727
2019-11-06 04:29:35,706 train 900 9.071724e-03 -0.454669
2019-11-06 04:32:51,774 train 950 9.070237e-03 -3.096050
2019-11-06 04:35:59,922 train 1000 9.067287e-03 -2.960357
2019-11-06 04:39:12,418 train 1050 9.072804e-03 -2.815345
2019-11-06 04:42:22,172 train 1100 9.064251e-03 -2.685197
2019-11-06 04:45:34,303 train 1150 9.069078e-03 -2.565361
2019-11-06 04:48:40,689 train 1200 9.067636e-03 -2.544848
2019-11-06 04:51:43,629 train 1250 9.050043e-03 -2.436747
2019-11-06 04:54:54,568 train 1300 9.040157e-03 -2.346389
2019-11-06 04:58:01,764 train 1350 9.033757e-03 -2.344393
2019-11-06 05:01:06,411 train 1400 9.034572e-03 -2.261125
2019-11-06 05:04:16,387 train 1450 9.040217e-03 -2.187849
2019-11-06 05:07:26,153 train 1500 9.034520e-03 -2.120671
2019-11-06 05:09:56,461 training loss; R2: 9.042910e-03 -2.078010
2019-11-06 05:09:56,948 valid 000 1.000674e-02 0.363993
2019-11-06 05:10:06,679 valid 050 8.207496e-03 -0.323243
2019-11-06 05:10:16,385 valid 100 8.267411e-03 -0.108817
2019-11-06 05:10:26,129 valid 150 8.318321e-03 -0.035714
2019-11-06 05:10:35,847 valid 200 8.415325e-03 0.009794
2019-11-06 05:10:45,558 valid 250 8.450287e-03 -0.243406
2019-11-06 05:10:55,273 valid 300 8.453358e-03 -0.191941
2019-11-06 05:11:05,021 valid 350 8.445867e-03 -0.159510
2019-11-06 05:11:14,738 valid 400 8.413830e-03 -0.122188
2019-11-06 05:11:24,459 valid 450 8.462287e-03 -0.098695
2019-11-06 05:11:34,167 valid 500 8.455227e-03 -0.096467
2019-11-06 05:11:43,881 valid 550 8.452161e-03 -0.087193
2019-11-06 05:11:53,595 valid 600 8.433214e-03 -0.067046
2019-11-06 05:12:03,302 valid 650 8.416873e-03 -0.049481
2019-11-06 05:12:13,025 valid 700 8.408666e-03 -0.041578
2019-11-06 05:12:22,688 valid 750 8.396912e-03 -0.039365
2019-11-06 05:12:32,530 valid 800 8.401777e-03 -0.031962
2019-11-06 05:12:44,259 valid 850 8.407976e-03 -0.048207
2019-11-06 05:12:56,122 valid 900 8.402616e-03 -0.044214
2019-11-06 05:13:08,040 valid 950 8.397958e-03 -0.060954
2019-11-06 05:13:19,927 valid 1000 8.397316e-03 -0.053414
2019-11-06 05:13:31,804 valid 1050 8.387165e-03 -0.605982
2019-11-06 05:13:43,688 valid 1100 8.376825e-03 -0.807118
2019-11-06 05:13:55,118 valid 1150 8.372667e-03 -0.774840
2019-11-06 05:14:04,916 valid 1200 8.373052e-03 -0.746114
2019-11-06 05:14:14,690 valid 1250 8.372058e-03 -0.712347
2019-11-06 05:14:24,455 valid 1300 8.381110e-03 -0.683803
2019-11-06 05:14:34,218 valid 1350 8.385837e-03 -0.657382
2019-11-06 05:14:44,030 valid 1400 8.393340e-03 -0.636429
2019-11-06 05:14:53,768 valid 1450 8.410517e-03 -0.625260
2019-11-06 05:15:03,360 valid 1500 8.406226e-03 -0.601897
2019-11-06 05:15:10,930 validation loss; R2: 8.415803e-03 -0.587936
2019-11-06 05:15:11,073 epoch 22 lr 1.000000e-03
2019-11-06 05:15:11,074 genotype = Genotype(normal=[('dil_conv_3x3', 0), ('dil_conv_3x3', 1), ('dil_conv_5x5', 2), ('dil_conv_5x5', 1), ('max_pool_3x3', 0), ('sep_conv_5x5', 2), ('sep_conv_3x3', 2), ('dil_conv_5x5', 4)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 0), ('max_pool_2x2', 1), ('max_pool_3x3', 0), ('dil_conv_5x5', 2), ('dil_conv_5x5', 3), ('dil_conv_5x5', 2), ('dil_conv_5x5', 0), ('dil_conv_5x5', 4)], reduce_concat=range(2, 6))
2019-11-06 05:15:11,076 
alphas_normal = Variable containing:
 0.1216  0.1411  0.1064  0.0294  0.0275  0.1846  0.0134  0.2331  0.1430
 0.8461  0.0121  0.0103  0.0096  0.0099  0.0146  0.0218  0.0663  0.0092
 0.4520  0.1336  0.1066  0.0386  0.0387  0.0338  0.0721  0.0994  0.0251
 0.4330  0.0291  0.0403  0.1148  0.0229  0.0503  0.0462  0.0233  0.2402
 0.4285  0.0466  0.0289  0.0224  0.0245  0.0340  0.0173  0.0197  0.3780
 0.0266  0.0314  0.1280  0.3883  0.0232  0.0297  0.0210  0.0147  0.3371
 0.6773  0.0256  0.0348  0.1119  0.0199  0.0619  0.0261  0.0299  0.0126
 0.5719  0.0435  0.0415  0.0642  0.0171  0.0950  0.1266  0.0228  0.0174
 0.7223  0.0285  0.0308  0.0499  0.0135  0.0504  0.0323  0.0257  0.0465
 0.7872  0.0234  0.0279  0.0285  0.0166  0.0484  0.0419  0.0088  0.0174
 0.6027  0.0257  0.0355  0.0777  0.0209  0.0547  0.0390  0.1266  0.0171
 0.4715  0.0503  0.0595  0.1240  0.0190  0.2276  0.0206  0.0146  0.0128
 0.8135  0.0176  0.0165  0.0217  0.0111  0.0679  0.0288  0.0115  0.0113
 0.4588  0.0328  0.0306  0.0347  0.0174  0.0370  0.1448  0.0632  0.1807
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-06 05:15:11,078 
alphas_reduce = Variable containing:
 0.0308  0.0170  0.0748  0.6195  0.0447  0.0303  0.0617  0.0212  0.1000
 0.4775  0.0556  0.1442  0.1224  0.0364  0.0411  0.0630  0.0246  0.0353
 0.0254  0.0130  0.0430  0.8066  0.0352  0.0199  0.0193  0.0175  0.0200
 0.8356  0.0113  0.0207  0.0215  0.0172  0.0418  0.0196  0.0176  0.0146
 0.0340  0.0313  0.0334  0.0374  0.0264  0.0282  0.0306  0.0232  0.7556
 0.2270  0.0574  0.1055  0.0845  0.1953  0.0745  0.0614  0.1187  0.0757
 0.6468  0.0257  0.0761  0.0455  0.0512  0.0365  0.0419  0.0391  0.0370
 0.3428  0.0637  0.0478  0.0398  0.0528  0.0932  0.0598  0.0516  0.2485
 0.0324  0.0223  0.0187  0.0186  0.0235  0.0496  0.3220  0.0225  0.4905
 0.0541  0.0265  0.0571  0.0670  0.0532  0.1054  0.0505  0.0951  0.4911
 0.1857  0.0347  0.1024  0.1022  0.0659  0.3113  0.0475  0.0453  0.1049
 0.0696  0.0734  0.1140  0.1765  0.0548  0.0638  0.1018  0.3102  0.0361
 0.1003  0.0775  0.0853  0.1087  0.0578  0.1043  0.2533  0.0536  0.1593
 0.0645  0.0500  0.0547  0.0802  0.0455  0.0343  0.1662  0.0417  0.4630
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-06 05:15:16,303 train 000 8.567299e-03 0.479499
2019-11-06 05:18:21,408 train 050 9.152059e-03 0.123552
2019-11-06 05:21:25,203 train 100 8.994690e-03 0.057295
2019-11-06 05:24:38,810 train 150 8.816010e-03 -0.909324
2019-11-06 05:27:48,008 train 200 8.917084e-03 -0.664621
2019-11-06 05:30:59,937 train 250 8.919814e-03 -3.861753
2019-11-06 05:34:06,035 train 300 8.894846e-03 -3.216061
2019-11-06 05:37:15,370 train 350 8.962318e-03 -2.789321
2019-11-06 05:40:16,078 train 400 9.018313e-03 -2.455345
2019-11-06 05:43:26,568 train 450 8.989847e-03 -2.195013
2019-11-06 05:46:31,094 train 500 9.049663e-03 -1.982780
2019-11-06 05:49:35,977 train 550 9.062655e-03 -1.809873
2019-11-06 05:52:43,624 train 600 9.081217e-03 -1.661139
2019-11-06 05:55:42,781 train 650 9.093126e-03 -1.571559
2019-11-06 05:58:45,634 train 700 9.067753e-03 -1.459273
2019-11-06 06:01:46,635 train 750 9.069969e-03 -1.380989
2019-11-06 06:04:51,624 train 800 9.035186e-03 -1.289749
2019-11-06 06:07:52,087 train 850 9.056824e-03 -1.219652
2019-11-06 06:11:00,770 train 900 9.029836e-03 -1.172523
2019-11-06 06:14:02,003 train 950 9.030790e-03 -1.119881
2019-11-06 06:17:09,386 train 1000 9.042490e-03 -1.102513
2019-11-06 06:20:12,930 train 1050 9.056965e-03 -1.049085
2019-11-06 06:23:15,468 train 1100 9.059863e-03 -1.001624
2019-11-06 06:26:19,113 train 1150 9.062141e-03 -0.961011
2019-11-06 06:29:21,181 train 1200 9.059289e-03 -0.918866
2019-11-06 06:32:29,346 train 1250 9.059951e-03 -0.879525
2019-11-06 06:35:29,249 train 1300 9.054602e-03 -0.846316
2019-11-06 06:38:35,633 train 1350 9.038360e-03 -0.824773
2019-11-06 06:41:47,753 train 1400 9.021889e-03 -0.879899
2019-11-06 06:44:53,437 train 1450 9.022521e-03 -0.852838
2019-11-06 06:48:00,360 train 1500 9.031230e-03 -0.836803
2019-11-06 06:50:24,812 training loss; R2: 9.034228e-03 -0.816820
2019-11-06 06:50:25,298 valid 000 8.258938e-03 0.275692
2019-11-06 06:50:35,061 valid 050 8.657506e-03 -0.027275
2019-11-06 06:50:44,815 valid 100 8.581794e-03 0.073234
2019-11-06 06:50:54,564 valid 150 8.702951e-03 0.050258
2019-11-06 06:51:04,327 valid 200 8.687239e-03 0.081201
2019-11-06 06:51:14,086 valid 250 8.702226e-03 0.077799
2019-11-06 06:51:23,850 valid 300 8.715322e-03 0.075445
2019-11-06 06:51:33,595 valid 350 8.713068e-03 0.083195
2019-11-06 06:51:43,336 valid 400 8.734858e-03 0.034087
2019-11-06 06:51:53,077 valid 450 8.744487e-03 0.044314
2019-11-06 06:52:02,832 valid 500 8.747733e-03 0.063405
2019-11-06 06:52:12,569 valid 550 8.749093e-03 0.059163
2019-11-06 06:52:22,317 valid 600 8.769287e-03 0.021857
2019-11-06 06:52:32,086 valid 650 8.777857e-03 0.033010
2019-11-06 06:52:41,844 valid 700 8.785412e-03 0.046359
2019-11-06 06:52:51,620 valid 750 8.782783e-03 0.049625
2019-11-06 06:53:01,377 valid 800 8.792580e-03 0.054960
2019-11-06 06:53:11,133 valid 850 8.788414e-03 0.057363
2019-11-06 06:53:20,891 valid 900 8.802786e-03 0.028071
2019-11-06 06:53:30,800 valid 950 8.788046e-03 0.029886
2019-11-06 06:53:40,733 valid 1000 8.797072e-03 0.036980
2019-11-06 06:53:50,582 valid 1050 8.764881e-03 0.043857
2019-11-06 06:54:00,416 valid 1100 8.779743e-03 0.037666
2019-11-06 06:54:10,220 valid 1150 8.765500e-03 0.036469
2019-11-06 06:54:20,072 valid 1200 8.763564e-03 0.029280
2019-11-06 06:54:29,947 valid 1250 8.758752e-03 -0.003678
2019-11-06 06:54:39,783 valid 1300 8.765512e-03 -0.001960
2019-11-06 06:54:49,548 valid 1350 8.767665e-03 -0.019106
2019-11-06 06:54:59,340 valid 1400 8.789351e-03 -0.016298
2019-11-06 06:55:09,098 valid 1450 8.801121e-03 -0.015290
2019-11-06 06:55:18,875 valid 1500 8.816571e-03 -0.015784
2019-11-06 06:55:26,450 validation loss; R2: 8.818038e-03 -0.016667
2019-11-06 06:55:26,587 epoch 23 lr 1.000000e-03
2019-11-06 06:55:26,588 genotype = Genotype(normal=[('sep_conv_3x3', 0), ('dil_conv_3x3', 1), ('dil_conv_5x5', 2), ('dil_conv_5x5', 1), ('max_pool_3x3', 0), ('sep_conv_5x5', 2), ('sep_conv_3x3', 2), ('dil_conv_5x5', 4)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 0), ('max_pool_3x3', 1), ('dil_conv_5x5', 2), ('max_pool_3x3', 0), ('dil_conv_5x5', 3), ('dil_conv_5x5', 2), ('dil_conv_5x5', 0), ('dil_conv_5x5', 4)], reduce_concat=range(2, 6))
2019-11-06 06:55:26,589 
alphas_normal = Variable containing:
 0.1546  0.1785  0.1091  0.0340  0.0319  0.1886  0.0143  0.1577  0.1313
 0.8276  0.0142  0.0109  0.0094  0.0115  0.0095  0.0151  0.0936  0.0082
 0.3514  0.1902  0.1143  0.0433  0.0499  0.0244  0.0802  0.1250  0.0213
 0.3380  0.0362  0.0431  0.1158  0.0253  0.0507  0.0547  0.0221  0.3140
 0.3494  0.0481  0.0304  0.0220  0.0292  0.0330  0.0176  0.0166  0.4538
 0.0372  0.0363  0.1171  0.3846  0.0248  0.0488  0.0130  0.0125  0.3257
 0.7409  0.0240  0.0286  0.0920  0.0173  0.0426  0.0230  0.0219  0.0097
 0.5685  0.0426  0.0393  0.0542  0.0189  0.0867  0.1402  0.0286  0.0210
 0.6394  0.0449  0.0447  0.0526  0.0242  0.0533  0.0331  0.0527  0.0551
 0.8185  0.0247  0.0235  0.0242  0.0177  0.0376  0.0272  0.0122  0.0144
 0.6398  0.0264  0.0299  0.0719  0.0210  0.0455  0.0246  0.0948  0.0461
 0.4642  0.0597  0.0719  0.1432  0.0220  0.1956  0.0233  0.0101  0.0099
 0.8295  0.0166  0.0150  0.0179  0.0117  0.0611  0.0183  0.0132  0.0167
 0.5089  0.0463  0.0314  0.0364  0.0210  0.0373  0.1158  0.0670  0.1359
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-06 06:55:26,591 
alphas_reduce = Variable containing:
 0.0400  0.0262  0.0928  0.6430  0.0437  0.0201  0.0418  0.0160  0.0765
 0.4741  0.0662  0.1335  0.1348  0.0345  0.0287  0.0671  0.0303  0.0308
 0.0386  0.0165  0.0764  0.7260  0.0443  0.0259  0.0244  0.0164  0.0314
 0.8430  0.0156  0.0221  0.0209  0.0188  0.0330  0.0144  0.0168  0.0154
 0.0242  0.0203  0.0202  0.0224  0.0188  0.0232  0.0205  0.0244  0.8260
 0.2494  0.0624  0.0827  0.0649  0.1412  0.0866  0.0711  0.1732  0.0685
 0.5659  0.0250  0.0652  0.0500  0.0514  0.0422  0.0749  0.0542  0.0712
 0.3226  0.0474  0.0353  0.0315  0.0445  0.0821  0.0661  0.0366  0.3339
 0.0312  0.0219  0.0197  0.0179  0.0237  0.0500  0.2007  0.0200  0.6149
 0.0406  0.0241  0.0381  0.0409  0.0396  0.1042  0.0432  0.0872  0.5821
 0.1315  0.0372  0.0886  0.0962  0.0605  0.3892  0.0383  0.0464  0.1122
 0.1027  0.0655  0.0735  0.1110  0.0465  0.0479  0.1138  0.3977  0.0413
 0.0865  0.0563  0.0635  0.0777  0.0511  0.1449  0.2646  0.0673  0.1880
 0.0739  0.0512  0.0502  0.0652  0.0499  0.0451  0.0733  0.0451  0.5461
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-06 06:55:31,531 train 000 7.362501e-03 -0.397119
2019-11-06 06:58:33,056 train 050 8.998246e-03 -0.296593
2019-11-06 07:01:33,870 train 100 8.890557e-03 -0.112663
2019-11-06 07:04:42,818 train 150 8.901242e-03 -0.072682
2019-11-06 07:07:46,859 train 200 8.935467e-03 -0.093155
2019-11-06 07:10:47,774 train 250 8.899911e-03 -0.114036
2019-11-06 07:13:48,740 train 300 8.892860e-03 -0.100591
2019-11-06 07:16:53,792 train 350 8.890890e-03 -0.097672
2019-11-06 07:20:02,246 train 400 8.960157e-03 -0.106533
2019-11-06 07:23:06,401 train 450 8.924939e-03 -0.103887
2019-11-06 07:26:06,889 train 500 8.930638e-03 -2.008587
2019-11-06 07:29:11,136 train 550 8.986897e-03 -1.832693
2019-11-06 07:32:13,015 train 600 8.985106e-03 -1.674633
2019-11-06 07:35:12,982 train 650 8.959920e-03 -1.553465
2019-11-06 07:38:13,156 train 700 8.945963e-03 -1.498449
2019-11-06 07:41:14,902 train 750 8.982541e-03 -1.398868
2019-11-06 07:44:15,493 train 800 8.973589e-03 -1.474505
2019-11-06 07:47:20,220 train 850 9.014991e-03 -1.387931
2019-11-06 07:50:25,182 train 900 9.000361e-03 -1.325826
2019-11-06 07:53:29,291 train 950 8.998980e-03 -1.255051
2019-11-06 07:56:35,210 train 1000 8.982165e-03 -1.383434
2019-11-06 07:59:43,539 train 1050 8.987911e-03 -1.314685
2019-11-06 08:02:44,000 train 1100 8.991166e-03 -1.261173
2019-11-06 08:05:54,783 train 1150 8.993172e-03 -1.202819
2019-11-06 08:08:57,548 train 1200 8.988629e-03 -1.148198
2019-11-06 08:12:06,649 train 1250 8.985889e-03 -1.100290
2019-11-06 08:15:10,520 train 1300 8.992458e-03 -1.054539
2019-11-06 08:18:13,044 train 1350 8.988844e-03 -1.025418
2019-11-06 08:21:15,598 train 1400 8.995745e-03 -0.989457
2019-11-06 08:24:21,528 train 1450 9.002550e-03 -0.951411
2019-11-06 08:27:28,641 train 1500 9.008349e-03 -0.921169
2019-11-06 08:29:53,486 training loss; R2: 9.009425e-03 -0.894947
2019-11-06 08:29:53,983 valid 000 1.187500e-02 0.091253
2019-11-06 08:30:03,728 valid 050 1.222921e-02 0.058663
2019-11-06 08:30:13,515 valid 100 1.202518e-02 0.105959
2019-11-06 08:30:23,285 valid 150 1.205563e-02 0.019058
2019-11-06 08:30:33,048 valid 200 1.211760e-02 0.013793
2019-11-06 08:30:42,792 valid 250 1.218398e-02 0.039607
2019-11-06 08:30:52,550 valid 300 1.211205e-02 0.043698
2019-11-06 08:31:02,317 valid 350 1.202578e-02 0.045029
2019-11-06 08:31:12,100 valid 400 1.200824e-02 0.054916
2019-11-06 08:31:21,957 valid 450 1.197807e-02 0.040237
2019-11-06 08:31:31,853 valid 500 1.197775e-02 0.051059
2019-11-06 08:31:41,656 valid 550 1.194759e-02 0.060604
2019-11-06 08:31:51,444 valid 600 1.195729e-02 0.053515
2019-11-06 08:32:01,257 valid 650 1.199953e-02 0.060271
2019-11-06 08:32:11,018 valid 700 1.198341e-02 0.064622
2019-11-06 08:32:20,828 valid 750 1.197683e-02 0.071929
2019-11-06 08:32:30,607 valid 800 1.197052e-02 0.058443
2019-11-06 08:32:40,399 valid 850 1.195590e-02 0.032322
2019-11-06 08:32:50,193 valid 900 1.196657e-02 0.036658
2019-11-06 08:32:59,987 valid 950 1.197250e-02 0.042625
2019-11-06 08:33:09,782 valid 1000 1.195576e-02 -0.033284
2019-11-06 08:33:19,549 valid 1050 1.195556e-02 -0.034941
2019-11-06 08:33:29,312 valid 1100 1.196078e-02 -0.050575
2019-11-06 08:33:39,098 valid 1150 1.195857e-02 -0.042285
2019-11-06 08:33:48,831 valid 1200 1.195649e-02 -0.041783
2019-11-06 08:33:58,568 valid 1250 1.197190e-02 -0.036072
2019-11-06 08:34:08,308 valid 1300 1.199184e-02 -0.033120
2019-11-06 08:34:18,154 valid 1350 1.198292e-02 -0.026069
2019-11-06 08:34:28,015 valid 1400 1.199185e-02 -0.024640
2019-11-06 08:34:37,741 valid 1450 1.198584e-02 -0.017962
2019-11-06 08:34:47,663 valid 1500 1.197722e-02 -0.013439
2019-11-06 08:34:55,442 validation loss; R2: 1.198329e-02 -0.008523
2019-11-06 08:34:55,583 epoch 24 lr 1.000000e-03
2019-11-06 08:34:55,584 genotype = Genotype(normal=[('dil_conv_3x3', 0), ('dil_conv_3x3', 1), ('dil_conv_5x5', 2), ('dil_conv_5x5', 1), ('max_pool_3x3', 0), ('max_pool_3x3', 1), ('sep_conv_3x3', 2), ('dil_conv_5x5', 4)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 0), ('max_pool_2x2', 1), ('max_pool_3x3', 0), ('dil_conv_5x5', 2), ('dil_conv_5x5', 3), ('dil_conv_5x5', 2), ('dil_conv_5x5', 0), ('dil_conv_5x5', 4)], reduce_concat=range(2, 6))
2019-11-06 08:34:55,586 
alphas_normal = Variable containing:
 0.1461  0.1714  0.0941  0.0331  0.0287  0.1870  0.0168  0.1949  0.1279
 0.8240  0.0149  0.0118  0.0124  0.0106  0.0148  0.0169  0.0876  0.0070
 0.4192  0.1494  0.1180  0.0627  0.0417  0.0308  0.0749  0.0862  0.0171
 0.3691  0.0292  0.0443  0.1367  0.0208  0.0468  0.0501  0.0220  0.2809
 0.3632  0.0444  0.0379  0.0339  0.0229  0.0478  0.0113  0.0128  0.4258
 0.0544  0.0370  0.1032  0.3661  0.0246  0.0316  0.0123  0.0121  0.3588
 0.6536  0.0276  0.0431  0.1402  0.0188  0.0652  0.0196  0.0218  0.0102
 0.6216  0.0369  0.0334  0.0517  0.0159  0.0849  0.1192  0.0194  0.0169
 0.7073  0.0282  0.0351  0.0414  0.0155  0.0565  0.0299  0.0221  0.0639
 0.7905  0.0288  0.0267  0.0256  0.0216  0.0476  0.0268  0.0179  0.0145
 0.6483  0.0236  0.0274  0.0640  0.0188  0.0629  0.0228  0.1045  0.0276
 0.4866  0.0657  0.0578  0.1389  0.0254  0.1812  0.0217  0.0123  0.0104
 0.8586  0.0154  0.0137  0.0149  0.0120  0.0491  0.0135  0.0118  0.0111
 0.5113  0.0397  0.0287  0.0352  0.0235  0.0349  0.1097  0.0770  0.1400
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-06 08:34:55,588 
alphas_reduce = Variable containing:
 0.0435  0.0340  0.1020  0.5735  0.0442  0.0240  0.0580  0.0277  0.0931
 0.4462  0.0686  0.1524  0.1388  0.0294  0.0331  0.0695  0.0280  0.0339
 0.0359  0.0150  0.0371  0.7692  0.0391  0.0270  0.0243  0.0228  0.0294
 0.8114  0.0149  0.0296  0.0334  0.0253  0.0393  0.0161  0.0153  0.0148
 0.0388  0.0347  0.0306  0.0368  0.0323  0.0272  0.0537  0.0235  0.7224
 0.1651  0.0559  0.1215  0.1012  0.2200  0.0971  0.0573  0.1156  0.0663
 0.6415  0.0260  0.0767  0.0493  0.0519  0.0365  0.0307  0.0232  0.0643
 0.3291  0.0586  0.0452  0.0394  0.0564  0.0894  0.0253  0.0239  0.3327
 0.0376  0.0268  0.0206  0.0200  0.0279  0.0587  0.2122  0.0151  0.5810
 0.0406  0.0199  0.0448  0.0494  0.0505  0.0609  0.0497  0.0649  0.6193
 0.1231  0.0309  0.0999  0.1136  0.0474  0.3789  0.0513  0.0379  0.1170
 0.0794  0.0785  0.0950  0.1442  0.0666  0.0647  0.1001  0.3250  0.0465
 0.0753  0.0791  0.0724  0.1012  0.0658  0.1390  0.2289  0.0552  0.1830
 0.0811  0.0629  0.0618  0.0703  0.0668  0.0477  0.0925  0.0541  0.4630
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-06 08:35:00,620 train 000 8.050251e-03 -0.186175
2019-11-06 08:38:01,675 train 050 8.828568e-03 -0.420673
2019-11-06 08:41:01,456 train 100 9.054442e-03 -0.224412
2019-11-06 08:44:10,752 train 150 9.117224e-03 -0.224082
2019-11-06 08:47:16,616 train 200 8.996906e-03 -1.464816
2019-11-06 08:50:24,097 train 250 9.062798e-03 -1.175268
2019-11-06 08:53:27,820 train 300 8.998650e-03 -1.021134
2019-11-06 08:56:29,650 train 350 8.985209e-03 -0.925210
2019-11-06 08:59:34,102 train 400 8.959742e-03 -0.814451
2019-11-06 09:02:34,111 train 450 8.980849e-03 -0.739937
2019-11-06 09:05:37,222 train 500 8.972831e-03 -0.659967
2019-11-06 09:08:41,293 train 550 8.975776e-03 -0.583095
2019-11-06 09:11:44,692 train 600 8.969414e-03 -0.541576
2019-11-06 09:14:54,508 train 650 8.979831e-03 -1.726979
