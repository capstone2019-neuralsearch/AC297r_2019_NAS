2019-11-04 16:16:51,722 gpu device = 1
2019-11-04 16:16:51,723 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=128, cutout=False, cutout_length=16, data='../data', dataset='GalaxyZoo', drop_path_prob=0.3, epochs=200, fc1_size=1024, fc2_size=1024, gpu=1, grad_clip=5, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', random=False, report_freq=50, save='eval-GalaxyZoo-Run04-C16_L08_WD1E-8_DROP30_BS128-20191104-161651', seed=0, val_portion=0.1, weight_decay=1e-08)
2019-11-04 16:16:54,962 param size = 1.553237MB
2019-11-04 16:16:54,966 epoch 0 lr 1.000000e-03
2019-11-04 16:16:57,902 train 000 3.859863e-02 -3.220188
2019-11-04 16:17:15,703 train 050 2.567233e-02 -0.635410
2019-11-04 16:17:34,779 train 100 2.217580e-02 -0.504633
2019-11-04 16:17:53,632 train 150 2.055926e-02 -0.444916
2019-11-04 16:18:12,318 train 200 1.960953e-02 -0.426518
2019-11-04 16:18:31,225 train 250 1.898807e-02 -0.406934
2019-11-04 16:18:50,307 train 300 1.849761e-02 -0.388531
2019-11-04 16:19:09,027 train 350 1.811099e-02 -0.386159
2019-11-04 16:19:27,734 train 400 1.779654e-02 -0.378058
2019-11-04 16:19:40,408 training loss; R2: 1.766971e-02 -0.383709
2019-11-04 16:19:41,425 valid 000 2.769662e-02 -0.339991
2019-11-04 16:19:58,794 validation loss; R2: 2.623993e-02 -0.394972
2019-11-04 16:19:58,816 epoch 1 lr 9.999383e-04
2019-11-04 16:20:00,043 train 000 1.733593e-02 -0.083448
2019-11-04 16:20:18,740 train 050 1.579948e-02 -0.185892
2019-11-04 16:20:37,471 train 100 1.574905e-02 -0.297206
2019-11-04 16:20:56,185 train 150 1.549946e-02 -0.349832
2019-11-04 16:21:14,925 train 200 1.543051e-02 -0.404483
2019-11-04 16:21:33,627 train 250 1.535354e-02 -0.412824
2019-11-04 16:21:52,359 train 300 1.521955e-02 -0.397724
2019-11-04 16:22:11,120 train 350 1.503726e-02 -0.402207
2019-11-04 16:22:29,864 train 400 1.494409e-02 -0.371395
2019-11-04 16:22:41,812 training loss; R2: 1.489438e-02 -0.359623
2019-11-04 16:22:42,809 valid 000 1.784307e-02 -0.563762
2019-11-04 16:23:00,139 validation loss; R2: 1.719023e-02 -0.237338
2019-11-04 16:23:00,166 epoch 2 lr 9.997533e-04
2019-11-04 16:23:01,304 train 000 1.285876e-02 -0.072641
2019-11-04 16:23:20,062 train 050 1.477197e-02 -0.272603
2019-11-04 16:23:38,757 train 100 1.472438e-02 -0.334793
2019-11-04 16:23:57,433 train 150 1.470328e-02 -0.303553
2019-11-04 16:24:16,092 train 200 1.460645e-02 -0.317165
2019-11-04 16:24:34,766 train 250 1.439422e-02 -0.327589
2019-11-04 16:24:53,417 train 300 1.429810e-02 -0.315728
2019-11-04 16:25:12,089 train 350 1.423795e-02 -0.346238
2019-11-04 16:25:30,759 train 400 1.412087e-02 -0.357233
2019-11-04 16:25:42,671 training loss; R2: 1.405670e-02 -0.366128
2019-11-04 16:25:43,697 valid 000 1.373633e-02 -0.807785
2019-11-04 16:26:01,037 validation loss; R2: 1.392981e-02 -2.255223
2019-11-04 16:26:01,066 epoch 3 lr 9.994449e-04
2019-11-04 16:26:02,229 train 000 1.365322e-02 0.235299
2019-11-04 16:26:20,942 train 050 1.377038e-02 -0.342319
2019-11-04 16:26:39,610 train 100 1.352560e-02 -0.408261
2019-11-04 16:26:58,285 train 150 1.334910e-02 -0.363187
2019-11-04 16:27:16,967 train 200 1.323482e-02 -0.345670
2019-11-04 16:27:35,658 train 250 1.328055e-02 -0.327708
2019-11-04 16:27:54,540 train 300 1.320456e-02 -0.317368
2019-11-04 16:28:13,222 train 350 1.318332e-02 -0.322264
2019-11-04 16:28:32,049 train 400 1.317467e-02 -0.323578
2019-11-04 16:28:44,103 training loss; R2: 1.316902e-02 -0.307270
2019-11-04 16:28:45,176 valid 000 2.595815e-02 -2.585653
2019-11-04 16:29:02,843 validation loss; R2: 2.206207e-02 -4.079800
2019-11-04 16:29:02,871 epoch 4 lr 9.990134e-04
2019-11-04 16:29:03,979 train 000 1.371301e-02 -0.081861
2019-11-04 16:29:22,914 train 050 1.485891e-02 -0.404917
2019-11-04 16:29:41,682 train 100 1.926491e-02 -0.419773
2019-11-04 16:30:00,328 train 150 1.940327e-02 -0.387356
2019-11-04 16:30:18,972 train 200 1.914558e-02 -0.377007
2019-11-04 16:30:37,713 train 250 1.892088e-02 -0.361022
2019-11-04 16:30:56,451 train 300 1.884586e-02 -0.371771
2019-11-04 16:31:15,167 train 350 1.842020e-02 -0.371499
2019-11-04 16:31:33,869 train 400 1.805609e-02 -0.365989
2019-11-04 16:31:45,850 training loss; R2: 1.781095e-02 -0.358453
2019-11-04 16:31:46,829 valid 000 1.921108e-02 -1.197793
2019-11-04 16:32:04,163 validation loss; R2: 1.773116e-02 -1.158559
2019-11-04 16:32:04,189 epoch 5 lr 9.984587e-04
2019-11-04 16:32:05,325 train 000 1.440141e-02 -0.550800
2019-11-04 16:32:24,077 train 050 1.495002e-02 -0.409946
2019-11-04 16:32:42,954 train 100 1.445137e-02 -0.374205
2019-11-04 16:33:01,619 train 150 1.414754e-02 -0.326178
2019-11-04 16:33:20,295 train 200 1.392688e-02 -0.327985
2019-11-04 16:33:39,238 train 250 1.376432e-02 -0.319917
2019-11-04 16:33:58,253 train 300 1.361598e-02 -0.321084
2019-11-04 16:34:16,934 train 350 1.347585e-02 -0.307993
2019-11-04 16:34:35,595 train 400 1.338796e-02 -0.325318
2019-11-04 16:34:47,714 training loss; R2: 1.332767e-02 -0.310290
2019-11-04 16:34:48,778 valid 000 1.500567e-02 -0.394804
2019-11-04 16:35:06,180 validation loss; R2: 1.356868e-02 -1.189865
2019-11-04 16:35:06,206 epoch 6 lr 9.977810e-04
2019-11-04 16:35:07,342 train 000 1.130963e-02 -1.003295
2019-11-04 16:35:26,338 train 050 1.260315e-02 -0.260025
2019-11-04 16:35:45,409 train 100 1.255634e-02 -0.372380
2019-11-04 16:36:04,170 train 150 1.252871e-02 -0.378130
2019-11-04 16:36:22,904 train 200 1.245905e-02 -0.342426
2019-11-04 16:36:41,929 train 250 1.247558e-02 -0.303756
2019-11-04 16:37:01,458 train 300 1.250443e-02 -0.298207
2019-11-04 16:37:20,683 train 350 1.252760e-02 -0.294070
2019-11-04 16:37:39,724 train 400 1.252476e-02 -0.297233
2019-11-04 16:37:51,944 training loss; R2: 1.251450e-02 -0.301982
2019-11-04 16:37:52,912 valid 000 1.461017e-02 0.192547
2019-11-04 16:38:10,839 validation loss; R2: 1.270764e-02 -0.474561
2019-11-04 16:38:10,867 epoch 7 lr 9.969805e-04
2019-11-04 16:38:12,026 train 000 1.272521e-02 -0.120465
2019-11-04 16:38:31,415 train 050 1.250250e-02 -0.291766
2019-11-04 16:38:50,656 train 100 1.238547e-02 -0.333571
2019-11-04 16:39:09,857 train 150 1.239834e-02 -0.442787
2019-11-04 16:39:29,077 train 200 1.245925e-02 -0.377005
2019-11-04 16:39:48,273 train 250 1.916395e-02 -1.176619
2019-11-04 16:40:07,453 train 300 3.188896e-02 -3.962227
2019-11-04 16:40:26,647 train 350 4.005184e-02 -6.467382
2019-11-04 16:40:45,811 train 400 4.584896e-02 -8.509892
2019-11-04 16:40:58,054 training loss; R2: 4.886731e-02 -9.523854
2019-11-04 16:40:59,052 valid 000 8.278236e-02 -23.408222
2019-11-04 16:41:16,814 validation loss; R2: 8.482933e-02 -24.131839
2019-11-04 16:41:16,841 epoch 8 lr 9.960574e-04
2019-11-04 16:41:18,071 train 000 8.178743e-02 -27.878108
2019-11-04 16:41:37,224 train 050 8.840546e-02 -23.356964
2019-11-04 16:41:56,332 train 100 9.391948e-02 -22.237579
2019-11-04 16:42:15,432 train 150 9.674013e-02 -21.250345
2019-11-04 16:42:34,512 train 200 1.007130e-01 -20.347565
2019-11-04 16:42:53,692 train 250 1.005970e-01 -20.268969
2019-11-04 16:43:12,790 train 300 9.852017e-02 -21.000466
2019-11-04 16:43:31,897 train 350 9.688354e-02 -21.646125
2019-11-04 16:43:50,983 train 400 9.529107e-02 -21.932533
2019-11-04 16:44:03,189 training loss; R2: 9.450654e-02 -22.159180
2019-11-04 16:44:04,164 valid 000 8.365224e-02 -41.721630
2019-11-04 16:44:22,297 validation loss; R2: 8.482933e-02 -24.921220
2019-11-04 16:44:22,320 epoch 9 lr 9.950118e-04
2019-11-04 16:44:23,421 train 000 8.105399e-02 -22.045660
2019-11-04 16:44:42,589 train 050 8.509928e-02 -24.249341
2019-11-04 16:45:02,004 train 100 8.512745e-02 -24.822934
2019-11-04 16:45:21,373 train 150 8.524462e-02 -24.441039
2019-11-04 16:45:40,338 train 200 8.535132e-02 -24.052150
2019-11-04 16:45:59,471 train 250 8.513463e-02 -24.300942
2019-11-04 16:46:18,928 train 300 8.495421e-02 -24.926279
2019-11-04 16:46:37,948 train 350 8.491230e-02 -24.780820
2019-11-04 16:46:57,038 train 400 8.490626e-02 -24.486594
2019-11-04 16:47:09,395 training loss; R2: 8.488713e-02 -24.461890
2019-11-04 16:47:10,450 valid 000 8.287776e-02 -38.341173
2019-11-04 16:47:28,302 validation loss; R2: 8.482933e-02 -23.862963
2019-11-04 16:47:28,330 epoch 10 lr 9.938442e-04
2019-11-04 16:47:29,405 train 000 8.474468e-02 -6.713855
2019-11-04 16:47:48,747 train 050 8.527548e-02 -25.639254
2019-11-04 16:48:07,952 train 100 8.528207e-02 -24.310023
2019-11-04 16:48:27,120 train 150 8.521821e-02 -24.174300
2019-11-04 16:48:46,227 train 200 8.512117e-02 -24.675927
2019-11-04 16:49:05,301 train 250 8.508619e-02 -24.705111
2019-11-04 16:49:24,342 train 300 8.505630e-02 -24.698718
2019-11-04 16:49:43,410 train 350 8.497649e-02 -24.234707
2019-11-04 16:50:02,453 train 400 8.498709e-02 -24.244910
2019-11-04 16:50:14,641 training loss; R2: 8.498237e-02 -24.252456
2019-11-04 16:50:15,635 valid 000 8.221478e-02 -25.562446
2019-11-04 16:50:33,759 validation loss; R2: 8.482933e-02 -24.203800
2019-11-04 16:50:33,790 epoch 11 lr 9.925547e-04
2019-11-04 16:50:34,900 train 000 8.098295e-02 -27.824708
2019-11-04 16:50:53,871 train 050 8.497237e-02 -22.725018
2019-11-04 16:51:13,041 train 100 8.502938e-02 -23.690769
2019-11-04 16:51:32,282 train 150 8.512846e-02 -23.537150
2019-11-04 16:51:51,579 train 200 8.508270e-02 -24.028610
2019-11-04 16:52:10,655 train 250 8.509276e-02 -24.466951
2019-11-04 16:52:29,739 train 300 8.487148e-02 -24.230036
2019-11-04 16:52:48,739 train 350 8.488301e-02 -24.132158
2019-11-04 16:53:07,844 train 400 8.491357e-02 -24.234320
2019-11-04 16:53:20,054 training loss; R2: 8.491428e-02 -24.222997
2019-11-04 16:53:21,014 valid 000 8.718676e-02 -5.898821
2019-11-04 16:53:38,950 validation loss; R2: 8.482933e-02 -23.902022
2019-11-04 16:53:38,975 epoch 12 lr 9.911436e-04
2019-11-04 16:53:40,102 train 000 8.703674e-02 -30.204261
2019-11-04 16:53:59,458 train 050 8.414679e-02 -24.514093
2019-11-04 16:54:18,627 train 100 8.432980e-02 -23.102083
2019-11-04 16:54:37,723 train 150 8.444273e-02 -23.590697
2019-11-04 16:54:56,859 train 200 8.455373e-02 -24.256927
2019-11-04 16:55:15,958 train 250 8.473819e-02 -24.241059
2019-11-04 16:55:35,045 train 300 8.472667e-02 -24.371259
2019-11-04 16:55:54,143 train 350 8.488473e-02 -24.390080
2019-11-04 16:56:13,260 train 400 8.500712e-02 -24.631165
2019-11-04 16:56:25,467 training loss; R2: 8.494912e-02 -24.585353
2019-11-04 16:56:26,499 valid 000 8.659271e-02 -42.955187
2019-11-04 16:56:44,294 validation loss; R2: 8.482933e-02 -24.142066
2019-11-04 16:56:44,322 epoch 13 lr 9.896114e-04
2019-11-04 16:56:45,435 train 000 8.451498e-02 -50.687929
2019-11-04 16:57:04,561 train 050 8.515905e-02 -24.249363
2019-11-04 16:57:23,893 train 100 8.536912e-02 -23.684183
2019-11-04 16:57:43,152 train 150 8.509829e-02 -23.883596
2019-11-04 16:58:02,360 train 200 8.483168e-02 -23.629785
2019-11-04 16:58:21,375 train 250 8.482711e-02 -23.614875
2019-11-04 16:58:40,719 train 300 8.484780e-02 -23.959368
2019-11-04 16:59:00,061 train 350 8.477847e-02 -23.894148
2019-11-04 16:59:19,181 train 400 8.481071e-02 -24.022640
2019-11-04 16:59:31,391 training loss; R2: 8.492446e-02 -24.137471
2019-11-04 16:59:32,404 valid 000 8.872609e-02 -28.294965
2019-11-04 16:59:50,422 validation loss; R2: 8.482933e-02 -25.134930
2019-11-04 16:59:50,455 epoch 14 lr 9.879584e-04
2019-11-04 16:59:51,612 train 000 8.528715e-02 -21.132373
2019-11-04 17:00:11,166 train 050 8.426975e-02 -25.210317
2019-11-04 17:00:30,299 train 100 8.457703e-02 -23.245120
2019-11-04 17:00:49,441 train 150 8.475221e-02 -23.172757
2019-11-04 17:01:08,541 train 200 8.495246e-02 -24.056665
2019-11-04 17:01:27,612 train 250 8.493358e-02 -24.316363
2019-11-04 17:01:46,678 train 300 8.490123e-02 -24.357008
2019-11-04 17:02:05,750 train 350 8.483533e-02 -24.710371
2019-11-04 17:02:24,826 train 400 8.494086e-02 -24.554229
2019-11-04 17:02:36,993 training loss; R2: 8.495947e-02 -24.311736
2019-11-04 17:02:38,001 valid 000 8.441412e-02 -32.539269
2019-11-04 17:02:55,986 validation loss; R2: 8.482933e-02 -24.755325
2019-11-04 17:02:56,012 epoch 15 lr 9.861850e-04
2019-11-04 17:02:57,096 train 000 8.726089e-02 -27.989588
2019-11-04 17:03:16,329 train 050 8.511375e-02 -24.030770
2019-11-04 17:03:35,437 train 100 8.531905e-02 -25.521322
2019-11-04 17:03:54,482 train 150 8.501961e-02 -25.480572
2019-11-04 17:04:13,523 train 200 8.491625e-02 -25.271291
2019-11-04 17:04:32,306 train 250 8.496923e-02 -25.570964
2019-11-04 17:04:51,094 train 300 8.496970e-02 -25.383995
2019-11-04 17:05:09,871 train 350 8.501014e-02 -25.239774
2019-11-04 17:05:28,790 train 400 8.501548e-02 -24.853926
2019-11-04 17:05:40,797 training loss; R2: 8.501040e-02 -24.791153
2019-11-04 17:05:41,787 valid 000 8.217560e-02 -44.803718
2019-11-04 17:05:59,400 validation loss; R2: 8.482933e-02 -25.839638
2019-11-04 17:05:59,428 epoch 16 lr 9.842916e-04
2019-11-04 17:06:00,556 train 000 8.250612e-02 -41.194548
2019-11-04 17:06:19,653 train 050 8.522348e-02 -24.975849
2019-11-04 17:06:38,490 train 100 8.521211e-02 -24.516318
2019-11-04 17:06:57,382 train 150 8.514380e-02 -24.523377
2019-11-04 17:07:16,213 train 200 8.505093e-02 -24.317191
2019-11-04 17:07:35,134 train 250 8.509867e-02 -24.624324
2019-11-04 17:07:54,004 train 300 8.504100e-02 -24.910226
2019-11-04 17:08:12,931 train 350 8.500614e-02 -24.779935
2019-11-04 17:08:31,858 train 400 8.489396e-02 -24.738596
2019-11-04 17:08:44,044 training loss; R2: 8.492750e-02 -24.756808
2019-11-04 17:08:45,040 valid 000 7.996911e-02 -16.841034
2019-11-04 17:09:03,036 validation loss; R2: 8.482933e-02 -24.797897
2019-11-04 17:09:03,064 epoch 17 lr 9.822787e-04
2019-11-04 17:09:04,210 train 000 8.500312e-02 -10.710697
2019-11-04 17:09:23,363 train 050 8.588489e-02 -25.501251
2019-11-04 17:09:42,529 train 100 8.542877e-02 -25.073753
2019-11-04 17:10:01,834 train 150 8.522765e-02 -25.125553
2019-11-04 17:10:21,095 train 200 8.506381e-02 -24.910634
2019-11-04 17:10:40,354 train 250 8.499585e-02 -24.554057
2019-11-04 17:10:59,249 train 300 8.488903e-02 -24.115394
2019-11-04 17:11:18,578 train 350 8.480808e-02 -24.389029
2019-11-04 17:11:37,860 train 400 8.479008e-02 -24.456564
2019-11-04 17:11:49,987 training loss; R2: 8.489639e-02 -24.457970
2019-11-04 17:11:51,009 valid 000 8.664420e-02 -21.517462
2019-11-04 17:12:09,248 validation loss; R2: 8.482933e-02 -24.045900
2019-11-04 17:12:09,280 epoch 18 lr 9.801468e-04
2019-11-04 17:12:10,459 train 000 8.493367e-02 -26.639025
2019-11-04 17:12:29,466 train 050 8.453588e-02 -24.761967
2019-11-04 17:12:48,742 train 100 8.426663e-02 -23.989357
2019-11-04 17:13:08,210 train 150 8.455103e-02 -24.537205
2019-11-04 17:13:27,393 train 200 8.466905e-02 -24.538876
2019-11-04 17:13:46,616 train 250 8.468702e-02 -24.689898
2019-11-04 17:14:06,099 train 300 8.478041e-02 -24.566774
2019-11-04 17:14:25,308 train 350 8.479959e-02 -24.331703
2019-11-04 17:14:44,255 train 400 8.482382e-02 -24.247577
2019-11-04 17:14:56,587 training loss; R2: 8.485808e-02 -24.402359
2019-11-04 17:14:57,559 valid 000 8.807309e-02 -31.068176
2019-11-04 17:15:15,979 validation loss; R2: 8.482933e-02 -23.866709
2019-11-04 17:15:16,010 epoch 19 lr 9.778965e-04
2019-11-04 17:15:17,137 train 000 8.661320e-02 -5.997677
2019-11-04 17:15:36,430 train 050 8.473989e-02 -24.628416
2019-11-04 17:15:55,619 train 100 8.473815e-02 -24.706940
2019-11-04 17:16:14,799 train 150 8.483516e-02 -24.662711
2019-11-04 17:16:34,003 train 200 8.498315e-02 -24.195477
2019-11-04 17:16:53,405 train 250 8.497990e-02 -24.308594
2019-11-04 17:17:12,622 train 300 8.499922e-02 -24.530176
2019-11-04 17:17:31,649 train 350 8.506853e-02 -24.558323
2019-11-04 17:17:50,791 train 400 8.494191e-02 -24.501601
2019-11-04 17:18:03,106 training loss; R2: 8.485808e-02 -24.604689
2019-11-04 17:18:04,077 valid 000 8.612054e-02 -15.437332
2019-11-04 17:18:21,981 validation loss; R2: 8.482933e-02 -24.535136
2019-11-04 17:18:22,010 epoch 20 lr 9.755283e-04
2019-11-04 17:18:23,136 train 000 8.634178e-02 -37.552051
2019-11-04 17:18:42,334 train 050 8.439639e-02 -24.581915
2019-11-04 17:19:01,445 train 100 8.473243e-02 -24.009602
2019-11-04 17:19:20,466 train 150 8.505954e-02 -25.291539
2019-11-04 17:19:39,720 train 200 8.502972e-02 -25.061189
2019-11-04 17:19:58,821 train 250 8.511446e-02 -24.957500
2019-11-04 17:20:17,746 train 300 8.502906e-02 -25.059079
2019-11-04 17:20:36,837 train 350 8.497499e-02 -24.731069
2019-11-04 17:20:56,161 train 400 8.483287e-02 -24.596960
2019-11-04 17:21:08,442 training loss; R2: 8.485808e-02 -24.609698
2019-11-04 17:21:09,445 valid 000 8.478851e-02 -29.355741
2019-11-04 17:21:27,369 validation loss; R2: 8.482933e-02 -25.194525
2019-11-04 17:21:27,397 epoch 21 lr 9.730427e-04
2019-11-04 17:21:28,456 train 000 8.302054e-02 -46.754462
2019-11-04 17:21:47,762 train 050 8.509564e-02 -24.725456
2019-11-04 17:22:07,031 train 100 8.525897e-02 -25.672456
2019-11-04 17:22:26,249 train 150 8.507940e-02 -25.727323
2019-11-04 17:22:45,404 train 200 8.504998e-02 -24.846612
2019-11-04 17:23:04,541 train 250 8.493161e-02 -24.187262
2019-11-04 17:23:23,651 train 300 8.491778e-02 -24.199993
2019-11-04 17:23:42,713 train 350 8.489514e-02 -24.212898
2019-11-04 17:24:01,887 train 400 8.488564e-02 -24.218607
2019-11-04 17:24:14,099 training loss; R2: 8.485808e-02 -24.060826
2019-11-04 17:24:15,128 valid 000 8.057077e-02 -26.541290
2019-11-04 17:24:33,146 validation loss; R2: 8.482933e-02 -24.602946
2019-11-04 17:24:33,181 epoch 22 lr 9.704404e-04
2019-11-04 17:24:34,338 train 000 8.347761e-02 -23.392995
2019-11-04 17:24:53,619 train 050 8.440854e-02 -26.638829
2019-11-04 17:25:12,774 train 100 8.454257e-02 -25.302239
2019-11-04 17:25:31,929 train 150 8.474091e-02 -24.911731
2019-11-04 17:25:51,135 train 200 8.503254e-02 -25.514453
2019-11-04 17:26:10,351 train 250 8.494629e-02 -25.067593
2019-11-04 17:26:29,653 train 300 8.494890e-02 -24.575491
2019-11-04 17:26:48,922 train 350 8.491739e-02 -24.871047
2019-11-04 17:27:08,153 train 400 8.486391e-02 -24.742966
2019-11-04 17:27:20,491 training loss; R2: 8.485808e-02 -24.579409
2019-11-04 17:27:21,517 valid 000 8.282159e-02 -29.437715
2019-11-04 17:27:39,694 validation loss; R2: 8.482933e-02 -24.971856
2019-11-04 17:27:39,729 epoch 23 lr 9.677220e-04
2019-11-04 17:27:40,825 train 000 8.497989e-02 -19.236229
2019-11-04 17:28:00,140 train 050 8.485341e-02 -25.508905
2019-11-04 17:28:19,258 train 100 8.490544e-02 -25.241455
2019-11-04 17:28:38,776 train 150 8.487683e-02 -25.439236
2019-11-04 17:28:57,851 train 200 8.468519e-02 -24.881904
2019-11-04 17:29:17,023 train 250 8.470132e-02 -25.157423
2019-11-04 17:29:36,150 train 300 8.473694e-02 -25.341178
2019-11-04 17:29:55,287 train 350 8.482135e-02 -25.114487
2019-11-04 17:30:14,707 train 400 8.489073e-02 -24.935858
2019-11-04 17:30:27,041 training loss; R2: 8.485854e-02 -24.770500
2019-11-04 17:30:28,054 valid 000 8.102314e-02 -41.654250
2019-11-04 17:30:46,000 validation loss; R2: 8.482933e-02 -25.953602
2019-11-04 17:30:46,027 epoch 24 lr 9.648882e-04
2019-11-04 17:30:47,209 train 000 8.675086e-02 -18.743086
2019-11-04 17:31:06,711 train 050 8.561712e-02 -24.712031
2019-11-04 17:31:26,040 train 100 8.477470e-02 -24.609290
2019-11-04 17:31:45,240 train 150 8.487909e-02 -23.936379
2019-11-04 17:32:04,532 train 200 8.492435e-02 -24.409742
2019-11-04 17:32:23,780 train 250 8.475079e-02 -24.052649
2019-11-04 17:32:42,833 train 300 8.467365e-02 -24.091664
2019-11-04 17:33:01,835 train 350 8.470900e-02 -24.135024
2019-11-04 17:33:20,923 train 400 8.482571e-02 -24.259389
2019-11-04 17:33:33,197 training loss; R2: 8.485808e-02 -24.530933
2019-11-04 17:33:34,181 valid 000 8.163099e-02 -33.905526
2019-11-04 17:33:52,183 validation loss; R2: 8.482933e-02 -25.599552
2019-11-04 17:33:52,209 epoch 25 lr 9.619398e-04
2019-11-04 17:33:53,262 train 000 8.748348e-02 -39.499374
2019-11-04 17:34:12,815 train 050 8.510437e-02 -26.038398
2019-11-04 17:34:32,082 train 100 8.481316e-02 -24.594920
2019-11-04 17:34:51,382 train 150 8.456194e-02 -24.055100
2019-11-04 17:35:10,582 train 200 8.470501e-02 -23.992220
2019-11-04 17:35:29,779 train 250 8.474132e-02 -23.856983
2019-11-04 17:35:48,947 train 300 8.472472e-02 -24.631374
2019-11-04 17:36:08,032 train 350 8.477553e-02 -24.638551
2019-11-04 17:36:27,136 train 400 8.482536e-02 -24.602677
2019-11-04 17:36:39,307 training loss; R2: 8.485808e-02 -24.651983
2019-11-04 17:36:40,333 valid 000 9.127821e-02 -33.668242
2019-11-04 17:36:58,453 validation loss; R2: 8.482933e-02 -24.898620
2019-11-04 17:36:58,483 epoch 26 lr 9.588773e-04
2019-11-04 17:36:59,658 train 000 9.267952e-02 -33.819955
2019-11-04 17:37:18,879 train 050 8.528397e-02 -29.085077
2019-11-04 17:37:37,816 train 100 8.505371e-02 -27.028683
2019-11-04 17:37:56,667 train 150 8.543221e-02 -26.158199
2019-11-04 17:38:15,417 train 200 8.895958e-02 -25.074118
2019-11-04 17:38:34,331 train 250 9.053365e-02 -24.750752
2019-11-04 17:38:53,112 train 300 9.021679e-02 -25.355050
2019-11-04 17:39:11,925 train 350 9.096026e-02 -25.057708
2019-11-04 17:39:30,726 train 400 9.112533e-02 -24.526007
2019-11-04 17:39:42,830 training loss; R2: 9.121980e-02 -24.118092
2019-11-04 17:39:43,864 valid 000 9.487426e-02 -30.416062
2019-11-04 17:40:01,796 validation loss; R2: 8.851536e-02 -24.193379
2019-11-04 17:40:01,826 epoch 27 lr 9.557016e-04
2019-11-04 17:40:02,971 train 000 9.323152e-02 -13.952902
2019-11-04 17:40:22,012 train 050 9.392266e-02 -22.187072
2019-11-04 17:40:41,397 train 100 9.396609e-02 -21.657818
2019-11-04 17:41:00,534 train 150 9.635543e-02 -21.038480
2019-11-04 17:41:19,566 train 200 9.771004e-02 -21.086116
2019-11-04 17:41:38,918 train 250 9.837736e-02 -21.034965
2019-11-04 17:41:57,943 train 300 1.005843e-01 -20.828396
2019-11-04 17:42:16,927 train 350 1.025230e-01 -20.285188
2019-11-04 17:42:36,344 train 400 1.059353e-01 -19.415394
2019-11-04 17:42:48,392 training loss; R2: 1.077240e-01 -18.976153
2019-11-04 17:42:49,386 valid 000 1.280044e-01 -13.003714
2019-11-04 17:43:07,170 validation loss; R2: 1.329640e-01 -12.421725
2019-11-04 17:43:07,195 epoch 28 lr 9.524135e-04
2019-11-04 17:43:08,283 train 000 1.331564e-01 -11.615999
2019-11-04 17:43:27,641 train 050 1.326547e-01 -12.300903
2019-11-04 17:43:46,930 train 100 1.326802e-01 -12.370561
2019-11-04 17:44:06,078 train 150 1.325308e-01 -12.098642
2019-11-04 17:44:25,236 train 200 1.323994e-01 -12.268695
2019-11-04 17:44:44,360 train 250 1.310960e-01 -12.456264
2019-11-04 17:45:03,519 train 300 1.283148e-01 -13.216767
2019-11-04 17:45:22,655 train 350 1.260210e-01 -13.764290
2019-11-04 17:45:41,750 train 400 1.237743e-01 -14.084160
2019-11-04 17:45:53,910 training loss; R2: 1.214877e-01 -14.462104
2019-11-04 17:45:54,950 valid 000 8.066598e-02 -13.994601
2019-11-04 17:46:12,635 validation loss; R2: 8.533634e-02 -24.463293
2019-11-04 17:46:12,667 epoch 29 lr 9.490138e-04
2019-11-04 17:46:13,803 train 000 9.052695e-02 -21.754692
2019-11-04 17:46:32,769 train 050 9.212373e-02 -21.919717
2019-11-04 17:46:51,759 train 100 9.195976e-02 -21.126495
2019-11-04 17:47:10,673 train 150 9.218736e-02 -20.796903
2019-11-04 17:47:29,648 train 200 9.232329e-02 -21.468852
2019-11-04 17:47:48,624 train 250 9.206164e-02 -21.662758
2019-11-04 17:48:07,592 train 300 9.212877e-02 -21.876187
2019-11-04 17:48:26,567 train 350 9.205713e-02 -21.945486
2019-11-04 17:48:45,542 train 400 9.209396e-02 -22.268650
2019-11-04 17:48:57,665 training loss; R2: 9.243806e-02 -22.072881
2019-11-04 17:48:58,593 valid 000 1.346413e-01 -14.962610
2019-11-04 17:49:16,567 validation loss; R2: 1.328135e-01 -12.285979
2019-11-04 17:49:16,597 epoch 30 lr 9.455033e-04
2019-11-04 17:49:17,834 train 000 9.765759e-02 -17.781483
2019-11-04 17:49:36,985 train 050 1.017722e-01 -19.697727
2019-11-04 17:49:56,058 train 100 1.018564e-01 -19.938184
2019-11-04 17:50:15,467 train 150 1.004808e-01 -20.262472
2019-11-04 17:50:34,791 train 200 1.015897e-01 -19.725571
2019-11-04 17:50:53,994 train 250 1.024460e-01 -19.528409
2019-11-04 17:51:13,095 train 300 1.022896e-01 -19.603799
2019-11-04 17:51:32,250 train 350 1.022233e-01 -19.623980
2019-11-04 17:51:51,380 train 400 1.022023e-01 -19.517749
2019-11-04 17:52:03,697 training loss; R2: 1.023596e-01 -19.611405
2019-11-04 17:52:04,731 valid 000 9.010352e-02 -19.374851
2019-11-04 17:52:22,544 validation loss; R2: 8.944805e-02 -23.032853
2019-11-04 17:52:22,573 epoch 31 lr 9.418828e-04
2019-11-04 17:52:23,692 train 000 9.691258e-02 -22.140805
2019-11-04 17:52:43,007 train 050 1.004939e-01 -20.091157
2019-11-04 17:53:02,057 train 100 1.014405e-01 -19.721728
2019-11-04 17:53:21,358 train 150 1.012280e-01 -19.580892
2019-11-04 17:53:40,659 train 200 1.007528e-01 -19.789408
2019-11-04 17:53:59,888 train 250 1.010134e-01 -19.952657
2019-11-04 17:54:18,953 train 300 1.001588e-01 -19.933469
2019-11-04 17:54:38,190 train 350 9.950152e-02 -19.979934
2019-11-04 17:54:57,630 train 400 9.912383e-02 -20.205362
2019-11-04 17:55:09,912 training loss; R2: 9.896406e-02 -20.216044
2019-11-04 17:55:10,938 valid 000 9.282452e-02 -31.814916
2019-11-04 17:55:28,646 validation loss; R2: 8.988832e-02 -22.370108
2019-11-04 17:55:28,675 epoch 32 lr 9.381533e-04
2019-11-04 17:55:29,783 train 000 9.340637e-02 -24.832962
2019-11-04 17:55:49,269 train 050 9.798280e-02 -19.750528
2019-11-04 17:56:08,515 train 100 9.812857e-02 -20.781114
2019-11-04 17:56:27,666 train 150 9.951247e-02 -20.346492
2019-11-04 17:56:46,700 train 200 1.010275e-01 -19.852798
2019-11-04 17:57:05,551 train 250 1.029025e-01 -19.177623
2019-11-04 17:57:24,365 train 300 1.024851e-01 -19.502856
2019-11-04 17:57:43,183 train 350 1.019469e-01 -19.756889
2019-11-04 17:58:02,034 train 400 1.011963e-01 -19.981504
2019-11-04 17:58:14,181 training loss; R2: 1.008009e-01 -19.865574
2019-11-04 17:58:15,180 valid 000 8.644378e-02 -11.669751
2019-11-04 17:58:32,734 validation loss; R2: 8.844806e-02 -24.700129
2019-11-04 17:58:32,764 epoch 33 lr 9.343158e-04
2019-11-04 17:58:33,880 train 000 1.001242e-01 -22.807733
2019-11-04 17:58:52,864 train 050 9.533781e-02 -21.379804
2019-11-04 17:59:12,117 train 100 9.354224e-02 -21.381517
2019-11-04 17:59:31,241 train 150 9.276722e-02 -22.164926
2019-11-04 17:59:50,283 train 200 9.191575e-02 -22.587681
2019-11-04 18:00:09,503 train 250 9.139215e-02 -22.505264
2019-11-04 18:00:28,589 train 300 9.096357e-02 -22.350267
2019-11-04 18:00:47,607 train 350 9.065459e-02 -22.493211
2019-11-04 18:01:06,953 train 400 9.032327e-02 -22.499324
2019-11-04 18:01:19,128 training loss; R2: 9.012842e-02 -22.518310
2019-11-04 18:01:20,156 valid 000 9.099738e-02 -32.116426
2019-11-04 18:01:38,107 validation loss; R2: 8.484287e-02 -24.690688
2019-11-04 18:01:38,135 epoch 34 lr 9.303710e-04
2019-11-04 18:01:39,297 train 000 8.292484e-02 -25.298908
2019-11-04 18:01:58,639 train 050 9.031813e-02 -22.349516
2019-11-04 18:02:17,839 train 100 9.182853e-02 -21.943126
2019-11-04 18:02:36,986 train 150 9.223444e-02 -22.297300
2019-11-04 18:02:56,136 train 200 9.318908e-02 -22.220350
2019-11-04 18:03:15,276 train 250 9.293239e-02 -21.838420
2019-11-04 18:03:34,362 train 300 9.245768e-02 -21.992238
2019-11-04 18:03:53,380 train 350 9.235932e-02 -22.047230
2019-11-04 18:04:12,372 train 400 9.244195e-02 -22.252244
2019-11-04 18:04:24,513 training loss; R2: 9.242025e-02 -22.107650
2019-11-04 18:04:25,502 valid 000 8.297403e-02 -12.864196
2019-11-04 18:04:43,565 validation loss; R2: 8.703513e-02 -24.368407
2019-11-04 18:04:43,590 epoch 35 lr 9.263201e-04
2019-11-04 18:04:44,750 train 000 9.201930e-02 -26.014309
2019-11-04 18:05:03,881 train 050 9.223868e-02 -21.633771
2019-11-04 18:05:23,018 train 100 9.329189e-02 -22.105248
2019-11-04 18:05:42,442 train 150 9.315510e-02 -21.950286
2019-11-04 18:06:01,600 train 200 9.267052e-02 -22.013212
2019-11-04 18:06:20,539 train 250 9.218469e-02 -22.002542
2019-11-04 18:06:39,701 train 300 9.192588e-02 -22.164160
2019-11-04 18:06:59,021 train 350 9.170015e-02 -22.244160
2019-11-04 18:07:18,130 train 400 9.154359e-02 -22.237115
2019-11-04 18:07:30,662 training loss; R2: 9.208588e-02 -22.201048
2019-11-04 18:07:31,708 valid 000 9.224531e-02 -23.636549
2019-11-04 18:07:49,842 validation loss; R2: 9.333941e-02 -22.367973
2019-11-04 18:07:49,871 epoch 36 lr 9.221640e-04
2019-11-04 18:07:50,995 train 000 1.053389e-01 -21.705516
2019-11-04 18:08:10,671 train 050 1.031077e-01 -19.593772
2019-11-04 18:08:30,305 train 100 1.052211e-01 -18.603304
2019-11-04 18:08:49,907 train 150 1.056196e-01 -18.328778
2019-11-04 18:09:09,526 train 200 1.057401e-01 -18.580897
2019-11-04 18:09:29,114 train 250 1.061737e-01 -18.574106
2019-11-04 18:09:48,666 train 300 1.059428e-01 -18.755859
2019-11-04 18:10:08,200 train 350 1.056647e-01 -18.598047
2019-11-04 18:10:27,777 train 400 1.061177e-01 -18.610441
2019-11-04 18:10:40,252 training loss; R2: 1.061559e-01 -18.534434
2019-11-04 18:10:41,258 valid 000 9.285638e-02 -18.403642
2019-11-04 18:10:59,121 validation loss; R2: 9.603027e-02 -20.989358
2019-11-04 18:10:59,150 epoch 37 lr 9.179037e-04
2019-11-04 18:11:00,264 train 000 1.023041e-01 -17.098672
2019-11-04 18:11:19,728 train 050 1.039085e-01 -19.675547
2019-11-04 18:11:39,121 train 100 1.025313e-01 -19.117686
2019-11-04 18:11:58,362 train 150 1.021603e-01 -19.160386
2019-11-04 18:12:17,355 train 200 1.017616e-01 -19.351690
2019-11-04 18:12:36,248 train 250 1.008584e-01 -19.777080
2019-11-04 18:12:55,136 train 300 1.003491e-01 -19.941538
2019-11-04 18:13:14,083 train 350 9.933354e-02 -20.055907
2019-11-04 18:13:33,030 train 400 9.843939e-02 -20.244754
2019-11-04 18:13:45,138 training loss; R2: 9.791490e-02 -20.392544
2019-11-04 18:13:46,158 valid 000 8.451208e-02 -6.586610
2019-11-04 18:14:03,778 validation loss; R2: 8.531767e-02 -23.714476
2019-11-04 18:14:03,807 epoch 38 lr 9.135403e-04
2019-11-04 18:14:04,917 train 000 9.363453e-02 -8.274888
2019-11-04 18:14:24,154 train 050 9.276522e-02 -21.958556
2019-11-04 18:14:43,131 train 100 9.235419e-02 -22.159888
2019-11-04 18:15:02,000 train 150 9.351211e-02 -21.774165
2019-11-04 18:15:20,808 train 200 9.401966e-02 -21.745514
2019-11-04 18:15:39,693 train 250 9.443538e-02 -21.424847
2019-11-04 18:15:58,608 train 300 9.468669e-02 -21.085143
2019-11-04 18:16:17,667 train 350 9.472759e-02 -21.404341
2019-11-04 18:16:36,685 train 400 9.461489e-02 -21.293948
2019-11-04 18:16:48,771 training loss; R2: 9.468840e-02 -21.433582
2019-11-04 18:16:49,764 valid 000 8.579586e-02 -11.589775
2019-11-04 18:17:07,848 validation loss; R2: 9.146355e-02 -23.937862
2019-11-04 18:17:07,880 epoch 39 lr 9.090749e-04
2019-11-04 18:17:08,987 train 000 9.449216e-02 -13.723845
2019-11-04 18:17:28,209 train 050 9.512929e-02 -20.440945
2019-11-04 18:17:47,264 train 100 9.543689e-02 -21.549391
2019-11-04 18:18:06,287 train 150 9.537085e-02 -21.640156
2019-11-04 18:18:25,417 train 200 9.472543e-02 -21.680434
2019-11-04 18:18:44,736 train 250 9.411398e-02 -21.581599
2019-11-04 18:19:04,090 train 300 9.358602e-02 -21.963381
2019-11-04 18:19:23,498 train 350 9.283001e-02 -22.103588
2019-11-04 18:19:42,500 train 400 9.213517e-02 -22.405834
2019-11-04 18:19:54,586 training loss; R2: 9.185064e-02 -22.527319
2019-11-04 18:19:55,554 valid 000 8.790234e-02 -41.102664
2019-11-04 18:20:13,392 validation loss; R2: 8.482933e-02 -24.950766
2019-11-04 18:20:13,426 epoch 40 lr 9.045085e-04
2019-11-04 18:20:14,532 train 000 8.581055e-02 -32.758730
2019-11-04 18:20:33,850 train 050 8.811742e-02 -22.550800
2019-11-04 18:20:53,017 train 100 8.782509e-02 -23.604105
2019-11-04 18:21:12,101 train 150 8.781210e-02 -23.755545
2019-11-04 18:21:31,128 train 200 8.789393e-02 -23.539829
2019-11-04 18:21:50,146 train 250 8.793007e-02 -23.923423
2019-11-04 18:22:09,173 train 300 8.799991e-02 -23.537854
2019-11-04 18:22:28,221 train 350 8.789153e-02 -23.177410
2019-11-04 18:22:47,276 train 400 8.780312e-02 -23.358573
2019-11-04 18:22:59,444 training loss; R2: 8.790165e-02 -23.465757
2019-11-04 18:23:00,466 valid 000 8.518609e-02 -7.493609
2019-11-04 18:23:18,120 validation loss; R2: 8.482933e-02 -23.985146
2019-11-04 18:23:18,147 epoch 41 lr 8.998423e-04
2019-11-04 18:23:19,252 train 000 9.083812e-02 -34.722581
2019-11-04 18:23:38,360 train 050 8.752080e-02 -22.086059
2019-11-04 18:23:57,507 train 100 8.795502e-02 -22.567234
2019-11-04 18:24:16,650 train 150 8.801245e-02 -22.939133
2019-11-04 18:24:35,995 train 200 8.813290e-02 -22.631172
2019-11-04 18:24:55,162 train 250 8.815091e-02 -22.891367
2019-11-04 18:25:14,218 train 300 8.800732e-02 -23.312255
2019-11-04 18:25:33,258 train 350 8.793993e-02 -23.823574
2019-11-04 18:25:52,339 train 400 8.782125e-02 -23.610670
2019-11-04 18:26:04,477 training loss; R2: 8.781080e-02 -23.572065
2019-11-04 18:26:05,464 valid 000 8.368032e-02 -34.893935
2019-11-04 18:26:23,611 validation loss; R2: 8.482933e-02 -23.926148
2019-11-04 18:26:23,649 epoch 42 lr 8.950775e-04
2019-11-04 18:26:24,812 train 000 8.915009e-02 -21.092645
2019-11-04 18:26:43,915 train 050 8.796180e-02 -23.970329
2019-11-04 18:27:02,916 train 100 8.791301e-02 -24.257693
2019-11-04 18:27:22,120 train 150 8.766476e-02 -23.695444
2019-11-04 18:27:41,352 train 200 8.775452e-02 -23.896614
2019-11-04 18:28:00,635 train 250 8.768435e-02 -24.063704
2019-11-04 18:28:19,684 train 300 8.752966e-02 -23.868523
2019-11-04 18:28:38,739 train 350 8.762669e-02 -24.065655
2019-11-04 18:28:57,979 train 400 8.765828e-02 -24.044532
2019-11-04 18:29:10,345 training loss; R2: 8.763532e-02 -23.946661
2019-11-04 18:29:11,370 valid 000 8.398654e-02 -9.035235
2019-11-04 18:29:29,462 validation loss; R2: 8.482933e-02 -24.457847
2019-11-04 18:29:29,493 epoch 43 lr 8.902152e-04
2019-11-04 18:29:30,634 train 000 9.272161e-02 -16.266877
2019-11-04 18:29:50,073 train 050 8.794050e-02 -24.933075
2019-11-04 18:30:09,231 train 100 8.779352e-02 -25.013713
2019-11-04 18:30:28,408 train 150 8.763661e-02 -24.502907
2019-11-04 18:30:47,616 train 200 8.768215e-02 -24.334148
2019-11-04 18:31:06,659 train 250 8.753356e-02 -24.312326
2019-11-04 18:31:25,690 train 300 8.753243e-02 -24.051954
2019-11-04 18:31:44,724 train 350 8.757056e-02 -23.966588
2019-11-04 18:32:03,798 train 400 8.763347e-02 -23.765922
2019-11-04 18:32:15,966 training loss; R2: 8.760122e-02 -23.897092
2019-11-04 18:32:16,967 valid 000 8.337311e-02 -29.398504
2019-11-04 18:32:34,734 validation loss; R2: 8.482933e-02 -24.947063
2019-11-04 18:32:34,764 epoch 44 lr 8.852566e-04
2019-11-04 18:32:35,877 train 000 8.148365e-02 -6.912335
2019-11-04 18:32:54,890 train 050 8.687227e-02 -22.972469
2019-11-04 18:33:14,089 train 100 8.710573e-02 -23.524679
2019-11-04 18:33:33,458 train 150 8.730348e-02 -23.790460
2019-11-04 18:33:52,489 train 200 8.739239e-02 -23.736543
2019-11-04 18:34:11,515 train 250 8.758879e-02 -23.933655
2019-11-04 18:34:30,799 train 300 8.750696e-02 -23.802163
2019-11-04 18:34:50,051 train 350 8.752724e-02 -23.813170
2019-11-04 18:35:09,129 train 400 8.762700e-02 -23.661516
2019-11-04 18:35:21,266 training loss; R2: 8.767083e-02 -23.690186
2019-11-04 18:35:22,310 valid 000 8.680020e-02 -21.874524
2019-11-04 18:35:40,222 validation loss; R2: 8.482933e-02 -23.982532
2019-11-04 18:35:40,253 epoch 45 lr 8.802030e-04
2019-11-04 18:35:41,404 train 000 8.876546e-02 -23.147558
2019-11-04 18:36:00,853 train 050 8.759672e-02 -23.321908
2019-11-04 18:36:20,160 train 100 8.769725e-02 -23.006746
2019-11-04 18:36:39,447 train 150 8.759254e-02 -23.261000
2019-11-04 18:36:58,679 train 200 8.772560e-02 -23.185496
2019-11-04 18:37:17,847 train 250 8.770432e-02 -23.417081
2019-11-04 18:37:36,924 train 300 8.781818e-02 -23.677892
2019-11-04 18:37:56,069 train 350 8.792264e-02 -23.728716
2019-11-04 18:38:15,178 train 400 8.786484e-02 -23.782966
2019-11-04 18:38:27,229 training loss; R2: 8.787651e-02 -23.700899
2019-11-04 18:38:28,268 valid 000 8.442975e-02 -19.405167
2019-11-04 18:38:45,971 validation loss; R2: 8.482933e-02 -24.502318
2019-11-04 18:38:46,001 epoch 46 lr 8.750555e-04
2019-11-04 18:38:47,154 train 000 8.940329e-02 -21.705294
2019-11-04 18:39:06,244 train 050 8.778708e-02 -23.436205
2019-11-04 18:39:25,466 train 100 8.769278e-02 -24.039096
