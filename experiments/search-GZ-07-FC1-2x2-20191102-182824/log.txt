2019-11-02 18:28:24,864 gpu device = 3
2019-11-02 18:28:24,864 args = Namespace(arch_learning_rate=0.01, arch_weight_decay=1e-06, batch_size=16, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.3, epochs=50, gpu=3, grad_clip=5, gz_regression=False, init_channels=16, layers=8, learning_rate=0.001, learning_rate_min=0.0001, model_path='saved_models', momentum=0.9, optimizer='Adam', report_freq=50, save='search-GZ_FC1_2x2-20191102-182824', seed=2, train_portion=0.8, unrolled=True, weight_decay=1e-08)
2019-11-02 18:28:28,396 param size = 2.229141MB
2019-11-02 18:28:28,409 epoch 0 lr 1.000000e-03
2019-11-02 18:28:28,410 genotype = Genotype(normal=[('sep_conv_5x5', 1), ('max_pool_3x3', 0), ('skip_connect', 2), ('sep_conv_3x3', 1), ('max_pool_3x3', 2), ('max_pool_3x3', 3), ('dil_conv_3x3', 4), ('skip_connect', 2)], normal_concat=range(2, 6), reduce=[('sep_conv_5x5', 1), ('dil_conv_5x5', 0), ('skip_connect', 2), ('skip_connect', 1), ('skip_connect', 1), ('avg_pool_3x3', 2), ('max_pool_2x2', 0), ('dil_conv_5x5', 2)], reduce_concat=range(2, 6))
2019-11-02 18:28:28,412 
alphas_normal = Variable containing:
 0.1112  0.1111  0.1111  0.1112  0.1112  0.1111  0.1111  0.1110  0.1111
 0.1110  0.1111  0.1110  0.1111  0.1112  0.1108  0.1114  0.1111  0.1112
 0.1109  0.1111  0.1111  0.1111  0.1111  0.1111  0.1112  0.1111  0.1112
 0.1112  0.1111  0.1109  0.1111  0.1112  0.1112  0.1112  0.1110  0.1111
 0.1110  0.1114  0.1108  0.1112  0.1112  0.1112  0.1112  0.1111  0.1110
 0.1110  0.1112  0.1113  0.1110  0.1111  0.1112  0.1110  0.1111  0.1111
 0.1110  0.1112  0.1112  0.1110  0.1111  0.1111  0.1111  0.1111  0.1111
 0.1109  0.1111  0.1111  0.1113  0.1112  0.1111  0.1112  0.1109  0.1111
 0.1111  0.1110  0.1110  0.1113  0.1111  0.1110  0.1111  0.1113  0.1111
 0.1113  0.1112  0.1110  0.1110  0.1111  0.1112  0.1111  0.1111  0.1111
 0.1111  0.1110  0.1112  0.1111  0.1112  0.1112  0.1111  0.1112  0.1109
 0.1111  0.1113  0.1112  0.1111  0.1111  0.1110  0.1112  0.1111  0.1110
 0.1111  0.1112  0.1111  0.1111  0.1110  0.1112  0.1111  0.1111  0.1111
 0.1110  0.1109  0.1112  0.1110  0.1111  0.1110  0.1112  0.1114  0.1112
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-02 18:28:28,414 
alphas_reduce = Variable containing:
 0.1111  0.1110  0.1112  0.1111  0.1111  0.1111  0.1111  0.1112  0.1112
 0.1110  0.1111  0.1111  0.1111  0.1111  0.1110  0.1113  0.1111  0.1111
 0.1110  0.1113  0.1112  0.1111  0.1109  0.1111  0.1112  0.1112  0.1112
 0.1109  0.1113  0.1111  0.1110  0.1112  0.1110  0.1111  0.1112  0.1112
 0.1111  0.1113  0.1109  0.1110  0.1111  0.1109  0.1112  0.1112  0.1112
 0.1112  0.1111  0.1109  0.1111  0.1111  0.1111  0.1112  0.1111  0.1111
 0.1112  0.1113  0.1110  0.1109  0.1113  0.1109  0.1112  0.1112  0.1112
 0.1111  0.1111  0.1111  0.1111  0.1113  0.1110  0.1111  0.1111  0.1112
 0.1111  0.1111  0.1111  0.1112  0.1111  0.1111  0.1112  0.1112  0.1111
 0.1112  0.1110  0.1114  0.1108  0.1112  0.1110  0.1113  0.1112  0.1109
 0.1111  0.1113  0.1111  0.1111  0.1111  0.1111  0.1112  0.1112  0.1109
 0.1112  0.1110  0.1110  0.1111  0.1110  0.1113  0.1111  0.1110  0.1113
 0.1111  0.1112  0.1111  0.1111  0.1110  0.1109  0.1113  0.1112  0.1112
 0.1112  0.1112  0.1111  0.1112  0.1110  0.1110  0.1110  0.1111  0.1111
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-02 18:28:36,949 train 000 4.056057e-02 -1.333567
2019-11-02 18:31:29,303 train 050 3.117585e-02 -3.452024
2019-11-02 18:34:19,633 train 100 2.681441e-02 -3.004700
2019-11-02 18:37:13,978 train 150 2.502074e-02 -2.527798
2019-11-02 18:40:09,642 train 200 2.387792e-02 -2.500854
2019-11-02 18:43:01,677 train 250 2.320252e-02 -2.331285
2019-11-02 18:46:02,287 train 300 2.265134e-02 -2.119237
2019-11-02 18:48:54,558 train 350 2.226758e-02 -1.995068
2019-11-02 18:51:44,592 train 400 2.196736e-02 -1.912853
2019-11-02 18:54:36,582 train 450 2.162114e-02 -1.865567
2019-11-02 18:57:28,070 train 500 2.137909e-02 -1.891741
2019-11-02 19:00:22,556 train 550 2.096081e-02 -1.910830
2019-11-02 19:03:15,391 train 600 2.061823e-02 -1.970339
2019-11-02 19:06:09,871 train 650 2.030589e-02 -1.966948
2019-11-02 19:09:02,989 train 700 2.006701e-02 -1.942485
2019-11-02 19:11:51,322 train 750 1.982047e-02 -1.887236
2019-11-02 19:14:39,118 train 800 1.957086e-02 -2.485726
2019-11-02 19:17:27,097 train 850 1.934049e-02 -2.406361
2019-11-02 19:20:15,008 train 900 1.918132e-02 -2.366648
2019-11-02 19:23:02,562 train 950 1.898896e-02 -2.301800
2019-11-02 19:25:51,535 train 1000 1.881541e-02 -3.106261
2019-11-02 19:28:39,804 train 1050 1.865264e-02 -3.021154
2019-11-02 19:31:27,459 train 1100 1.855038e-02 -2.953267
2019-11-02 19:34:14,367 train 1150 1.845376e-02 -2.897583
2019-11-02 19:37:02,337 train 1200 1.830642e-02 -2.885420
2019-11-02 19:39:50,010 train 1250 1.819404e-02 -2.879084
2019-11-02 19:42:37,560 train 1300 1.807827e-02 -2.827790
2019-11-02 19:45:25,822 train 1350 1.793217e-02 -2.767371
2019-11-02 19:48:13,944 train 1400 1.783540e-02 -2.726037
2019-11-02 19:51:00,783 train 1450 1.775511e-02 -2.701960
2019-11-02 19:53:49,106 train 1500 1.763274e-02 -2.693785
2019-11-02 19:56:36,907 train 1550 1.750339e-02 -2.651384
2019-11-02 19:59:24,286 train 1600 1.739705e-02 -2.605462
2019-11-02 20:02:14,520 train 1650 1.729712e-02 -2.558895
2019-11-02 20:05:06,887 train 1700 1.721489e-02 -2.524360
2019-11-02 20:08:00,652 train 1750 1.712763e-02 -2.481521
2019-11-02 20:10:53,231 train 1800 1.703073e-02 -2.443042
2019-11-02 20:13:46,476 train 1850 1.695902e-02 -2.436811
2019-11-02 20:16:44,363 train 1900 1.686549e-02 -2.483309
2019-11-02 20:19:43,166 train 1950 1.678896e-02 -2.446907
2019-11-02 20:22:36,182 train 2000 1.671581e-02 -2.429121
2019-11-02 20:25:28,173 train 2050 1.664304e-02 -2.396063
2019-11-02 20:28:26,416 train 2100 1.657656e-02 -2.576728
2019-11-02 20:31:25,859 train 2150 1.650559e-02 -2.541213
2019-11-02 20:34:19,638 train 2200 1.643313e-02 -2.511684
2019-11-02 20:37:13,641 train 2250 1.635586e-02 -2.478264
2019-11-02 20:40:11,208 train 2300 1.628467e-02 -4.713953
2019-11-02 20:43:03,095 train 2350 1.621910e-02 -4.638345
2019-11-02 20:45:59,517 train 2400 1.615061e-02 -4.567549
2019-11-02 20:48:53,533 train 2450 1.609754e-02 -4.503670
2019-11-02 20:51:48,366 train 2500 1.604469e-02 -4.466106
2019-11-02 20:54:45,220 train 2550 1.599168e-02 -4.406454
2019-11-02 20:57:40,497 train 2600 1.593401e-02 -4.351258
2019-11-02 21:00:36,228 train 2650 1.587057e-02 -4.291081
2019-11-02 21:03:34,829 train 2700 1.581811e-02 -4.231941
2019-11-02 21:06:32,474 train 2750 1.577026e-02 -4.188608
2019-11-02 21:09:29,873 train 2800 1.570506e-02 -4.140446
2019-11-02 21:12:23,859 train 2850 1.565457e-02 -4.084127
2019-11-02 21:15:19,027 train 2900 1.560852e-02 -4.045552
2019-11-02 21:18:18,143 train 2950 1.556273e-02 -3.996528
2019-11-02 21:21:15,511 train 3000 1.552016e-02 -3.952890
2019-11-02 21:24:10,208 train 3050 1.547886e-02 -3.921118
2019-11-02 21:25:53,901 training loss; R2: 1.545609e-02 -3.908004
2019-11-02 21:25:54,401 valid 000 1.164415e-02 -1.724140
2019-11-02 21:26:04,479 valid 050 1.215148e-02 -2.675074
2019-11-02 21:26:14,714 valid 100 1.202078e-02 -2.567249
2019-11-02 21:26:24,998 valid 150 1.213800e-02 -4.267094
2019-11-02 21:26:35,284 valid 200 1.224322e-02 -3.513152
2019-11-02 21:26:45,569 valid 250 1.237464e-02 -3.157396
2019-11-02 21:26:55,860 valid 300 1.240868e-02 -3.295361
2019-11-02 21:27:06,154 valid 350 1.244090e-02 -3.144303
2019-11-02 21:27:16,466 valid 400 1.247057e-02 -35.490834
2019-11-02 21:27:26,758 valid 450 1.247137e-02 -32.270788
2019-11-02 21:27:37,040 valid 500 1.251799e-02 -29.392787
2019-11-02 21:27:47,326 valid 550 1.252776e-02 -26.933953
2019-11-02 21:27:57,558 valid 600 1.251425e-02 -26.357897
2019-11-02 21:28:07,651 valid 650 1.250890e-02 -24.475055
2019-11-02 21:28:17,745 valid 700 1.251082e-02 -22.894968
2019-11-02 21:28:27,794 valid 750 1.248674e-02 -21.507147
2019-11-02 21:28:32,257 validation loss; R2: 1.251029e-02 -21.023342
2019-11-02 21:28:32,383 epoch 1 lr 9.991120e-04
2019-11-02 21:28:32,384 genotype = Genotype(normal=[('max_pool_3x3', 0), ('dil_conv_3x3', 1), ('max_pool_2x2', 0), ('dil_conv_5x5', 2), ('max_pool_2x2', 0), ('dil_conv_5x5', 2), ('max_pool_3x3', 0), ('dil_conv_5x5', 3)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 0), ('max_pool_3x3', 1), ('max_pool_3x3', 0), ('max_pool_3x3', 2), ('dil_conv_5x5', 2), ('dil_conv_5x5', 3), ('max_pool_3x3', 2), ('max_pool_3x3', 4)], reduce_concat=range(2, 6))
2019-11-02 21:28:32,386 
alphas_normal = Variable containing:
 0.1335  0.0849  0.1702  0.1818  0.0341  0.1135  0.0830  0.1350  0.0640
 0.4279  0.0891  0.0614  0.0381  0.0344  0.0824  0.0578  0.1058  0.1031
 0.0679  0.0941  0.2121  0.2006  0.0401  0.1315  0.0458  0.0967  0.1112
 0.4358  0.0871  0.1053  0.0578  0.0336  0.0944  0.0867  0.0429  0.0562
 0.4248  0.0668  0.0386  0.0318  0.0253  0.1045  0.0363  0.0797  0.1921
 0.1620  0.0673  0.2355  0.2204  0.0383  0.0823  0.0478  0.0909  0.0554
 0.2386  0.0711  0.1587  0.1187  0.0381  0.0737  0.0757  0.0845  0.1410
 0.4680  0.0617  0.0616  0.0605  0.0269  0.0393  0.0437  0.0535  0.1849
 0.3382  0.0606  0.0866  0.0979  0.0315  0.0890  0.0627  0.1160  0.1175
 0.1136  0.0611  0.2041  0.2794  0.0459  0.0912  0.0736  0.0687  0.0625
 0.4012  0.0429  0.0791  0.0786  0.0304  0.1415  0.0764  0.0513  0.0987
 0.4126  0.1187  0.1035  0.1027  0.0424  0.0593  0.0376  0.0456  0.0776
 0.2420  0.0549  0.0700  0.0789  0.0344  0.1165  0.0715  0.1137  0.2182
 0.4322  0.0578  0.0804  0.0939  0.0379  0.0546  0.0550  0.0945  0.0936
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-02 21:28:32,388 
alphas_reduce = Variable containing:
 0.0466  0.0338  0.1346  0.4752  0.0508  0.1272  0.0565  0.0283  0.0469
 0.1606  0.1524  0.1175  0.3213  0.0367  0.0627  0.0446  0.0519  0.0522
 0.0635  0.0335  0.1205  0.6042  0.0523  0.0291  0.0323  0.0323  0.0324
 0.1920  0.1744  0.1011  0.1348  0.0561  0.0695  0.0894  0.1133  0.0694
 0.0720  0.0593  0.1227  0.3812  0.0401  0.0407  0.0420  0.0457  0.1963
 0.1270  0.1151  0.1286  0.2208  0.0794  0.0749  0.0781  0.0689  0.1072
 0.1198  0.1968  0.1373  0.1939  0.0619  0.0911  0.0589  0.0995  0.0409
 0.0631  0.0546  0.1289  0.1379  0.0389  0.0353  0.0308  0.0443  0.4662
 0.0802  0.0638  0.1433  0.1471  0.0523  0.0562  0.0586  0.0403  0.3583
 0.1074  0.1165  0.1026  0.2204  0.0723  0.0850  0.0680  0.1111  0.1166
 0.1864  0.0986  0.0707  0.1293  0.0495  0.1092  0.0678  0.1990  0.0893
 0.0560  0.0429  0.0734  0.6022  0.0354  0.0345  0.0355  0.0410  0.0791
 0.0728  0.0499  0.1015  0.3283  0.0484  0.0549  0.0352  0.0575  0.2515
 0.0870  0.0465  0.0967  0.3915  0.0518  0.1571  0.0495  0.0493  0.0708
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-02 21:28:36,995 train 000 1.238621e-02 -0.884040
2019-11-02 21:31:30,511 train 050 1.350294e-02 -1.459292
2019-11-02 21:34:28,179 train 100 1.335398e-02 -1.966251
2019-11-02 21:37:22,315 train 150 1.359753e-02 -1.796680
2019-11-02 21:40:21,545 train 200 1.329049e-02 -1.727051
2019-11-02 21:43:15,029 train 250 1.300185e-02 -1.592797
2019-11-02 21:46:06,269 train 300 1.296889e-02 -1.553183
2019-11-02 21:48:56,629 train 350 1.286630e-02 -1.570157
2019-11-02 21:51:51,417 train 400 1.290741e-02 -1.529144
2019-11-02 21:54:53,689 train 450 1.289181e-02 -1.522513
2019-11-02 21:57:48,052 train 500 1.284235e-02 -1.564886
2019-11-02 22:00:44,077 train 550 1.275985e-02 -1.591491
2019-11-02 22:03:40,973 train 600 1.268934e-02 -1.531868
2019-11-02 22:06:36,680 train 650 1.264410e-02 -1.530100
2019-11-02 22:09:29,887 train 700 1.260333e-02 -1.516985
2019-11-02 22:12:22,104 train 750 1.260673e-02 -1.514447
2019-11-02 22:15:16,532 train 800 1.259773e-02 -1.743040
2019-11-02 22:18:08,948 train 850 1.260928e-02 -1.730466
2019-11-02 22:21:05,959 train 900 1.262834e-02 -1.757968
2019-11-02 22:23:59,994 train 950 1.262351e-02 -1.772478
2019-11-02 22:26:59,966 train 1000 1.260728e-02 -1.767564
2019-11-02 22:29:55,474 train 1050 1.258892e-02 -1.733178
2019-11-02 22:32:51,066 train 1100 1.258447e-02 -1.722555
2019-11-02 22:35:49,415 train 1150 1.260557e-02 -1.849815
2019-11-02 22:38:46,227 train 1200 1.258280e-02 -1.826912
2019-11-02 22:41:42,691 train 1250 1.259277e-02 -1.831451
2019-11-02 22:44:44,661 train 1300 1.259964e-02 -1.860657
2019-11-02 22:47:40,204 train 1350 1.257899e-02 -2.936016
2019-11-02 22:50:34,510 train 1400 1.255625e-02 -2.872280
2019-11-02 22:53:36,958 train 1450 1.252800e-02 -2.865234
2019-11-02 22:56:31,046 train 1500 1.252313e-02 -2.812887
2019-11-02 22:59:23,138 train 1550 1.251521e-02 -2.774383
2019-11-02 23:02:16,383 train 1600 1.250498e-02 -2.731521
2019-11-02 23:05:10,249 train 1650 1.250574e-02 -2.704459
2019-11-02 23:08:02,133 train 1700 1.249894e-02 -2.663385
2019-11-02 23:10:53,997 train 1750 1.249465e-02 -2.625244
2019-11-02 23:13:49,774 train 1800 1.247364e-02 -2.600153
2019-11-02 23:16:48,139 train 1850 1.246755e-02 -2.567506
2019-11-02 23:19:41,527 train 1900 1.244886e-02 -2.562436
2019-11-02 23:22:36,697 train 1950 1.244149e-02 -2.522664
2019-11-02 23:25:33,536 train 2000 1.242480e-02 -2.600677
2019-11-02 23:28:30,245 train 2050 1.241906e-02 -2.604260
2019-11-02 23:31:28,627 train 2100 1.241014e-02 -2.582766
2019-11-02 23:34:22,854 train 2150 1.239039e-02 -2.557305
2019-11-02 23:37:21,229 train 2200 1.238099e-02 -2.553520
2019-11-02 23:40:14,975 train 2250 1.237512e-02 -2.528944
2019-11-02 23:43:07,534 train 2300 1.235673e-02 -2.496949
2019-11-02 23:46:02,716 train 2350 1.234977e-02 -2.475108
2019-11-02 23:48:55,781 train 2400 1.235675e-02 -2.453643
2019-11-02 23:51:51,486 train 2450 1.234492e-02 -2.435876
2019-11-02 23:54:47,476 train 2500 1.233161e-02 -2.431867
2019-11-02 23:57:45,382 train 2550 1.231514e-02 -2.461301
2019-11-03 00:00:40,080 train 2600 1.231066e-02 -2.439466
2019-11-03 00:03:35,737 train 2650 1.230679e-02 -2.548241
2019-11-03 00:06:35,208 train 2700 1.229482e-02 -2.516615
2019-11-03 00:09:37,376 train 2750 1.228343e-02 -2.492873
2019-11-03 00:12:34,816 train 2800 1.226926e-02 -2.477198
2019-11-03 00:15:28,830 train 2850 1.225563e-02 -2.492795
2019-11-03 00:18:22,731 train 2900 1.224975e-02 -2.491562
2019-11-03 00:21:16,174 train 2950 1.223667e-02 -2.477491
2019-11-03 00:24:09,465 train 3000 1.221840e-02 -2.451518
2019-11-03 00:27:05,842 train 3050 1.221703e-02 -2.433572
2019-11-03 00:28:43,801 training loss; R2: 1.220276e-02 -2.424201
2019-11-03 00:28:44,295 valid 000 1.398387e-02 -0.352149
2019-11-03 00:28:54,318 valid 050 1.098982e-02 -3.089106
2019-11-03 00:29:04,339 valid 100 1.079348e-02 -2.005043
2019-11-03 00:29:14,360 valid 150 1.079039e-02 -1.702933
2019-11-03 00:29:24,412 valid 200 1.073855e-02 -1.551056
2019-11-03 00:29:34,459 valid 250 1.079258e-02 -1.652705
2019-11-03 00:29:44,523 valid 300 1.078966e-02 -1.501176
2019-11-03 00:29:54,505 valid 350 1.083819e-02 -1.490085
2019-11-03 00:30:04,510 valid 400 1.078147e-02 -1.761765
2019-11-03 00:30:14,516 valid 450 1.076738e-02 -1.681365
2019-11-03 00:30:24,546 valid 500 1.078957e-02 -1.604491
2019-11-03 00:30:34,598 valid 550 1.080741e-02 -1.562023
2019-11-03 00:30:44,634 valid 600 1.083575e-02 -1.551056
2019-11-03 00:30:54,665 valid 650 1.082975e-02 -1.512162
2019-11-03 00:31:04,721 valid 700 1.082874e-02 -1.500885
2019-11-03 00:31:14,738 valid 750 1.083491e-02 -1.456111
2019-11-03 00:31:18,535 validation loss; R2: 1.082409e-02 -1.447469
2019-11-03 00:31:18,667 epoch 2 lr 9.964516e-04
2019-11-03 00:31:18,668 genotype = Genotype(normal=[('max_pool_2x2', 0), ('dil_conv_5x5', 1), ('max_pool_2x2', 0), ('dil_conv_5x5', 2), ('dil_conv_5x5', 3), ('max_pool_2x2', 0), ('max_pool_3x3', 0), ('dil_conv_3x3', 3)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 0), ('max_pool_3x3', 1), ('max_pool_3x3', 0), ('max_pool_3x3', 2), ('dil_conv_5x5', 2), ('dil_conv_5x5', 3), ('max_pool_3x3', 2), ('max_pool_3x3', 4)], reduce_concat=range(2, 6))
2019-11-03 00:31:18,670 
alphas_normal = Variable containing:
 0.2205  0.0789  0.2030  0.1465  0.0237  0.0695  0.0830  0.1103  0.0645
 0.7252  0.0361  0.0267  0.0163  0.0159  0.0409  0.0201  0.0549  0.0640
 0.0964  0.0951  0.2617  0.2044  0.0331  0.0763  0.0377  0.0829  0.1123
 0.6271  0.0565  0.0631  0.0370  0.0217  0.0392  0.0653  0.0379  0.0521
 0.6154  0.0365  0.0199  0.0149  0.0129  0.0634  0.0209  0.0640  0.1520
 0.1998  0.0677  0.1751  0.1512  0.0345  0.1164  0.0563  0.1056  0.0932
 0.3962  0.0549  0.1185  0.1205  0.0322  0.0547  0.0416  0.0858  0.0957
 0.6809  0.0386  0.0308  0.0287  0.0183  0.0176  0.0209  0.0344  0.1297
 0.4931  0.0280  0.0287  0.0322  0.0168  0.0305  0.0316  0.1455  0.1936
 0.1665  0.0645  0.1420  0.2018  0.0423  0.1077  0.0648  0.1210  0.0893
 0.4885  0.0559  0.0776  0.1042  0.0373  0.0897  0.0435  0.0216  0.0817
 0.5654  0.0830  0.0719  0.0558  0.0298  0.0586  0.0198  0.0389  0.0768
 0.5565  0.0309  0.0292  0.0280  0.0178  0.1042  0.0402  0.1188  0.0744
 0.6645  0.0386  0.0302  0.0288  0.0197  0.0287  0.0217  0.1124  0.0553
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-03 00:31:18,671 
alphas_reduce = Variable containing:
 0.0601  0.0248  0.2065  0.4716  0.0482  0.0852  0.0414  0.0266  0.0357
 0.1473  0.0849  0.1363  0.3929  0.0538  0.0645  0.0335  0.0459  0.0409
 0.0733  0.0348  0.1145  0.6202  0.0571  0.0284  0.0224  0.0294  0.0198
 0.2491  0.1457  0.0912  0.1532  0.0687  0.0664  0.1057  0.0805  0.0396
 0.0560  0.0435  0.0741  0.3991  0.0363  0.0291  0.0398  0.0255  0.2967
 0.2354  0.0637  0.1528  0.2715  0.0713  0.0734  0.0485  0.0385  0.0450
 0.0954  0.1675  0.1160  0.2518  0.0701  0.0953  0.0516  0.1081  0.0443
 0.0322  0.0275  0.0513  0.0610  0.0242  0.0207  0.0264  0.0221  0.7345
 0.0604  0.0411  0.0895  0.1095  0.0367  0.0400  0.0320  0.0210  0.5698
 0.1212  0.1392  0.0601  0.1340  0.0460  0.2455  0.0807  0.0947  0.0785
 0.2418  0.1039  0.0722  0.1707  0.0641  0.0876  0.0457  0.1324  0.0817
 0.0358  0.0277  0.0348  0.7560  0.0267  0.0255  0.0217  0.0220  0.0498
 0.0687  0.0407  0.0639  0.3243  0.0372  0.0448  0.0356  0.0280  0.3567
 0.0484  0.0535  0.0529  0.6440  0.0476  0.0468  0.0356  0.0314  0.0398
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-03 00:31:22,327 train 000 8.409988e-03 -1.823850
2019-11-03 00:34:17,691 train 050 1.143842e-02 -1.562877
2019-11-03 00:37:09,518 train 100 1.125505e-02 -1.671228
2019-11-03 00:40:06,097 train 150 1.139399e-02 -2.407227
2019-11-03 00:43:01,855 train 200 1.138405e-02 -2.311249
2019-11-03 00:45:55,869 train 250 1.157453e-02 -2.127373
2019-11-03 00:48:55,761 train 300 1.164457e-02 -2.156866
2019-11-03 00:51:53,684 train 350 1.157757e-02 -2.043424
2019-11-03 00:54:50,452 train 400 1.165829e-02 -1.963469
2019-11-03 00:57:43,257 train 450 1.169033e-02 -2.341345
2019-11-03 01:00:37,393 train 500 1.168708e-02 -2.226333
2019-11-03 01:03:34,072 train 550 1.165022e-02 -2.152680
2019-11-03 01:06:26,920 train 600 1.169368e-02 -2.079639
2019-11-03 01:09:23,045 train 650 1.166489e-02 -2.080583
2019-11-03 01:12:15,882 train 700 1.164908e-02 -2.007315
2019-11-03 01:15:07,885 train 750 1.169950e-02 -1.952140
2019-11-03 01:18:01,920 train 800 1.173714e-02 -1.923963
2019-11-03 01:20:58,730 train 850 1.173842e-02 -1.907005
2019-11-03 01:23:53,815 train 900 1.172030e-02 -1.921557
2019-11-03 01:26:47,329 train 950 1.171397e-02 -1.892164
2019-11-03 01:29:39,386 train 1000 1.173199e-02 -1.891847
2019-11-03 01:32:50,062 train 1050 1.176291e-02 -1.850918
2019-11-03 01:35:44,262 train 1100 1.176051e-02 -1.821356
2019-11-03 01:38:38,500 train 1150 1.174822e-02 -1.924562
2019-11-03 01:41:36,906 train 1200 1.175274e-02 -1.897539
2019-11-03 01:44:35,634 train 1250 1.175011e-02 -1.871831
2019-11-03 01:47:32,148 train 1300 1.175710e-02 -1.840775
2019-11-03 01:50:25,446 train 1350 1.173864e-02 -1.830906
2019-11-03 01:53:19,000 train 1400 1.174205e-02 -1.832284
2019-11-03 01:56:15,658 train 1450 1.172328e-02 -1.812884
2019-11-03 01:59:07,837 train 1500 1.172764e-02 -2.568512
2019-11-03 01:02:02,486 train 1550 1.169075e-02 -2.522840
2019-11-03 01:05:01,817 train 1600 1.170945e-02 -2.477843
2019-11-03 01:07:58,955 train 1650 1.169486e-02 -2.431458
2019-11-03 01:10:53,999 train 1700 1.166728e-02 -2.766718
2019-11-03 01:13:48,643 train 1750 1.168102e-02 -2.762769
2019-11-03 01:16:45,389 train 1800 1.165022e-02 -2.713567
2019-11-03 01:19:40,585 train 1850 1.161999e-02 -2.684042
2019-11-03 01:22:35,543 train 1900 1.159080e-02 -2.650696
2019-11-03 01:25:29,664 train 1950 1.156921e-02 -2.787264
2019-11-03 01:28:26,654 train 2000 1.155786e-02 -2.745207
2019-11-03 01:31:22,917 train 2050 1.155462e-02 -2.707270
2019-11-03 01:34:23,685 train 2100 1.154573e-02 -2.671526
2019-11-03 01:37:17,416 train 2150 1.153913e-02 -2.644080
2019-11-03 01:40:14,024 train 2200 1.153119e-02 -2.656583
2019-11-03 01:43:07,208 train 2250 1.153139e-02 -2.636410
2019-11-03 01:46:00,267 train 2300 1.152913e-02 -2.613720
2019-11-03 01:48:56,229 train 2350 1.152570e-02 -2.584015
2019-11-03 01:51:54,941 train 2400 1.152478e-02 -2.610078
2019-11-03 01:54:52,912 train 2450 1.152660e-02 -5.543376
2019-11-03 01:57:48,876 train 2500 1.151719e-02 -5.519781
2019-11-03 02:00:42,217 train 2550 1.151915e-02 -5.460921
2019-11-03 02:03:36,326 train 2600 1.151898e-02 -5.407443
2019-11-03 02:06:28,060 train 2650 1.150463e-02 -5.351956
2019-11-03 02:09:22,422 train 2700 1.150020e-02 -5.275216
2019-11-03 02:12:19,676 train 2750 1.148946e-02 -5.202786
2019-11-03 02:15:16,723 train 2800 1.147680e-02 -5.127189
2019-11-03 02:18:16,725 train 2850 1.147265e-02 -5.063777
2019-11-03 02:21:12,627 train 2900 1.146383e-02 -5.008387
2019-11-03 02:24:05,394 train 2950 1.147021e-02 -4.939174
2019-11-03 02:26:59,823 train 3000 1.146649e-02 -4.872207
2019-11-03 02:29:55,783 train 3050 1.145922e-02 -4.811334
2019-11-03 02:31:33,636 training loss; R2: 1.146108e-02 -4.783256
2019-11-03 02:31:34,163 valid 000 6.621017e-03 -0.856765
2019-11-03 02:31:44,271 valid 050 1.029788e-02 -1.251995
2019-11-03 02:31:54,356 valid 100 1.029623e-02 -1.069970
2019-11-03 02:32:04,432 valid 150 1.033005e-02 -6.107101
2019-11-03 02:32:14,523 valid 200 1.035656e-02 -4.890055
2019-11-03 02:32:24,635 valid 250 1.041250e-02 -4.130355
2019-11-03 02:32:34,735 valid 300 1.040734e-02 -3.633391
2019-11-03 02:32:44,802 valid 350 1.039523e-02 -3.253272
2019-11-03 02:32:54,896 valid 400 1.042035e-02 -2.979818
2019-11-03 02:33:04,962 valid 450 1.037956e-02 -2.738682
2019-11-03 02:33:15,053 valid 500 1.035201e-02 -2.596962
2019-11-03 02:33:25,199 valid 550 1.034721e-02 -2.552096
2019-11-03 02:33:35,329 valid 600 1.035102e-02 -2.408197
2019-11-03 02:33:45,468 valid 650 1.035300e-02 -2.289751
2019-11-03 02:33:55,585 valid 700 1.039590e-02 -2.180319
2019-11-03 02:34:05,691 valid 750 1.039641e-02 -2.090512
2019-11-03 02:34:09,519 validation loss; R2: 1.040572e-02 -2.073800
2019-11-03 02:34:09,722 epoch 3 lr 9.920293e-04
2019-11-03 02:34:09,723 genotype = Genotype(normal=[('max_pool_3x3', 0), ('dil_conv_5x5', 1), ('max_pool_3x3', 0), ('dil_conv_5x5', 2), ('dil_conv_5x5', 3), ('max_pool_3x3', 0), ('max_pool_3x3', 0), ('dil_conv_3x3', 4)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 0), ('max_pool_3x3', 1), ('max_pool_3x3', 0), ('max_pool_3x3', 2), ('dil_conv_5x5', 3), ('dil_conv_5x5', 2), ('max_pool_3x3', 2), ('max_pool_3x3', 4)], reduce_concat=range(2, 6))
2019-11-03 02:34:09,725 
alphas_normal = Variable containing:
 0.1558  0.0573  0.1983  0.2914  0.0187  0.0669  0.0515  0.0949  0.0652
 0.7059  0.0486  0.0267  0.0151  0.0259  0.0443  0.0235  0.0429  0.0670
 0.0517  0.0713  0.3060  0.3455  0.0259  0.0584  0.0253  0.0441  0.0717
 0.6771  0.0786  0.0612  0.0254  0.0290  0.0322  0.0471  0.0217  0.0277
 0.5141  0.0408  0.0239  0.0169  0.0208  0.1089  0.0280  0.0640  0.1825
 0.1124  0.0469  0.1568  0.2473  0.0281  0.1222  0.0806  0.1036  0.1020
 0.4410  0.0392  0.1066  0.0922  0.0278  0.0448  0.0382  0.1038  0.1063
 0.5496  0.0564  0.0735  0.0522  0.0387  0.0161  0.0335  0.0326  0.1473
 0.3688  0.0437  0.0503  0.0433  0.0315  0.0368  0.0472  0.1196  0.2587
 0.0986  0.0344  0.1015  0.3927  0.0280  0.0985  0.0658  0.0990  0.0815
 0.5373  0.0391  0.0683  0.0665  0.0314  0.0658  0.0414  0.0223  0.1280
 0.3462  0.0779  0.1943  0.1237  0.0490  0.0495  0.0266  0.0248  0.1081
 0.2894  0.0625  0.0826  0.0700  0.0437  0.1857  0.0753  0.0819  0.1089
 0.4235  0.0804  0.0807  0.0681  0.0494  0.0191  0.0195  0.2147  0.0447
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-03 02:34:09,727 
alphas_reduce = Variable containing:
 0.0516  0.0204  0.2023  0.5471  0.0763  0.0558  0.0177  0.0158  0.0130
 0.1350  0.1179  0.0982  0.2944  0.0258  0.1468  0.0437  0.0341  0.1041
 0.0587  0.0191  0.1075  0.7125  0.0488  0.0151  0.0107  0.0174  0.0102
 0.5001  0.0590  0.0605  0.0790  0.0368  0.0546  0.1237  0.0356  0.0509
 0.0402  0.0283  0.0616  0.6231  0.0237  0.0237  0.0300  0.0175  0.1519
 0.0996  0.0289  0.2676  0.3727  0.1161  0.0292  0.0332  0.0245  0.0283
 0.1326  0.1292  0.1657  0.3037  0.0623  0.0644  0.0470  0.0581  0.0370
 0.0627  0.0666  0.0828  0.0960  0.0366  0.0219  0.0143  0.0182  0.6010
 0.0332  0.0382  0.0543  0.0622  0.0291  0.0201  0.0175  0.0122  0.7331
 0.1445  0.0361  0.0848  0.1165  0.0666  0.3471  0.0565  0.1070  0.0409
 0.2940  0.0768  0.1014  0.2014  0.0669  0.0615  0.0745  0.0718  0.0518
 0.0173  0.0171  0.0245  0.8749  0.0147  0.0132  0.0115  0.0125  0.0144
 0.0438  0.0386  0.0734  0.5622  0.0304  0.0247  0.0194  0.0275  0.1800
 0.0414  0.0218  0.0473  0.7557  0.0222  0.0367  0.0219  0.0243  0.0288
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-03 02:34:13,394 train 000 1.338880e-02 -0.012038
2019-11-03 02:37:14,892 train 050 1.094666e-02 -1.208005
2019-11-03 02:40:10,685 train 100 1.096510e-02 -1.295657
2019-11-03 02:43:05,830 train 150 1.094124e-02 -1.370594
2019-11-03 02:46:07,588 train 200 1.098852e-02 -1.364807
2019-11-03 02:49:03,675 train 250 1.110209e-02 -1.437091
2019-11-03 02:51:57,295 train 300 1.115904e-02 -2.041783
2019-11-03 02:54:51,117 train 350 1.116737e-02 -1.953794
2019-11-03 02:57:47,062 train 400 1.117599e-02 -1.850801
2019-11-03 03:00:40,264 train 450 1.120320e-02 -1.786828
2019-11-03 03:03:33,891 train 500 1.124910e-02 -1.776974
2019-11-03 03:06:27,136 train 550 1.120609e-02 -1.699842
2019-11-03 03:09:26,749 train 600 1.118219e-02 -1.703604
2019-11-03 03:12:18,888 train 650 1.122713e-02 -1.633590
2019-11-03 03:15:17,404 train 700 1.122015e-02 -1.649277
2019-11-03 03:18:15,141 train 750 1.121100e-02 -1.712906
2019-11-03 03:21:13,151 train 800 1.120852e-02 -1.689699
2019-11-03 03:24:12,053 train 850 1.119318e-02 -1.660506
2019-11-03 03:27:10,938 train 900 1.116797e-02 -1.672751
2019-11-03 03:30:06,892 train 950 1.118128e-02 -1.677787
2019-11-03 03:33:01,540 train 1000 1.117026e-02 -1.661301
2019-11-03 03:36:00,977 train 1050 1.116216e-02 -1.648502
2019-11-03 03:38:59,870 train 1100 1.116803e-02 -1.623041
2019-11-03 03:41:51,934 train 1150 1.116302e-02 -1.603002
2019-11-03 03:44:46,114 train 1200 1.114636e-02 -2.304866
2019-11-03 03:47:40,660 train 1250 1.114981e-02 -2.262610
2019-11-03 03:50:33,478 train 1300 1.113713e-02 -2.215414
2019-11-03 03:53:29,647 train 1350 1.114176e-02 -2.191021
2019-11-03 03:56:27,060 train 1400 1.114172e-02 -2.206347
2019-11-03 03:59:20,838 train 1450 1.111463e-02 -2.176235
2019-11-03 04:02:15,429 train 1500 1.115445e-02 -3.317891
2019-11-03 04:05:10,596 train 1550 1.114489e-02 -3.286268
2019-11-03 04:08:05,579 train 1600 1.114298e-02 -3.299193
2019-11-03 04:11:03,989 train 1650 1.113096e-02 -3.235189
2019-11-03 04:14:00,353 train 1700 1.111304e-02 -3.180128
2019-11-03 04:16:55,636 train 1750 1.111462e-02 -3.121645
2019-11-03 04:19:50,676 train 1800 1.110803e-02 -3.063598
2019-11-03 04:22:46,324 train 1850 1.111162e-02 -3.294940
2019-11-03 04:25:39,719 train 1900 1.111260e-02 -3.249997
2019-11-03 04:28:33,948 train 1950 1.111495e-02 -3.211726
2019-11-03 04:31:29,260 train 2000 1.111666e-02 -3.157489
2019-11-03 04:34:24,777 train 2050 1.109537e-02 -3.110356
2019-11-03 04:37:18,526 train 2100 1.108731e-02 -3.181731
2019-11-03 04:40:14,318 train 2150 1.109501e-02 -3.221039
2019-11-03 04:43:09,952 train 2200 1.109637e-02 -3.181880
2019-11-03 04:46:03,388 train 2250 1.110751e-02 -3.150130
2019-11-03 04:49:00,493 train 2300 1.110057e-02 -3.267721
2019-11-03 04:51:54,059 train 2350 1.109835e-02 -3.284503
2019-11-03 04:54:51,945 train 2400 1.109409e-02 -3.245722
2019-11-03 04:57:44,929 train 2450 1.109879e-02 -3.205528
2019-11-03 05:00:39,367 train 2500 1.107610e-02 -3.164105
2019-11-03 05:03:32,765 train 2550 1.106672e-02 -3.148583
2019-11-03 05:06:27,916 train 2600 1.105804e-02 -3.113086
2019-11-03 05:09:27,532 train 2650 1.105278e-02 -3.262100
2019-11-03 05:12:28,484 train 2700 1.104430e-02 -3.223539
2019-11-03 05:15:22,491 train 2750 1.103322e-02 -3.196208
2019-11-03 05:18:19,707 train 2800 1.103831e-02 -3.174314
2019-11-03 05:21:11,962 train 2850 1.104077e-02 -3.141095
2019-11-03 05:24:03,649 train 2900 1.103397e-02 -3.139067
2019-11-03 05:27:02,543 train 2950 1.102892e-02 -3.107547
2019-11-03 05:29:58,629 train 3000 1.102315e-02 -3.075179
2019-11-03 05:32:58,406 train 3050 1.101597e-02 -3.151523
2019-11-03 05:34:35,399 training loss; R2: 1.101174e-02 -3.139267
2019-11-03 05:34:35,869 valid 000 8.692196e-03 -2.670386
2019-11-03 05:34:45,924 valid 050 1.039508e-02 -1.152177
2019-11-03 05:34:55,984 valid 100 1.023181e-02 -1.829841
2019-11-03 05:35:05,990 valid 150 1.020590e-02 -1.729178
2019-11-03 05:35:15,987 valid 200 1.013639e-02 -1.584389
2019-11-03 05:35:25,955 valid 250 1.016541e-02 -1.557077
2019-11-03 05:35:35,921 valid 300 1.008879e-02 -1.566954
2019-11-03 05:35:45,910 valid 350 1.002790e-02 -1.499811
2019-11-03 05:35:55,901 valid 400 1.002782e-02 -1.439478
2019-11-03 05:36:05,875 valid 450 1.001264e-02 -1.407884
2019-11-03 05:36:15,886 valid 500 9.982381e-03 -1.365938
2019-11-03 05:36:25,907 valid 550 1.002094e-02 -1.360659
2019-11-03 05:36:35,930 valid 600 9.990504e-03 -1.485689
2019-11-03 05:36:45,890 valid 650 9.984319e-03 -1.463876
2019-11-03 05:36:55,861 valid 700 9.975845e-03 -1.508460
2019-11-03 05:37:05,828 valid 750 9.974645e-03 -1.493795
2019-11-03 05:37:09,586 validation loss; R2: 9.965691e-03 -1.502316
2019-11-03 05:37:09,772 epoch 4 lr 9.858624e-04
2019-11-03 05:37:09,773 genotype = Genotype(normal=[('max_pool_3x3', 0), ('dil_conv_5x5', 1), ('max_pool_3x3', 0), ('dil_conv_5x5', 2), ('dil_conv_5x5', 3), ('max_pool_3x3', 0), ('max_pool_3x3', 0), ('sep_conv_3x3', 3)], normal_concat=range(2, 6), reduce=[('max_pool_2x2', 0), ('max_pool_3x3', 1), ('max_pool_3x3', 0), ('max_pool_3x3', 2), ('dil_conv_5x5', 3), ('dil_conv_5x5', 2), ('max_pool_3x3', 2), ('max_pool_3x3', 4)], reduce_concat=range(2, 6))
2019-11-03 05:37:09,775 
alphas_normal = Variable containing:
 0.1123  0.0478  0.1929  0.3092  0.0176  0.0935  0.0663  0.1007  0.0597
 0.7302  0.0401  0.0236  0.0146  0.0227  0.0462  0.0343  0.0269  0.0612
 0.0370  0.0545  0.2516  0.4371  0.0320  0.0532  0.0192  0.0461  0.0693
 0.7745  0.0411  0.0314  0.0155  0.0191  0.0233  0.0572  0.0103  0.0277
 0.5024  0.0402  0.0252  0.0182  0.0231  0.1226  0.0325  0.0627  0.1732
 0.0740  0.0551  0.1548  0.3150  0.0391  0.1446  0.0685  0.0700  0.0788
 0.5167  0.0314  0.0828  0.0750  0.0262  0.0374  0.0402  0.1145  0.0759
 0.5934  0.0454  0.0713  0.0444  0.0321  0.0152  0.0319  0.0211  0.1451
 0.3167  0.0408  0.0449  0.0331  0.0293  0.0496  0.0514  0.1101  0.3242
 0.0786  0.0317  0.0600  0.4166  0.0287  0.0565  0.0879  0.1313  0.1086
 0.5785  0.0235  0.0437  0.0438  0.0214  0.0523  0.0634  0.0140  0.1593
 0.3116  0.0812  0.2839  0.1301  0.0426  0.0295  0.0214  0.0293  0.0704
 0.2204  0.0420  0.0610  0.0479  0.0289  0.3376  0.1124  0.0564  0.0935
 0.7086  0.0311  0.0315  0.0292  0.0182  0.0179  0.0141  0.1068  0.0426
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-03 05:37:09,777 
alphas_reduce = Variable containing:
 0.0358  0.0197  0.3550  0.3246  0.1747  0.0642  0.0069  0.0111  0.0078
 0.0705  0.1614  0.0864  0.2803  0.0332  0.1403  0.0279  0.0472  0.1527
 0.0320  0.0147  0.1120  0.7205  0.0865  0.0084  0.0061  0.0115  0.0081
 0.3648  0.0563  0.0812  0.0810  0.0573  0.0805  0.1530  0.0450  0.0808
 0.0466  0.0498  0.0738  0.4859  0.0384  0.0245  0.0522  0.0201  0.2086
 0.0524  0.0193  0.3871  0.3571  0.1117  0.0207  0.0174  0.0173  0.0170
 0.0829  0.1134  0.1500  0.4073  0.0566  0.0586  0.0464  0.0571  0.0275
 0.0723  0.1008  0.1137  0.0908  0.0460  0.0177  0.0162  0.0132  0.5292
 0.0269  0.0266  0.0308  0.0289  0.0227  0.0206  0.0110  0.0088  0.8237
 0.0916  0.0323  0.1166  0.1348  0.1484  0.3406  0.0224  0.0932  0.0201
 0.2742  0.1295  0.0681  0.1485  0.0630  0.0572  0.0783  0.1345  0.0467
 0.0253  0.0250  0.0264  0.8473  0.0199  0.0174  0.0116  0.0115  0.0155
 0.0688  0.0699  0.1166  0.5361  0.0380  0.0339  0.0269  0.0211  0.0887
 0.1216  0.0399  0.0516  0.6165  0.0321  0.0359  0.0377  0.0218  0.0428
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-03 05:37:13,483 train 000 9.431827e-03 -0.074106
2019-11-03 05:40:11,542 train 050 1.026452e-02 -1.250030
2019-11-03 05:43:03,308 train 100 1.032739e-02 -1.347324
2019-11-03 05:45:59,381 train 150 1.027873e-02 -1.570580
2019-11-03 05:48:53,152 train 200 1.041175e-02 -1.502346
2019-11-03 05:51:47,671 train 250 1.041398e-02 -1.463945
2019-11-03 05:54:38,677 train 300 1.048962e-02 -1.429994
2019-11-03 05:57:31,588 train 350 1.059447e-02 -1.568479
2019-11-03 06:00:30,291 train 400 1.061545e-02 -1.598883
2019-11-03 06:03:26,967 train 450 1.057302e-02 -1.540752
2019-11-03 06:06:20,640 train 500 1.058954e-02 -1.674676
2019-11-03 06:09:15,056 train 550 1.060062e-02 -4.312740
2019-11-03 06:12:07,526 train 600 1.060293e-02 -4.101517
2019-11-03 06:15:06,568 train 650 1.061735e-02 -3.882307
2019-11-03 06:18:00,605 train 700 1.058937e-02 -4.521014
2019-11-03 06:20:54,411 train 750 1.058067e-02 -4.296462
2019-11-03 06:23:51,629 train 800 1.056766e-02 -4.097872
2019-11-03 06:26:44,451 train 850 1.058774e-02 -4.650323
2019-11-03 06:29:43,486 train 900 1.065250e-02 -5.048569
2019-11-03 06:32:39,818 train 950 1.067011e-02 -4.856240
2019-11-03 06:35:33,222 train 1000 1.066406e-02 -4.672729
2019-11-03 06:38:29,685 train 1050 1.067608e-02 -12.162532
2019-11-03 06:41:25,218 train 1100 1.068460e-02 -11.711963
2019-11-03 06:44:20,204 train 1150 1.068192e-02 -11.257300
2019-11-03 06:47:16,897 train 1200 1.069067e-02 -10.834514
2019-11-03 06:50:13,322 train 1250 1.069126e-02 -10.454526
2019-11-03 06:53:09,507 train 1300 1.070123e-02 -10.126307
2019-11-03 06:56:07,815 train 1350 1.068013e-02 -9.869199
2019-11-03 06:59:02,093 train 1400 1.066345e-02 -9.573522
2019-11-03 07:01:56,545 train 1450 1.066715e-02 -9.286367
2019-11-03 07:04:48,810 train 1500 1.065157e-02 -9.035088
2019-11-03 07:07:48,277 train 1550 1.065185e-02 -8.793519
2019-11-03 07:10:40,916 train 1600 1.066154e-02 -8.550346
2019-11-03 07:13:47,196 train 1650 1.065458e-02 -8.416875
2019-11-03 07:16:41,184 train 1700 1.065699e-02 -8.202324
2019-11-03 07:19:36,641 train 1750 1.066275e-02 -8.001637
2019-11-03 07:22:39,116 train 1800 1.067752e-02 -7.823490
2019-11-03 07:25:33,142 train 1850 1.067753e-02 -7.646895
2019-11-03 07:28:24,803 train 1900 1.067741e-02 -7.513920
2019-11-03 07:31:18,811 train 1950 1.067680e-02 -7.350473
2019-11-03 07:34:11,057 train 2000 1.066117e-02 -7.195419
2019-11-03 07:37:04,502 train 2050 1.066948e-02 -7.042093
2019-11-03 07:39:59,283 train 2100 1.067070e-02 -6.896400
2019-11-03 07:42:57,226 train 2150 1.067261e-02 -6.770113
2019-11-03 07:45:50,311 train 2200 1.068154e-02 -6.707714
2019-11-03 07:48:45,506 train 2250 1.066917e-02 -6.605085
2019-11-03 07:51:42,335 train 2300 1.068048e-02 -6.497545
2019-11-03 07:54:37,057 train 2350 1.067760e-02 -6.390069
2019-11-03 07:57:31,803 train 2400 1.067901e-02 -6.302396
2019-11-03 08:00:30,125 train 2450 1.067955e-02 -6.195356
2019-11-03 08:03:23,159 train 2500 1.067985e-02 -6.217795
2019-11-03 08:06:15,632 train 2550 1.067619e-02 -6.135611
2019-11-03 08:09:11,190 train 2600 1.068380e-02 -6.036257
2019-11-03 08:12:04,719 train 2650 1.067925e-02 -5.945527
2019-11-03 08:15:00,645 train 2700 1.067718e-02 -5.852869
2019-11-03 08:17:56,934 train 2750 1.067035e-02 -5.769443
2019-11-03 08:20:50,583 train 2800 1.066676e-02 -5.694033
2019-11-03 08:23:46,763 train 2850 1.066312e-02 -5.618486
2019-11-03 08:26:40,271 train 2900 1.066152e-02 -5.539690
2019-11-03 08:29:37,427 train 2950 1.065987e-02 -5.479143
2019-11-03 08:32:32,060 train 3000 1.065636e-02 -5.414674
2019-11-03 08:35:29,164 train 3050 1.064825e-02 -5.354190
2019-11-03 08:37:08,228 training loss; R2: 1.065677e-02 -5.314201
2019-11-03 08:37:08,746 valid 000 7.318192e-03 -1.381618
2019-11-03 08:37:18,813 valid 050 9.811775e-03 -1.496157
2019-11-03 08:37:28,861 valid 100 1.008399e-02 -1.149457
2019-11-03 08:37:38,935 valid 150 1.019036e-02 -1.130543
2019-11-03 08:37:49,000 valid 200 1.015932e-02 -1.178428
2019-11-03 08:37:59,952 valid 250 1.005031e-02 -1.116204
2019-11-03 08:38:11,992 valid 300 1.008451e-02 -2.165405
2019-11-03 08:38:23,993 valid 350 1.008943e-02 -2.007937
2019-11-03 08:38:35,365 valid 400 1.008070e-02 -1.916050
2019-11-03 08:38:45,474 valid 450 1.007256e-02 -1.771646
2019-11-03 08:38:55,579 valid 500 1.007005e-02 -1.696915
2019-11-03 08:39:05,660 valid 550 1.008755e-02 -1.649053
2019-11-03 08:39:15,765 valid 600 1.013121e-02 -1.587544
2019-11-03 08:39:25,878 valid 650 1.011026e-02 -1.570910
2019-11-03 08:39:35,981 valid 700 1.012589e-02 -1.697508
2019-11-03 08:39:46,113 valid 750 1.012371e-02 -1.709365
2019-11-03 08:39:49,903 validation loss; R2: 1.011795e-02 -1.683705
2019-11-03 08:39:50,094 epoch 5 lr 9.779754e-04
2019-11-03 08:39:50,095 genotype = Genotype(normal=[('max_pool_3x3', 0), ('dil_conv_5x5', 1), ('max_pool_3x3', 0), ('dil_conv_5x5', 2), ('max_pool_3x3', 0), ('dil_conv_5x5', 3), ('sep_conv_3x3', 3), ('max_pool_3x3', 0)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 1), ('max_pool_3x3', 0), ('max_pool_3x3', 0), ('max_pool_3x3', 2), ('dil_conv_5x5', 3), ('dil_conv_5x5', 2), ('max_pool_3x3', 2), ('sep_conv_3x3', 0)], reduce_concat=range(2, 6))
2019-11-03 08:39:50,097 
alphas_normal = Variable containing:
 0.0813  0.0623  0.2530  0.3683  0.0326  0.0595  0.0348  0.0585  0.0497
 0.7301  0.0321  0.0201  0.0146  0.0195  0.0384  0.0470  0.0183  0.0797
 0.0420  0.0911  0.2629  0.3680  0.0504  0.0462  0.0152  0.0520  0.0723
 0.7785  0.0408  0.0260  0.0165  0.0224  0.0158  0.0566  0.0169  0.0265
 0.5499  0.0331  0.0232  0.0174  0.0249  0.1032  0.0296  0.0668  0.1519
 0.0658  0.0965  0.1576  0.2730  0.0808  0.1310  0.0400  0.0661  0.0893
 0.6140  0.0324  0.0553  0.0639  0.0258  0.0256  0.0525  0.0949  0.0355
 0.6371  0.0451  0.0532  0.0341  0.0400  0.0173  0.0246  0.0243  0.1242
 0.4632  0.0426  0.0277  0.0232  0.0305  0.0283  0.0602  0.0974  0.2270
 0.0514  0.0415  0.0540  0.3906  0.0461  0.0445  0.1741  0.0735  0.1243
 0.6892  0.0182  0.0246  0.0263  0.0167  0.0337  0.0451  0.0145  0.1318
 0.2953  0.0918  0.2781  0.1213  0.0531  0.0241  0.0316  0.0282  0.0766
 0.2184  0.0403  0.0462  0.0382  0.0301  0.4047  0.1024  0.0592  0.0605
 0.7556  0.0231  0.0184  0.0174  0.0165  0.0201  0.0106  0.0987  0.0395
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-03 08:39:50,098 
alphas_reduce = Variable containing:
 0.0366  0.0422  0.2256  0.2743  0.2289  0.1471  0.0119  0.0162  0.0172
 0.0702  0.1604  0.1369  0.3130  0.0484  0.0936  0.0325  0.0521  0.0930
 0.0358  0.0241  0.0933  0.6429  0.1407  0.0172  0.0140  0.0203  0.0117
 0.4320  0.0603  0.0947  0.0706  0.0885  0.0726  0.0916  0.0373  0.0522
 0.0513  0.0513  0.0805  0.5054  0.0593  0.0228  0.0531  0.0259  0.1504
 0.0940  0.0313  0.3469  0.2607  0.1320  0.0270  0.0384  0.0362  0.0336
 0.0804  0.1209  0.1724  0.2896  0.1083  0.0712  0.0453  0.0807  0.0311
 0.0825  0.0957  0.1300  0.1021  0.0707  0.0316  0.0513  0.0280  0.4079
 0.0167  0.0176  0.0197  0.0184  0.0183  0.0140  0.0131  0.0119  0.8704
 0.0473  0.0274  0.0486  0.0523  0.0551  0.6225  0.0250  0.0982  0.0236
 0.2026  0.3106  0.0597  0.0794  0.0699  0.0453  0.0408  0.1395  0.0522
 0.0684  0.0485  0.0607  0.6456  0.0405  0.0388  0.0391  0.0304  0.0279
 0.1751  0.0466  0.1188  0.3513  0.0419  0.0508  0.0666  0.0365  0.1125
 0.1766  0.0339  0.0734  0.4160  0.0460  0.0386  0.1053  0.0314  0.0789
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-03 08:39:54,050 train 000 9.694488e-03 -0.881333
2019-11-03 08:42:47,035 train 050 1.049929e-02 -1.380542
2019-11-03 08:45:39,698 train 100 1.048577e-02 -1.175743
2019-11-03 08:48:34,076 train 150 1.055509e-02 -3.540446
2019-11-03 08:51:31,711 train 200 1.058189e-02 -5.323970
2019-11-03 08:54:30,364 train 250 1.055264e-02 -4.544444
2019-11-03 08:57:25,961 train 300 1.053414e-02 -4.025891
2019-11-03 09:00:19,513 train 350 1.050732e-02 -3.702775
2019-11-03 09:03:15,169 train 400 1.057037e-02 -3.391259
2019-11-03 09:06:06,666 train 450 1.061806e-02 -3.222765
2019-11-03 09:09:02,341 train 500 1.061263e-02 -3.093696
2019-11-03 09:11:55,966 train 550 1.060328e-02 -2.957205
2019-11-03 09:14:48,755 train 600 1.060488e-02 -2.829299
2019-11-03 09:17:44,932 train 650 1.058508e-02 -2.724581
2019-11-03 09:20:46,715 train 700 1.064306e-02 -2.649661
2019-11-03 09:23:45,775 train 750 1.059591e-02 -2.587419
2019-11-03 09:26:44,487 train 800 1.058611e-02 -2.515410
2019-11-03 09:29:42,774 train 850 1.059056e-02 -2.464671
2019-11-03 09:32:42,952 train 900 1.058509e-02 -2.395045
2019-11-03 09:35:42,630 train 950 1.056493e-02 -2.324701
2019-11-03 09:38:42,628 train 1000 1.055436e-02 -2.277924
2019-11-03 09:41:42,719 train 1050 1.050550e-02 -2.416020
2019-11-03 09:44:42,571 train 1100 1.048917e-02 -2.421244
2019-11-03 09:47:42,574 train 1150 1.047703e-02 -2.372206
2019-11-03 09:50:42,592 train 1200 1.048213e-02 -2.332619
2019-11-03 09:53:37,739 train 1250 1.049110e-02 -2.286949
2019-11-03 09:56:22,517 train 1300 1.045732e-02 -2.248978
2019-11-03 09:59:07,597 train 1350 1.044433e-02 -2.225578
2019-11-03 10:01:59,480 train 1400 1.042019e-02 -2.201353
2019-11-03 10:04:50,278 train 1450 1.043192e-02 -2.221655
2019-11-03 10:07:35,926 train 1500 1.041937e-02 -2.191177
2019-11-03 10:10:22,992 train 1550 1.042285e-02 -2.168558
2019-11-03 10:13:08,225 train 1600 1.043007e-02 -2.137858
2019-11-03 10:15:53,960 train 1650 1.043460e-02 -2.107776
2019-11-03 10:18:39,487 train 1700 1.043625e-02 -2.079566
2019-11-03 10:21:25,612 train 1750 1.044597e-02 -2.049429
2019-11-03 10:24:12,028 train 1800 1.044165e-02 -2.115843
2019-11-03 10:26:57,948 train 1850 1.044118e-02 -2.090099
2019-11-03 10:29:43,977 train 1900 1.045958e-02 -2.099953
2019-11-03 10:32:29,198 train 1950 1.044610e-02 -2.082737
2019-11-03 10:35:13,583 train 2000 1.045202e-02 -2.055460
2019-11-03 10:37:58,714 train 2050 1.044528e-02 -2.037265
2019-11-03 10:40:43,159 train 2100 1.044349e-02 -2.014321
2019-11-03 10:43:27,719 train 2150 1.043235e-02 -1.992963
2019-11-03 10:46:11,683 train 2200 1.043341e-02 -1.970081
2019-11-03 10:48:57,853 train 2250 1.043344e-02 -1.949931
2019-11-03 10:51:53,004 train 2300 1.043575e-02 -1.935739
2019-11-03 10:54:48,085 train 2350 1.043781e-02 -1.916284
2019-11-03 10:57:43,226 train 2400 1.042821e-02 -1.914479
2019-11-03 11:00:38,536 train 2450 1.043297e-02 -1.897599
2019-11-03 11:03:33,669 train 2500 1.042812e-02 -1.882604
2019-11-03 11:06:26,887 train 2550 1.043387e-02 -1.873024
2019-11-03 11:09:10,186 train 2600 1.043555e-02 -1.865046
2019-11-03 11:11:52,791 train 2650 1.043810e-02 -1.898958
2019-11-03 11:14:35,535 train 2700 1.044283e-02 -1.889970
2019-11-03 11:17:34,427 train 2750 1.043948e-02 -2.235146
2019-11-03 11:20:19,597 train 2800 1.044681e-02 -2.207903
2019-11-03 11:23:04,426 train 2850 1.044756e-02 -2.193356
2019-11-03 11:25:52,143 train 2900 1.044403e-02 -2.173990
2019-11-03 11:28:53,409 train 2950 1.043463e-02 -2.284388
2019-11-03 11:31:38,608 train 3000 1.043548e-02 -2.268435
2019-11-03 11:34:23,954 train 3050 1.043122e-02 -2.255998
2019-11-03 11:35:56,438 training loss; R2: 1.043767e-02 -2.243471
2019-11-03 11:35:56,919 valid 000 1.076712e-02 -0.372430
2019-11-03 11:36:06,420 valid 050 9.542909e-03 -4.458465
2019-11-03 11:36:15,855 valid 100 9.817793e-03 -2.690626
2019-11-03 11:36:25,306 valid 150 9.674851e-03 -1.992656
2019-11-03 11:36:34,747 valid 200 9.871875e-03 -1.673528
2019-11-03 11:36:44,209 valid 250 9.860314e-03 -19.932216
2019-11-03 11:36:53,663 valid 300 9.884475e-03 -16.735132
2019-11-03 11:37:03,126 valid 350 9.906569e-03 -14.886327
2019-11-03 11:37:12,587 valid 400 9.973737e-03 -13.124422
2019-11-03 11:37:22,024 valid 450 1.002580e-02 -11.842988
2019-11-03 11:37:31,456 valid 500 9.993242e-03 -10.793052
2019-11-03 11:37:40,885 valid 550 1.000344e-02 -9.980875
2019-11-03 11:37:50,314 valid 600 1.001610e-02 -9.230268
2019-11-03 11:37:59,742 valid 650 1.000779e-02 -8.602903
2019-11-03 11:38:09,165 valid 700 1.001755e-02 -8.068448
2019-11-03 11:38:18,572 valid 750 1.003102e-02 -7.578797
2019-11-03 11:38:22,132 validation loss; R2: 1.000546e-02 -7.420662
2019-11-03 11:38:22,263 epoch 6 lr 9.683994e-04
2019-11-03 11:38:22,264 genotype = Genotype(normal=[('max_pool_3x3', 0), ('dil_conv_5x5', 1), ('max_pool_3x3', 0), ('dil_conv_5x5', 2), ('max_pool_3x3', 0), ('dil_conv_5x5', 3), ('sep_conv_5x5', 0), ('sep_conv_3x3', 3)], normal_concat=range(2, 6), reduce=[('avg_pool_3x3', 0), ('max_pool_3x3', 1), ('max_pool_3x3', 2), ('max_pool_3x3', 0), ('dil_conv_5x5', 3), ('max_pool_3x3', 1), ('sep_conv_3x3', 0), ('max_pool_3x3', 2)], reduce_concat=range(2, 6))
2019-11-03 11:38:22,266 
alphas_normal = Variable containing:
 0.0644  0.0961  0.1971  0.3889  0.0442  0.0757  0.0295  0.0525  0.0517
 0.7544  0.0242  0.0152  0.0117  0.0181  0.0249  0.0402  0.0149  0.0964
 0.0320  0.1193  0.2979  0.3361  0.0499  0.0229  0.0181  0.0380  0.0858
 0.8334  0.0275  0.0204  0.0146  0.0207  0.0097  0.0260  0.0177  0.0300
 0.5833  0.0383  0.0274  0.0192  0.0320  0.0471  0.0396  0.0563  0.1568
 0.0671  0.0813  0.1334  0.2700  0.0664  0.1533  0.0386  0.0732  0.1167
 0.6507  0.0257  0.0494  0.0527  0.0271  0.0278  0.0308  0.1113  0.0246
 0.5840  0.0396  0.0617  0.0444  0.0418  0.0292  0.0261  0.0248  0.1484
 0.5257  0.0398  0.0303  0.0296  0.0320  0.0194  0.0783  0.0720  0.1730
 0.0503  0.0272  0.0286  0.2855  0.0262  0.0277  0.3236  0.0656  0.1653
 0.6845  0.0193  0.0301  0.0257  0.0193  0.0229  0.0406  0.0141  0.1436
 0.2858  0.0529  0.2703  0.1187  0.0392  0.0223  0.0964  0.0255  0.0888
 0.2873  0.0341  0.0385  0.0399  0.0266  0.3201  0.1192  0.0856  0.0486
 0.7687  0.0186  0.0153  0.0151  0.0139  0.0198  0.0109  0.0985  0.0392
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-03 11:38:22,267 
alphas_reduce = Variable containing:
 0.0366  0.0309  0.2147  0.1987  0.3285  0.1555  0.0087  0.0136  0.0129
 0.1071  0.2238  0.0818  0.2470  0.0353  0.0819  0.0383  0.0557  0.1291
 0.0388  0.0311  0.0686  0.5164  0.2740  0.0213  0.0132  0.0222  0.0144
 0.5116  0.0726  0.0488  0.0465  0.0450  0.1143  0.0726  0.0417  0.0471
 0.0437  0.0358  0.0562  0.6302  0.0489  0.0234  0.0334  0.0199  0.1084
 0.1373  0.0532  0.3280  0.1959  0.1460  0.0222  0.0336  0.0416  0.0422
 0.0754  0.0873  0.1311  0.4175  0.0614  0.0691  0.0531  0.0642  0.0408
 0.1272  0.0871  0.1090  0.0906  0.0611  0.0490  0.0469  0.0388  0.3903
 0.0113  0.0093  0.0102  0.0104  0.0097  0.0103  0.0116  0.0080  0.9192
 0.0278  0.0255  0.0285  0.0319  0.0352  0.7683  0.0173  0.0490  0.0165
 0.1967  0.3878  0.0436  0.0647  0.0489  0.0417  0.0388  0.1076  0.0703
 0.0591  0.0339  0.0337  0.7082  0.0291  0.0448  0.0287  0.0387  0.0239
 0.1630  0.0386  0.0871  0.4683  0.0390  0.0479  0.0535  0.0313  0.0714
 0.0877  0.0300  0.0332  0.5131  0.0361  0.0390  0.1441  0.0273  0.0895
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-03 11:38:25,797 train 000 1.099823e-02 -0.621481
2019-11-03 11:41:12,327 train 050 1.048895e-02 -1.192501
2019-11-03 11:43:58,167 train 100 1.017262e-02 -1.342132
2019-11-03 11:46:41,602 train 150 1.018376e-02 -1.367004
2019-11-03 11:49:26,850 train 200 1.029672e-02 -1.386581
2019-11-03 11:52:12,544 train 250 1.027122e-02 -1.308687
2019-11-03 11:55:08,806 train 300 1.030417e-02 -1.333663
2019-11-03 11:57:57,563 train 350 1.023904e-02 -1.290674
2019-11-03 12:00:41,322 train 400 1.026224e-02 -1.303652
2019-11-03 12:03:26,678 train 450 1.022820e-02 -1.629109
2019-11-03 12:06:12,608 train 500 1.027995e-02 -1.562288
2019-11-03 12:08:59,296 train 550 1.026522e-02 -1.564270
2019-11-03 12:11:49,441 train 600 1.027250e-02 -1.548449
2019-11-03 12:14:33,763 train 650 1.019744e-02 -1.600167
2019-11-03 12:17:20,557 train 700 1.024953e-02 -2.041343
2019-11-03 12:20:09,476 train 750 1.024723e-02 -2.000341
2019-11-03 12:22:55,167 train 800 1.023589e-02 -2.027158
2019-11-03 12:25:40,646 train 850 1.023753e-02 -1.977591
2019-11-03 12:28:26,233 train 900 1.026665e-02 -1.982395
2019-11-03 12:31:11,913 train 950 1.028269e-02 -1.932845
2019-11-03 12:33:58,164 train 1000 1.030095e-02 -2.051153
2019-11-03 12:36:41,131 train 1050 1.028425e-02 -1.996060
2019-11-03 12:39:23,624 train 1100 1.030512e-02 -2.118572
2019-11-03 12:42:06,343 train 1150 1.030295e-02 -2.079403
2019-11-03 12:44:51,661 train 1200 1.030969e-02 -2.060998
2019-11-03 12:47:37,544 train 1250 1.029348e-02 -2.022909
2019-11-03 12:50:23,336 train 1300 1.027585e-02 -2.047026
2019-11-03 12:53:09,768 train 1350 1.028277e-02 -2.106937
2019-11-03 12:56:04,211 train 1400 1.027309e-02 -2.074007
2019-11-03 12:58:51,654 train 1450 1.026002e-02 -2.060313
2019-11-03 13:01:37,548 train 1500 1.026382e-02 -2.035078
2019-11-03 13:04:38,041 train 1550 1.025840e-02 -2.449818
2019-11-03 13:07:38,792 train 1600 1.025204e-02 -2.441014
2019-11-03 13:10:42,950 train 1650 1.026199e-02 -2.444348
2019-11-03 13:13:50,372 train 1700 1.027582e-02 -2.404734
2019-11-03 13:16:57,622 train 1750 1.027763e-02 -2.365504
2019-11-03 13:20:04,715 train 1800 1.028563e-02 -2.331671
2019-11-03 13:23:04,431 train 1850 1.029658e-02 -2.325017
2019-11-03 13:25:59,504 train 1900 1.029192e-02 -2.291252
2019-11-03 13:28:43,463 train 1950 1.027948e-02 -2.272614
2019-11-03 13:31:26,159 train 2000 1.028558e-02 -2.264179
2019-11-03 13:34:09,385 train 2050 1.027834e-02 -2.228691
2019-11-03 13:36:53,514 train 2100 1.027367e-02 -2.208234
2019-11-03 13:39:39,266 train 2150 1.026871e-02 -2.213260
2019-11-03 13:42:24,891 train 2200 1.026300e-02 -2.199746
2019-11-03 13:45:09,565 train 2250 1.025411e-02 -2.183275
2019-11-03 13:47:52,234 train 2300 1.025142e-02 -2.207978
2019-11-03 13:50:35,005 train 2350 1.025962e-02 -2.194461
2019-11-03 13:53:17,562 train 2400 1.026750e-02 -2.237219
2019-11-03 13:56:02,932 train 2450 1.026874e-02 -2.221840
2019-11-03 13:58:48,231 train 2500 1.026720e-02 -2.757488
2019-11-03 14:01:33,007 train 2550 1.026530e-02 -2.724222
2019-11-03 14:04:17,816 train 2600 1.025668e-02 -3.477745
2019-11-03 14:07:07,986 train 2650 1.025802e-02 -3.533683
2019-11-03 14:09:56,361 train 2700 1.024297e-02 -3.487645
2019-11-03 14:12:44,869 train 2750 1.024005e-02 -3.448081
2019-11-03 14:15:40,662 train 2800 1.024657e-02 -3.409485
2019-11-03 14:18:26,815 train 2850 1.024539e-02 -3.376653
2019-11-03 14:21:12,971 train 2900 1.024187e-02 -3.354285
2019-11-03 14:23:59,218 train 2950 1.023267e-02 -3.323034
2019-11-03 14:26:59,570 train 3000 1.023869e-02 -3.298653
2019-11-03 14:29:59,587 train 3050 1.023960e-02 -3.262789
2019-11-03 14:31:40,450 training loss; R2: 1.024341e-02 -3.240257
2019-11-03 14:31:40,931 valid 000 7.015228e-03 -0.322013
2019-11-03 14:31:52,326 valid 050 9.296607e-03 -1.068996
2019-11-03 14:32:03,701 valid 100 9.529894e-03 -1.008381
2019-11-03 14:32:15,019 valid 150 9.689537e-03 -0.946344
2019-11-03 14:32:26,330 valid 200 9.593042e-03 -0.953035
2019-11-03 14:32:37,635 valid 250 9.500049e-03 -0.950171
2019-11-03 14:32:48,946 valid 300 9.580635e-03 -416.210167
2019-11-03 14:33:00,245 valid 350 9.484472e-03 -357.080799
2019-11-03 14:33:11,555 valid 400 9.524870e-03 -312.757483
2019-11-03 14:33:22,839 valid 450 9.507701e-03 -278.230350
2019-11-03 14:33:34,120 valid 500 9.528419e-03 -250.587841
2019-11-03 14:33:45,415 valid 550 9.532447e-03 -228.065692
2019-11-03 14:33:56,712 valid 600 9.576762e-03 -215.302925
2019-11-03 14:34:08,019 valid 650 9.594077e-03 -198.848664
2019-11-03 14:34:19,326 valid 700 9.583718e-03 -184.716176
2019-11-03 14:34:30,631 valid 750 9.595926e-03 -172.578796
2019-11-03 14:34:34,904 validation loss; R2: 9.585210e-03 -168.416474
2019-11-03 14:34:35,042 epoch 7 lr 9.571722e-04
2019-11-03 14:34:35,043 genotype = Genotype(normal=[('max_pool_3x3', 0), ('dil_conv_5x5', 1), ('max_pool_2x2', 0), ('dil_conv_5x5', 2), ('max_pool_3x3', 0), ('dil_conv_5x5', 2), ('max_pool_3x3', 0), ('sep_conv_3x3', 3)], normal_concat=range(2, 6), reduce=[('avg_pool_3x3', 0), ('skip_connect', 1), ('max_pool_3x3', 0), ('max_pool_3x3', 2), ('dil_conv_5x5', 3), ('dil_conv_5x5', 2), ('max_pool_3x3', 2), ('sep_conv_3x3', 0)], reduce_concat=range(2, 6))
2019-11-03 14:34:35,045 
alphas_normal = Variable containing:
 0.0516  0.0543  0.1476  0.5368  0.0256  0.0551  0.0314  0.0518  0.0457
 0.7232  0.0290  0.0200  0.0177  0.0246  0.0420  0.0256  0.0256  0.0925
 0.0401  0.1130  0.3280  0.3213  0.0436  0.0336  0.0266  0.0272  0.0667
 0.7791  0.0340  0.0270  0.0204  0.0269  0.0137  0.0297  0.0198  0.0494
 0.5086  0.0424  0.0381  0.0241  0.0414  0.0786  0.0311  0.0503  0.1854
 0.0603  0.0876  0.1466  0.3071  0.0616  0.1715  0.0808  0.0337  0.0507
 0.6282  0.0280  0.0557  0.0624  0.0293  0.0330  0.0344  0.1053  0.0237
 0.4596  0.0442  0.0772  0.0604  0.0480  0.0397  0.0350  0.0303  0.2057
 0.4522  0.0422  0.0405  0.0370  0.0382  0.0202  0.1369  0.0584  0.1743
 0.0661  0.0337  0.0442  0.3978  0.0273  0.0368  0.1979  0.0516  0.1446
 0.6817  0.0162  0.0237  0.0226  0.0154  0.0225  0.0972  0.0143  0.1065
 0.1819  0.0538  0.3576  0.1934  0.0471  0.0212  0.0520  0.0273  0.0657
 0.2751  0.0309  0.0365  0.0446  0.0270  0.3723  0.1238  0.0578  0.0321
 0.6952  0.0191  0.0165  0.0182  0.0138  0.0396  0.0150  0.1119  0.0707
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-03 14:34:35,047 
alphas_reduce = Variable containing:
 0.0309  0.0265  0.2176  0.2284  0.3305  0.1331  0.0079  0.0136  0.0115
 0.1364  0.2087  0.0680  0.2072  0.0251  0.1237  0.0295  0.0667  0.1345
 0.0424  0.0189  0.0804  0.5677  0.2412  0.0146  0.0080  0.0170  0.0097
 0.5678  0.0581  0.0514  0.0419  0.0419  0.0874  0.0433  0.0511  0.0571
 0.0482  0.0438  0.0603  0.5255  0.0499  0.0229  0.0325  0.0296  0.1872
 0.1188  0.0169  0.4760  0.1840  0.1305  0.0135  0.0208  0.0169  0.0226
 0.1492  0.0963  0.2389  0.2814  0.0730  0.0425  0.0313  0.0572  0.0302
 0.1373  0.0721  0.0887  0.0696  0.0479  0.0282  0.0307  0.0177  0.5079
 0.0207  0.0165  0.0179  0.0179  0.0160  0.0153  0.0163  0.0115  0.8680
 0.0542  0.0303  0.0434  0.0486  0.0475  0.6908  0.0246  0.0434  0.0172
 0.3268  0.3527  0.0518  0.0607  0.0515  0.0336  0.0299  0.0538  0.0391
 0.0479  0.0310  0.0396  0.7403  0.0272  0.0246  0.0213  0.0277  0.0404
 0.1059  0.0407  0.0895  0.5808  0.0338  0.0304  0.0272  0.0199  0.0717
 0.0518  0.0303  0.0502  0.6515  0.0337  0.0220  0.0980  0.0157  0.0467
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-03 14:34:38,941 train 000 1.071195e-02 -1.212397
2019-11-03 14:37:39,401 train 050 1.010894e-02 -1.333685
2019-11-03 14:40:39,499 train 100 9.929543e-03 -1.202851
2019-11-03 14:43:39,672 train 150 9.863364e-03 -1.083333
2019-11-03 14:46:26,780 train 200 9.871511e-03 -1.083367
2019-11-03 14:49:26,916 train 250 9.958872e-03 -1.094462
2019-11-03 14:52:13,294 train 300 9.913364e-03 -6.310458
2019-11-03 14:54:59,432 train 350 9.929018e-03 -5.583475
2019-11-03 14:57:45,476 train 400 9.853950e-03 -5.039030
2019-11-03 15:00:35,461 train 450 9.882826e-03 -4.617995
2019-11-03 15:03:30,862 train 500 9.997722e-03 -4.322862
2019-11-03 15:06:24,133 train 550 9.996575e-03 -4.028621
2019-11-03 15:09:09,930 train 600 9.985408e-03 -3.807310
2019-11-03 15:11:56,075 train 650 9.970002e-03 -3.596313
2019-11-03 15:14:42,215 train 700 9.915315e-03 -3.456275
2019-11-03 15:17:27,817 train 750 9.927861e-03 -3.318692
2019-11-03 15:20:12,615 train 800 9.958478e-03 -3.172201
2019-11-03 15:22:57,206 train 850 9.983235e-03 -3.203122
2019-11-03 15:25:42,717 train 900 9.968418e-03 -3.077408
2019-11-03 15:28:28,249 train 950 9.976213e-03 -3.155839
2019-11-03 15:31:13,101 train 1000 9.987376e-03 -3.045518
2019-11-03 15:33:58,530 train 1050 9.970603e-03 -2.967685
2019-11-03 15:36:43,970 train 1100 9.968904e-03 -2.871430
2019-11-03 15:39:27,100 train 1150 9.984993e-03 -2.784711
2019-11-03 15:42:19,078 train 1200 9.978308e-03 -2.718213
2019-11-03 15:45:04,206 train 1250 1.000665e-02 -2.696187
2019-11-03 15:47:49,341 train 1300 1.003514e-02 -2.656169
2019-11-03 15:50:34,668 train 1350 1.003825e-02 -2.603657
2019-11-03 15:53:19,612 train 1400 1.003376e-02 -2.539898
2019-11-03 15:56:04,402 train 1450 1.003602e-02 -2.588005
2019-11-03 15:58:54,542 train 1500 1.004520e-02 -2.547243
2019-11-03 16:01:58,206 train 1550 1.001721e-02 -2.510273
2019-11-03 16:05:04,248 train 1600 1.001971e-02 -2.468161
2019-11-03 16:07:58,836 train 1650 1.000733e-02 -2.421814
2019-11-03 16:10:58,445 train 1700 1.001667e-02 -2.379863
2019-11-03 16:13:50,460 train 1750 1.001709e-02 -2.348355
2019-11-03 16:16:50,739 train 1800 1.001422e-02 -9.019736
2019-11-03 16:19:50,777 train 1850 1.002298e-02 -8.832891
2019-11-03 16:22:51,083 train 1900 1.002290e-02 -8.631223
2019-11-03 16:25:36,592 train 1950 1.004010e-02 -8.442509
2019-11-03 16:28:36,606 train 2000 1.003011e-02 -8.284355
2019-11-03 16:31:36,880 train 2050 1.003830e-02 -8.109116
2019-11-03 16:34:27,613 train 2100 1.003951e-02 -7.970801
2019-11-03 16:37:13,248 train 2150 1.004460e-02 -7.812080
2019-11-03 16:39:58,804 train 2200 1.004519e-02 -8.428202
2019-11-03 16:42:44,037 train 2250 1.005197e-02 -8.274225
2019-11-03 16:45:29,240 train 2300 1.004033e-02 -8.119230
2019-11-03 16:48:13,889 train 2350 1.003575e-02 -7.968252
2019-11-03 16:50:59,197 train 2400 1.003783e-02 -7.829009
2019-11-03 16:53:59,354 train 2450 1.004201e-02 -7.726539
2019-11-03 16:56:59,249 train 2500 1.002982e-02 -7.598219
2019-11-03 16:59:44,651 train 2550 1.004022e-02 -7.482722
2019-11-03 17:02:29,912 train 2600 1.003696e-02 -7.351754
2019-11-03 17:05:15,757 train 2650 1.004368e-02 -7.250166
2019-11-03 17:08:01,829 train 2700 1.004065e-02 -7.138670
2019-11-03 17:10:50,676 train 2750 1.003375e-02 -7.024172
2019-11-03 17:13:45,512 train 2800 1.002820e-02 -6.927232
2019-11-03 17:16:38,115 train 2850 1.003231e-02 -7.115911
2019-11-03 17:19:23,735 train 2900 1.003379e-02 -7.034918
2019-11-03 17:22:24,128 train 2950 1.003138e-02 -6.961446
2019-11-03 17:25:21,978 train 3000 1.003069e-02 -7.163304
2019-11-03 17:28:07,118 train 3050 1.002633e-02 -7.062590
2019-11-03 17:29:39,438 training loss; R2: 1.002057e-02 -7.010363
2019-11-03 17:29:39,920 valid 000 1.024032e-02 -1.874845
2019-11-03 17:29:49,411 valid 050 1.052279e-02 -1.526909
2019-11-03 17:29:58,915 valid 100 1.040359e-02 -1.188495
2019-11-03 17:30:08,393 valid 150 1.054686e-02 -1.198665
2019-11-03 17:30:17,895 valid 200 1.048355e-02 -1.048381
2019-11-03 17:30:27,368 valid 250 1.050743e-02 -0.994793
2019-11-03 17:30:36,832 valid 300 1.060046e-02 -1.352730
2019-11-03 17:30:46,295 valid 350 1.059966e-02 -1.298625
2019-11-03 17:30:55,767 valid 400 1.057551e-02 -1.243625
2019-11-03 17:31:05,216 valid 450 1.060292e-02 -5.684025
2019-11-03 17:31:14,650 valid 500 1.057220e-02 -5.225772
2019-11-03 17:31:24,098 valid 550 1.055567e-02 -5.985486
2019-11-03 17:31:33,556 valid 600 1.055730e-02 -5.555674
2019-11-03 17:31:43,020 valid 650 1.054156e-02 -5.184025
2019-11-03 17:31:52,474 valid 700 1.052332e-02 -4.870857
2019-11-03 17:32:01,891 valid 750 1.051937e-02 -4.606486
2019-11-03 17:32:05,448 validation loss; R2: 1.052158e-02 -4.518873
2019-11-03 17:32:05,576 epoch 8 lr 9.443380e-04
2019-11-03 17:32:05,577 genotype = Genotype(normal=[('max_pool_3x3', 0), ('dil_conv_5x5', 1), ('max_pool_3x3', 0), ('dil_conv_5x5', 2), ('max_pool_3x3', 0), ('dil_conv_5x5', 2), ('sep_conv_3x3', 3), ('max_pool_2x2', 2)], normal_concat=range(2, 6), reduce=[('avg_pool_3x3', 0), ('max_pool_3x3', 1), ('max_pool_3x3', 0), ('max_pool_3x3', 2), ('dil_conv_5x5', 3), ('max_pool_2x2', 0), ('sep_conv_3x3', 0), ('max_pool_3x3', 2)], reduce_concat=range(2, 6))
2019-11-03 17:32:05,579 
alphas_normal = Variable containing:
 0.0392  0.0385  0.1197  0.6333  0.0217  0.0466  0.0205  0.0471  0.0334
 0.7048  0.0233  0.0208  0.0188  0.0193  0.0316  0.0330  0.0233  0.1252
 0.0503  0.1168  0.2737  0.3718  0.0357  0.0376  0.0280  0.0314  0.0549
 0.7881  0.0315  0.0294  0.0189  0.0250  0.0127  0.0273  0.0142  0.0529
 0.4740  0.0382  0.0299  0.0205  0.0359  0.0734  0.0315  0.0442  0.2524
 0.0638  0.0880  0.1793  0.3713  0.0510  0.1342  0.0385  0.0361  0.0377
 0.5672  0.0253  0.0475  0.0556  0.0262  0.0306  0.0381  0.1850  0.0245
 0.3823  0.0509  0.0841  0.0606  0.0551  0.0400  0.0437  0.0306  0.2528
 0.5359  0.0329  0.0298  0.0232  0.0270  0.0175  0.1598  0.0501  0.1237
 0.0480  0.0294  0.0340  0.2934  0.0254  0.0448  0.2994  0.0612  0.1644
 0.7198  0.0121  0.0150  0.0144  0.0113  0.0161  0.0956  0.0136  0.1022
 0.1587  0.0606  0.3407  0.1856  0.0553  0.0297  0.0503  0.0345  0.0846
 0.2596  0.0298  0.0320  0.0357  0.0252  0.4069  0.1027  0.0724  0.0357
 0.7525  0.0186  0.0157  0.0156  0.0141  0.0225  0.0146  0.0921  0.0543
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-03 17:32:05,580 
alphas_reduce = Variable containing:
 0.0455  0.0297  0.2513  0.1705  0.3087  0.1558  0.0077  0.0111  0.0197
 0.0997  0.1855  0.0678  0.2247  0.0281  0.1115  0.0495  0.0542  0.1790
 0.0671  0.0223  0.0585  0.5774  0.2098  0.0194  0.0119  0.0190  0.0146
 0.5593  0.0634  0.0536  0.0418  0.0523  0.0948  0.0410  0.0357  0.0581
 0.0647  0.0461  0.0555  0.5002  0.0465  0.0209  0.0271  0.0264  0.2126
 0.1407  0.0213  0.5029  0.1225  0.0907  0.0219  0.0265  0.0314  0.0421
 0.1524  0.0876  0.2157  0.2350  0.0932  0.0497  0.0741  0.0485  0.0439
 0.1798  0.0780  0.0653  0.0573  0.0479  0.0544  0.0329  0.0167  0.4677
 0.0064  0.0054  0.0057  0.0057  0.0054  0.0047  0.0102  0.0044  0.9520
 0.0460  0.0274  0.0443  0.0477  0.0410  0.7074  0.0274  0.0333  0.0255
 0.2540  0.3957  0.0675  0.0702  0.0688  0.0389  0.0302  0.0494  0.0253
 0.0499  0.0411  0.0618  0.6865  0.0369  0.0318  0.0206  0.0303  0.0412
 0.1264  0.0530  0.0861  0.5286  0.0463  0.0332  0.0294  0.0221  0.0749
 0.0425  0.0631  0.0593  0.5273  0.0696  0.0130  0.1114  0.0177  0.0961
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-03 17:32:09,138 train 000 1.100390e-02 -0.287000
2019-11-03 17:34:54,007 train 050 1.012374e-02 -1.129210
2019-11-03 17:37:38,427 train 100 9.755658e-03 -0.985794
2019-11-03 17:40:38,703 train 150 9.589816e-03 -0.951963
2019-11-03 17:43:38,898 train 200 9.566506e-03 -1.710961
2019-11-03 17:46:39,129 train 250 9.583984e-03 -1.636232
2019-11-03 17:49:39,410 train 300 9.663997e-03 -1.571407
2019-11-03 17:52:24,925 train 350 9.683697e-03 -1.493228
2019-11-03 17:55:25,231 train 400 9.736877e-03 -3.214620
2019-11-03 17:58:08,159 train 450 9.722967e-03 -3.012753
2019-11-03 18:00:54,058 train 500 9.737749e-03 -2.829836
2019-11-03 18:03:39,652 train 550 9.755941e-03 -2.695964
2019-11-03 18:06:25,032 train 600 9.742993e-03 -2.656408
2019-11-03 18:09:14,633 train 650 9.792625e-03 -2.556083
2019-11-03 18:12:00,300 train 700 9.806184e-03 -2.445124
2019-11-03 18:14:45,675 train 750 9.797646e-03 -2.508078
2019-11-03 18:17:40,530 train 800 9.805064e-03 -2.407879
2019-11-03 18:20:25,779 train 850 9.803300e-03 -2.323758
2019-11-03 18:23:10,392 train 900 9.834530e-03 -2.253290
2019-11-03 18:25:55,814 train 950 9.816963e-03 -2.186267
2019-11-03 18:28:41,377 train 1000 9.831026e-03 -2.174225
2019-11-03 18:31:26,807 train 1050 9.827759e-03 -2.275311
2019-11-03 18:34:12,260 train 1100 9.808592e-03 -8.493025
2019-11-03 18:36:57,511 train 1150 9.834979e-03 -8.474038
2019-11-03 18:39:42,776 train 1200 9.847042e-03 -8.171000
2019-11-03 18:42:28,331 train 1250 9.863250e-03 -7.882325
2019-11-03 18:45:10,866 train 1300 9.862479e-03 -7.621470
2019-11-03 18:47:53,542 train 1350 9.858359e-03 -7.394552
2019-11-03 18:50:36,202 train 1400 9.867456e-03 -9.341708
2019-11-03 18:53:18,992 train 1450 9.874007e-03 -9.107828
2019-11-03 18:56:01,854 train 1500 9.886614e-03 -8.851520
2019-11-03 18:58:47,481 train 1550 9.873291e-03 -8.604269
2019-11-03 19:01:32,096 train 1600 9.875171e-03 -8.685334
2019-11-03 19:04:15,144 train 1650 9.881203e-03 -8.456827
2019-11-03 19:06:58,142 train 1700 9.881743e-03 -8.252857
2019-11-03 19:09:41,296 train 1750 9.876952e-03 -8.055664
2019-11-03 19:12:34,924 train 1800 9.883528e-03 -7.868450
2019-11-03 19:15:28,354 train 1850 9.901257e-03 -7.683881
2019-11-03 19:18:23,114 train 1900 9.907271e-03 -7.516072
2019-11-03 19:21:17,900 train 1950 9.894811e-03 -7.349515
2019-11-03 19:24:09,106 train 2000 9.883862e-03 -7.195006
2019-11-03 19:26:55,301 train 2050 9.874628e-03 -7.049092
2019-11-03 19:29:41,214 train 2100 9.873242e-03 -6.909151
2019-11-03 19:32:26,782 train 2150 9.865869e-03 -6.785346
2019-11-03 19:35:13,118 train 2200 9.864276e-03 -6.654147
2019-11-03 19:37:59,129 train 2250 9.861960e-03 -6.532338
2019-11-03 19:40:44,698 train 2300 9.857634e-03 -7.659851
2019-11-03 19:43:30,585 train 2350 9.867775e-03 -7.520539
2019-11-03 19:46:16,201 train 2400 9.861895e-03 -7.384095
2019-11-03 19:49:01,548 train 2450 9.861039e-03 -7.259839
2019-11-03 19:51:59,691 train 2500 9.851938e-03 -7.149866
2019-11-03 19:55:00,173 train 2550 9.842173e-03 -7.083271
2019-11-03 19:57:46,525 train 2600 9.847840e-03 -6.974851
2019-11-03 20:00:32,419 train 2650 9.847380e-03 -6.860947
2019-11-03 20:03:18,392 train 2700 9.842900e-03 -6.755083
2019-11-03 20:06:03,815 train 2750 9.839780e-03 -6.656059
2019-11-03 20:08:49,625 train 2800 9.836804e-03 -6.652576
2019-11-03 20:11:35,708 train 2850 9.830256e-03 -6.573749
2019-11-03 20:14:21,803 train 2900 9.825184e-03 -6.475590
2019-11-03 20:17:07,642 train 2950 9.818149e-03 -6.392523
2019-11-03 20:20:00,447 train 3000 9.825338e-03 -6.310734
2019-11-03 20:22:46,829 train 3050 9.830026e-03 -6.229687
2019-11-03 20:24:19,615 training loss; R2: 9.833055e-03 -6.181931
2019-11-03 20:24:20,104 valid 000 1.277579e-02 -1.651853
2019-11-03 20:24:29,595 valid 050 9.415201e-03 -1.298859
2019-11-03 20:24:39,067 valid 100 9.163561e-03 -4.285355
2019-11-03 20:24:49,423 valid 150 9.358824e-03 -9.599071
2019-11-03 20:25:00,771 valid 200 9.328328e-03 -8.047998
2019-11-03 20:25:12,117 valid 250 9.373227e-03 -6.708576
2019-11-03 20:25:23,492 valid 300 9.442421e-03 -22.055876
2019-11-03 20:25:34,172 valid 350 9.433419e-03 -19.073211
2019-11-03 20:25:43,624 valid 400 9.417081e-03 -16.897384
2019-11-03 20:25:53,013 valid 450 9.454627e-03 -15.524239
2019-11-03 20:26:02,425 valid 500 9.399759e-03 -14.085454
2019-11-03 20:26:11,859 valid 550 9.407071e-03 -12.956517
2019-11-03 20:26:21,267 valid 600 9.407482e-03 -11.997567
2019-11-03 20:26:30,699 valid 650 9.398555e-03 -19.469700
2019-11-03 20:26:40,136 valid 700 9.403722e-03 -18.203971
2019-11-03 20:26:49,553 valid 750 9.402856e-03 -17.100028
2019-11-03 20:26:53,104 validation loss; R2: 9.420259e-03 -16.715280
2019-11-03 20:26:53,234 epoch 9 lr 9.299476e-04
2019-11-03 20:26:53,235 genotype = Genotype(normal=[('max_pool_3x3', 0), ('dil_conv_5x5', 1), ('max_pool_3x3', 0), ('dil_conv_5x5', 2), ('max_pool_3x3', 0), ('dil_conv_5x5', 2), ('max_pool_2x2', 2), ('sep_conv_3x3', 3)], normal_concat=range(2, 6), reduce=[('max_pool_2x2', 0), ('max_pool_3x3', 1), ('max_pool_3x3', 0), ('max_pool_3x3', 2), ('dil_conv_5x5', 3), ('max_pool_2x2', 0), ('sep_conv_3x3', 0), ('max_pool_3x3', 2)], reduce_concat=range(2, 6))
2019-11-03 20:26:53,237 
alphas_normal = Variable containing:
 0.0369  0.0325  0.1015  0.6561  0.0203  0.0446  0.0191  0.0636  0.0253
 0.6635  0.0263  0.0246  0.0205  0.0187  0.0641  0.0414  0.0249  0.1162
 0.0380  0.1487  0.2230  0.3487  0.0427  0.0420  0.0540  0.0333  0.0697
 0.7647  0.0372  0.0362  0.0227  0.0261  0.0135  0.0268  0.0247  0.0479
 0.3654  0.0539  0.0401  0.0233  0.0588  0.1223  0.0563  0.0413  0.2387
 0.0413  0.0931  0.1418  0.3617  0.0647  0.1676  0.0395  0.0409  0.0495
 0.5502  0.0299  0.0517  0.0765  0.0248  0.0272  0.0249  0.1752  0.0397
 0.2720  0.0702  0.1045  0.0845  0.0810  0.0393  0.0621  0.0567  0.2297
 0.3060  0.0649  0.0518  0.0402  0.0535  0.0421  0.1909  0.0669  0.1837
 0.0495  0.0319  0.0405  0.2165  0.0282  0.0501  0.2771  0.0655  0.2408
 0.6492  0.0156  0.0176  0.0193  0.0134  0.0224  0.1040  0.0163  0.1421
 0.1192  0.0637  0.3771  0.1825  0.0605  0.0466  0.0461  0.0291  0.0752
 0.2072  0.0417  0.0419  0.0463  0.0324  0.3363  0.1511  0.0953  0.0476
 0.6226  0.0316  0.0286  0.0269  0.0256  0.0388  0.0291  0.0912  0.1056
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-03 20:26:53,238 
alphas_reduce = Variable containing:
 0.0362  0.0325  0.3011  0.1579  0.2970  0.1330  0.0082  0.0190  0.0151
 0.1138  0.1552  0.0627  0.2311  0.0273  0.1178  0.0416  0.0615  0.1889
 0.0833  0.0228  0.0504  0.5278  0.2528  0.0196  0.0128  0.0155  0.0150
 0.6004  0.0800  0.0309  0.0309  0.0304  0.0626  0.0568  0.0509  0.0571
 0.0536  0.0406  0.0634  0.5174  0.0514  0.0228  0.0409  0.0453  0.1647
 0.1313  0.0200  0.5198  0.1416  0.0840  0.0220  0.0242  0.0272  0.0299
 0.2218  0.0932  0.2101  0.2164  0.0723  0.0362  0.0531  0.0451  0.0519
 0.2826  0.0886  0.0657  0.0665  0.0544  0.0366  0.0305  0.0191  0.3559
 0.0087  0.0076  0.0085  0.0087  0.0082  0.0071  0.0097  0.0092  0.9324
 0.0321  0.0190  0.0312  0.0363  0.0327  0.7848  0.0171  0.0268  0.0202
 0.2318  0.4210  0.0389  0.0454  0.0488  0.0462  0.0719  0.0600  0.0359
 0.0576  0.0523  0.0543  0.6413  0.0509  0.0324  0.0331  0.0385  0.0395
 0.1056  0.0557  0.0915  0.5386  0.0479  0.0353  0.0406  0.0272  0.0576
 0.0315  0.0410  0.0542  0.5418  0.0564  0.0195  0.1258  0.0247  0.1052
[torch.cuda.FloatTensor of size 14x9 (GPU 3)]

2019-11-03 20:26:56,823 train 000 9.556019e-03 -1.098783
2019-11-03 20:29:42,164 train 050 9.517931e-03 -7.441897
2019-11-03 20:32:27,653 train 100 9.538480e-03 -4.397486
