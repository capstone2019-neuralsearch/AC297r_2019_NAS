2019-11-04 09:48:33,029 gpu device = 3
2019-11-04 09:48:33,030 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=64, cutout=False, cutout_length=16, data='../data', dataset='GalaxyZoo', drop_path_prob=0.5, epochs=2000, gpu=3, grad_clip=5, init_channels=32, layers=11, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', random=False, report_freq=50, save='eval-GalaxyZoo-Run03-C32_L8_WD1E-5_DROP50_BS64-20191104-094832', seed=0, weight_decay=1e-05)
2019-11-04 09:48:36,492 param size = 11.557605MB
2019-11-04 09:48:36,494 epoch 0 lr 1.000000e-03
2019-11-04 09:48:39,099 train 000 4.462736e-02 -4.656060
2019-11-04 09:48:49,409 train 050 3.559164e-02 -3.989976
2019-11-04 09:48:59,930 train 100 3.171301e-02 -3.213264
2019-11-04 09:49:10,445 train 150 2.901750e-02 -2.334223
2019-11-04 09:49:21,005 train 200 2.745505e-02 -1.954718
2019-11-04 09:49:31,552 train 250 2.661964e-02 -1.800757
2019-11-04 09:49:42,099 train 300 2.579988e-02 -1.634652
2019-11-04 09:49:52,668 train 350 2.524384e-02 -1.548344
2019-11-04 09:50:03,241 train 400 2.507312e-02 -1.451566
2019-11-04 09:50:13,769 train 450 2.468048e-02 -1.373704
2019-11-04 09:50:24,240 train 500 2.483125e-02 -1.311292
2019-11-04 09:50:34,722 train 550 2.459843e-02 -1.267025
2019-11-04 09:50:45,242 train 600 2.439287e-02 -1.219762
2019-11-04 09:50:55,797 train 650 2.422062e-02 -1.162926
2019-11-04 09:51:06,317 train 700 2.407561e-02 -1.133714
2019-11-04 09:51:16,865 train 750 2.383434e-02 -1.146822
2019-11-04 09:51:27,402 train 800 2.361272e-02 -1.120800
2019-11-04 09:51:37,866 train 850 2.338715e-02 -1.146933
2019-11-04 09:51:41,668 training loss; R2: 2.330614e-02 -1.136332
2019-11-04 09:51:42,210 valid 000 1.569024e-02 -0.030146
2019-11-04 09:51:52,433 valid 050 1.725752e-02 -0.426746
2019-11-04 09:52:01,487 validation loss; R2: 1.730521e-02 -0.557618
2019-11-04 09:52:01,569 epoch 1 lr 9.999994e-04
2019-11-04 09:52:02,415 train 000 1.991826e-02 -0.339840
2019-11-04 09:52:12,928 train 050 1.910051e-02 -1.310783
2019-11-04 09:52:23,521 train 100 1.995549e-02 -1.077650
2019-11-04 09:52:34,074 train 150 2.070273e-02 -1.264232
2019-11-04 09:52:44,605 train 200 2.061938e-02 -1.176767
2019-11-04 09:52:55,139 train 250 2.053395e-02 -1.146728
2019-11-04 09:53:05,709 train 300 2.034766e-02 -1.124048
2019-11-04 09:53:16,252 train 350 2.006245e-02 -1.116349
2019-11-04 09:53:26,807 train 400 2.027562e-02 -1.108414
2019-11-04 09:53:37,329 train 450 2.019658e-02 -1.069839
2019-11-04 09:53:47,866 train 500 1.997625e-02 -1.076781
2019-11-04 09:53:58,388 train 550 1.970150e-02 -1.072339
2019-11-04 09:54:08,883 train 600 1.944503e-02 -1.047216
2019-11-04 09:54:19,360 train 650 1.928497e-02 -1.048777
2019-11-04 09:54:29,867 train 700 1.927751e-02 -1.026744
2019-11-04 09:54:40,332 train 750 1.921448e-02 -1.031421
2019-11-04 09:54:50,809 train 800 1.907551e-02 -1.018070
2019-11-04 09:55:01,284 train 850 1.890467e-02 -1.010083
2019-11-04 09:55:04,500 training loss; R2: 1.885528e-02 -1.008528
2019-11-04 09:55:05,119 valid 000 1.640241e-02 -0.029277
2019-11-04 09:55:15,296 valid 050 1.783474e-02 -0.868040
2019-11-04 09:55:24,289 validation loss; R2: 1.797277e-02 -0.980000
2019-11-04 09:55:24,402 epoch 2 lr 9.999975e-04
2019-11-04 09:55:25,084 train 000 1.622531e-02 -0.411858
2019-11-04 09:55:35,706 train 050 1.640949e-02 -0.835298
2019-11-04 09:55:46,461 train 100 1.615754e-02 -1.185157
2019-11-04 09:55:57,041 train 150 1.601210e-02 -1.095464
2019-11-04 09:56:07,568 train 200 1.616937e-02 -0.989229
2019-11-04 09:56:18,167 train 250 1.651621e-02 -0.973981
2019-11-04 09:56:28,780 train 300 1.683814e-02 -0.964028
2019-11-04 09:56:39,341 train 350 1.704203e-02 -0.940263
2019-11-04 09:56:49,923 train 400 1.709492e-02 -0.912643
2019-11-04 09:57:00,488 train 450 1.704999e-02 -0.895361
2019-11-04 09:57:11,116 train 500 1.701657e-02 -0.865392
2019-11-04 09:57:21,758 train 550 1.698440e-02 -0.909264
2019-11-04 09:57:32,369 train 600 1.683232e-02 -0.910212
2019-11-04 09:57:42,998 train 650 1.661168e-02 -0.883011
2019-11-04 09:57:53,558 train 700 1.640516e-02 -0.867470
2019-11-04 09:58:04,134 train 750 1.622961e-02 -0.860617
2019-11-04 09:58:14,746 train 800 1.607855e-02 -0.844233
2019-11-04 09:58:25,367 train 850 1.591755e-02 -0.835972
2019-11-04 09:58:28,587 training loss; R2: 1.587490e-02 -0.830385
2019-11-04 09:58:29,114 valid 000 1.206063e-02 0.118755
2019-11-04 09:58:39,316 valid 050 1.356764e-02 -0.277091
2019-11-04 09:58:48,347 validation loss; R2: 1.362655e-02 -0.394133
2019-11-04 09:58:48,455 epoch 3 lr 9.999944e-04
2019-11-04 09:58:49,167 train 000 1.365861e-02 -0.679166
2019-11-04 09:58:59,815 train 050 1.507810e-02 -0.899534
2019-11-04 09:59:10,427 train 100 1.441217e-02 -0.869609
2019-11-04 09:59:21,003 train 150 1.415193e-02 -0.803732
2019-11-04 09:59:31,612 train 200 1.400476e-02 -0.800706
2019-11-04 09:59:42,228 train 250 1.392340e-02 -0.789665
2019-11-04 09:59:52,841 train 300 1.379200e-02 -0.800005
2019-11-04 10:00:03,435 train 350 1.376493e-02 -0.806849
2019-11-04 10:00:14,037 train 400 1.370589e-02 -0.988436
2019-11-04 10:00:24,564 train 450 1.365059e-02 -0.950456
2019-11-04 10:00:35,094 train 500 1.359755e-02 -0.920779
2019-11-04 10:00:45,640 train 550 1.353105e-02 -0.901107
2019-11-04 10:00:56,199 train 600 1.347757e-02 -0.918032
2019-11-04 10:01:06,745 train 650 1.344065e-02 -0.894387
2019-11-04 10:01:17,280 train 700 1.337975e-02 -0.927128
2019-11-04 10:01:27,958 train 750 1.332436e-02 -0.914834
2019-11-04 10:01:38,660 train 800 1.327224e-02 -0.898715
2019-11-04 10:01:49,258 train 850 1.322584e-02 -0.886922
2019-11-04 10:01:52,487 training loss; R2: 1.322789e-02 -0.880634
2019-11-04 10:01:53,053 valid 000 1.226531e-02 0.065884
2019-11-04 10:02:03,200 valid 050 1.281235e-02 -0.455709
2019-11-04 10:02:12,120 validation loss; R2: 1.278609e-02 -0.623484
2019-11-04 10:02:12,226 epoch 4 lr 9.999901e-04
2019-11-04 10:02:12,971 train 000 1.487191e-02 -1.117409
2019-11-04 10:02:23,545 train 050 1.296177e-02 -0.721374
2019-11-04 10:02:34,081 train 100 1.287887e-02 -0.918632
2019-11-04 10:02:44,651 train 150 1.272034e-02 -0.791950
2019-11-04 10:02:55,230 train 200 1.271704e-02 -0.756075
2019-11-04 10:03:05,806 train 250 1.273811e-02 -0.807198
2019-11-04 10:03:16,365 train 300 1.277092e-02 -0.799813
2019-11-04 10:03:26,850 train 350 1.286517e-02 -0.765532
2019-11-04 10:03:37,322 train 400 1.285854e-02 -0.771047
2019-11-04 10:03:47,835 train 450 1.283772e-02 -0.741744
2019-11-04 10:03:58,371 train 500 1.279105e-02 -0.716492
2019-11-04 10:04:08,906 train 550 1.277323e-02 -0.770733
2019-11-04 10:04:19,405 train 600 1.274326e-02 -0.749092
2019-11-04 10:04:29,901 train 650 1.271908e-02 -0.747523
2019-11-04 10:04:40,448 train 700 1.269391e-02 -0.749531
2019-11-04 10:04:51,019 train 750 1.266951e-02 -0.751932
2019-11-04 10:05:01,598 train 800 1.264169e-02 -0.756685
2019-11-04 10:05:12,166 train 850 1.261004e-02 -0.746066
2019-11-04 10:05:15,367 training loss; R2: 1.260833e-02 -0.742805
2019-11-04 10:05:15,898 valid 000 1.153377e-02 0.164617
2019-11-04 10:05:26,152 valid 050 1.297365e-02 -0.301991
2019-11-04 10:05:35,152 validation loss; R2: 1.308354e-02 -0.410306
2019-11-04 10:05:35,272 epoch 5 lr 9.999846e-04
2019-11-04 10:05:36,042 train 000 1.213337e-02 -1.888143
2019-11-04 10:05:46,616 train 050 1.219194e-02 -0.826325
2019-11-04 10:05:57,251 train 100 1.231899e-02 -1.000632
2019-11-04 10:06:07,862 train 150 1.227697e-02 -0.929453
2019-11-04 10:06:18,412 train 200 1.214669e-02 -0.888954
2019-11-04 10:06:29,008 train 250 1.215170e-02 -0.858280
2019-11-04 10:06:39,628 train 300 1.216256e-02 -0.881016
2019-11-04 10:06:50,192 train 350 1.263136e-02 -0.859394
2019-11-04 10:07:00,781 train 400 1.303206e-02 -0.818429
2019-11-04 10:07:11,348 train 450 1.317311e-02 -0.847891
2019-11-04 10:07:21,936 train 500 1.324220e-02 -0.988814
2019-11-04 10:07:32,509 train 550 1.324160e-02 -0.975965
2019-11-04 10:07:43,104 train 600 1.319104e-02 -0.991206
2019-11-04 10:07:53,678 train 650 1.315724e-02 -0.961509
2019-11-04 10:08:04,276 train 700 1.310513e-02 -0.937174
2019-11-04 10:08:14,859 train 750 1.304932e-02 -0.947294
2019-11-04 10:08:25,443 train 800 1.300598e-02 -0.941001
2019-11-04 10:08:36,007 train 850 1.295418e-02 -0.954906
2019-11-04 10:08:39,212 training loss; R2: 1.294077e-02 -0.946195
2019-11-04 10:08:39,775 valid 000 1.328647e-02 -0.103566
2019-11-04 10:08:49,942 valid 050 1.308435e-02 -0.905471
2019-11-04 10:08:58,953 validation loss; R2: 1.297850e-02 -1.178337
2019-11-04 10:08:59,050 epoch 6 lr 9.999778e-04
2019-11-04 10:08:59,820 train 000 1.122277e-02 -0.672292
2019-11-04 10:09:10,257 train 050 1.215321e-02 -0.739377
2019-11-04 10:09:20,745 train 100 1.221868e-02 -0.832508
2019-11-04 10:09:31,235 train 150 1.216915e-02 -0.841587
2019-11-04 10:09:41,763 train 200 1.212819e-02 -0.837753
2019-11-04 10:09:52,330 train 250 1.205370e-02 -0.775824
2019-11-04 10:10:02,848 train 300 1.202086e-02 -0.796764
2019-11-04 10:10:13,367 train 350 1.205844e-02 -0.788054
2019-11-04 10:10:23,929 train 400 1.207407e-02 -0.809444
2019-11-04 10:10:34,465 train 450 1.206982e-02 -0.802819
2019-11-04 10:10:45,027 train 500 1.203934e-02 -0.799139
2019-11-04 10:10:55,508 train 550 1.201784e-02 -0.782007
2019-11-04 10:11:06,004 train 600 1.204220e-02 -0.755399
2019-11-04 10:11:16,478 train 650 1.204923e-02 -1.909402
2019-11-04 10:11:27,006 train 700 1.206084e-02 -1.833036
2019-11-04 10:11:37,567 train 750 1.207458e-02 -1.735049
2019-11-04 10:11:48,112 train 800 1.207842e-02 -1.683050
2019-11-04 10:11:58,565 train 850 1.206872e-02 -1.610279
2019-11-04 10:12:01,810 training loss; R2: 1.206198e-02 -1.593405
2019-11-04 10:12:02,372 valid 000 1.461954e-02 -0.176665
2019-11-04 10:12:12,504 valid 050 1.491163e-02 -1.009216
2019-11-04 10:12:21,450 validation loss; R2: 1.482100e-02 -1.300260
2019-11-04 10:12:21,559 epoch 7 lr 9.999698e-04
2019-11-04 10:12:22,241 train 000 1.267734e-02 -0.281373
2019-11-04 10:12:32,628 train 050 1.158107e-02 -0.684851
2019-11-04 10:12:43,093 train 100 1.183215e-02 -0.700400
2019-11-04 10:12:53,531 train 150 1.186671e-02 -0.683469
2019-11-04 10:13:04,164 train 200 1.199686e-02 -0.804575
2019-11-04 10:13:14,749 train 250 1.216021e-02 -0.765592
2019-11-04 10:13:25,336 train 300 1.219657e-02 -0.797356
2019-11-04 10:13:35,888 train 350 1.221819e-02 -0.781153
2019-11-04 10:13:46,406 train 400 1.221105e-02 -0.756783
2019-11-04 10:13:56,893 train 450 1.225832e-02 -0.750233
2019-11-04 10:14:07,427 train 500 1.230038e-02 -0.745553
2019-11-04 10:14:17,923 train 550 1.232613e-02 -0.772204
2019-11-04 10:14:28,474 train 600 1.230967e-02 -0.781653
2019-11-04 10:14:38,961 train 650 1.227425e-02 -0.797673
2019-11-04 10:14:49,473 train 700 1.239131e-02 -0.806828
2019-11-04 10:14:59,969 train 750 1.242876e-02 -0.808576
2019-11-04 10:15:10,468 train 800 1.243516e-02 -0.812003
2019-11-04 10:15:20,986 train 850 1.242542e-02 -0.800415
2019-11-04 10:15:24,171 training loss; R2: 1.243138e-02 -0.824234
2019-11-04 10:15:24,701 valid 000 1.961067e-02 -0.415197
2019-11-04 10:15:34,926 valid 050 2.000049e-02 -1.629130
2019-11-04 10:15:43,981 validation loss; R2: 1.984819e-02 -1.956145
2019-11-04 10:15:44,091 epoch 8 lr 9.999605e-04
2019-11-04 10:15:44,820 train 000 1.271620e-02 -0.438631
2019-11-04 10:15:55,287 train 050 1.263705e-02 -0.644800
2019-11-04 10:16:05,800 train 100 1.247217e-02 -0.802705
2019-11-04 10:16:16,380 train 150 1.232162e-02 -0.753692
2019-11-04 10:16:26,898 train 200 1.229507e-02 -0.709677
2019-11-04 10:16:37,454 train 250 1.216415e-02 -0.882330
2019-11-04 10:16:47,988 train 300 1.210464e-02 -0.851939
2019-11-04 10:16:58,447 train 350 1.207204e-02 -0.805984
2019-11-04 10:17:08,882 train 400 1.206152e-02 -0.891030
2019-11-04 10:17:19,329 train 450 1.207479e-02 -0.849228
2019-11-04 10:17:29,854 train 500 1.206064e-02 -0.809407
2019-11-04 10:17:40,402 train 550 1.203636e-02 -0.795878
2019-11-04 10:17:50,936 train 600 1.206975e-02 -0.771199
2019-11-04 10:18:01,457 train 650 1.214336e-02 -0.758896
2019-11-04 10:18:11,965 train 700 1.215825e-02 -0.729976
2019-11-04 10:18:22,446 train 750 1.216075e-02 -0.720294
2019-11-04 10:18:32,993 train 800 1.215399e-02 -0.711734
2019-11-04 10:18:43,559 train 850 1.216184e-02 -0.719941
2019-11-04 10:18:46,814 training loss; R2: 1.216862e-02 -0.718193
2019-11-04 10:18:47,363 valid 000 1.246437e-02 -0.030245
2019-11-04 10:18:57,731 valid 050 1.301838e-02 -0.669400
2019-11-04 10:19:06,848 validation loss; R2: 1.303575e-02 -0.884091
2019-11-04 10:19:06,959 epoch 9 lr 9.999500e-04
2019-11-04 10:19:07,796 train 000 1.280214e-02 -0.200946
2019-11-04 10:19:18,426 train 050 1.208602e-02 -0.762028
2019-11-04 10:19:28,980 train 100 1.196277e-02 -1.583918
2019-11-04 10:19:39,441 train 150 1.194968e-02 -1.380074
2019-11-04 10:19:49,979 train 200 1.197062e-02 -1.231649
2019-11-04 10:20:00,400 train 250 1.196157e-02 -1.097523
2019-11-04 10:20:10,912 train 300 1.191861e-02 -1.053284
2019-11-04 10:20:21,430 train 350 1.189278e-02 -1.014075
2019-11-04 10:20:31,942 train 400 1.185009e-02 -1.014660
2019-11-04 10:20:42,398 train 450 1.182336e-02 -1.013132
2019-11-04 10:20:52,922 train 500 1.182899e-02 -0.962137
2019-11-04 10:21:03,403 train 550 1.183890e-02 -0.936347
2019-11-04 10:21:13,895 train 600 1.181418e-02 -0.927968
2019-11-04 10:21:24,358 train 650 1.178923e-02 -0.914422
2019-11-04 10:21:34,848 train 700 1.178227e-02 -0.922611
2019-11-04 10:21:45,317 train 750 1.179300e-02 -0.920542
2019-11-04 10:21:55,827 train 800 1.179019e-02 -0.911868
2019-11-04 10:22:06,316 train 850 1.177679e-02 -0.889748
2019-11-04 10:22:09,571 training loss; R2: 1.178310e-02 -0.884170
2019-11-04 10:22:10,123 valid 000 1.410501e-02 0.099284
2019-11-04 10:22:20,328 valid 050 1.448329e-02 -0.181886
2019-11-04 10:22:29,355 validation loss; R2: 1.443778e-02 -0.260030
2019-11-04 10:22:29,471 epoch 10 lr 9.999383e-04
2019-11-04 10:22:30,188 train 000 1.063758e-02 -0.523638
2019-11-04 10:22:40,698 train 050 1.191654e-02 -0.630485
2019-11-04 10:22:51,279 train 100 1.192818e-02 -0.630111
2019-11-04 10:23:01,803 train 150 1.185761e-02 -0.625725
2019-11-04 10:23:12,359 train 200 1.182233e-02 -0.691491
2019-11-04 10:23:22,900 train 250 1.173313e-02 -0.776036
2019-11-04 10:23:33,433 train 300 1.177367e-02 -0.721174
2019-11-04 10:23:43,958 train 350 1.179260e-02 -0.731525
2019-11-04 10:23:54,493 train 400 1.180684e-02 -0.721015
2019-11-04 10:24:05,022 train 450 1.191382e-02 -0.700063
2019-11-04 10:24:15,553 train 500 1.195699e-02 -0.685351
2019-11-04 10:24:26,048 train 550 1.200499e-02 -0.676145
2019-11-04 10:24:36,592 train 600 1.199473e-02 -0.672754
2019-11-04 10:24:47,147 train 650 1.199248e-02 -0.696764
2019-11-04 10:24:57,671 train 700 1.199964e-02 -0.716417
2019-11-04 10:25:08,223 train 750 1.198148e-02 -0.714000
2019-11-04 10:25:18,794 train 800 1.197077e-02 -0.705166
2019-11-04 10:25:29,398 train 850 1.195490e-02 -0.705810
2019-11-04 10:25:32,651 training loss; R2: 1.194281e-02 -0.699559
2019-11-04 10:25:33,184 valid 000 1.297202e-02 -0.062957
2019-11-04 10:25:43,324 valid 050 1.314314e-02 -0.842202
2019-11-04 10:25:52,319 validation loss; R2: 1.312941e-02 -1.067780
2019-11-04 10:25:52,431 epoch 11 lr 9.999254e-04
2019-11-04 10:25:53,159 train 000 1.365934e-02 -9.073173
2019-11-04 10:26:03,548 train 050 1.148568e-02 -2.308598
2019-11-04 10:26:13,951 train 100 1.154362e-02 -1.867886
2019-11-04 10:26:24,374 train 150 1.176965e-02 -1.481175
2019-11-04 10:26:34,808 train 200 1.194780e-02 -1.296444
2019-11-04 10:26:45,225 train 250 1.359546e-02 -1.299391
2019-11-04 10:26:55,670 train 300 1.413256e-02 -1.170027
2019-11-04 10:27:06,161 train 350 1.421784e-02 -1.106265
2019-11-04 10:27:16,680 train 400 1.420377e-02 -1.034223
2019-11-04 10:27:27,189 train 450 1.408161e-02 -1.019746
2019-11-04 10:27:37,704 train 500 1.413135e-02 -1.019609
2019-11-04 10:27:48,203 train 550 1.416081e-02 -1.004924
2019-11-04 10:27:58,680 train 600 1.410769e-02 -1.133054
2019-11-04 10:28:09,172 train 650 1.404783e-02 -1.114368
2019-11-04 10:28:19,667 train 700 1.403179e-02 -1.090912
2019-11-04 10:28:30,134 train 750 1.397893e-02 -1.059662
2019-11-04 10:28:40,629 train 800 1.391114e-02 -1.055761
2019-11-04 10:28:51,120 train 850 1.381470e-02 -1.029580
2019-11-04 10:28:54,357 training loss; R2: 1.378726e-02 -1.027769
2019-11-04 10:28:54,880 valid 000 2.049655e-02 -0.265367
2019-11-04 10:29:05,053 valid 050 2.215381e-02 -1.182095
2019-11-04 10:29:14,117 validation loss; R2: 2.192274e-02 -1.399456
2019-11-04 10:29:14,227 epoch 12 lr 9.999112e-04
2019-11-04 10:29:15,002 train 000 1.152589e-02 -0.681926
2019-11-04 10:29:25,685 train 050 1.289801e-02 -0.827098
2019-11-04 10:29:36,401 train 100 1.303177e-02 -0.913483
2019-11-04 10:29:47,098 train 150 1.317397e-02 -0.788161
2019-11-04 10:29:57,770 train 200 1.305774e-02 -0.799993
2019-11-04 10:30:08,419 train 250 1.297675e-02 -0.843880
2019-11-04 10:30:19,068 train 300 1.292540e-02 -0.788840
2019-11-04 10:30:29,758 train 350 1.282644e-02 -0.737615
2019-11-04 10:30:40,450 train 400 1.272248e-02 -0.783039
2019-11-04 10:30:51,158 train 450 1.262546e-02 -0.756675
2019-11-04 10:31:01,899 train 500 1.259589e-02 -0.746255
2019-11-04 10:31:12,587 train 550 1.258201e-02 -0.752553
2019-11-04 10:31:23,217 train 600 1.254066e-02 -0.753594
2019-11-04 10:31:33,801 train 650 1.252457e-02 -0.745049
2019-11-04 10:31:44,326 train 700 1.250126e-02 -0.745990
2019-11-04 10:31:54,799 train 750 1.243860e-02 -0.750579
2019-11-04 10:32:05,275 train 800 1.239736e-02 -0.776370
2019-11-04 10:32:15,731 train 850 1.238404e-02 -0.771149
2019-11-04 10:32:18,940 training loss; R2: 1.237300e-02 -0.765545
2019-11-04 10:32:19,515 valid 000 1.199884e-02 -0.040407
2019-11-04 10:32:29,691 valid 050 1.216393e-02 -0.584773
2019-11-04 10:32:38,678 validation loss; R2: 1.225119e-02 -0.795527
2019-11-04 10:32:38,790 epoch 13 lr 9.998958e-04
2019-11-04 10:32:39,484 train 000 1.210739e-02 -0.260155
2019-11-04 10:32:50,071 train 050 1.223527e-02 -0.525938
2019-11-04 10:33:00,655 train 100 1.219722e-02 -0.496837
2019-11-04 10:33:11,190 train 150 1.215812e-02 -1.316228
2019-11-04 10:33:21,739 train 200 1.210758e-02 -1.209224
2019-11-04 10:33:32,271 train 250 1.209096e-02 -1.144353
2019-11-04 10:33:42,802 train 300 1.213644e-02 -1.066645
2019-11-04 10:33:53,340 train 350 1.217109e-02 -0.969178
2019-11-04 10:34:03,851 train 400 1.212357e-02 -0.916978
2019-11-04 10:34:14,383 train 450 1.210870e-02 -0.877619
2019-11-04 10:34:24,963 train 500 1.207561e-02 -0.888985
2019-11-04 10:34:35,535 train 550 1.205387e-02 -0.882636
2019-11-04 10:34:46,114 train 600 1.204872e-02 -0.902740
2019-11-04 10:34:56,682 train 650 1.202821e-02 -0.872417
2019-11-04 10:35:07,243 train 700 1.203530e-02 -1.109843
2019-11-04 10:35:17,813 train 750 1.202709e-02 -1.127278
2019-11-04 10:35:28,323 train 800 1.201020e-02 -1.095373
2019-11-04 10:35:38,885 train 850 1.199264e-02 -1.061894
2019-11-04 10:35:42,136 training loss; R2: 1.199091e-02 -1.053022
2019-11-04 10:35:42,676 valid 000 1.586349e-02 -0.365220
2019-11-04 10:35:52,894 valid 050 1.777157e-02 -1.662456
2019-11-04 10:36:01,940 validation loss; R2: 1.758613e-02 -2.022002
2019-11-04 10:36:02,063 epoch 14 lr 9.998791e-04
2019-11-04 10:36:02,772 train 000 1.439861e-02 -0.099294
2019-11-04 10:36:13,332 train 050 1.213059e-02 -0.526861
2019-11-04 10:36:23,824 train 100 1.203842e-02 -0.477088
2019-11-04 10:36:34,274 train 150 1.211947e-02 -0.671480
2019-11-04 10:36:44,767 train 200 1.204358e-02 -0.639500
2019-11-04 10:36:55,315 train 250 1.206161e-02 -0.690280
2019-11-04 10:37:05,949 train 300 1.199645e-02 -0.757326
2019-11-04 10:37:16,513 train 350 1.199463e-02 -0.731965
2019-11-04 10:37:27,065 train 400 1.199670e-02 -0.711270
2019-11-04 10:37:37,631 train 450 1.197080e-02 -0.766340
2019-11-04 10:37:48,213 train 500 1.196730e-02 -0.739679
2019-11-04 10:37:58,771 train 550 1.199236e-02 -0.735924
2019-11-04 10:38:09,312 train 600 1.197616e-02 -0.722160
2019-11-04 10:38:19,866 train 650 1.197217e-02 -0.738253
2019-11-04 10:38:30,428 train 700 1.196492e-02 -0.722450
2019-11-04 10:38:40,973 train 750 1.197020e-02 -0.757891
2019-11-04 10:38:51,547 train 800 1.196952e-02 -0.764227
2019-11-04 10:39:02,112 train 850 1.199412e-02 -0.761084
2019-11-04 10:39:05,376 training loss; R2: 1.198786e-02 -0.763022
2019-11-04 10:39:05,915 valid 000 2.262176e-02 -0.098318
2019-11-04 10:39:15,981 valid 050 2.389799e-02 -0.383053
2019-11-04 10:39:24,944 validation loss; R2: 2.437123e-02 -0.394891
2019-11-04 10:39:25,077 epoch 15 lr 9.998612e-04
2019-11-04 10:39:25,811 train 000 1.103964e-02 0.108064
2019-11-04 10:39:36,417 train 050 1.197151e-02 -0.585926
2019-11-04 10:39:46,943 train 100 1.167973e-02 -0.975148
2019-11-04 10:39:57,435 train 150 1.175359e-02 -0.893410
2019-11-04 10:40:07,986 train 200 1.172243e-02 -1.231776
2019-11-04 10:40:18,571 train 250 1.169456e-02 -1.117135
2019-11-04 10:40:29,185 train 300 1.175383e-02 -1.009164
2019-11-04 10:40:39,798 train 350 1.186728e-02 -1.185825
2019-11-04 10:40:50,362 train 400 1.197533e-02 -1.134957
2019-11-04 10:41:00,884 train 450 1.203038e-02 -1.100355
2019-11-04 10:41:11,424 train 500 1.206507e-02 -1.077509
2019-11-04 10:41:21,970 train 550 1.218382e-02 -1.035667
2019-11-04 10:41:32,496 train 600 1.232139e-02 -0.979645
2019-11-04 10:41:43,073 train 650 1.236515e-02 -0.958411
2019-11-04 10:41:53,612 train 700 1.241024e-02 -0.953981
2019-11-04 10:42:04,137 train 750 1.241224e-02 -0.957442
2019-11-04 10:42:14,604 train 800 1.238376e-02 -0.957674
2019-11-04 10:42:25,071 train 850 1.235003e-02 -0.949528
2019-11-04 10:42:28,302 training loss; R2: 1.235673e-02 -1.001470
2019-11-04 10:42:28,835 valid 000 4.213444e-02 -1.788854
2019-11-04 10:42:38,988 valid 050 4.705299e-02 -4.983591
2019-11-04 10:42:47,940 validation loss; R2: 4.668173e-02 -5.750688
2019-11-04 10:42:48,052 epoch 16 lr 9.998421e-04
2019-11-04 10:42:48,774 train 000 1.060424e-02 0.095342
2019-11-04 10:42:59,177 train 050 1.212778e-02 -0.435840
2019-11-04 10:43:09,802 train 100 1.209952e-02 -0.607972
2019-11-04 10:43:20,411 train 150 1.216920e-02 -0.684636
2019-11-04 10:43:30,908 train 200 1.206314e-02 -0.724328
2019-11-04 10:43:41,559 train 250 1.204145e-02 -0.759513
2019-11-04 10:43:52,182 train 300 1.205149e-02 -0.753295
2019-11-04 10:44:02,791 train 350 1.204531e-02 -0.709783
2019-11-04 10:44:13,460 train 400 1.201152e-02 -0.681059
2019-11-04 10:44:24,026 train 450 1.202669e-02 -0.671800
2019-11-04 10:44:34,678 train 500 1.202373e-02 -0.686635
2019-11-04 10:44:45,391 train 550 1.201384e-02 -0.681787
2019-11-04 10:44:56,014 train 600 1.200372e-02 -0.732244
2019-11-04 10:45:06,739 train 650 1.199447e-02 -0.737542
2019-11-04 10:45:17,380 train 700 1.199653e-02 -0.748377
2019-11-04 10:45:28,009 train 750 1.199204e-02 -0.754372
2019-11-04 10:45:38,760 train 800 1.200915e-02 -0.761103
2019-11-04 10:45:49,280 train 850 1.200729e-02 -0.760422
2019-11-04 10:45:52,498 training loss; R2: 1.200250e-02 -0.772657
2019-11-04 10:45:53,037 valid 000 2.082997e-02 -0.156424
2019-11-04 10:46:03,181 valid 050 2.279759e-02 -0.502403
2019-11-04 10:46:12,163 validation loss; R2: 2.285535e-02 -0.575749
2019-11-04 10:46:12,276 epoch 17 lr 9.998217e-04
2019-11-04 10:46:12,972 train 000 1.081185e-02 -0.096176
2019-11-04 10:46:23,514 train 050 1.197854e-02 -0.738399
2019-11-04 10:46:33,981 train 100 1.186011e-02 -0.764617
2019-11-04 10:46:44,523 train 150 1.177489e-02 -0.734945
2019-11-04 10:46:55,098 train 200 1.187746e-02 -0.661508
2019-11-04 10:47:05,636 train 250 1.190991e-02 -0.708476
2019-11-04 10:47:16,157 train 300 1.194616e-02 -0.692553
2019-11-04 10:47:26,724 train 350 1.196724e-02 -0.721173
2019-11-04 10:47:37,319 train 400 1.205577e-02 -0.739015
2019-11-04 10:47:47,907 train 450 1.217753e-02 -0.746767
2019-11-04 10:47:58,508 train 500 1.220493e-02 -0.793801
2019-11-04 10:48:09,109 train 550 1.220823e-02 -0.776922
2019-11-04 10:48:19,713 train 600 1.223117e-02 -0.752433
2019-11-04 10:48:30,267 train 650 1.219726e-02 -0.723404
2019-11-04 10:48:40,821 train 700 1.220570e-02 -0.741848
2019-11-04 10:48:51,433 train 750 1.219191e-02 -0.744711
2019-11-04 10:49:02,040 train 800 1.219547e-02 -0.741188
2019-11-04 10:49:12,628 train 850 1.223747e-02 -0.759023
2019-11-04 10:49:15,877 training loss; R2: 1.223004e-02 -0.758260
2019-11-04 10:49:16,409 valid 000 5.291059e-02 -3.248493
2019-11-04 10:49:26,556 valid 050 5.898556e-02 -9.943379
2019-11-04 10:49:35,589 validation loss; R2: 5.826923e-02 -11.509809
2019-11-04 10:49:35,710 epoch 18 lr 9.998002e-04
2019-11-04 10:49:36,416 train 000 1.160348e-02 -1.336430
2019-11-04 10:49:46,834 train 050 1.234708e-02 -0.481129
2019-11-04 10:49:57,319 train 100 1.231159e-02 -0.482464
2019-11-04 10:50:07,850 train 150 1.226910e-02 -0.550551
2019-11-04 10:50:18,403 train 200 1.219724e-02 -0.592061
2019-11-04 10:50:28,916 train 250 1.215309e-02 -0.642613
2019-11-04 10:50:39,456 train 300 1.216880e-02 -0.639030
2019-11-04 10:50:49,978 train 350 1.218258e-02 -0.629350
2019-11-04 10:51:00,477 train 400 1.219366e-02 -0.633059
2019-11-04 10:51:10,944 train 450 1.222250e-02 -0.603710
2019-11-04 10:51:21,440 train 500 1.229209e-02 -0.639763
2019-11-04 10:51:31,948 train 550 1.227457e-02 -0.641215
2019-11-04 10:51:42,431 train 600 1.225177e-02 -0.643340
2019-11-04 10:51:52,943 train 650 1.224727e-02 -0.656663
2019-11-04 10:52:03,429 train 700 1.222529e-02 -0.654181
2019-11-04 10:52:13,895 train 750 1.221776e-02 -0.657158
2019-11-04 10:52:24,410 train 800 1.220328e-02 -0.663198
2019-11-04 10:52:34,895 train 850 1.220259e-02 -0.664124
2019-11-04 10:52:38,115 training loss; R2: 1.219411e-02 -0.661470
2019-11-04 10:52:38,644 valid 000 3.300864e-02 -0.393114
2019-11-04 10:52:48,996 valid 050 3.302287e-02 -1.391177
2019-11-04 10:52:58,071 validation loss; R2: 3.354257e-02 -1.355452
2019-11-04 10:52:58,189 epoch 19 lr 9.997773e-04
2019-11-04 10:52:58,880 train 000 1.135992e-02 -2.047445
2019-11-04 10:53:09,340 train 050 1.209654e-02 -0.550669
2019-11-04 10:53:19,811 train 100 1.213782e-02 -0.584766
2019-11-04 10:53:30,206 train 150 1.202417e-02 -0.620011
2019-11-04 10:53:40,603 train 200 1.201542e-02 -0.668287
2019-11-04 10:53:50,994 train 250 1.202997e-02 -0.695950
2019-11-04 10:54:01,400 train 300 1.206299e-02 -0.680584
2019-11-04 10:54:11,799 train 350 1.213015e-02 -0.725754
2019-11-04 10:54:22,228 train 400 1.217858e-02 -0.720847
2019-11-04 10:54:32,679 train 450 1.219577e-02 -0.728143
2019-11-04 10:54:43,184 train 500 1.223201e-02 -0.717084
2019-11-04 10:54:53,660 train 550 1.228897e-02 -0.688228
2019-11-04 10:55:04,139 train 600 1.233733e-02 -0.684662
2019-11-04 10:55:14,624 train 650 1.236134e-02 -0.698230
2019-11-04 10:55:25,117 train 700 1.235861e-02 -0.689199
2019-11-04 10:55:35,578 train 750 1.231172e-02 -0.685914
2019-11-04 10:55:46,026 train 800 1.230219e-02 -0.677860
2019-11-04 10:55:56,453 train 850 1.231677e-02 -0.694558
2019-11-04 10:55:59,644 training loss; R2: 1.231024e-02 -0.691097
2019-11-04 10:56:00,246 valid 000 4.044582e-02 -1.755606
2019-11-04 10:56:10,494 valid 050 4.582195e-02 -3.860453
2019-11-04 10:56:19,618 validation loss; R2: 4.532344e-02 -4.367096
2019-11-04 10:56:19,747 epoch 20 lr 9.997533e-04
2019-11-04 10:56:20,537 train 000 1.114009e-02 -0.191350
2019-11-04 10:56:31,026 train 050 1.200047e-02 -0.910721
2019-11-04 10:56:41,627 train 100 1.207073e-02 -0.944285
2019-11-04 10:56:52,278 train 150 1.203881e-02 -0.899970
2019-11-04 10:57:02,861 train 200 1.200253e-02 -0.787639
2019-11-04 10:57:13,442 train 250 1.202425e-02 -0.731851
2019-11-04 10:57:24,042 train 300 1.257784e-02 -1.910931
2019-11-04 10:57:34,646 train 350 1.255337e-02 -1.713751
2019-11-04 10:57:45,292 train 400 1.246848e-02 -1.584496
2019-11-04 10:57:55,920 train 450 1.241795e-02 -1.498130
2019-11-04 10:58:06,560 train 500 1.234336e-02 -1.387375
2019-11-04 10:58:17,159 train 550 1.231285e-02 -1.323485
2019-11-04 10:58:27,744 train 600 1.226647e-02 -1.306171
2019-11-04 10:58:38,306 train 650 1.221913e-02 -1.241604
2019-11-04 10:58:48,890 train 700 1.220209e-02 -1.207686
2019-11-04 10:58:59,538 train 750 1.217264e-02 -1.195227
2019-11-04 10:59:10,144 train 800 1.216116e-02 -1.164266
2019-11-04 10:59:20,743 train 850 1.215455e-02 -1.136668
2019-11-04 10:59:24,011 training loss; R2: 1.216276e-02 -1.124135
2019-11-04 10:59:24,630 valid 000 3.586844e-02 -1.501611
2019-11-04 10:59:34,915 valid 050 4.075847e-02 -1.684758
2019-11-04 10:59:44,029 validation loss; R2: 4.060817e-02 -1.751137
2019-11-04 10:59:44,142 epoch 21 lr 9.997280e-04
2019-11-04 10:59:44,904 train 000 1.335704e-02 -0.539372
2019-11-04 10:59:55,478 train 050 1.204973e-02 -0.705752
2019-11-04 11:00:05,994 train 100 1.217758e-02 -0.791634
2019-11-04 11:00:16,555 train 150 1.215722e-02 -0.677165
2019-11-04 11:00:27,180 train 200 1.193731e-02 -0.708162
2019-11-04 11:00:37,754 train 250 1.190796e-02 -0.701918
2019-11-04 11:00:48,245 train 300 1.185973e-02 -0.660618
2019-11-04 11:00:58,724 train 350 1.181081e-02 -0.678409
2019-11-04 11:01:09,370 train 400 1.182590e-02 -0.656424
2019-11-04 11:01:19,934 train 450 1.186179e-02 -0.672468
2019-11-04 11:01:30,486 train 500 1.190079e-02 -0.747455
2019-11-04 11:01:41,023 train 550 1.192714e-02 -0.731106
2019-11-04 11:01:51,633 train 600 1.193188e-02 -0.727536
2019-11-04 11:02:02,208 train 650 1.194566e-02 -0.741273
2019-11-04 11:02:12,722 train 700 1.200552e-02 -0.805722
2019-11-04 11:02:23,217 train 750 1.201810e-02 -0.789461
2019-11-04 11:02:33,857 train 800 1.201344e-02 -0.786349
2019-11-04 11:02:44,449 train 850 1.201045e-02 -0.768736
2019-11-04 11:02:47,711 training loss; R2: 1.200900e-02 -0.769885
2019-11-04 11:02:48,304 valid 000 2.358742e-02 -0.099551
2019-11-04 11:02:58,585 valid 050 2.783107e-02 -0.707898
2019-11-04 11:03:07,629 validation loss; R2: 2.791126e-02 -0.810361
2019-11-04 11:03:07,751 epoch 22 lr 9.997015e-04
2019-11-04 11:03:08,508 train 000 1.114719e-02 -5.699400
2019-11-04 11:03:19,044 train 050 1.214799e-02 -1.050528
2019-11-04 11:03:29,638 train 100 1.197241e-02 -0.775962
2019-11-04 11:03:40,242 train 150 1.183781e-02 -1.027249
2019-11-04 11:03:50,981 train 200 1.172345e-02 -0.978277
2019-11-04 11:04:01,633 train 250 1.176166e-02 -0.931814
2019-11-04 11:04:12,231 train 300 1.174021e-02 -0.887583
2019-11-04 11:04:22,790 train 350 1.180903e-02 -0.870904
2019-11-04 11:04:33,440 train 400 1.183186e-02 -0.836954
2019-11-04 11:04:44,057 train 450 1.178577e-02 -0.926708
2019-11-04 11:04:54,586 train 500 1.179215e-02 -0.888002
2019-11-04 11:05:05,181 train 550 1.176560e-02 -0.890438
2019-11-04 11:05:15,838 train 600 1.176721e-02 -0.858769
2019-11-04 11:05:26,354 train 650 1.178691e-02 -0.844442
2019-11-04 11:05:36,921 train 700 1.181005e-02 -19.020902
2019-11-04 11:05:47,553 train 750 1.180160e-02 -17.781858
2019-11-04 11:05:58,186 train 800 1.181390e-02 -17.627286
2019-11-04 11:06:08,723 train 850 1.180237e-02 -16.632044
2019-11-04 11:06:12,016 training loss; R2: 1.181058e-02 -16.343668
2019-11-04 11:06:12,561 valid 000 2.405717e-02 -0.001613
2019-11-04 11:06:22,718 valid 050 2.584057e-02 -0.318450
2019-11-04 11:06:31,973 validation loss; R2: 2.614893e-02 -0.348120
2019-11-04 11:06:32,104 epoch 23 lr 9.996737e-04
2019-11-04 11:06:32,813 train 000 1.216160e-02 -0.937682
2019-11-04 11:06:43,330 train 050 1.198294e-02 -0.838943
2019-11-04 11:06:53,853 train 100 1.190553e-02 -0.732156
2019-11-04 11:07:04,490 train 150 1.179059e-02 -0.746729
2019-11-04 11:07:15,150 train 200 1.184134e-02 -0.775561
2019-11-04 11:07:25,819 train 250 1.185755e-02 -0.755211
2019-11-04 11:07:36,359 train 300 1.186182e-02 -0.733124
2019-11-04 11:07:46,945 train 350 1.189218e-02 -0.708525
2019-11-04 11:07:57,522 train 400 1.186330e-02 -0.724375
2019-11-04 11:08:08,064 train 450 1.185894e-02 -0.710897
2019-11-04 11:08:18,647 train 500 1.186511e-02 -0.694549
2019-11-04 11:08:29,140 train 550 1.188460e-02 -0.687087
2019-11-04 11:08:39,606 train 600 1.186544e-02 -0.682218
2019-11-04 11:08:50,022 train 650 1.183483e-02 -0.696680
2019-11-04 11:09:00,478 train 700 1.181576e-02 -0.697544
2019-11-04 11:09:10,919 train 750 1.181210e-02 -0.682868
2019-11-04 11:09:21,378 train 800 1.181608e-02 -7.306029
2019-11-04 11:09:31,826 train 850 1.180779e-02 -6.991137
2019-11-04 11:09:35,025 training loss; R2: 1.179772e-02 -6.880833
2019-11-04 11:09:35,618 valid 000 2.780871e-02 -0.246274
2019-11-04 11:09:45,721 valid 050 2.786264e-02 -0.265200
2019-11-04 11:09:54,627 validation loss; R2: 2.813429e-02 -0.282391
2019-11-04 11:09:54,745 epoch 24 lr 9.996447e-04
2019-11-04 11:09:55,496 train 000 1.262899e-02 -0.098240
2019-11-04 11:10:06,071 train 050 1.161146e-02 -0.615833
2019-11-04 11:10:16,715 train 100 1.145820e-02 -0.631957
2019-11-04 11:10:27,265 train 150 1.153934e-02 -0.680119
2019-11-04 11:10:37,895 train 200 1.155002e-02 -0.690382
2019-11-04 11:10:48,485 train 250 1.154367e-02 -0.698550
2019-11-04 11:10:59,098 train 300 1.155194e-02 -0.701002
2019-11-04 11:11:09,651 train 350 1.154947e-02 -0.708050
2019-11-04 11:11:20,210 train 400 1.153159e-02 -0.699443
2019-11-04 11:11:30,758 train 450 1.153395e-02 -0.726360
2019-11-04 11:11:41,344 train 500 1.153439e-02 -2.598934
2019-11-04 11:11:51,878 train 550 1.153645e-02 -2.465157
2019-11-04 11:12:02,436 train 600 1.154228e-02 -2.342316
2019-11-04 11:12:12,980 train 650 1.154694e-02 -2.216504
2019-11-04 11:12:23,591 train 700 1.159200e-02 -2.124339
2019-11-04 11:12:34,175 train 750 1.159760e-02 -2.013550
2019-11-04 11:12:44,762 train 800 1.158602e-02 -1.938802
2019-11-04 11:12:55,309 train 850 1.158157e-02 -1.871609
2019-11-04 11:12:58,510 training loss; R2: 1.158332e-02 -1.844838
2019-11-04 11:12:59,042 valid 000 4.296348e-02 -1.092606
2019-11-04 11:13:09,199 valid 050 4.357598e-02 -1.852788
2019-11-04 11:13:18,215 validation loss; R2: 4.309050e-02 -2.078252
2019-11-04 11:13:18,331 epoch 25 lr 9.996145e-04
2019-11-04 11:13:19,045 train 000 1.419632e-02 -0.900859
2019-11-04 11:13:29,510 train 050 1.153409e-02 -1.016486
2019-11-04 11:13:39,997 train 100 1.135069e-02 -0.918544
2019-11-04 11:13:50,435 train 150 1.143355e-02 -0.916856
2019-11-04 11:14:00,920 train 200 1.149947e-02 -0.924098
2019-11-04 11:14:11,402 train 250 1.157121e-02 -0.836176
2019-11-04 11:14:21,895 train 300 1.162088e-02 -0.765575
2019-11-04 11:14:32,358 train 350 1.156829e-02 -0.749857
2019-11-04 11:14:42,858 train 400 1.152669e-02 -0.733053
2019-11-04 11:14:53,326 train 450 1.151230e-02 -0.710945
2019-11-04 11:15:03,824 train 500 1.150262e-02 -0.728444
2019-11-04 11:15:14,275 train 550 1.152199e-02 -1.266942
2019-11-04 11:15:24,771 train 600 1.149983e-02 -1.243366
2019-11-04 11:15:35,330 train 650 1.149140e-02 -1.273364
2019-11-04 11:15:45,908 train 700 1.147669e-02 -1.246580
2019-11-04 11:15:56,484 train 750 1.148707e-02 -1.223514
2019-11-04 11:16:07,071 train 800 1.148583e-02 -1.186172
2019-11-04 11:16:17,624 train 850 1.152833e-02 -1.184113
2019-11-04 11:16:20,832 training loss; R2: 1.153521e-02 -1.205331
2019-11-04 11:16:21,381 valid 000 1.441981e-02 0.092980
2019-11-04 11:16:31,615 valid 050 1.416573e-02 -0.248775
2019-11-04 11:16:40,600 validation loss; R2: 1.424164e-02 -0.291014
2019-11-04 11:16:40,713 epoch 26 lr 9.995831e-04
2019-11-04 11:16:41,468 train 000 1.019089e-02 -0.064167
2019-11-04 11:16:51,995 train 050 1.160635e-02 -0.719332
2019-11-04 11:17:02,640 train 100 1.160535e-02 -0.775109
2019-11-04 11:17:13,174 train 150 1.156813e-02 -0.813093
2019-11-04 11:17:23,665 train 200 1.152146e-02 -0.894934
2019-11-04 11:17:34,191 train 250 1.149287e-02 -0.825758
2019-11-04 11:17:44,769 train 300 1.146170e-02 -0.824677
2019-11-04 11:17:55,206 train 350 1.143760e-02 -0.813483
2019-11-04 11:18:05,741 train 400 1.140292e-02 -0.776650
2019-11-04 11:18:16,294 train 450 1.137585e-02 -0.764857
2019-11-04 11:18:26,824 train 500 1.137731e-02 -0.751343
2019-11-04 11:18:37,300 train 550 1.134549e-02 -0.733235
2019-11-04 11:18:47,789 train 600 1.134166e-02 -0.717557
2019-11-04 11:18:58,272 train 650 1.132749e-02 -0.731813
2019-11-04 11:19:08,873 train 700 1.133124e-02 -0.720324
2019-11-04 11:19:19,424 train 750 1.132835e-02 -0.713811
2019-11-04 11:19:29,968 train 800 1.131937e-02 -0.788173
2019-11-04 11:19:40,451 train 850 1.131704e-02 -0.774090
2019-11-04 11:19:43,700 training loss; R2: 1.132283e-02 -0.768093
2019-11-04 11:19:44,246 valid 000 1.656215e-02 0.057060
2019-11-04 11:19:54,372 valid 050 1.647992e-02 -0.167543
2019-11-04 11:20:03,406 validation loss; R2: 1.649716e-02 -0.205103
2019-11-04 11:20:03,505 epoch 27 lr 9.995504e-04
2019-11-04 11:20:04,304 train 000 9.755449e-03 0.073244
2019-11-04 11:20:14,814 train 050 1.174599e-02 -0.976474
2019-11-04 11:20:25,319 train 100 1.164950e-02 -0.671451
2019-11-04 11:20:35,872 train 150 1.159672e-02 -0.678214
2019-11-04 11:20:46,459 train 200 1.151608e-02 -0.709662
2019-11-04 11:20:57,009 train 250 1.146739e-02 -0.670203
2019-11-04 11:21:07,561 train 300 1.148369e-02 -0.750730
2019-11-04 11:21:18,089 train 350 1.149585e-02 -0.740003
2019-11-04 11:21:28,678 train 400 1.148904e-02 -0.744161
2019-11-04 11:21:39,257 train 450 1.150388e-02 -0.736605
2019-11-04 11:21:49,816 train 500 1.150818e-02 -0.717002
2019-11-04 11:22:00,315 train 550 1.151858e-02 -0.712729
2019-11-04 11:22:10,845 train 600 1.157424e-02 -0.698207
2019-11-04 11:22:21,341 train 650 1.159725e-02 -0.693831
2019-11-04 11:22:31,840 train 700 1.160177e-02 -0.686108
2019-11-04 11:22:42,343 train 750 1.160581e-02 -0.705448
2019-11-04 11:22:52,839 train 800 1.160488e-02 -0.700065
2019-11-04 11:23:03,299 train 850 1.159795e-02 -0.692260
2019-11-04 11:23:06,523 training loss; R2: 1.158873e-02 -0.692753
2019-11-04 11:23:07,133 valid 000 1.838744e-02 -0.070345
2019-11-04 11:23:17,389 valid 050 1.769705e-02 -0.548612
2019-11-04 11:23:26,350 validation loss; R2: 1.780077e-02 -0.604776
2019-11-04 11:23:26,467 epoch 28 lr 9.995165e-04
2019-11-04 11:23:27,222 train 000 1.089488e-02 -0.974222
2019-11-04 11:23:37,799 train 050 1.170069e-02 -0.697875
2019-11-04 11:23:48,440 train 100 1.150319e-02 -0.720293
2019-11-04 11:23:59,011 train 150 1.160831e-02 -0.765097
2019-11-04 11:24:09,618 train 200 1.160912e-02 -0.775516
2019-11-04 11:24:20,196 train 250 1.151219e-02 -0.788822
2019-11-04 11:24:30,811 train 300 1.146981e-02 -0.782217
2019-11-04 11:24:41,369 train 350 1.155837e-02 -0.914739
2019-11-04 11:24:51,975 train 400 1.149840e-02 -0.904223
2019-11-04 11:25:02,535 train 450 1.150124e-02 -0.859557
2019-11-04 11:25:13,112 train 500 1.153957e-02 -0.847297
2019-11-04 11:25:23,656 train 550 1.151849e-02 -0.847859
2019-11-04 11:25:34,226 train 600 1.150420e-02 -0.818966
2019-11-04 11:25:44,783 train 650 1.150648e-02 -0.802572
2019-11-04 11:25:55,364 train 700 1.152394e-02 -0.788244
2019-11-04 11:26:05,948 train 750 1.149906e-02 -0.798048
2019-11-04 11:26:16,544 train 800 1.149106e-02 -0.806120
2019-11-04 11:26:27,045 train 850 1.147969e-02 -0.799598
2019-11-04 11:26:30,287 training loss; R2: 1.147482e-02 -0.799580
2019-11-04 11:26:30,822 valid 000 1.220212e-02 0.099812
2019-11-04 11:26:40,967 valid 050 1.425419e-02 -0.370281
2019-11-04 11:26:50,042 validation loss; R2: 1.439671e-02 -0.474604
2019-11-04 11:26:50,158 epoch 29 lr 9.994813e-04
2019-11-04 11:26:50,861 train 000 1.204211e-02 -0.196221
2019-11-04 11:27:01,456 train 050 1.169762e-02 -0.911207
2019-11-04 11:27:12,024 train 100 1.168969e-02 -0.800280
2019-11-04 11:27:22,513 train 150 1.154735e-02 -0.740106
2019-11-04 11:27:33,022 train 200 1.147859e-02 -0.775456
2019-11-04 11:27:43,559 train 250 1.142860e-02 -0.764645
2019-11-04 11:27:54,120 train 300 1.140458e-02 -0.740794
2019-11-04 11:28:04,672 train 350 1.137819e-02 -0.717691
2019-11-04 11:28:15,265 train 400 1.139360e-02 -0.726775
2019-11-04 11:28:25,807 train 450 1.138221e-02 -0.724154
2019-11-04 11:28:36,361 train 500 1.137735e-02 -0.751629
2019-11-04 11:28:46,933 train 550 1.138106e-02 -0.723072
2019-11-04 11:28:57,547 train 600 1.142621e-02 -0.741397
2019-11-04 11:29:08,082 train 650 1.147084e-02 -0.738046
2019-11-04 11:29:18,609 train 700 1.153577e-02 -0.738687
2019-11-04 11:29:29,085 train 750 1.153559e-02 -0.722044
2019-11-04 11:29:39,585 train 800 1.152328e-02 -0.729354
2019-11-04 11:29:50,078 train 850 1.151184e-02 -0.733517
2019-11-04 11:29:53,327 training loss; R2: 1.150928e-02 -0.727212
2019-11-04 11:29:53,956 valid 000 2.086818e-02 -0.116944
2019-11-04 11:30:04,185 valid 050 2.019114e-02 -0.449202
2019-11-04 11:30:13,212 validation loss; R2: 2.032552e-02 -0.476940
2019-11-04 11:30:13,332 epoch 30 lr 9.994449e-04
2019-11-04 11:30:14,025 train 000 1.005250e-02 -1.775582
2019-11-04 11:30:24,675 train 050 1.121557e-02 -0.829546
2019-11-04 11:30:35,317 train 100 1.138153e-02 -0.653314
2019-11-04 11:30:45,909 train 150 1.136305e-02 -0.700654
2019-11-04 11:30:56,442 train 200 1.154716e-02 -0.682579
2019-11-04 11:31:06,924 train 250 1.156541e-02 -0.690970
2019-11-04 11:31:17,500 train 300 1.158261e-02 -0.668975
2019-11-04 11:31:28,057 train 350 1.156873e-02 -0.631299
2019-11-04 11:31:38,630 train 400 1.157671e-02 -0.843642
2019-11-04 11:31:49,186 train 450 1.158893e-02 -0.825975
2019-11-04 11:31:59,727 train 500 1.159289e-02 -0.795193
2019-11-04 11:32:10,281 train 550 1.159463e-02 -0.770766
2019-11-04 11:32:20,835 train 600 1.157733e-02 -0.758638
2019-11-04 11:32:31,381 train 650 1.157156e-02 -0.769042
2019-11-04 11:32:41,930 train 700 1.156547e-02 -0.782826
2019-11-04 11:32:52,442 train 750 1.154731e-02 -0.817711
2019-11-04 11:33:02,987 train 800 1.151852e-02 -0.803797
2019-11-04 11:33:13,560 train 850 1.150277e-02 -0.796795
2019-11-04 11:33:16,755 training loss; R2: 1.149299e-02 -0.798431
2019-11-04 11:33:17,341 valid 000 1.661065e-02 0.003403
2019-11-04 11:33:27,545 valid 050 1.571041e-02 -0.648046
2019-11-04 11:33:36,490 validation loss; R2: 1.572611e-02 -0.735769
2019-11-04 11:33:36,613 epoch 31 lr 9.994073e-04
2019-11-04 11:33:37,359 train 000 1.212184e-02 -1.933594
2019-11-04 11:33:47,850 train 050 1.145045e-02 -0.702681
2019-11-04 11:33:58,337 train 100 1.152025e-02 -0.766649
2019-11-04 11:34:08,771 train 150 1.156875e-02 -0.725047
2019-11-04 11:34:19,219 train 200 1.151481e-02 -0.714763
2019-11-04 11:34:29,674 train 250 1.142205e-02 -0.703333
2019-11-04 11:34:40,147 train 300 1.137125e-02 -0.731121
2019-11-04 11:34:50,624 train 350 1.137192e-02 -0.747975
2019-11-04 11:35:01,168 train 400 1.133507e-02 -0.772112
2019-11-04 11:35:11,702 train 450 1.130027e-02 -0.749626
2019-11-04 11:35:22,273 train 500 1.134100e-02 -0.794023
2019-11-04 11:35:32,827 train 550 1.137339e-02 -0.794682
2019-11-04 11:35:43,426 train 600 1.140676e-02 -0.770390
2019-11-04 11:35:53,955 train 650 1.140796e-02 -0.757413
2019-11-04 11:36:04,530 train 700 1.138670e-02 -0.761488
2019-11-04 11:36:15,051 train 750 1.139769e-02 -0.792418
2019-11-04 11:36:25,564 train 800 1.140673e-02 -0.802973
2019-11-04 11:36:36,058 train 850 1.138288e-02 -0.792465
2019-11-04 11:36:39,248 training loss; R2: 1.138846e-02 -0.791559
2019-11-04 11:36:39,835 valid 000 1.064675e-02 0.040445
2019-11-04 11:36:49,976 valid 050 1.162082e-02 -0.638857
2019-11-04 11:36:59,042 validation loss; R2: 1.161880e-02 -0.851106
2019-11-04 11:36:59,173 epoch 32 lr 9.993685e-04
2019-11-04 11:36:59,957 train 000 1.100061e-02 0.179321
2019-11-04 11:37:10,669 train 050 1.138882e-02 -2.518668
2019-11-04 11:37:21,417 train 100 1.126233e-02 -1.519246
2019-11-04 11:37:32,218 train 150 1.131643e-02 -1.316017
2019-11-04 11:37:43,026 train 200 1.130433e-02 -1.158683
2019-11-04 11:37:53,671 train 250 1.130076e-02 -1.012623
2019-11-04 11:38:04,285 train 300 1.126871e-02 -0.979477
2019-11-04 11:38:14,843 train 350 1.123506e-02 -1.004204
2019-11-04 11:38:25,400 train 400 1.123201e-02 -0.943131
2019-11-04 11:38:35,941 train 450 1.121814e-02 -1.182712
2019-11-04 11:38:46,502 train 500 1.122395e-02 -2.108313
2019-11-04 11:38:57,043 train 550 1.119282e-02 -2.005001
2019-11-04 11:39:07,605 train 600 1.116175e-02 -1.907310
2019-11-04 11:39:18,141 train 650 1.116962e-02 -1.801526
2019-11-04 11:39:28,710 train 700 1.116431e-02 -1.734212
2019-11-04 11:39:39,230 train 750 1.114556e-02 -1.652996
2019-11-04 11:39:49,780 train 800 1.115174e-02 -1.581470
2019-11-04 11:40:00,326 train 850 1.114908e-02 -1.546397
2019-11-04 11:40:03,515 training loss; R2: 1.115346e-02 -1.523899
2019-11-04 11:40:04,096 valid 000 1.439101e-02 -0.250641
2019-11-04 11:40:14,296 valid 050 1.530547e-02 -1.389137
2019-11-04 11:40:23,216 validation loss; R2: 1.536361e-02 -1.708973
2019-11-04 11:40:23,327 epoch 33 lr 9.993284e-04
2019-11-04 11:40:24,022 train 000 1.129587e-02 0.158438
2019-11-04 11:40:34,526 train 050 1.118883e-02 -0.784919
2019-11-04 11:40:45,103 train 100 1.116825e-02 -0.803189
2019-11-04 11:40:55,659 train 150 1.130559e-02 -0.766792
2019-11-04 11:41:06,237 train 200 1.133780e-02 -0.827289
2019-11-04 11:41:16,795 train 250 1.136123e-02 -32.544305
2019-11-04 11:41:27,363 train 300 1.135651e-02 -27.307738
2019-11-04 11:41:37,909 train 350 1.136256e-02 -23.530753
2019-11-04 11:41:48,453 train 400 1.135298e-02 -20.711295
2019-11-04 11:41:58,923 train 450 1.135186e-02 -18.501990
2019-11-04 11:42:09,430 train 500 1.142010e-02 -16.731312
2019-11-04 11:42:19,926 train 550 1.137962e-02 -15.280869
2019-11-04 11:42:30,403 train 600 1.142197e-02 -14.069842
2019-11-04 11:42:40,838 train 650 1.145773e-02 -13.082985
2019-11-04 11:42:51,332 train 700 1.144453e-02 -12.207158
2019-11-04 11:43:01,858 train 750 1.141414e-02 -11.438825
2019-11-04 11:43:12,452 train 800 1.139656e-02 -10.763158
2019-11-04 11:43:23,014 train 850 1.138408e-02 -10.192072
2019-11-04 11:43:26,311 training loss; R2: 1.137752e-02 -10.020567
2019-11-04 11:43:26,942 valid 000 1.197452e-02 -0.027660
2019-11-04 11:43:37,127 valid 050 1.313659e-02 -0.786869
2019-11-04 11:43:46,176 validation loss; R2: 1.303155e-02 -1.002060
2019-11-04 11:43:46,303 epoch 34 lr 9.992871e-04
2019-11-04 11:43:47,077 train 000 9.521375e-03 -0.976762
2019-11-04 11:43:57,608 train 050 1.122569e-02 -0.454910
2019-11-04 11:44:08,141 train 100 1.106616e-02 -0.654603
2019-11-04 11:44:18,627 train 150 1.126100e-02 -0.756369
2019-11-04 11:44:29,108 train 200 1.121140e-02 -0.740204
2019-11-04 11:44:39,552 train 250 1.117576e-02 -0.744304
2019-11-04 11:44:50,005 train 300 1.114199e-02 -1.020717
2019-11-04 11:45:00,432 train 350 1.117597e-02 -0.948366
2019-11-04 11:45:10,896 train 400 1.115543e-02 -0.961281
2019-11-04 11:45:21,334 train 450 1.117552e-02 -0.931560
2019-11-04 11:45:31,773 train 500 1.117537e-02 -6.281634
2019-11-04 11:45:42,208 train 550 1.117741e-02 -5.985077
2019-11-04 11:45:52,651 train 600 1.114867e-02 -5.521408
2019-11-04 11:46:03,092 train 650 1.117533e-02 -5.155346
2019-11-04 11:46:13,535 train 700 1.118832e-02 -4.839942
2019-11-04 11:46:23,957 train 750 1.117911e-02 -4.575727
2019-11-04 11:46:34,372 train 800 1.117911e-02 -4.326843
2019-11-04 11:46:44,787 train 850 1.117643e-02 -4.119683
2019-11-04 11:46:48,003 training loss; R2: 1.118681e-02 -4.055511
2019-11-04 11:46:48,542 valid 000 1.191143e-02 -0.158415
2019-11-04 11:46:58,637 valid 050 1.250954e-02 -1.275969
2019-11-04 11:47:07,576 validation loss; R2: 1.259498e-02 -1.641324
2019-11-04 11:47:07,712 epoch 35 lr 9.992445e-04
2019-11-04 11:47:08,406 train 000 1.256708e-02 -0.344885
2019-11-04 11:47:18,845 train 050 1.102562e-02 -0.839872
2019-11-04 11:47:29,295 train 100 1.100394e-02 -0.761958
2019-11-04 11:47:39,980 train 150 1.111677e-02 -0.760461
2019-11-04 11:47:50,551 train 200 1.110610e-02 -0.784538
2019-11-04 11:48:01,119 train 250 1.105249e-02 -0.804434
2019-11-04 11:48:11,688 train 300 1.106527e-02 -0.750212
2019-11-04 11:48:22,255 train 350 1.101254e-02 -0.799181
2019-11-04 11:48:32,805 train 400 1.099880e-02 -0.794393
2019-11-04 11:48:43,346 train 450 1.097661e-02 -0.776302
2019-11-04 11:48:53,895 train 500 1.099591e-02 -0.736948
2019-11-04 11:49:04,446 train 550 1.101011e-02 -0.735870
2019-11-04 11:49:15,031 train 600 1.099590e-02 -0.878773
2019-11-04 11:49:25,583 train 650 1.098944e-02 -0.881697
2019-11-04 11:49:36,139 train 700 1.101999e-02 -13.513567
2019-11-04 11:49:46,700 train 750 1.101848e-02 -12.647075
2019-11-04 11:49:57,235 train 800 1.103832e-02 -11.898813
2019-11-04 11:50:07,790 train 850 1.106271e-02 -11.223776
2019-11-04 11:50:11,050 training loss; R2: 1.106022e-02 -11.041770
2019-11-04 11:50:11,587 valid 000 1.149688e-02 -0.027200
2019-11-04 11:50:21,715 valid 050 1.232416e-02 -0.876303
2019-11-04 11:50:30,823 validation loss; R2: 1.232488e-02 -1.122671
2019-11-04 11:50:30,936 epoch 36 lr 9.992008e-04
2019-11-04 11:50:31,714 train 000 1.191014e-02 -2.151156
2019-11-04 11:50:42,285 train 050 1.120823e-02 -0.628024
2019-11-04 11:50:52,912 train 100 1.113374e-02 -0.523082
2019-11-04 11:51:03,394 train 150 1.107048e-02 -0.589980
2019-11-04 11:51:13,896 train 200 1.109363e-02 -0.657913
2019-11-04 11:51:24,373 train 250 1.108955e-02 -0.645348
2019-11-04 11:51:34,888 train 300 1.111596e-02 -0.672169
2019-11-04 11:51:45,385 train 350 1.107200e-02 -0.669296
2019-11-04 11:51:55,889 train 400 1.103124e-02 -0.685806
2019-11-04 11:52:06,370 train 450 1.100119e-02 -0.690397
2019-11-04 11:52:16,856 train 500 1.099090e-02 -0.697384
2019-11-04 11:52:27,315 train 550 1.102181e-02 -0.708529
2019-11-04 11:52:37,787 train 600 1.104912e-02 -0.711237
2019-11-04 11:52:48,195 train 650 1.103845e-02 -0.738789
2019-11-04 11:52:58,770 train 700 1.103568e-02 -0.841269
2019-11-04 11:53:09,306 train 750 1.113464e-02 -0.834705
2019-11-04 11:53:19,865 train 800 1.115177e-02 -0.876667
2019-11-04 11:53:30,443 train 850 1.115491e-02 -0.853554
2019-11-04 11:53:33,719 training loss; R2: 1.114548e-02 -0.850527
2019-11-04 11:53:34,310 valid 000 1.403914e-02 -0.301935
2019-11-04 11:53:44,530 valid 050 1.419705e-02 -1.379650
2019-11-04 11:53:53,525 validation loss; R2: 1.411202e-02 -1.724536
2019-11-04 11:53:53,652 epoch 37 lr 9.991558e-04
2019-11-04 11:53:54,404 train 000 9.336958e-03 -0.149335
2019-11-04 11:54:04,996 train 050 1.077099e-02 -0.762788
2019-11-04 11:54:15,572 train 100 1.097767e-02 -0.727003
2019-11-04 11:54:26,159 train 150 1.098722e-02 -0.743108
2019-11-04 11:54:36,758 train 200 1.099422e-02 -0.744701
2019-11-04 11:54:47,344 train 250 1.090098e-02 -0.769463
2019-11-04 11:54:57,938 train 300 1.089524e-02 -0.749650
2019-11-04 11:55:08,470 train 350 1.092819e-02 -1.087021
2019-11-04 11:55:19,047 train 400 1.095587e-02 -1.014865
2019-11-04 11:55:29,605 train 450 1.099281e-02 -0.989955
2019-11-04 11:55:40,188 train 500 1.105401e-02 -0.995553
2019-11-04 11:55:50,773 train 550 1.108709e-02 -0.954081
2019-11-04 11:56:01,363 train 600 1.109971e-02 -0.950709
2019-11-04 11:56:11,922 train 650 1.108503e-02 -0.934064
2019-11-04 11:56:22,506 train 700 1.107875e-02 -0.916188
2019-11-04 11:56:33,093 train 750 1.108155e-02 -0.895796
2019-11-04 11:56:43,617 train 800 1.108791e-02 -0.889382
2019-11-04 11:56:54,109 train 850 1.108763e-02 -0.877709
2019-11-04 11:56:57,438 training loss; R2: 1.108293e-02 -0.872238
2019-11-04 11:56:57,975 valid 000 1.263477e-02 0.142024
2019-11-04 11:57:08,138 valid 050 1.197926e-02 -0.251213
2019-11-04 11:57:17,116 validation loss; R2: 1.199165e-02 -0.354533
2019-11-04 11:57:17,245 epoch 38 lr 9.991095e-04
2019-11-04 11:57:18,006 train 000 1.096239e-02 0.079810
2019-11-04 11:57:28,573 train 050 1.125006e-02 -0.796664
2019-11-04 11:57:39,169 train 100 1.107419e-02 -0.755216
2019-11-04 11:57:49,672 train 150 1.114728e-02 -0.748855
2019-11-04 11:58:00,259 train 200 1.108706e-02 -0.687569
2019-11-04 11:58:10,817 train 250 1.103328e-02 -0.724846
2019-11-04 11:58:21,396 train 300 1.098547e-02 -0.672319
2019-11-04 11:58:31,922 train 350 1.094745e-02 -0.654387
2019-11-04 11:58:42,487 train 400 1.093801e-02 -0.660080
2019-11-04 11:58:53,050 train 450 1.091041e-02 -0.677086
2019-11-04 11:59:03,632 train 500 1.090093e-02 -0.947994
2019-11-04 11:59:14,244 train 550 1.088663e-02 -0.912075
2019-11-04 11:59:24,861 train 600 1.088307e-02 -0.895561
2019-11-04 11:59:35,428 train 650 1.089765e-02 -0.909741
2019-11-04 11:59:45,912 train 700 1.092376e-02 -0.921620
2019-11-04 11:59:56,388 train 750 1.092878e-02 -0.891369
2019-11-04 12:00:06,851 train 800 1.093026e-02 -0.880273
2019-11-04 12:00:17,314 train 850 1.094341e-02 -0.874364
2019-11-04 12:00:20,544 training loss; R2: 1.094295e-02 -0.869058
2019-11-04 12:00:21,132 valid 000 1.056681e-02 -0.159062
2019-11-04 12:00:31,285 valid 050 1.246589e-02 -1.193065
2019-11-04 12:00:40,278 validation loss; R2: 1.242422e-02 -1.492983
2019-11-04 12:00:40,394 epoch 39 lr 9.990621e-04
2019-11-04 12:00:41,166 train 000 1.206230e-02 -1.026082
2019-11-04 12:00:51,691 train 050 1.081398e-02 -0.963407
2019-11-04 12:01:02,346 train 100 1.100143e-02 -0.737372
2019-11-04 12:01:12,955 train 150 1.091136e-02 -0.696114
2019-11-04 12:01:23,574 train 200 1.093828e-02 -0.638596
2019-11-04 12:01:34,190 train 250 1.088418e-02 -0.625712
2019-11-04 12:01:44,809 train 300 1.093245e-02 -0.617343
2019-11-04 12:01:55,393 train 350 1.093560e-02 -0.947582
2019-11-04 12:02:05,992 train 400 1.091903e-02 -0.985116
2019-11-04 12:02:16,484 train 450 1.094038e-02 -0.969870
2019-11-04 12:02:26,940 train 500 1.093109e-02 -0.996766
2019-11-04 12:02:37,351 train 550 1.090357e-02 -0.967075
2019-11-04 12:02:47,781 train 600 1.095568e-02 -0.961461
2019-11-04 12:02:58,225 train 650 1.100666e-02 -0.925299
2019-11-04 12:03:08,697 train 700 1.101608e-02 -0.899067
2019-11-04 12:03:19,148 train 750 1.101846e-02 -0.892164
2019-11-04 12:03:29,602 train 800 1.101282e-02 -0.898018
2019-11-04 12:03:40,031 train 850 1.099757e-02 -0.946194
2019-11-04 12:03:43,258 training loss; R2: 1.099974e-02 -0.941850
2019-11-04 12:03:43,790 valid 000 2.246694e-02 -0.282574
2019-11-04 12:03:53,898 valid 050 2.291314e-02 -0.964425
2019-11-04 12:04:02,841 validation loss; R2: 2.292593e-02 -1.146184
2019-11-04 12:04:02,967 epoch 40 lr 9.990134e-04
2019-11-04 12:04:03,723 train 000 9.885844e-03 -2.159852
2019-11-04 12:04:14,164 train 050 1.078238e-02 -0.717514
2019-11-04 12:04:24,606 train 100 1.083060e-02 -0.757742
2019-11-04 12:04:35,070 train 150 1.080037e-02 -0.774321
2019-11-04 12:04:45,636 train 200 1.089067e-02 -0.831304
2019-11-04 12:04:56,141 train 250 1.093066e-02 -0.855328
2019-11-04 12:05:06,570 train 300 1.092257e-02 -0.858190
2019-11-04 12:05:17,041 train 350 1.095330e-02 -0.825084
2019-11-04 12:05:27,603 train 400 1.095923e-02 -0.789890
2019-11-04 12:05:38,067 train 450 1.094136e-02 -0.781651
2019-11-04 12:05:48,506 train 500 1.094599e-02 -0.753467
2019-11-04 12:05:59,005 train 550 1.095132e-02 -0.744541
2019-11-04 12:06:09,567 train 600 1.097249e-02 -0.733351
2019-11-04 12:06:19,994 train 650 1.098142e-02 -0.722997
2019-11-04 12:06:30,429 train 700 1.096892e-02 -0.739627
2019-11-04 12:06:40,933 train 750 1.098153e-02 -0.750940
2019-11-04 12:06:51,432 train 800 1.098082e-02 -0.752642
2019-11-04 12:07:01,845 train 850 1.099376e-02 -0.735466
2019-11-04 12:07:05,039 training loss; R2: 1.098922e-02 -0.737112
2019-11-04 12:07:05,633 valid 000 1.062296e-02 0.025539
2019-11-04 12:07:15,838 valid 050 1.104082e-02 -0.732808
2019-11-04 12:07:24,805 validation loss; R2: 1.096632e-02 -0.949317
2019-11-04 12:07:24,925 epoch 41 lr 9.989634e-04
2019-11-04 12:07:25,667 train 000 1.015295e-02 -0.526508
2019-11-04 12:07:36,230 train 050 1.091474e-02 -2.520321
2019-11-04 12:07:46,790 train 100 1.087343e-02 -1.523957
2019-11-04 12:07:57,384 train 150 1.087707e-02 -1.244851
2019-11-04 12:08:07,916 train 200 1.092600e-02 -1.111462
2019-11-04 12:08:18,455 train 250 1.088878e-02 -1.054320
2019-11-04 12:08:29,021 train 300 1.088491e-02 -1.017843
2019-11-04 12:08:39,561 train 350 1.087046e-02 -0.995273
2019-11-04 12:08:50,170 train 400 1.084780e-02 -0.948959
2019-11-04 12:09:00,756 train 450 1.085377e-02 -0.904950
2019-11-04 12:09:11,354 train 500 1.089286e-02 -0.886493
2019-11-04 12:09:21,931 train 550 1.089069e-02 -0.884496
2019-11-04 12:09:32,532 train 600 1.087862e-02 -0.890575
2019-11-04 12:09:43,115 train 650 1.085911e-02 -0.895371
2019-11-04 12:09:53,712 train 700 1.085992e-02 -0.885230
2019-11-04 12:10:04,277 train 750 1.086729e-02 -0.872756
2019-11-04 12:10:14,830 train 800 1.086612e-02 -0.859237
2019-11-04 12:10:25,392 train 850 1.085960e-02 -0.839198
2019-11-04 12:10:28,622 training loss; R2: 1.085559e-02 -0.836736
2019-11-04 12:10:29,205 valid 000 1.105157e-02 -0.047463
2019-11-04 12:10:39,310 valid 050 1.159281e-02 -0.850182
2019-11-04 12:10:48,248 validation loss; R2: 1.151079e-02 -1.124839
2019-11-04 12:10:48,354 epoch 42 lr 9.989123e-04
2019-11-04 12:10:49,107 train 000 1.128270e-02 0.056399
2019-11-04 12:10:59,646 train 050 1.073084e-02 -0.707250
2019-11-04 12:11:10,236 train 100 1.070510e-02 -0.722114
2019-11-04 12:11:20,740 train 150 1.082396e-02 -0.740435
2019-11-04 12:11:31,182 train 200 1.084635e-02 -0.716761
2019-11-04 12:11:41,612 train 250 1.085102e-02 -0.719860
2019-11-04 12:11:52,045 train 300 1.102131e-02 -0.944186
2019-11-04 12:12:02,523 train 350 1.113257e-02 -0.960492
2019-11-04 12:12:13,072 train 400 1.112806e-02 -0.941837
2019-11-04 12:12:23,616 train 450 1.110872e-02 -0.918810
2019-11-04 12:12:34,052 train 500 1.106050e-02 -0.895069
2019-11-04 12:12:44,465 train 550 1.103920e-02 -0.873335
2019-11-04 12:12:54,890 train 600 1.102981e-02 -0.878666
2019-11-04 12:13:05,321 train 650 1.100293e-02 -0.861355
2019-11-04 12:13:15,807 train 700 1.100984e-02 -0.846098
2019-11-04 12:13:26,289 train 750 1.100493e-02 -0.866356
2019-11-04 12:13:36,795 train 800 1.101178e-02 -0.873823
2019-11-04 12:13:47,371 train 850 1.102531e-02 -0.865976
2019-11-04 12:13:50,607 training loss; R2: 1.119717e-02 -0.860769
2019-11-04 12:13:51,141 valid 000 4.671537e-02 -2.026013
2019-11-04 12:14:01,366 valid 050 5.129066e-02 -5.692520
2019-11-04 12:14:10,393 validation loss; R2: 5.074326e-02 -6.657813
2019-11-04 12:14:10,502 epoch 43 lr 9.988599e-04
2019-11-04 12:14:11,189 train 000 1.594331e-02 -1.670807
2019-11-04 12:14:21,737 train 050 1.179675e-02 -0.355855
2019-11-04 12:14:32,214 train 100 1.137536e-02 -0.566106
2019-11-04 12:14:42,648 train 150 1.124493e-02 -0.654857
2019-11-04 12:14:53,107 train 200 1.111777e-02 -0.607851
2019-11-04 12:15:03,702 train 250 1.107460e-02 -0.592072
2019-11-04 12:15:14,302 train 300 1.109411e-02 -0.582042
2019-11-04 12:15:24,887 train 350 1.103540e-02 -0.585429
2019-11-04 12:15:35,443 train 400 1.100842e-02 -0.641146
2019-11-04 12:15:45,992 train 450 1.101367e-02 -0.667333
2019-11-04 12:15:56,567 train 500 1.098324e-02 -0.726622
2019-11-04 12:16:07,114 train 550 1.096407e-02 -0.706067
2019-11-04 12:16:17,681 train 600 1.093882e-02 -0.718688
2019-11-04 12:16:28,220 train 650 1.093400e-02 -0.740616
2019-11-04 12:16:38,793 train 700 1.091637e-02 -0.730727
2019-11-04 12:16:49,344 train 750 1.089989e-02 -3.880202
2019-11-04 12:16:59,922 train 800 1.088677e-02 -3.677081
2019-11-04 12:17:10,478 train 850 1.087420e-02 -3.864165
2019-11-04 12:17:13,722 training loss; R2: 1.086519e-02 -3.799020
2019-11-04 12:17:14,263 valid 000 1.153737e-02 -0.056234
2019-11-04 12:17:24,497 valid 050 1.130486e-02 -0.910123
2019-11-04 12:17:33,546 validation loss; R2: 1.123051e-02 -1.176153
2019-11-04 12:17:33,665 epoch 44 lr 9.988063e-04
2019-11-04 12:17:34,415 train 000 1.093355e-02 -0.695026
2019-11-04 12:17:44,918 train 050 1.068411e-02 -0.748138
2019-11-04 12:17:55,460 train 100 1.069008e-02 -0.770071
2019-11-04 12:18:06,049 train 150 1.075359e-02 -0.762280
2019-11-04 12:18:16,509 train 200 1.076410e-02 -0.710289
2019-11-04 12:18:27,017 train 250 1.078935e-02 -0.683842
2019-11-04 12:18:37,588 train 300 1.080610e-02 -0.683492
2019-11-04 12:18:48,069 train 350 1.079802e-02 -0.664227
2019-11-04 12:18:58,538 train 400 1.079431e-02 -0.643453
2019-11-04 12:19:09,056 train 450 1.078293e-02 -0.688558
2019-11-04 12:19:19,579 train 500 1.081970e-02 -0.684949
2019-11-04 12:19:29,952 train 550 1.084708e-02 -0.666314
2019-11-04 12:19:40,511 train 600 1.084082e-02 -0.674319
2019-11-04 12:19:50,961 train 650 1.084634e-02 -0.702348
2019-11-04 12:20:01,395 train 700 1.082440e-02 -0.690036
2019-11-04 12:20:11,959 train 750 1.078947e-02 -0.686631
2019-11-04 12:20:22,492 train 800 1.078382e-02 -0.702773
2019-11-04 12:20:32,949 train 850 1.077597e-02 -0.710954
2019-11-04 12:20:36,176 training loss; R2: 1.076963e-02 -0.713285
2019-11-04 12:20:36,773 valid 000 1.183267e-02 -0.097081
2019-11-04 12:20:46,932 valid 050 1.225103e-02 -1.149209
2019-11-04 12:20:55,983 validation loss; R2: 1.220108e-02 -1.454582
2019-11-04 12:20:56,097 epoch 45 lr 9.987514e-04
2019-11-04 12:20:56,853 train 000 8.694972e-03 -1.719677
2019-11-04 12:21:07,452 train 050 1.048883e-02 -0.781899
2019-11-04 12:21:17,958 train 100 1.102021e-02 -0.674699
2019-11-04 12:21:28,408 train 150 1.108555e-02 -0.698634
2019-11-04 12:21:38,977 train 200 1.115907e-02 -0.794456
2019-11-04 12:21:49,555 train 250 1.104822e-02 -0.863980
2019-11-04 12:22:00,152 train 300 1.102291e-02 -0.812913
2019-11-04 12:22:10,713 train 350 1.100733e-02 -0.773605
2019-11-04 12:22:21,272 train 400 1.097130e-02 -0.796679
2019-11-04 12:22:31,749 train 450 1.097810e-02 -0.800274
2019-11-04 12:22:42,278 train 500 1.097943e-02 -0.787778
2019-11-04 12:22:52,766 train 550 1.097554e-02 -0.857570
2019-11-04 12:23:03,333 train 600 1.096900e-02 -0.891042
2019-11-04 12:23:13,890 train 650 1.096575e-02 -0.927992
2019-11-04 12:23:24,451 train 700 1.094664e-02 -0.907160
2019-11-04 12:23:34,972 train 750 1.094802e-02 -0.884328
2019-11-04 12:23:45,513 train 800 1.093201e-02 -0.876392
2019-11-04 12:23:56,061 train 850 1.092412e-02 -0.870070
2019-11-04 12:23:59,299 training loss; R2: 1.092564e-02 -0.868942
2019-11-04 12:23:59,850 valid 000 1.018696e-02 0.131209
2019-11-04 12:24:10,118 valid 050 1.128181e-02 -0.459377
2019-11-04 12:24:19,178 validation loss; R2: 1.137816e-02 -0.606552
2019-11-04 12:24:19,287 epoch 46 lr 9.986953e-04
2019-11-04 12:24:20,000 train 000 9.984476e-03 -0.371284
2019-11-04 12:24:30,503 train 050 1.090057e-02 -0.334767
2019-11-04 12:24:41,031 train 100 1.093812e-02 -0.535197
2019-11-04 12:24:51,587 train 150 1.088014e-02 -0.593326
2019-11-04 12:25:02,146 train 200 1.081864e-02 -0.688688
2019-11-04 12:25:12,739 train 250 1.079181e-02 -0.640013
2019-11-04 12:25:23,347 train 300 1.081904e-02 -0.719666
2019-11-04 12:25:33,947 train 350 1.083628e-02 -0.724544
2019-11-04 12:25:44,554 train 400 1.084835e-02 -1.719433
2019-11-04 12:25:55,101 train 450 1.085180e-02 -63.063177
2019-11-04 12:26:05,693 train 500 1.085190e-02 -57.052316
2019-11-04 12:26:16,241 train 550 1.082682e-02 -51.933086
2019-11-04 12:26:26,823 train 600 1.082039e-02 -47.653557
2019-11-04 12:26:37,389 train 650 1.082499e-02 -44.031328
2019-11-04 12:26:47,977 train 700 1.084445e-02 -40.961580
2019-11-04 12:26:58,547 train 750 1.083638e-02 -38.273492
2019-11-04 12:27:09,088 train 800 1.081538e-02 -35.938417
2019-11-04 12:27:19,657 train 850 1.081427e-02 -33.879890
2019-11-04 12:27:22,910 training loss; R2: 1.083181e-02 -33.293168
2019-11-04 12:27:23,443 valid 000 1.116958e-02 -0.083389
2019-11-04 12:27:33,651 valid 050 1.156308e-02 -1.013713
2019-11-04 12:27:42,823 validation loss; R2: 1.154619e-02 -1.305209
2019-11-04 12:27:42,937 epoch 47 lr 9.986380e-04
2019-11-04 12:27:43,667 train 000 1.328407e-02 -0.015888
2019-11-04 12:27:54,314 train 050 1.056324e-02 -0.683592
2019-11-04 12:28:04,838 train 100 1.070847e-02 -0.668844
2019-11-04 12:28:15,423 train 150 1.076007e-02 -0.644272
2019-11-04 12:28:25,998 train 200 1.087630e-02 -0.664419
2019-11-04 12:28:36,512 train 250 1.088313e-02 -0.704348
2019-11-04 12:28:47,034 train 300 1.079240e-02 -0.739675
2019-11-04 12:28:57,547 train 350 1.082003e-02 -0.726808
2019-11-04 12:29:08,097 train 400 1.081933e-02 -0.682397
2019-11-04 12:29:18,594 train 450 1.081336e-02 -0.773175
2019-11-04 12:29:29,126 train 500 1.083548e-02 -0.764482
2019-11-04 12:29:39,642 train 550 1.088028e-02 -0.788994
2019-11-04 12:29:50,118 train 600 1.086094e-02 -0.853323
2019-11-04 12:30:00,567 train 650 1.088953e-02 -0.840089
2019-11-04 12:30:11,041 train 700 1.088059e-02 -0.821494
2019-11-04 12:30:21,478 train 750 1.086279e-02 -0.815770
2019-11-04 12:30:31,956 train 800 1.085060e-02 -0.839664
2019-11-04 12:30:42,408 train 850 1.083680e-02 -0.813977
2019-11-04 12:30:45,615 training loss; R2: 1.082754e-02 -0.812998
2019-11-04 12:30:46,204 valid 000 1.215582e-02 -0.114088
2019-11-04 12:30:56,363 valid 050 1.317954e-02 -1.108456
2019-11-04 12:31:05,364 validation loss; R2: 1.316215e-02 -1.414017
2019-11-04 12:31:05,490 epoch 48 lr 9.985795e-04
2019-11-04 12:31:06,205 train 000 1.051045e-02 0.203096
2019-11-04 12:31:16,766 train 050 1.084911e-02 -0.774281
2019-11-04 12:31:27,249 train 100 1.073901e-02 -0.654802
2019-11-04 12:31:37,716 train 150 1.079250e-02 -1.404120
2019-11-04 12:31:48,211 train 200 1.073609e-02 -1.171729
2019-11-04 12:31:58,655 train 250 1.079227e-02 -1.044244
2019-11-04 12:32:09,117 train 300 1.079588e-02 -0.965809
2019-11-04 12:32:19,554 train 350 1.081059e-02 -0.936487
2019-11-04 12:32:29,972 train 400 1.077641e-02 -0.902822
2019-11-04 12:32:40,399 train 450 1.074945e-02 -0.905409
2019-11-04 12:32:50,866 train 500 1.075606e-02 -0.858356
2019-11-04 12:33:01,288 train 550 1.073317e-02 -0.845821
2019-11-04 12:33:11,709 train 600 1.073568e-02 -0.825646
2019-11-04 12:33:22,148 train 650 1.072991e-02 -0.803916
2019-11-04 12:33:32,627 train 700 1.071354e-02 -0.792098
2019-11-04 12:33:43,067 train 750 1.072416e-02 -0.780281
2019-11-04 12:33:53,547 train 800 1.072826e-02 -0.781446
2019-11-04 12:34:03,994 train 850 1.072915e-02 -0.765355
2019-11-04 12:34:07,200 training loss; R2: 1.073190e-02 -0.765668
2019-11-04 12:34:07,795 valid 000 1.055232e-02 0.117267
2019-11-04 12:34:18,020 valid 050 1.096669e-02 -0.523552
2019-11-04 12:34:26,917 validation loss; R2: 1.085223e-02 -0.688690
2019-11-04 12:34:27,033 epoch 49 lr 9.985197e-04
2019-11-04 12:34:27,792 train 000 1.270754e-02 0.220971
2019-11-04 12:34:38,282 train 050 1.094871e-02 -0.743450
2019-11-04 12:34:48,747 train 100 1.086781e-02 -0.797003
2019-11-04 12:34:59,239 train 150 1.076194e-02 -0.708358
2019-11-04 12:35:09,695 train 200 1.078047e-02 -0.739513
2019-11-04 12:35:20,116 train 250 1.079005e-02 -0.761831
2019-11-04 12:35:30,558 train 300 1.084446e-02 -0.878066
2019-11-04 12:35:41,082 train 350 1.082288e-02 -0.828205
2019-11-04 12:35:51,554 train 400 1.087267e-02 -0.800988
2019-11-04 12:36:02,001 train 450 1.087796e-02 -0.781255
2019-11-04 12:36:12,469 train 500 1.087148e-02 -0.786838
2019-11-04 12:36:22,902 train 550 1.087430e-02 -0.768924
2019-11-04 12:36:33,365 train 600 1.089373e-02 -0.760065
2019-11-04 12:36:43,807 train 650 1.090069e-02 -0.760020
2019-11-04 12:36:54,278 train 700 1.092355e-02 -0.746159
2019-11-04 12:37:04,744 train 750 1.094126e-02 -0.742438
2019-11-04 12:37:15,211 train 800 1.094641e-02 -0.744818
2019-11-04 12:37:25,647 train 850 1.092760e-02 -0.733685
2019-11-04 12:37:28,863 training loss; R2: 1.092385e-02 -0.755752
2019-11-04 12:37:29,406 valid 000 1.233227e-02 -0.145014
2019-11-04 12:37:39,650 valid 050 1.332546e-02 -1.067828
2019-11-04 12:37:48,648 validation loss; R2: 1.333436e-02 -1.359061
2019-11-04 12:37:48,772 epoch 50 lr 9.984587e-04
2019-11-04 12:37:49,530 train 000 1.004080e-02 -0.038431
2019-11-04 12:38:00,040 train 050 1.074873e-02 -0.801283
2019-11-04 12:38:10,464 train 100 1.070205e-02 -0.767233
2019-11-04 12:38:20,844 train 150 1.069807e-02 -0.800376
2019-11-04 12:38:31,282 train 200 1.071789e-02 -0.797915
2019-11-04 12:38:41,759 train 250 1.070564e-02 -0.786494
2019-11-04 12:38:52,288 train 300 1.077376e-02 -0.741499
2019-11-04 12:39:02,798 train 350 1.074493e-02 -0.745590
2019-11-04 12:39:13,312 train 400 1.075572e-02 -0.705108
2019-11-04 12:39:23,806 train 450 1.074493e-02 -0.704824
2019-11-04 12:39:34,303 train 500 1.075573e-02 -0.728299
2019-11-04 12:39:44,756 train 550 1.078854e-02 -0.706213
2019-11-04 12:39:55,194 train 600 1.081297e-02 -0.711750
2019-11-04 12:40:05,632 train 650 1.080517e-02 -0.695548
2019-11-04 12:40:16,109 train 700 1.080704e-02 -0.691024
2019-11-04 12:40:26,568 train 750 1.080357e-02 -0.816548
2019-11-04 12:40:37,008 train 800 1.080919e-02 -0.804653
2019-11-04 12:40:47,459 train 850 1.081781e-02 -0.803446
2019-11-04 12:40:50,679 training loss; R2: 1.081962e-02 -0.797009
2019-11-04 12:40:51,206 valid 000 9.954466e-03 0.037414
2019-11-04 12:41:01,583 valid 050 1.056819e-02 -0.759513
2019-11-04 12:41:10,684 validation loss; R2: 1.052212e-02 -0.978457
2019-11-04 12:41:10,816 epoch 51 lr 9.983964e-04
2019-11-04 12:41:11,576 train 000 9.834281e-03 -0.407557
2019-11-04 12:41:22,214 train 050 1.091003e-02 -0.582351
2019-11-04 12:41:32,898 train 100 1.095576e-02 -0.611298
2019-11-04 12:41:43,466 train 150 1.089958e-02 -0.572274
2019-11-04 12:41:54,072 train 200 1.086557e-02 -0.678537
2019-11-04 12:42:04,723 train 250 1.083142e-02 -0.719437
2019-11-04 12:42:15,350 train 300 1.080438e-02 -0.690427
2019-11-04 12:42:25,885 train 350 1.079245e-02 -0.691931
2019-11-04 12:42:36,383 train 400 1.085632e-02 -0.652554
2019-11-04 12:42:46,879 train 450 1.085817e-02 -0.640334
2019-11-04 12:42:57,373 train 500 1.085437e-02 -0.655332
2019-11-04 12:43:07,935 train 550 1.086537e-02 -0.887773
2019-11-04 12:43:18,520 train 600 1.088764e-02 -0.869539
2019-11-04 12:43:29,070 train 650 1.088116e-02 -0.866735
2019-11-04 12:43:39,652 train 700 1.086674e-02 -0.847260
2019-11-04 12:43:50,229 train 750 1.085067e-02 -0.829811
2019-11-04 12:44:00,819 train 800 1.086382e-02 -0.813418
2019-11-04 12:44:11,393 train 850 1.088378e-02 -0.804282
2019-11-04 12:44:14,636 training loss; R2: 1.088398e-02 -0.804637
2019-11-04 12:44:15,230 valid 000 9.140628e-03 0.007396
2019-11-04 12:44:25,514 valid 050 1.065019e-02 -0.985250
2019-11-04 12:44:34,502 validation loss; R2: 1.068760e-02 -1.259807
2019-11-04 12:44:34,618 epoch 52 lr 9.983330e-04
2019-11-04 12:44:35,318 train 000 1.134646e-02 -1.397185
2019-11-04 12:44:45,892 train 050 1.101896e-02 -0.954170
2019-11-04 12:44:56,344 train 100 1.095991e-02 -0.796179
2019-11-04 12:45:06,782 train 150 1.097933e-02 -0.630350
2019-11-04 12:45:17,340 train 200 1.106757e-02 -0.719256
2019-11-04 12:45:27,847 train 250 1.108755e-02 -0.745656
2019-11-04 12:45:38,351 train 300 1.111940e-02 -0.724940
2019-11-04 12:45:48,838 train 350 1.108596e-02 -0.700188
2019-11-04 12:45:59,416 train 400 1.122786e-02 -0.682810
2019-11-04 12:46:09,958 train 450 1.133807e-02 -0.678002
2019-11-04 12:46:20,526 train 500 1.144074e-02 -0.675748
2019-11-04 12:46:31,072 train 550 1.148923e-02 -0.809616
2019-11-04 12:46:41,639 train 600 1.150207e-02 -0.808970
2019-11-04 12:46:52,167 train 650 1.150265e-02 -0.782109
2019-11-04 12:47:02,738 train 700 1.148962e-02 -0.775631
2019-11-04 12:47:13,268 train 750 1.145800e-02 -0.787406
2019-11-04 12:47:23,820 train 800 1.142009e-02 -0.791226
2019-11-04 12:47:34,338 train 850 1.138506e-02 -0.778960
2019-11-04 12:47:37,584 training loss; R2: 1.138390e-02 -0.802015
2019-11-04 12:47:38,127 valid 000 9.614686e-03 0.147964
2019-11-04 12:47:48,382 valid 050 1.071251e-02 -0.531017
2019-11-04 12:47:57,420 validation loss; R2: 1.068687e-02 -0.703532
2019-11-04 12:47:57,544 epoch 53 lr 9.982683e-04
2019-11-04 12:47:58,248 train 000 1.055310e-02 0.225136
2019-11-04 12:48:08,874 train 050 1.124567e-02 -1.066419
2019-11-04 12:48:19,420 train 100 1.132209e-02 -0.962090
2019-11-04 12:48:30,063 train 150 1.119438e-02 -0.836358
2019-11-04 12:48:40,647 train 200 1.118909e-02 -0.910133
2019-11-04 12:48:51,230 train 250 1.113086e-02 -0.974392
2019-11-04 12:49:01,822 train 300 1.104539e-02 -1.008260
2019-11-04 12:49:12,382 train 350 1.098927e-02 -0.967642
2019-11-04 12:49:22,953 train 400 1.097181e-02 -0.903684
2019-11-04 12:49:33,518 train 450 1.096894e-02 -0.916705
2019-11-04 12:49:44,113 train 500 1.094619e-02 -0.890581
2019-11-04 12:49:54,645 train 550 1.091111e-02 -0.904047
2019-11-04 12:50:05,227 train 600 1.089623e-02 -0.895057
2019-11-04 12:50:15,746 train 650 1.090957e-02 -0.862302
2019-11-04 12:50:26,279 train 700 1.091189e-02 -0.869832
2019-11-04 12:50:36,791 train 750 1.095965e-02 -0.860500
2019-11-04 12:50:47,337 train 800 1.099233e-02 -0.860847
2019-11-04 12:50:57,838 train 850 1.102797e-02 -0.839500
2019-11-04 12:51:01,083 training loss; R2: 1.108440e-02 -0.836096
2019-11-04 12:51:01,645 valid 000 1.669095e-02 -0.162273
2019-11-04 12:51:11,838 valid 050 1.720677e-02 -0.739484
2019-11-04 12:51:20,825 validation loss; R2: 1.718363e-02 -0.857804
2019-11-04 12:51:20,942 epoch 54 lr 9.982023e-04
2019-11-04 12:51:21,636 train 000 1.485495e-02 -2.166160
2019-11-04 12:51:32,250 train 050 1.284363e-02 -0.779426
2019-11-04 12:51:42,784 train 100 1.222668e-02 -0.686226
2019-11-04 12:51:53,318 train 150 1.192233e-02 -0.641317
2019-11-04 12:52:03,843 train 200 1.180717e-02 -0.645915
2019-11-04 12:52:14,343 train 250 1.163736e-02 -0.668243
2019-11-04 12:52:24,895 train 300 1.154605e-02 -0.706998
2019-11-04 12:52:35,456 train 350 1.145864e-02 -0.672195
2019-11-04 12:52:46,043 train 400 1.141555e-02 -0.703830
2019-11-04 12:52:56,615 train 450 1.136302e-02 -0.709453
2019-11-04 12:53:07,219 train 500 1.134079e-02 -0.669962
2019-11-04 12:53:17,806 train 550 1.133764e-02 -0.668673
2019-11-04 12:53:28,400 train 600 1.132395e-02 -0.668240
2019-11-04 12:53:38,946 train 650 1.131034e-02 -0.679217
2019-11-04 12:53:49,490 train 700 1.131551e-02 -0.671649
2019-11-04 12:54:00,016 train 750 1.143341e-02 -0.672919
2019-11-04 12:54:10,554 train 800 1.151401e-02 -0.665067
2019-11-04 12:54:21,075 train 850 1.158768e-02 -0.670883
2019-11-04 12:54:24,313 training loss; R2: 1.159152e-02 -0.681882
2019-11-04 12:54:24,899 valid 000 2.026728e-02 -0.396687
2019-11-04 12:54:35,144 valid 050 2.023825e-02 -1.336592
2019-11-04 12:54:44,181 validation loss; R2: 2.011335e-02 -1.628222
2019-11-04 12:54:44,303 epoch 55 lr 9.981352e-04
2019-11-04 12:54:45,034 train 000 1.198632e-02 0.086426
2019-11-04 12:54:55,623 train 050 1.171874e-02 -0.969721
2019-11-04 12:55:06,207 train 100 1.164387e-02 -0.945485
2019-11-04 12:55:16,679 train 150 1.146875e-02 -1.291045
2019-11-04 12:55:27,231 train 200 1.152064e-02 -1.209357
2019-11-04 12:55:37,738 train 250 1.142536e-02 -1.040472
2019-11-04 12:55:48,303 train 300 1.134128e-02 -0.966951
2019-11-04 12:55:58,786 train 350 1.131526e-02 -0.900709
2019-11-04 12:56:09,344 train 400 1.126718e-02 -0.872432
2019-11-04 12:56:19,901 train 450 1.123963e-02 -0.824047
2019-11-04 12:56:30,519 train 500 1.120456e-02 -0.794701
2019-11-04 12:56:41,140 train 550 1.118203e-02 -0.775295
2019-11-04 12:56:51,746 train 600 1.119808e-02 -0.762298
2019-11-04 12:57:02,342 train 650 1.119068e-02 -0.776605
2019-11-04 12:57:12,925 train 700 1.117810e-02 -0.781857
2019-11-04 12:57:23,432 train 750 1.115359e-02 -0.782934
2019-11-04 12:57:33,937 train 800 1.114171e-02 -0.758010
2019-11-04 12:57:44,459 train 850 1.115289e-02 -0.762123
2019-11-04 12:57:47,718 training loss; R2: 1.115633e-02 -0.754073
2019-11-04 12:57:48,281 valid 000 1.085843e-02 0.121854
2019-11-04 12:57:58,495 valid 050 1.124975e-02 -0.601076
2019-11-04 12:58:07,509 validation loss; R2: 1.133291e-02 -0.748339
2019-11-04 12:58:07,627 epoch 56 lr 9.980668e-04
2019-11-04 12:58:08,394 train 000 1.013780e-02 -0.640287
2019-11-04 12:58:18,971 train 050 1.113169e-02 -1.070037
2019-11-04 12:58:29,511 train 100 1.108554e-02 -0.983880
2019-11-04 12:58:39,917 train 150 1.110629e-02 -3.315553
2019-11-04 12:58:50,332 train 200 1.116093e-02 -3.251133
2019-11-04 12:59:00,807 train 250 1.127201e-02 -2.753958
2019-11-04 12:59:11,315 train 300 1.146614e-02 -2.404365
2019-11-04 12:59:21,826 train 350 1.155049e-02 -2.122886
2019-11-04 12:59:32,328 train 400 1.168179e-02 -1.901914
2019-11-04 12:59:42,836 train 450 1.174189e-02 -1.790328
2019-11-04 12:59:53,378 train 500 1.173641e-02 -1.654616
2019-11-04 13:00:03,903 train 550 1.171713e-02 -1.563607
2019-11-04 13:00:14,434 train 600 1.167978e-02 -1.533156
2019-11-04 13:00:24,933 train 650 1.165175e-02 -1.463200
2019-11-04 13:00:35,409 train 700 1.161527e-02 -1.416930
2019-11-04 13:00:45,838 train 750 1.168679e-02 -1.361256
2019-11-04 13:00:56,310 train 800 1.176755e-02 -1.321688
2019-11-04 13:01:06,753 train 850 1.176125e-02 -1.280280
2019-11-04 13:01:09,972 training loss; R2: 1.176273e-02 -1.267994
2019-11-04 13:01:10,497 valid 000 1.894627e-02 -0.172962
2019-11-04 13:01:20,966 valid 050 2.109279e-02 -0.919181
2019-11-04 13:01:30,118 validation loss; R2: 2.113565e-02 -1.093663
2019-11-04 13:01:30,235 epoch 57 lr 9.979972e-04
2019-11-04 13:01:30,948 train 000 1.138159e-02 0.145622
2019-11-04 13:01:41,592 train 050 1.179729e-02 -1.374355
2019-11-04 13:01:52,083 train 100 1.160556e-02 -1.243555
2019-11-04 13:02:02,586 train 150 1.159139e-02 -0.968690
2019-11-04 13:02:13,040 train 200 1.156612e-02 -0.884020
2019-11-04 13:02:23,516 train 250 1.146904e-02 -0.895039
2019-11-04 13:02:33,986 train 300 1.140200e-02 -0.872533
2019-11-04 13:02:44,449 train 350 1.135789e-02 -0.849212
2019-11-04 13:02:54,921 train 400 1.133982e-02 -0.840559
2019-11-04 13:03:05,414 train 450 1.137008e-02 -0.862379
2019-11-04 13:03:15,944 train 500 1.137403e-02 -0.866192
2019-11-04 13:03:26,462 train 550 1.137338e-02 -0.890323
2019-11-04 13:03:37,002 train 600 1.137990e-02 -0.851935
2019-11-04 13:03:47,546 train 650 1.137820e-02 -0.816718
2019-11-04 13:03:58,072 train 700 1.137800e-02 -0.795265
2019-11-04 13:04:08,586 train 750 1.136978e-02 -0.798793
2019-11-04 13:04:19,116 train 800 1.138696e-02 -0.810482
2019-11-04 13:04:29,635 train 850 1.142793e-02 -0.794496
2019-11-04 13:04:32,873 training loss; R2: 1.144245e-02 -0.799219
2019-11-04 13:04:33,424 valid 000 1.204715e-02 0.059436
2019-11-04 13:04:43,609 valid 050 1.258409e-02 -0.503843
2019-11-04 13:04:52,611 validation loss; R2: 1.263472e-02 -0.666585
2019-11-04 13:04:52,727 epoch 58 lr 9.979264e-04
2019-11-04 13:04:53,473 train 000 1.333319e-02 -0.033930
2019-11-04 13:05:04,080 train 050 1.161514e-02 -0.864360
2019-11-04 13:05:14,851 train 100 1.147690e-02 -0.623834
2019-11-04 13:05:25,388 train 150 1.131960e-02 -0.592603
2019-11-04 13:05:35,904 train 200 1.129675e-02 -0.608601
2019-11-04 13:05:46,370 train 250 1.130963e-02 -0.554831
2019-11-04 13:05:57,114 train 300 1.130421e-02 -0.616500
2019-11-04 13:06:07,673 train 350 1.130466e-02 -0.660919
2019-11-04 13:06:18,225 train 400 1.130443e-02 -0.648270
2019-11-04 13:06:28,787 train 450 1.130898e-02 -0.896922
2019-11-04 13:06:39,383 train 500 1.129176e-02 -0.861685
2019-11-04 13:06:49,918 train 550 1.123278e-02 -0.849098
2019-11-04 13:07:00,489 train 600 1.121312e-02 -0.923193
2019-11-04 13:07:11,050 train 650 1.121066e-02 -0.906839
2019-11-04 13:07:21,616 train 700 1.120397e-02 -0.893324
2019-11-04 13:07:32,153 train 750 1.122981e-02 -0.902203
2019-11-04 13:07:42,704 train 800 1.124851e-02 -0.884714
2019-11-04 13:07:53,252 train 850 1.125692e-02 -0.870636
2019-11-04 13:07:56,506 training loss; R2: 1.125699e-02 -0.876703
2019-11-04 13:07:57,073 valid 000 1.842387e-02 -0.122752
2019-11-04 13:08:07,459 valid 050 2.075761e-02 -0.835422
2019-11-04 13:08:16,537 validation loss; R2: 2.094963e-02 -0.950452
2019-11-04 13:08:16,669 epoch 59 lr 9.978543e-04
2019-11-04 13:08:17,387 train 000 1.137496e-02 -0.363244
2019-11-04 13:08:28,033 train 050 1.165463e-02 -0.622738
2019-11-04 13:08:38,645 train 100 1.157626e-02 -0.676847
2019-11-04 13:08:49,225 train 150 1.148091e-02 -0.656567
2019-11-04 13:08:59,806 train 200 1.142374e-02 -0.670282
2019-11-04 13:09:10,342 train 250 1.144145e-02 -0.696490
2019-11-04 13:09:20,842 train 300 1.158529e-02 -0.669290
2019-11-04 13:09:31,319 train 350 1.171053e-02 -0.673333
2019-11-04 13:09:41,807 train 400 1.175054e-02 -0.690169
2019-11-04 13:09:52,290 train 450 1.184336e-02 -0.680579
2019-11-04 13:10:02,757 train 500 1.180942e-02 -0.656854
2019-11-04 13:10:13,194 train 550 1.176292e-02 -0.638842
2019-11-04 13:10:23,657 train 600 1.170954e-02 -0.638085
2019-11-04 13:10:34,080 train 650 1.169873e-02 -0.643949
2019-11-04 13:10:44,527 train 700 1.168159e-02 -0.647927
2019-11-04 13:10:54,949 train 750 1.166073e-02 -0.641303
2019-11-04 13:11:05,425 train 800 1.161951e-02 -0.636360
2019-11-04 13:11:15,825 train 850 1.160554e-02 -0.629952
2019-11-04 13:11:19,020 training loss; R2: 1.161075e-02 -0.628409
2019-11-04 13:11:19,576 valid 000 9.647391e-03 0.106231
2019-11-04 13:11:29,782 valid 050 1.101066e-02 -0.458820
2019-11-04 13:11:38,827 validation loss; R2: 1.102565e-02 -0.625247
2019-11-04 13:11:38,944 epoch 60 lr 9.977810e-04
2019-11-04 13:11:39,714 train 000 1.161658e-02 0.141057
2019-11-04 13:11:50,286 train 050 1.116482e-02 -1.153203
2019-11-04 13:12:00,852 train 100 1.121654e-02 -1.024208
2019-11-04 13:12:11,291 train 150 1.115120e-02 -0.848238
2019-11-04 13:12:21,699 train 200 1.122709e-02 -0.812134
2019-11-04 13:12:32,178 train 250 1.132175e-02 -0.766728
2019-11-04 13:12:42,686 train 300 1.144053e-02 -0.791577
2019-11-04 13:12:53,269 train 350 1.143689e-02 -0.786885
2019-11-04 13:13:03,866 train 400 1.144003e-02 -0.760881
2019-11-04 13:13:14,469 train 450 1.149868e-02 -0.733620
2019-11-04 13:13:25,068 train 500 1.151258e-02 -0.762008
2019-11-04 13:13:35,639 train 550 1.152360e-02 -0.779497
2019-11-04 13:13:46,237 train 600 1.152166e-02 -0.789512
2019-11-04 13:13:56,803 train 650 1.151494e-02 -0.771505
2019-11-04 13:14:07,370 train 700 1.148498e-02 -0.774831
2019-11-04 13:14:17,931 train 750 1.145415e-02 -0.756761
2019-11-04 13:14:28,515 train 800 1.142865e-02 -0.756148
2019-11-04 13:14:39,059 train 850 1.142168e-02 -0.747630
2019-11-04 13:14:42,308 training loss; R2: 1.145794e-02 -0.743378
2019-11-04 13:14:42,852 valid 000 3.267118e-02 -1.654299
2019-11-04 13:14:53,174 valid 050 3.279276e-02 -1.122589
2019-11-04 13:15:02,175 validation loss; R2: 3.285857e-02 -1.129216
2019-11-04 13:15:02,301 epoch 61 lr 9.977065e-04
2019-11-04 13:15:03,000 train 000 1.407381e-02 -0.506620
2019-11-04 13:15:13,571 train 050 1.247568e-02 -0.681490
2019-11-04 13:15:24,148 train 100 1.265220e-02 -0.631735
2019-11-04 13:15:34,590 train 150 1.251582e-02 -0.671590
2019-11-04 13:15:45,051 train 200 1.225078e-02 -0.717879
2019-11-04 13:15:55,500 train 250 1.210024e-02 -0.712764
2019-11-04 13:16:06,004 train 300 1.195317e-02 -0.682443
2019-11-04 13:16:16,548 train 350 1.185603e-02 -0.683431
2019-11-04 13:16:27,083 train 400 1.177121e-02 -0.693001
2019-11-04 13:16:37,599 train 450 1.171098e-02 -0.664219
2019-11-04 13:16:48,139 train 500 1.165058e-02 -0.686813
2019-11-04 13:16:58,652 train 550 1.161072e-02 -0.704451
2019-11-04 13:17:09,193 train 600 1.157796e-02 -0.691542
2019-11-04 13:17:19,711 train 650 1.156061e-02 -0.676595
2019-11-04 13:17:30,238 train 700 1.153581e-02 -0.691480
2019-11-04 13:17:40,689 train 750 1.151412e-02 -0.695237
2019-11-04 13:17:51,224 train 800 1.150641e-02 -0.712366
2019-11-04 13:18:01,724 train 850 1.147182e-02 -0.741034
2019-11-04 13:18:04,953 training loss; R2: 1.146943e-02 -0.741938
2019-11-04 13:18:05,505 valid 000 4.816694e-02 -1.051563
2019-11-04 13:18:15,771 valid 050 4.348254e-02 -0.949337
2019-11-04 13:18:24,993 validation loss; R2: 4.412963e-02 -0.925723
2019-11-04 13:18:25,113 epoch 62 lr 9.976307e-04
2019-11-04 13:18:25,916 train 000 1.181645e-02 -1.697404
2019-11-04 13:18:36,481 train 050 1.129820e-02 -0.840147
2019-11-04 13:18:47,231 train 100 1.130749e-02 -0.683096
2019-11-04 13:18:57,905 train 150 1.118491e-02 -0.739330
2019-11-04 13:19:08,468 train 200 1.115628e-02 -0.700318
2019-11-04 13:19:19,064 train 250 1.126506e-02 -0.760743
2019-11-04 13:19:29,622 train 300 1.127429e-02 -0.738326
2019-11-04 13:19:40,146 train 350 1.145469e-02 -0.755205
2019-11-04 13:19:50,744 train 400 1.152363e-02 -0.724486
2019-11-04 13:20:01,276 train 450 1.151774e-02 -0.703625
2019-11-04 13:20:11,781 train 500 1.148861e-02 -0.692992
2019-11-04 13:20:22,225 train 550 1.144754e-02 -0.707345
2019-11-04 13:20:32,715 train 600 1.142277e-02 -0.689381
2019-11-04 13:20:43,204 train 650 1.144250e-02 -0.707079
2019-11-04 13:20:53,740 train 700 1.146093e-02 -0.724253
2019-11-04 13:21:04,328 train 750 1.147497e-02 -0.723084
2019-11-04 13:21:14,948 train 800 1.157635e-02 -0.710363
2019-11-04 13:21:25,574 train 850 1.178445e-02 -0.701468
2019-11-04 13:21:28,832 training loss; R2: 1.188700e-02 -0.695562
2019-11-04 13:21:29,378 valid 000 3.242094e-02 -1.054631
2019-11-04 13:21:39,609 valid 050 3.690885e-02 -2.890310
2019-11-04 13:21:48,646 validation loss; R2: 3.654811e-02 -3.319148
2019-11-04 13:21:48,758 epoch 63 lr 9.975537e-04
2019-11-04 13:21:49,519 train 000 2.345864e-02 -0.423170
2019-11-04 13:22:00,045 train 050 1.841832e-02 -0.625795
2019-11-04 13:22:10,700 train 100 1.743027e-02 -0.841286
2019-11-04 13:22:21,176 train 150 1.647917e-02 -0.760458
2019-11-04 13:22:31,729 train 200 1.592165e-02 -0.690383
2019-11-04 13:22:42,240 train 250 1.541273e-02 -0.650225
2019-11-04 13:22:52,887 train 300 1.493416e-02 -0.632571
2019-11-04 13:23:03,600 train 350 1.456684e-02 -0.614157
2019-11-04 13:23:14,198 train 400 1.430672e-02 -0.595283
2019-11-04 13:23:24,842 train 450 1.403907e-02 -0.630888
2019-11-04 13:23:35,389 train 500 1.379184e-02 -0.649159
2019-11-04 13:23:45,951 train 550 1.358947e-02 -0.668851
2019-11-04 13:23:56,518 train 600 1.343146e-02 -0.661263
2019-11-04 13:24:07,073 train 650 1.338061e-02 -0.708967
2019-11-04 13:24:17,580 train 700 1.325792e-02 -0.702814
2019-11-04 13:24:28,057 train 750 1.315268e-02 -0.711094
2019-11-04 13:24:38,543 train 800 1.306434e-02 -0.700895
2019-11-04 13:24:49,025 train 850 1.297148e-02 -0.698949
2019-11-04 13:24:52,255 training loss; R2: 1.294130e-02 -0.701092
2019-11-04 13:24:52,827 valid 000 2.788993e-02 -0.110470
2019-11-04 13:25:03,001 valid 050 2.837942e-02 -0.276509
2019-11-04 13:25:12,030 validation loss; R2: 2.880295e-02 -0.293475
2019-11-04 13:25:12,143 epoch 64 lr 9.974755e-04
2019-11-04 13:25:12,912 train 000 1.202817e-02 -0.539993
2019-11-04 13:25:23,536 train 050 1.134257e-02 -0.724753
2019-11-04 13:25:34,166 train 100 1.144660e-02 -0.598331
2019-11-04 13:25:44,730 train 150 1.156545e-02 -0.611033
2019-11-04 13:25:55,196 train 200 1.161397e-02 -0.569410
2019-11-04 13:26:05,658 train 250 1.162627e-02 -0.595563
2019-11-04 13:26:16,164 train 300 1.162269e-02 -0.609441
2019-11-04 13:26:26,583 train 350 1.160598e-02 -0.664882
2019-11-04 13:26:37,012 train 400 1.160197e-02 -0.664257
2019-11-04 13:26:47,469 train 450 1.159893e-02 -0.650005
2019-11-04 13:26:57,909 train 500 1.160877e-02 -0.650982
2019-11-04 13:27:08,319 train 550 1.156587e-02 -0.649742
2019-11-04 13:27:18,720 train 600 1.154163e-02 -0.625769
2019-11-04 13:27:29,113 train 650 1.153895e-02 -0.640627
2019-11-04 13:27:39,513 train 700 1.165327e-02 -0.658350
2019-11-04 13:27:49,962 train 750 1.192870e-02 -0.671641
2019-11-04 13:28:00,396 train 800 1.201560e-02 -0.673679
2019-11-04 13:28:10,817 train 850 1.200727e-02 -0.672251
2019-11-04 13:28:14,024 training loss; R2: 1.201118e-02 -0.666958
2019-11-04 13:28:14,580 valid 000 1.082266e-02 0.039479
2019-11-04 13:28:25,040 valid 050 1.165808e-02 -0.709858
2019-11-04 13:28:34,162 validation loss; R2: 1.166705e-02 -0.934254
2019-11-04 13:28:34,271 epoch 65 lr 9.973961e-04
2019-11-04 13:28:35,032 train 000 1.185561e-02 -0.148980
2019-11-04 13:28:45,579 train 050 1.193983e-02 -0.724921
2019-11-04 13:28:56,192 train 100 1.193441e-02 -0.761579
2019-11-04 13:29:06,786 train 150 1.187069e-02 -0.729848
2019-11-04 13:29:17,344 train 200 1.179168e-02 -0.731812
2019-11-04 13:29:27,868 train 250 1.178369e-02 -24.785329
2019-11-04 13:29:38,456 train 300 1.170977e-02 -20.837769
2019-11-04 13:29:49,011 train 350 1.169938e-02 -17.985055
2019-11-04 13:29:59,563 train 400 1.170574e-02 -15.836801
2019-11-04 13:30:10,083 train 450 1.169429e-02 -14.180246
2019-11-04 13:30:20,630 train 500 1.164806e-02 -12.828080
2019-11-04 13:30:31,148 train 550 1.161377e-02 -11.732363
2019-11-04 13:30:41,736 train 600 1.162050e-02 -10.818186
2019-11-04 13:30:52,281 train 650 1.161385e-02 -10.039840
2019-11-04 13:31:02,869 train 700 1.159376e-02 -9.603867
2019-11-04 13:31:13,428 train 750 1.159539e-02 -9.013014
2019-11-04 13:31:24,055 train 800 1.156972e-02 -8.484704
2019-11-04 13:31:34,636 train 850 1.156958e-02 -8.032185
2019-11-04 13:31:37,896 training loss; R2: 1.156989e-02 -7.900695
2019-11-04 13:31:38,452 valid 000 1.001561e-02 0.128413
2019-11-04 13:31:48,747 valid 050 1.058970e-02 -0.436943
2019-11-04 13:31:57,735 validation loss; R2: 1.059437e-02 -0.599623
2019-11-04 13:31:57,859 epoch 66 lr 9.973154e-04
2019-11-04 13:31:58,555 train 000 9.746371e-03 0.326428
2019-11-04 13:32:09,087 train 050 1.142189e-02 -0.546651
2019-11-04 13:32:19,618 train 100 1.185061e-02 -0.850236
2019-11-04 13:32:30,198 train 150 1.187034e-02 -0.828930
2019-11-04 13:32:40,748 train 200 1.186793e-02 -0.761290
2019-11-04 13:32:51,225 train 250 1.182052e-02 -0.740992
2019-11-04 13:33:01,842 train 300 1.177709e-02 -0.787383
2019-11-04 13:33:12,390 train 350 1.177074e-02 -0.755982
2019-11-04 13:33:23,004 train 400 1.172761e-02 -0.752754
2019-11-04 13:33:33,588 train 450 1.171554e-02 -0.745325
2019-11-04 13:33:44,216 train 500 1.177103e-02 -0.753070
2019-11-04 13:33:54,777 train 550 1.171400e-02 -0.740192
2019-11-04 13:34:05,393 train 600 1.170150e-02 -0.714792
2019-11-04 13:34:15,983 train 650 1.168334e-02 -0.758150
2019-11-04 13:34:26,607 train 700 1.164744e-02 -0.780167
2019-11-04 13:34:37,200 train 750 1.163766e-02 -0.794258
2019-11-04 13:34:47,798 train 800 1.162217e-02 -0.798529
2019-11-04 13:34:58,393 train 850 1.159860e-02 -0.786041
2019-11-04 13:35:01,661 training loss; R2: 1.159463e-02 -0.783967
2019-11-04 13:35:02,240 valid 000 9.407666e-03 0.034081
2019-11-04 13:35:12,463 valid 050 1.047170e-02 -0.768425
2019-11-04 13:35:21,585 validation loss; R2: 1.042897e-02 -1.005132
2019-11-04 13:35:21,695 epoch 67 lr 9.972335e-04
2019-11-04 13:35:22,467 train 000 1.196338e-02 -0.536551
2019-11-04 13:35:33,011 train 050 1.127720e-02 -1.038731
2019-11-04 13:35:43,495 train 100 1.115372e-02 -0.834953
2019-11-04 13:35:53,961 train 150 1.116775e-02 -0.797408
2019-11-04 13:36:04,526 train 200 1.122641e-02 -0.724532
2019-11-04 13:36:14,995 train 250 1.131293e-02 -0.689240
2019-11-04 13:36:25,514 train 300 1.131462e-02 -0.688468
2019-11-04 13:36:36,023 train 350 1.131818e-02 -0.697395
2019-11-04 13:36:46,588 train 400 1.135166e-02 -0.659052
2019-11-04 13:36:57,097 train 450 1.135789e-02 -0.676730
2019-11-04 13:37:07,626 train 500 1.137128e-02 -0.653038
2019-11-04 13:37:18,119 train 550 1.133778e-02 -1.019068
2019-11-04 13:37:28,633 train 600 1.135850e-02 -0.992238
2019-11-04 13:37:39,158 train 650 1.137893e-02 -0.985433
2019-11-04 13:37:49,681 train 700 1.137860e-02 -0.957292
2019-11-04 13:38:00,167 train 750 1.137033e-02 -0.927201
2019-11-04 13:38:10,677 train 800 1.135754e-02 -0.913103
2019-11-04 13:38:21,215 train 850 1.136711e-02 -0.909379
2019-11-04 13:38:24,475 training loss; R2: 1.137196e-02 -0.907983
2019-11-04 13:38:25,015 valid 000 6.280954e-02 -1.215815
2019-11-04 13:38:35,359 valid 050 6.135309e-02 -1.098780
2019-11-04 13:38:44,427 validation loss; R2: 6.189781e-02 -1.115119
2019-11-04 13:38:44,545 epoch 68 lr 9.971504e-04
2019-11-04 13:38:45,267 train 000 1.063327e-02 -0.490241
2019-11-04 13:38:55,833 train 050 1.138187e-02 -0.542378
2019-11-04 13:39:06,303 train 100 1.137344e-02 -0.507613
2019-11-04 13:39:16,796 train 150 1.142581e-02 -0.555290
2019-11-04 13:39:27,405 train 200 1.134200e-02 -0.623543
2019-11-04 13:39:37,840 train 250 1.126156e-02 -0.602720
2019-11-04 13:39:48,381 train 300 1.124774e-02 -0.595046
2019-11-04 13:39:58,895 train 350 1.125565e-02 -0.570814
2019-11-04 13:40:09,415 train 400 1.127006e-02 -0.594139
2019-11-04 13:40:19,901 train 450 1.130128e-02 -0.632413
2019-11-04 13:40:30,421 train 500 1.134339e-02 -0.618015
2019-11-04 13:40:40,927 train 550 1.136071e-02 -0.659552
2019-11-04 13:40:51,446 train 600 1.136800e-02 -0.709134
2019-11-04 13:41:01,935 train 650 1.136544e-02 -0.701078
2019-11-04 13:41:12,422 train 700 1.137735e-02 -0.701608
2019-11-04 13:41:22,907 train 750 1.136913e-02 -0.697997
2019-11-04 13:41:33,422 train 800 1.137162e-02 -0.700736
2019-11-04 13:41:43,941 train 850 1.136934e-02 -0.681116
2019-11-04 13:41:47,183 training loss; R2: 1.137170e-02 -0.681049
2019-11-04 13:41:47,748 valid 000 1.032847e-02 0.128364
2019-11-04 13:41:58,259 valid 050 1.082215e-02 -0.387490
2019-11-04 13:42:07,421 validation loss; R2: 1.082893e-02 -0.533761
2019-11-04 13:42:07,537 epoch 69 lr 9.970660e-04
2019-11-04 13:42:08,279 train 000 9.453971e-03 0.096970
2019-11-04 13:42:18,967 train 050 1.101104e-02 -0.821580
2019-11-04 13:42:29,696 train 100 1.109035e-02 -0.762908
2019-11-04 13:42:40,281 train 150 1.118693e-02 -0.759624
2019-11-04 13:42:50,754 train 200 1.123386e-02 -0.721861
2019-11-04 13:43:01,192 train 250 1.134945e-02 -0.737111
2019-11-04 13:43:11,715 train 300 1.138070e-02 -0.775982
2019-11-04 13:43:22,183 train 350 1.138438e-02 -0.727555
2019-11-04 13:43:32,696 train 400 1.138223e-02 -0.949772
2019-11-04 13:43:43,180 train 450 1.141059e-02 -0.900402
2019-11-04 13:43:53,678 train 500 1.140245e-02 -0.875282
2019-11-04 13:44:04,137 train 550 1.139207e-02 -0.876221
2019-11-04 13:44:14,624 train 600 1.138659e-02 -0.858927
2019-11-04 13:44:25,188 train 650 1.140756e-02 -0.836097
2019-11-04 13:44:35,761 train 700 1.142188e-02 -0.852338
2019-11-04 13:44:46,219 train 750 1.142100e-02 -0.835220
2019-11-04 13:44:56,684 train 800 1.142659e-02 -0.830111
2019-11-04 13:45:07,130 train 850 1.141301e-02 -0.814170
2019-11-04 13:45:10,350 training loss; R2: 1.141270e-02 -0.805838
2019-11-04 13:45:10,914 valid 000 1.188883e-02 -0.002350
2019-11-04 13:45:21,383 valid 050 1.224442e-02 -0.728463
2019-11-04 13:45:30,541 validation loss; R2: 1.215315e-02 -0.950367
2019-11-04 13:45:30,650 epoch 70 lr 9.969805e-04
2019-11-04 13:45:31,393 train 000 1.154797e-02 -1.027619
2019-11-04 13:45:41,783 train 050 1.156306e-02 -0.570284
2019-11-04 13:45:52,343 train 100 1.129749e-02 -0.582507
2019-11-04 13:46:02,929 train 150 1.135658e-02 -0.644888
2019-11-04 13:46:13,471 train 200 1.125363e-02 -0.674203
2019-11-04 13:46:23,912 train 250 1.127320e-02 -0.686727
2019-11-04 13:46:34,545 train 300 1.126364e-02 -0.754779
2019-11-04 13:46:45,030 train 350 1.130253e-02 -0.704932
2019-11-04 13:46:55,721 train 400 1.132358e-02 -0.749016
2019-11-04 13:47:06,215 train 450 1.131758e-02 -0.726933
2019-11-04 13:47:16,787 train 500 1.128111e-02 -0.711520
2019-11-04 13:47:27,354 train 550 1.127457e-02 -0.721353
2019-11-04 13:47:37,881 train 600 1.132192e-02 -0.735356
2019-11-04 13:47:48,494 train 650 1.133753e-02 -0.751835
2019-11-04 13:47:59,025 train 700 1.133602e-02 -0.727660
2019-11-04 13:48:09,566 train 750 1.136001e-02 -0.707016
2019-11-04 13:48:20,148 train 800 1.135689e-02 -0.703370
2019-11-04 13:48:30,624 train 850 1.136132e-02 -0.706572
2019-11-04 13:48:33,866 training loss; R2: 1.136838e-02 -0.705776
2019-11-04 13:48:34,413 valid 000 1.076627e-02 0.060580
2019-11-04 13:48:44,746 valid 050 1.130987e-02 -0.515218
2019-11-04 13:48:54,157 validation loss; R2: 1.130389e-02 -0.719678
2019-11-04 13:48:54,265 epoch 71 lr 9.968937e-04
2019-11-04 13:48:55,088 train 000 1.239632e-02 -0.151131
2019-11-04 13:49:05,528 train 050 1.152240e-02 -0.541881
2019-11-04 13:49:16,103 train 100 1.170513e-02 -0.543911
2019-11-04 13:49:26,647 train 150 1.160689e-02 -0.593694
2019-11-04 13:49:37,191 train 200 1.180667e-02 -0.662868
2019-11-04 13:49:47,611 train 250 1.179744e-02 -0.616547
2019-11-04 13:49:58,109 train 300 1.176610e-02 -0.646514
2019-11-04 13:50:08,706 train 350 1.168293e-02 -0.743679
2019-11-04 13:50:19,309 train 400 1.163715e-02 -0.706859
2019-11-04 13:50:29,783 train 450 1.165019e-02 -0.712992
2019-11-04 13:50:40,407 train 500 1.162619e-02 -0.694896
2019-11-04 13:50:50,972 train 550 1.168536e-02 -0.688415
2019-11-04 13:51:01,524 train 600 1.179263e-02 -0.703661
2019-11-04 13:51:12,095 train 650 1.177718e-02 -0.696941
2019-11-04 13:51:22,647 train 700 1.180753e-02 -0.714139
2019-11-04 13:51:33,247 train 750 1.178323e-02 -0.720832
2019-11-04 13:51:43,877 train 800 1.177827e-02 -0.714289
2019-11-04 13:51:54,430 train 850 1.180157e-02 -0.718013
2019-11-04 13:51:57,672 training loss; R2: 1.180301e-02 -0.708921
2019-11-04 13:51:58,287 valid 000 1.404707e-02 -0.109960
2019-11-04 13:52:08,557 valid 050 1.396573e-02 -0.965576
2019-11-04 13:52:17,665 validation loss; R2: 1.391825e-02 -1.197452
2019-11-04 13:52:17,779 epoch 72 lr 9.968057e-04
2019-11-04 13:52:18,576 train 000 1.386170e-02 0.022234
2019-11-04 13:52:29,080 train 050 1.169424e-02 -1.036355
2019-11-04 13:52:39,531 train 100 1.150597e-02 -0.982680
2019-11-04 13:52:50,073 train 150 1.136654e-02 -0.906349
2019-11-04 13:53:00,654 train 200 1.134627e-02 -0.838578
2019-11-04 13:53:11,167 train 250 1.137486e-02 -0.802734
2019-11-04 13:53:21,652 train 300 1.135860e-02 -0.852598
2019-11-04 13:53:32,160 train 350 1.152486e-02 -0.828191
2019-11-04 13:53:42,725 train 400 1.159588e-02 -0.781262
2019-11-04 13:53:53,302 train 450 1.163763e-02 -0.768314
2019-11-04 13:54:03,870 train 500 1.167118e-02 -0.766123
2019-11-04 13:54:14,309 train 550 1.168472e-02 -0.757211
2019-11-04 13:54:24,831 train 600 1.170517e-02 -0.756182
2019-11-04 13:54:35,382 train 650 1.171978e-02 -0.752244
2019-11-04 13:54:45,814 train 700 1.170165e-02 -0.735058
2019-11-04 13:54:56,357 train 750 1.170321e-02 -0.711941
2019-11-04 13:55:06,942 train 800 1.169973e-02 -0.710316
2019-11-04 13:55:17,423 train 850 1.169231e-02 -0.695126
2019-11-04 13:55:20,636 training loss; R2: 1.167993e-02 -1.005292
2019-11-04 13:55:21,231 valid 000 1.116496e-02 0.150704
2019-11-04 13:55:31,763 valid 050 1.135391e-02 -0.352648
2019-11-04 13:55:40,890 validation loss; R2: 1.131846e-02 -0.493394
2019-11-04 13:55:41,010 epoch 73 lr 9.967164e-04
2019-11-04 13:55:41,779 train 000 9.692775e-03 -0.791640
2019-11-04 13:55:52,217 train 050 1.118880e-02 -0.940294
2019-11-04 13:56:02,924 train 100 1.128327e-02 -0.825209
2019-11-04 13:56:13,578 train 150 1.124028e-02 -0.850040
2019-11-04 13:56:24,167 train 200 1.125174e-02 -0.757339
2019-11-04 13:56:34,688 train 250 1.124508e-02 -0.700689
2019-11-04 13:56:45,217 train 300 1.129455e-02 -2.375239
2019-11-04 13:56:55,827 train 350 1.130853e-02 -2.126668
2019-11-04 13:57:06,405 train 400 1.131680e-02 -1.939937
2019-11-04 13:57:16,994 train 450 1.136101e-02 -1.804263
2019-11-04 13:57:27,581 train 500 1.137580e-02 -1.678645
2019-11-04 13:57:38,149 train 550 1.137030e-02 -1.633160
2019-11-04 13:57:48,742 train 600 1.135425e-02 -1.563177
2019-11-04 13:57:59,304 train 650 1.138067e-02 -1.489583
2019-11-04 13:58:09,864 train 700 1.138884e-02 -1.438668
2019-11-04 13:58:20,348 train 750 1.139138e-02 -1.408770
2019-11-04 13:58:30,880 train 800 1.139869e-02 -1.373834
2019-11-04 13:58:41,451 train 850 1.141579e-02 -1.329690
2019-11-04 13:58:44,708 training loss; R2: 1.141525e-02 -1.328032
2019-11-04 13:58:45,264 valid 000 2.874162e-02 -0.650722
2019-11-04 13:58:55,819 valid 050 2.948537e-02 -1.125672
2019-11-04 13:59:04,948 validation loss; R2: 2.940790e-02 -1.249873
2019-11-04 13:59:05,061 epoch 74 lr 9.966259e-04
2019-11-04 13:59:05,849 train 000 1.250841e-02 -0.207923
2019-11-04 13:59:16,434 train 050 1.158268e-02 -0.620591
2019-11-04 13:59:26,991 train 100 1.145730e-02 -0.642515
2019-11-04 13:59:37,499 train 150 1.148270e-02 -0.676949
2019-11-04 13:59:48,010 train 200 1.146358e-02 -0.725970
2019-11-04 13:59:58,540 train 250 1.147164e-02 -0.737994
2019-11-04 14:00:09,059 train 300 1.147461e-02 -0.702866
2019-11-04 14:00:19,630 train 350 1.142151e-02 -0.686024
2019-11-04 14:00:30,086 train 400 1.138694e-02 -0.649338
2019-11-04 14:00:40,526 train 450 1.139871e-02 -0.643520
2019-11-04 14:00:50,966 train 500 1.136096e-02 -0.615977
2019-11-04 14:01:01,446 train 550 1.140482e-02 -0.611892
2019-11-04 14:01:11,931 train 600 1.143353e-02 -0.601304
2019-11-04 14:01:22,415 train 650 1.143824e-02 -0.626441
2019-11-04 14:01:32,950 train 700 1.140670e-02 -0.675846
2019-11-04 14:01:43,440 train 750 1.141671e-02 -0.672466
2019-11-04 14:01:53,943 train 800 1.141341e-02 -0.664379
2019-11-04 14:02:04,462 train 850 1.142192e-02 -0.694359
2019-11-04 14:02:07,704 training loss; R2: 1.142094e-02 -0.696603
2019-11-04 14:02:08,283 valid 000 2.083940e-02 -0.175640
2019-11-04 14:02:18,495 valid 050 2.222801e-02 -0.629069
2019-11-04 14:02:27,491 validation loss; R2: 2.213668e-02 -0.740087
2019-11-04 14:02:27,619 epoch 75 lr 9.965342e-04
2019-11-04 14:02:28,333 train 000 1.137474e-02 -0.771772
2019-11-04 14:02:38,955 train 050 1.152028e-02 -7.020152
2019-11-04 14:02:49,526 train 100 1.152869e-02 -3.910194
2019-11-04 14:03:00,039 train 150 1.142368e-02 -2.768872
2019-11-04 14:03:10,599 train 200 1.146367e-02 -2.213193
2019-11-04 14:03:20,966 train 250 1.141055e-02 -1.879478
2019-11-04 14:03:31,368 train 300 1.142255e-02 -1.731817
2019-11-04 14:03:41,796 train 350 1.141045e-02 -1.608450
2019-11-04 14:03:52,301 train 400 1.141271e-02 -1.476884
2019-11-04 14:04:02,757 train 450 1.144797e-02 -1.402736
2019-11-04 14:04:13,203 train 500 1.142739e-02 -1.337291
2019-11-04 14:04:23,697 train 550 1.144115e-02 -1.284904
2019-11-04 14:04:34,176 train 600 1.142706e-02 -1.241293
2019-11-04 14:04:44,639 train 650 1.140522e-02 -1.221907
2019-11-04 14:04:55,114 train 700 1.140225e-02 -1.172792
2019-11-04 14:05:05,553 train 750 1.139918e-02 -1.124828
2019-11-04 14:05:16,014 train 800 1.141373e-02 -1.116766
2019-11-04 14:05:26,437 train 850 1.141652e-02 -1.109192
2019-11-04 14:05:29,654 training loss; R2: 1.142149e-02 -1.121368
2019-11-04 14:05:30,182 valid 000 1.341937e-02 0.147799
2019-11-04 14:05:40,442 valid 050 1.374562e-02 -0.405907
2019-11-04 14:05:49,697 validation loss; R2: 1.380979e-02 -0.556180
2019-11-04 14:05:49,822 epoch 76 lr 9.964413e-04
2019-11-04 14:05:50,566 train 000 1.112090e-02 -0.032255
2019-11-04 14:06:01,170 train 050 1.168403e-02 -0.554958
2019-11-04 14:06:11,812 train 100 1.239885e-02 -0.594301
2019-11-04 14:06:22,300 train 150 1.240566e-02 -0.613965
2019-11-04 14:06:32,805 train 200 1.278557e-02 -0.701003
2019-11-04 14:06:43,335 train 250 1.266418e-02 -0.723748
2019-11-04 14:06:53,763 train 300 1.250223e-02 -0.707077
2019-11-04 14:07:04,266 train 350 1.240079e-02 -0.720332
2019-11-04 14:07:14,742 train 400 1.229036e-02 -0.746123
2019-11-04 14:07:25,236 train 450 1.217642e-02 -0.743457
2019-11-04 14:07:35,736 train 500 1.210729e-02 -0.756479
2019-11-04 14:07:46,224 train 550 1.202622e-02 -0.736824
2019-11-04 14:07:56,734 train 600 1.197238e-02 -0.736277
2019-11-04 14:08:07,167 train 650 1.193240e-02 -0.740661
2019-11-04 14:08:17,571 train 700 1.192005e-02 -0.729274
2019-11-04 14:08:28,009 train 750 1.188616e-02 -0.713194
2019-11-04 14:08:38,518 train 800 1.184379e-02 -0.711701
2019-11-04 14:08:49,027 train 850 1.184803e-02 -0.723789
2019-11-04 14:08:52,275 training loss; R2: 1.183650e-02 -0.720243
2019-11-04 14:08:52,823 valid 000 1.183510e-02 -0.053657
2019-11-04 14:09:03,222 valid 050 1.297426e-02 -0.919224
2019-11-04 14:09:12,321 validation loss; R2: 1.296924e-02 -1.130412
2019-11-04 14:09:12,443 epoch 77 lr 9.963472e-04
2019-11-04 14:09:13,137 train 000 1.180975e-02 -0.230693
2019-11-04 14:09:23,814 train 050 1.134460e-02 -0.600359
2019-11-04 14:09:34,488 train 100 1.136425e-02 -0.589168
2019-11-04 14:09:45,048 train 150 1.130035e-02 -0.772118
2019-11-04 14:09:55,636 train 200 1.135053e-02 -0.791967
2019-11-04 14:10:06,154 train 250 1.146621e-02 -0.768471
2019-11-04 14:10:16,759 train 300 1.146668e-02 -0.756452
2019-11-04 14:10:27,318 train 350 1.143901e-02 -0.728573
2019-11-04 14:10:37,897 train 400 1.141392e-02 -0.702831
2019-11-04 14:10:48,479 train 450 1.136150e-02 -0.750030
2019-11-04 14:10:59,065 train 500 1.136373e-02 -0.777695
2019-11-04 14:11:09,638 train 550 1.134515e-02 -0.745953
2019-11-04 14:11:20,153 train 600 1.133675e-02 -0.722771
2019-11-04 14:11:30,745 train 650 1.137152e-02 -0.719510
2019-11-04 14:11:41,349 train 700 1.136677e-02 -0.709488
2019-11-04 14:11:51,957 train 750 1.139026e-02 -0.690358
2019-11-04 14:12:02,560 train 800 1.138313e-02 -0.674401
2019-11-04 14:12:13,157 train 850 1.138580e-02 -0.668090
2019-11-04 14:12:16,455 training loss; R2: 1.138199e-02 -0.667277
2019-11-04 14:12:16,993 valid 000 2.862202e-02 -0.640761
2019-11-04 14:12:27,215 valid 050 3.132160e-02 -1.439502
2019-11-04 14:12:36,145 validation loss; R2: 3.117821e-02 -1.639142
2019-11-04 14:12:36,265 epoch 78 lr 9.962518e-04
2019-11-04 14:12:36,984 train 000 1.207582e-02 -0.415882
2019-11-04 14:12:47,524 train 050 1.128781e-02 -0.738184
2019-11-04 14:12:58,069 train 100 1.139617e-02 -0.852641
2019-11-04 14:13:08,593 train 150 1.141206e-02 -0.764330
2019-11-04 14:13:19,109 train 200 1.139963e-02 -0.746899
2019-11-04 14:13:29,505 train 250 1.140298e-02 -0.682812
2019-11-04 14:13:40,032 train 300 1.138797e-02 -0.720017
2019-11-04 14:13:50,508 train 350 1.139535e-02 -0.688505
2019-11-04 14:14:01,001 train 400 1.136987e-02 -0.716754
2019-11-04 14:14:11,527 train 450 1.141539e-02 -0.675383
2019-11-04 14:14:22,056 train 500 1.140949e-02 -0.665625
2019-11-04 14:14:32,534 train 550 1.141472e-02 -0.681631
2019-11-04 14:14:42,999 train 600 1.140140e-02 -0.681245
2019-11-04 14:14:53,525 train 650 1.139894e-02 -0.662300
2019-11-04 14:15:04,041 train 700 1.142154e-02 -0.663023
2019-11-04 14:15:14,565 train 750 1.142550e-02 -0.666094
2019-11-04 14:15:25,056 train 800 1.143395e-02 -0.669652
2019-11-04 14:15:35,553 train 850 1.143701e-02 -0.677367
2019-11-04 14:15:38,774 training loss; R2: 1.144236e-02 -0.679843
2019-11-04 14:15:39,320 valid 000 3.485763e-02 -0.987481
2019-11-04 14:15:49,566 valid 050 3.924771e-02 -3.297927
2019-11-04 14:15:58,512 validation loss; R2: 3.907469e-02 -3.750849
2019-11-04 14:15:58,629 epoch 79 lr 9.961552e-04
2019-11-04 14:15:59,378 train 000 1.197832e-02 -0.450341
2019-11-04 14:16:10,033 train 050 1.158276e-02 -0.801045
2019-11-04 14:16:20,655 train 100 1.162002e-02 -0.786938
2019-11-04 14:16:31,326 train 150 1.172926e-02 -0.750159
2019-11-04 14:16:42,031 train 200 1.171334e-02 -0.816916
2019-11-04 14:16:52,527 train 250 1.170699e-02 -0.769516
2019-11-04 14:17:03,058 train 300 1.163213e-02 -0.716114
2019-11-04 14:17:13,500 train 350 1.160565e-02 -0.720772
2019-11-04 14:17:24,090 train 400 1.155513e-02 -0.695960
2019-11-04 14:17:34,638 train 450 1.154161e-02 -0.688079
2019-11-04 14:17:45,201 train 500 1.150414e-02 -0.716303
2019-11-04 14:17:55,753 train 550 1.154237e-02 -0.695676
2019-11-04 14:18:06,289 train 600 1.154167e-02 -0.708716
2019-11-04 14:18:16,823 train 650 1.150777e-02 -0.684445
2019-11-04 14:18:27,386 train 700 1.148628e-02 -0.689127
2019-11-04 14:18:37,926 train 750 1.146095e-02 -0.702821
2019-11-04 14:18:48,460 train 800 1.145677e-02 -0.725544
2019-11-04 14:18:58,978 train 850 1.146417e-02 -0.747387
2019-11-04 14:19:02,224 training loss; R2: 1.145741e-02 -0.752089
2019-11-04 14:19:02,831 valid 000 1.110924e-02 0.003832
2019-11-04 14:19:13,025 valid 050 1.195620e-02 -0.687942
2019-11-04 14:19:21,997 validation loss; R2: 1.196268e-02 -0.919764
2019-11-04 14:19:22,110 epoch 80 lr 9.960574e-04
2019-11-04 14:19:22,840 train 000 1.278106e-02 -1.438961
2019-11-04 14:19:33,377 train 050 1.152223e-02 -0.428868
2019-11-04 14:19:43,905 train 100 1.154869e-02 -0.554476
2019-11-04 14:19:54,456 train 150 1.157388e-02 -0.792455
2019-11-04 14:20:04,959 train 200 1.159387e-02 -0.892828
2019-11-04 14:20:15,448 train 250 1.157849e-02 -0.927289
2019-11-04 14:20:25,855 train 300 1.152875e-02 -0.902111
2019-11-04 14:20:36,290 train 350 1.151253e-02 -0.887552
2019-11-04 14:20:46,839 train 400 1.148890e-02 -0.831831
2019-11-04 14:20:57,479 train 450 1.146549e-02 -0.821679
2019-11-04 14:21:08,116 train 500 1.151629e-02 -0.813853
2019-11-04 14:21:18,806 train 550 1.148347e-02 -0.818658
2019-11-04 14:21:29,372 train 600 1.147250e-02 -0.797439
2019-11-04 14:21:39,858 train 650 1.145761e-02 -0.788177
2019-11-04 14:21:50,437 train 700 1.144357e-02 -0.771095
2019-11-04 14:22:00,986 train 750 1.142645e-02 -0.748347
2019-11-04 14:22:11,498 train 800 1.141205e-02 -0.741022
2019-11-04 14:22:22,003 train 850 1.140035e-02 -0.741592
2019-11-04 14:22:25,231 training loss; R2: 1.139603e-02 -0.744670
2019-11-04 14:22:25,766 valid 000 1.204070e-02 -0.054646
2019-11-04 14:22:35,961 valid 050 1.286650e-02 -0.785568
2019-11-04 14:22:44,995 validation loss; R2: 1.285662e-02 -1.027602
2019-11-04 14:22:45,103 epoch 81 lr 9.959583e-04
2019-11-04 14:22:45,848 train 000 1.166017e-02 -0.113972
2019-11-04 14:22:56,409 train 050 1.157410e-02 -0.708029
2019-11-04 14:23:06,933 train 100 1.141105e-02 -0.858907
2019-11-04 14:23:17,409 train 150 1.146206e-02 -0.853580
2019-11-04 14:23:27,887 train 200 1.142336e-02 -0.857662
2019-11-04 14:23:38,441 train 250 1.140479e-02 -0.843112
2019-11-04 14:23:49,014 train 300 1.144698e-02 -0.792753
2019-11-04 14:23:59,539 train 350 1.143056e-02 -0.766549
2019-11-04 14:24:10,076 train 400 1.141349e-02 -0.755643
2019-11-04 14:24:20,572 train 450 1.141666e-02 -0.742571
2019-11-04 14:24:31,091 train 500 1.142228e-02 -0.734236
2019-11-04 14:24:41,607 train 550 1.140626e-02 -0.710886
2019-11-04 14:24:52,149 train 600 1.140284e-02 -0.712314
2019-11-04 14:25:02,664 train 650 1.141181e-02 -0.703243
2019-11-04 14:25:13,174 train 700 1.139129e-02 -0.691449
2019-11-04 14:25:23,676 train 750 1.137107e-02 -0.710397
2019-11-04 14:25:34,260 train 800 1.135998e-02 -0.707734
2019-11-04 14:25:44,802 train 850 1.136118e-02 -0.730983
2019-11-04 14:25:48,040 training loss; R2: 1.136260e-02 -0.723231
2019-11-04 14:25:48,569 valid 000 1.403691e-02 0.004825
2019-11-04 14:25:58,920 valid 050 1.365231e-02 -0.631180
2019-11-04 14:26:08,096 validation loss; R2: 1.373573e-02 -0.750795
2019-11-04 14:26:08,211 epoch 82 lr 9.958580e-04
2019-11-04 14:26:08,960 train 000 1.048328e-02 -0.935850
2019-11-04 14:26:19,569 train 050 1.138443e-02 -0.665103
2019-11-04 14:26:30,111 train 100 1.131684e-02 -0.743827
2019-11-04 14:26:40,593 train 150 1.137344e-02 -0.833768
2019-11-04 14:26:51,154 train 200 1.139166e-02 -0.785350
2019-11-04 14:27:01,689 train 250 1.140252e-02 -0.870284
2019-11-04 14:27:12,225 train 300 1.142617e-02 -0.837475
2019-11-04 14:27:22,719 train 350 1.145019e-02 -0.811510
2019-11-04 14:27:33,233 train 400 1.143001e-02 -0.766075
2019-11-04 14:27:43,748 train 450 1.142169e-02 -0.778844
2019-11-04 14:27:54,294 train 500 1.141113e-02 -0.760565
2019-11-04 14:28:04,800 train 550 1.140636e-02 -0.754204
2019-11-04 14:28:15,246 train 600 1.139609e-02 -0.735363
2019-11-04 14:28:25,702 train 650 1.139996e-02 -0.747973
2019-11-04 14:28:36,190 train 700 1.140007e-02 -0.734558
2019-11-04 14:28:46,667 train 750 1.140078e-02 -0.766141
2019-11-04 14:28:57,212 train 800 1.149841e-02 -0.756554
2019-11-04 14:29:07,782 train 850 1.150132e-02 -0.761466
2019-11-04 14:29:11,044 training loss; R2: 1.149543e-02 -0.758635
2019-11-04 14:29:11,626 valid 000 1.916525e-02 -0.235216
2019-11-04 14:29:21,858 valid 050 2.104380e-02 -0.916813
2019-11-04 14:29:30,878 validation loss; R2: 2.098156e-02 -1.114878
2019-11-04 14:29:30,998 epoch 83 lr 9.957565e-04
2019-11-04 14:29:31,701 train 000 1.126356e-02 -0.023819
2019-11-04 14:29:42,325 train 050 1.201289e-02 -0.511692
2019-11-04 14:29:52,875 train 100 1.158807e-02 -0.570158
2019-11-04 14:30:03,406 train 150 1.148814e-02 -1.249301
2019-11-04 14:30:13,909 train 200 1.148928e-02 -1.054996
2019-11-04 14:30:24,471 train 250 1.147160e-02 -1.032323
2019-11-04 14:30:35,083 train 300 1.150432e-02 -0.971063
2019-11-04 14:30:45,661 train 350 1.147415e-02 -0.937563
2019-11-04 14:30:56,220 train 400 1.149680e-02 -0.948178
2019-11-04 14:31:06,793 train 450 1.151153e-02 -0.933891
2019-11-04 14:31:17,307 train 500 1.150022e-02 -0.936127
2019-11-04 14:31:27,879 train 550 1.149913e-02 -0.905690
2019-11-04 14:31:38,492 train 600 1.148901e-02 -0.894651
2019-11-04 14:31:49,104 train 650 1.145989e-02 -0.880999
2019-11-04 14:31:59,692 train 700 1.147492e-02 -0.858930
2019-11-04 14:32:10,273 train 750 1.146934e-02 -0.840731
2019-11-04 14:32:20,823 train 800 1.146898e-02 -0.825862
2019-11-04 14:32:31,329 train 850 1.146709e-02 -0.820179
2019-11-04 14:32:34,567 training loss; R2: 1.146832e-02 -0.818582
2019-11-04 14:32:35,106 valid 000 2.465714e-02 -0.930409
2019-11-04 14:32:45,398 valid 050 2.627326e-02 -2.333733
2019-11-04 14:32:54,373 validation loss; R2: 2.617236e-02 -2.749309
2019-11-04 14:32:54,489 epoch 84 lr 9.956538e-04
2019-11-04 14:32:55,183 train 000 9.565097e-03 0.116352
2019-11-04 14:33:05,496 train 050 1.172930e-02 -0.615204
2019-11-04 14:33:15,842 train 100 1.144738e-02 -0.568660
2019-11-04 14:33:26,348 train 150 1.133316e-02 -1.271577
2019-11-04 14:33:36,862 train 200 1.129904e-02 -1.158471
2019-11-04 14:33:47,405 train 250 1.129201e-02 -1.027121
2019-11-04 14:33:58,065 train 300 1.126087e-02 -0.976895
2019-11-04 14:34:08,682 train 350 1.128905e-02 -0.902759
2019-11-04 14:34:19,327 train 400 1.132050e-02 -0.864739
2019-11-04 14:34:29,962 train 450 1.131707e-02 -0.849386
2019-11-04 14:34:40,631 train 500 1.134500e-02 -0.822411
2019-11-04 14:34:51,290 train 550 1.134570e-02 -0.810868
2019-11-04 14:35:01,958 train 600 1.135098e-02 -0.847057
2019-11-04 14:35:12,612 train 650 1.132004e-02 -0.839246
2019-11-04 14:35:23,300 train 700 1.132084e-02 -0.810077
2019-11-04 14:35:33,960 train 750 1.133459e-02 -0.802251
2019-11-04 14:35:44,616 train 800 1.134819e-02 -0.797286
2019-11-04 14:35:55,208 train 850 1.136713e-02 -0.781476
2019-11-04 14:35:58,455 training loss; R2: 1.137229e-02 -0.778259
2019-11-04 14:35:59,035 valid 000 1.706744e-02 -0.283165
2019-11-04 14:36:09,345 valid 050 1.743527e-02 -1.039355
2019-11-04 14:36:18,399 validation loss; R2: 1.732104e-02 -1.284983
2019-11-04 14:36:18,515 epoch 85 lr 9.955499e-04
2019-11-04 14:36:19,252 train 000 9.541962e-03 -1.204983
2019-11-04 14:36:30,002 train 050 1.133765e-02 -2.313866
2019-11-04 14:36:40,905 train 100 1.144821e-02 -1.429945
2019-11-04 14:36:51,538 train 150 1.152736e-02 -1.289500
2019-11-04 14:37:02,156 train 200 1.144482e-02 -1.129406
2019-11-04 14:37:12,747 train 250 1.139723e-02 -0.972796
2019-11-04 14:37:23,256 train 300 1.133235e-02 -0.936590
2019-11-04 14:37:33,728 train 350 1.136114e-02 -0.878025
2019-11-04 14:37:44,150 train 400 1.141585e-02 -0.844081
2019-11-04 14:37:54,690 train 450 1.143268e-02 -0.868742
2019-11-04 14:38:05,252 train 500 1.143432e-02 -0.864867
2019-11-04 14:38:15,808 train 550 1.141203e-02 -0.904747
2019-11-04 14:38:26,373 train 600 1.139435e-02 -0.911346
2019-11-04 14:38:36,868 train 650 1.137385e-02 -0.898297
2019-11-04 14:38:47,390 train 700 1.135303e-02 -0.872751
2019-11-04 14:38:57,915 train 750 1.136884e-02 -0.866935
2019-11-04 14:39:08,451 train 800 1.136657e-02 -0.867334
2019-11-04 14:39:18,988 train 850 1.135101e-02 -0.858036
2019-11-04 14:39:22,224 training loss; R2: 1.136261e-02 -0.851527
2019-11-04 14:39:22,797 valid 000 2.176965e-02 -0.598614
2019-11-04 14:39:32,991 valid 050 2.393531e-02 -2.122173
2019-11-04 14:39:42,036 validation loss; R2: 2.372417e-02 -2.551776
2019-11-04 14:39:42,170 epoch 86 lr 9.954447e-04
2019-11-04 14:39:42,900 train 000 1.063504e-02 -0.252502
2019-11-04 14:39:53,240 train 050 1.127246e-02 -0.621818
2019-11-04 14:40:03,578 train 100 1.147279e-02 -5.376468
2019-11-04 14:40:14,157 train 150 1.153629e-02 -3.812234
2019-11-04 14:40:24,789 train 200 1.159103e-02 -3.051722
2019-11-04 14:40:35,451 train 250 1.156365e-02 -2.627913
2019-11-04 14:40:46,035 train 300 1.156629e-02 -2.308825
2019-11-04 14:40:56,600 train 350 1.153688e-02 -2.058375
2019-11-04 14:41:07,274 train 400 1.151496e-02 -1.886780
2019-11-04 14:41:17,894 train 450 1.150289e-02 -1.749609
2019-11-04 14:41:28,497 train 500 1.147561e-02 -1.650650
2019-11-04 14:41:39,073 train 550 1.145407e-02 -1.549676
2019-11-04 14:41:49,703 train 600 1.145345e-02 -1.469468
2019-11-04 14:42:00,234 train 650 1.145445e-02 -1.449172
2019-11-04 14:42:10,817 train 700 1.146161e-02 -1.419410
2019-11-04 14:42:21,424 train 750 1.144373e-02 -1.362936
2019-11-04 14:42:32,029 train 800 1.144173e-02 -1.308629
2019-11-04 14:42:42,683 train 850 1.143244e-02 -1.257164
2019-11-04 14:42:45,963 training loss; R2: 1.142382e-02 -1.244856
2019-11-04 14:42:46,535 valid 000 2.243868e-02 -0.710102
2019-11-04 14:42:56,781 valid 050 2.299197e-02 -2.187635
2019-11-04 14:43:05,830 validation loss; R2: 2.275223e-02 -2.670764
2019-11-04 14:43:05,943 epoch 87 lr 9.953383e-04
2019-11-04 14:43:06,666 train 000 1.732833e-02 -0.181908
2019-11-04 14:43:17,182 train 050 1.176302e-02 -0.685456
2019-11-04 14:43:27,677 train 100 1.163427e-02 -0.613963
2019-11-04 14:43:38,208 train 150 1.150708e-02 -0.770762
2019-11-04 14:43:48,681 train 200 1.151650e-02 -0.737965
2019-11-04 14:43:59,192 train 250 1.149669e-02 -0.826863
2019-11-04 14:44:09,713 train 300 1.149250e-02 -0.842540
2019-11-04 14:44:20,265 train 350 1.149597e-02 -0.912834
2019-11-04 14:44:30,736 train 400 1.149079e-02 -0.875020
2019-11-04 14:44:41,226 train 450 1.146559e-02 -0.828063
2019-11-04 14:44:51,715 train 500 1.144009e-02 -0.805293
2019-11-04 14:45:02,226 train 550 1.145862e-02 -0.777023
2019-11-04 14:45:12,715 train 600 1.144576e-02 -0.860357
2019-11-04 14:45:23,219 train 650 1.145503e-02 -0.882783
2019-11-04 14:45:33,724 train 700 1.144565e-02 -0.871713
2019-11-04 14:45:44,225 train 750 1.143756e-02 -0.859677
2019-11-04 14:45:54,736 train 800 1.142085e-02 -0.862900
2019-11-04 14:46:05,227 train 850 1.141523e-02 -0.855875
2019-11-04 14:46:08,441 training loss; R2: 1.141032e-02 -0.847945
2019-11-04 14:46:09,020 valid 000 3.012044e-02 -0.915690
2019-11-04 14:46:19,239 valid 050 3.251625e-02 -2.604192
2019-11-04 14:46:28,198 validation loss; R2: 3.199773e-02 -3.015322
2019-11-04 14:46:28,312 epoch 88 lr 9.952307e-04
2019-11-04 14:46:29,019 train 000 1.097534e-02 -0.229932
2019-11-04 14:46:39,626 train 050 1.140911e-02 -0.585092
2019-11-04 14:46:50,145 train 100 1.142082e-02 -0.589400
2019-11-04 14:47:00,692 train 150 1.139008e-02 -0.565249
2019-11-04 14:47:11,279 train 200 1.138362e-02 -0.612229
2019-11-04 14:47:21,820 train 250 1.139305e-02 -0.634523
2019-11-04 14:47:32,389 train 300 1.136032e-02 -0.597476
2019-11-04 14:47:42,920 train 350 1.136721e-02 -0.589225
2019-11-04 14:47:53,459 train 400 1.137868e-02 -0.616507
2019-11-04 14:48:03,907 train 450 1.138782e-02 -0.620647
2019-11-04 14:48:14,449 train 500 1.135641e-02 -0.623220
2019-11-04 14:48:24,867 train 550 1.138051e-02 -0.650020
2019-11-04 14:48:35,350 train 600 1.138296e-02 -0.665008
2019-11-04 14:48:45,865 train 650 1.139546e-02 -0.645140
2019-11-04 14:48:56,370 train 700 1.137648e-02 -0.639183
2019-11-04 14:49:06,867 train 750 1.139179e-02 -0.677749
2019-11-04 14:49:17,361 train 800 1.141130e-02 -0.663970
2019-11-04 14:49:27,876 train 850 1.142603e-02 -0.662554
2019-11-04 14:49:31,108 training loss; R2: 1.142448e-02 -0.661089
2019-11-04 14:49:31,695 valid 000 2.430716e-02 -0.148434
2019-11-04 14:49:41,911 valid 050 2.433950e-02 -0.480586
2019-11-04 14:49:50,967 validation loss; R2: 2.440247e-02 -0.520132
2019-11-04 14:49:51,082 epoch 89 lr 9.951219e-04
2019-11-04 14:49:51,795 train 000 1.150918e-02 0.171353
2019-11-04 14:50:02,383 train 050 1.160201e-02 -0.806864
2019-11-04 14:50:12,945 train 100 1.144448e-02 -0.751804
2019-11-04 14:50:23,495 train 150 1.141333e-02 -0.716563
2019-11-04 14:50:34,048 train 200 1.146112e-02 -0.848457
2019-11-04 14:50:44,586 train 250 1.150796e-02 -0.810257
2019-11-04 14:50:55,148 train 300 1.150078e-02 -0.773404
2019-11-04 14:51:05,648 train 350 1.149740e-02 -0.838288
2019-11-04 14:51:16,167 train 400 1.147243e-02 -0.986895
2019-11-04 14:51:26,690 train 450 1.147043e-02 -1.816415
2019-11-04 14:51:37,232 train 500 1.142924e-02 -1.688487
2019-11-04 14:51:47,820 train 550 1.139836e-02 -1.597703
2019-11-04 14:51:58,368 train 600 1.139086e-02 -1.519805
2019-11-04 14:52:08,897 train 650 1.137476e-02 -1.450825
2019-11-04 14:52:19,417 train 700 1.136728e-02 -1.408015
2019-11-04 14:52:29,933 train 750 1.136068e-02 -1.354400
2019-11-04 14:52:40,473 train 800 1.135062e-02 -1.305062
2019-11-04 14:52:50,980 train 850 1.133409e-02 -1.287802
2019-11-04 14:52:54,223 training loss; R2: 1.133703e-02 -1.273208
2019-11-04 14:52:54,761 valid 000 2.205231e-02 -0.221059
2019-11-04 14:53:05,064 valid 050 2.158707e-02 -0.587092
2019-11-04 14:53:14,151 validation loss; R2: 2.165097e-02 -0.686319
2019-11-04 14:53:14,269 epoch 90 lr 9.950118e-04
2019-11-04 14:53:14,975 train 000 1.477653e-02 -0.125738
2019-11-04 14:53:25,451 train 050 1.139096e-02 -0.480418
2019-11-04 14:53:35,914 train 100 1.121105e-02 -0.568201
2019-11-04 14:53:46,442 train 150 1.125305e-02 -0.750338
2019-11-04 14:53:57,013 train 200 1.124226e-02 -0.810605
2019-11-04 14:54:07,609 train 250 1.126896e-02 -0.762651
2019-11-04 14:54:18,201 train 300 1.120523e-02 -5.865110
2019-11-04 14:54:28,779 train 350 1.117828e-02 -5.095039
2019-11-04 14:54:39,345 train 400 1.119177e-02 -4.530056
2019-11-04 14:54:49,890 train 450 1.120671e-02 -4.120561
2019-11-04 14:55:00,462 train 500 1.120000e-02 -3.770259
2019-11-04 14:55:11,051 train 550 1.121627e-02 -3.495989
2019-11-04 14:55:21,644 train 600 1.125013e-02 -3.262471
2019-11-04 14:55:32,236 train 650 1.127135e-02 -3.047398
2019-11-04 14:55:42,840 train 700 1.127889e-02 -2.864543
2019-11-04 14:55:53,422 train 750 1.130075e-02 -2.747227
2019-11-04 14:56:03,958 train 800 1.130149e-02 -2.622912
2019-11-04 14:56:14,475 train 850 1.129657e-02 -2.507355
2019-11-04 14:56:17,705 training loss; R2: 1.128571e-02 -2.473947
2019-11-04 14:56:18,240 valid 000 2.918427e-02 -0.376591
2019-11-04 14:56:28,482 valid 050 2.879946e-02 -0.492088
2019-11-04 14:56:37,536 validation loss; R2: 2.900158e-02 -0.499116
2019-11-04 14:56:37,644 epoch 91 lr 9.949006e-04
2019-11-04 14:56:38,410 train 000 1.389646e-02 -0.808717
2019-11-04 14:56:48,975 train 050 1.142745e-02 -606.803931
2019-11-04 14:56:59,464 train 100 1.147415e-02 -306.733871
2019-11-04 14:57:09,977 train 150 1.141509e-02 -205.353704
2019-11-04 14:57:20,498 train 200 1.139429e-02 -154.400105
2019-11-04 14:57:31,017 train 250 1.137972e-02 -123.813382
2019-11-04 14:57:41,601 train 300 1.141040e-02 -103.382655
2019-11-04 14:57:52,174 train 350 1.138245e-02 -88.727882
2019-11-04 14:58:02,784 train 400 1.136623e-02 -77.760976
2019-11-04 14:58:13,386 train 450 1.136154e-02 -69.250601
2019-11-04 14:58:23,990 train 500 1.140275e-02 -62.389743
2019-11-04 14:58:34,590 train 550 1.140677e-02 -56.784270
2019-11-04 14:58:45,176 train 600 1.137317e-02 -52.120697
2019-11-04 14:58:55,756 train 650 1.133556e-02 -48.158011
2019-11-04 14:59:06,322 train 700 1.132556e-02 -44.781497
2019-11-04 14:59:16,869 train 750 1.130633e-02 -41.827500
2019-11-04 14:59:27,375 train 800 1.129066e-02 -39.272376
2019-11-04 14:59:37,850 train 850 1.129070e-02 -37.022868
2019-11-04 14:59:41,104 training loss; R2: 1.129712e-02 -36.391370
2019-11-04 14:59:41,651 valid 000 1.612606e-02 -0.119705
2019-11-04 14:59:51,877 valid 050 1.737262e-02 -0.962171
2019-11-04 15:00:00,845 validation loss; R2: 1.730058e-02 -1.150418
2019-11-04 15:00:00,955 epoch 92 lr 9.947881e-04
2019-11-04 15:00:01,691 train 000 9.734772e-03 -0.914062
2019-11-04 15:00:12,185 train 050 1.114996e-02 -1.498438
2019-11-04 15:00:22,716 train 100 1.101915e-02 -1.059395
2019-11-04 15:00:33,296 train 150 1.107513e-02 -0.901749
2019-11-04 15:00:43,826 train 200 1.113410e-02 -0.813151
2019-11-04 15:00:54,326 train 250 1.115961e-02 -0.770143
2019-11-04 15:01:04,810 train 300 1.120089e-02 -0.738111
2019-11-04 15:01:15,346 train 350 1.122292e-02 -0.764534
2019-11-04 15:01:25,835 train 400 1.120580e-02 -0.764067
2019-11-04 15:01:36,299 train 450 1.122620e-02 -0.729597
2019-11-04 15:01:46,789 train 500 1.121442e-02 -0.728117
2019-11-04 15:01:57,329 train 550 1.119540e-02 -0.755724
2019-11-04 15:02:07,883 train 600 1.118078e-02 -0.742418
2019-11-04 15:02:18,434 train 650 1.118211e-02 -0.779414
2019-11-04 15:02:28,959 train 700 1.119786e-02 -0.769779
2019-11-04 15:02:39,460 train 750 1.121950e-02 -0.780413
2019-11-04 15:02:49,949 train 800 1.123231e-02 -0.761791
2019-11-04 15:03:00,424 train 850 1.124534e-02 -0.770957
2019-11-04 15:03:03,641 training loss; R2: 1.124695e-02 -0.760266
2019-11-04 15:03:04,186 valid 000 2.446757e-02 -0.242714
2019-11-04 15:03:14,488 valid 050 2.336719e-02 -0.487224
2019-11-04 15:03:23,564 validation loss; R2: 2.360291e-02 -0.500373
2019-11-04 15:03:23,675 epoch 93 lr 9.946743e-04
2019-11-04 15:03:24,371 train 000 1.087415e-02 -0.147207
2019-11-04 15:03:34,661 train 050 1.093313e-02 -0.841467
2019-11-04 15:03:44,958 train 100 1.106728e-02 -0.813437
2019-11-04 15:03:55,557 train 150 1.114546e-02 -0.826037
2019-11-04 15:04:06,164 train 200 1.117646e-02 -0.826464
2019-11-04 15:04:16,727 train 250 1.117252e-02 -0.781871
2019-11-04 15:04:27,335 train 300 1.120792e-02 -0.752189
2019-11-04 15:04:37,796 train 350 1.119078e-02 -0.747375
2019-11-04 15:04:48,393 train 400 1.116429e-02 -1.169884
2019-11-04 15:04:58,856 train 450 1.116348e-02 -1.111166
2019-11-04 15:05:09,366 train 500 1.119361e-02 -1.055451
2019-11-04 15:05:19,791 train 550 1.116853e-02 -1.032714
2019-11-04 15:05:30,297 train 600 1.118394e-02 -1.001755
2019-11-04 15:05:40,743 train 650 1.117304e-02 -0.961162
2019-11-04 15:05:51,283 train 700 1.117976e-02 -0.940294
2019-11-04 15:06:01,810 train 750 1.116775e-02 -0.921333
2019-11-04 15:06:12,373 train 800 1.115120e-02 -0.913335
2019-11-04 15:06:22,920 train 850 1.116687e-02 -0.899787
2019-11-04 15:06:26,163 training loss; R2: 1.117277e-02 -0.896653
2019-11-04 15:06:26,701 valid 000 3.640271e-02 -0.605480
2019-11-04 15:06:36,961 valid 050 3.726942e-02 -0.822164
2019-11-04 15:06:45,984 validation loss; R2: 3.711555e-02 -0.814102
2019-11-04 15:06:46,095 epoch 94 lr 9.945594e-04
2019-11-04 15:06:46,789 train 000 1.224390e-02 0.082017
2019-11-04 15:06:57,382 train 050 1.217136e-02 -178.386775
2019-11-04 15:07:07,946 train 100 1.243592e-02 -90.481151
2019-11-04 15:07:18,455 train 150 1.229364e-02 -60.790639
2019-11-04 15:07:28,965 train 200 1.205022e-02 -45.795121
2019-11-04 15:07:39,495 train 250 1.192323e-02 -36.824743
2019-11-04 15:07:50,044 train 300 1.177099e-02 -30.798758
2019-11-04 15:08:00,515 train 350 1.168047e-02 -26.497639
2019-11-04 15:08:11,000 train 400 1.160988e-02 -23.259538
2019-11-04 15:08:21,457 train 450 1.155682e-02 -20.751303
2019-11-04 15:08:31,946 train 500 1.152419e-02 -18.733216
2019-11-04 15:08:42,461 train 550 1.144577e-02 -17.099686
2019-11-04 15:08:52,981 train 600 1.137188e-02 -15.731020
2019-11-04 15:09:03,522 train 650 1.134463e-02 -14.591757
2019-11-04 15:09:14,056 train 700 1.132131e-02 -13.600328
2019-11-04 15:09:24,619 train 750 1.129425e-02 -12.733281
2019-11-04 15:09:35,182 train 800 1.126426e-02 -11.986258
2019-11-04 15:09:45,756 train 850 1.127059e-02 -11.339536
2019-11-04 15:09:49,004 training loss; R2: 1.126967e-02 -11.145219
2019-11-04 15:09:49,546 valid 000 3.011002e-02 -0.767620
2019-11-04 15:09:59,795 valid 050 3.086233e-02 -1.214232
2019-11-04 15:10:08,831 validation loss; R2: 3.056043e-02 -1.339479
2019-11-04 15:10:08,944 epoch 95 lr 9.944432e-04
2019-11-04 15:10:09,638 train 000 9.649871e-03 -0.208061
2019-11-04 15:10:20,106 train 050 1.076064e-02 -0.662986
2019-11-04 15:10:30,694 train 100 1.080327e-02 -0.815941
2019-11-04 15:10:41,196 train 150 1.087941e-02 -0.780644
2019-11-04 15:10:51,784 train 200 1.092022e-02 -0.774617
2019-11-04 15:11:02,348 train 250 1.091963e-02 -0.790342
2019-11-04 15:11:12,913 train 300 1.100605e-02 -0.832990
2019-11-04 15:11:23,321 train 350 1.107463e-02 -0.785343
2019-11-04 15:11:33,692 train 400 1.107420e-02 -0.785831
2019-11-04 15:11:44,037 train 450 1.107190e-02 -0.775290
2019-11-04 15:11:54,493 train 500 1.106085e-02 -0.748001
2019-11-04 15:12:04,946 train 550 1.105505e-02 -0.730252
2019-11-04 15:12:15,423 train 600 1.106820e-02 -0.746188
2019-11-04 15:12:25,872 train 650 1.109459e-02 -0.726661
2019-11-04 15:12:36,332 train 700 1.108105e-02 -0.766649
2019-11-04 15:12:46,776 train 750 1.109547e-02 -0.760949
2019-11-04 15:12:57,203 train 800 1.109432e-02 -0.760687
2019-11-04 15:13:07,631 train 850 1.108402e-02 -0.751205
2019-11-04 15:13:10,837 training loss; R2: 1.108700e-02 -0.746483
2019-11-04 15:13:11,388 valid 000 3.526163e-02 -1.392727
2019-11-04 15:13:21,859 valid 050 4.002670e-02 -3.835252
2019-11-04 15:13:31,064 validation loss; R2: 3.956177e-02 -4.481036
2019-11-04 15:13:31,188 epoch 96 lr 9.943259e-04
2019-11-04 15:13:31,951 train 000 1.254822e-02 -0.464072
2019-11-04 15:13:42,475 train 050 1.125976e-02 -0.551593
2019-11-04 15:13:53,089 train 100 1.110472e-02 -0.690642
2019-11-04 15:14:03,625 train 150 1.107667e-02 -0.709121
2019-11-04 15:14:14,136 train 200 1.104315e-02 -0.648307
2019-11-04 15:14:24,671 train 250 1.103813e-02 -0.609025
2019-11-04 15:14:35,245 train 300 1.103474e-02 -0.608758
2019-11-04 15:14:45,699 train 350 1.103465e-02 -0.598074
2019-11-04 15:14:56,110 train 400 1.104415e-02 -0.599150
2019-11-04 15:15:06,662 train 450 1.105996e-02 -0.609908
2019-11-04 15:15:17,279 train 500 1.105003e-02 -0.631467
2019-11-04 15:15:27,875 train 550 1.106639e-02 -0.629345
2019-11-04 15:15:38,485 train 600 1.107196e-02 -0.638140
2019-11-04 15:15:49,096 train 650 1.112473e-02 -0.631687
2019-11-04 15:15:59,707 train 700 1.113089e-02 -0.628220
2019-11-04 15:16:10,293 train 750 1.111276e-02 -0.623756
2019-11-04 15:16:20,903 train 800 1.109511e-02 -0.646040
2019-11-04 15:16:31,486 train 850 1.109047e-02 -0.663432
2019-11-04 15:16:34,763 training loss; R2: 1.109445e-02 -0.660775
2019-11-04 15:16:35,313 valid 000 4.873789e-02 -6.916494
2019-11-04 15:16:45,535 valid 050 5.405391e-02 -11.541187
2019-11-04 15:16:54,616 validation loss; R2: 5.356671e-02 -12.290216
2019-11-04 15:16:54,727 epoch 97 lr 9.942073e-04
2019-11-04 15:16:55,436 train 000 1.099891e-02 0.202274
2019-11-04 15:17:05,940 train 050 1.110680e-02 -1.402528
2019-11-04 15:17:16,461 train 100 1.107286e-02 -0.948070
2019-11-04 15:17:27,000 train 150 1.103831e-02 -0.814872
2019-11-04 15:17:37,513 train 200 1.099788e-02 -0.820224
2019-11-04 15:17:48,028 train 250 1.103835e-02 -0.737506
2019-11-04 15:17:58,563 train 300 1.105490e-02 -0.694605
2019-11-04 15:18:09,057 train 350 1.105899e-02 -0.682682
2019-11-04 15:18:19,589 train 400 1.101815e-02 -0.660702
2019-11-04 15:18:30,076 train 450 1.099879e-02 -0.702763
2019-11-04 15:18:40,635 train 500 1.103706e-02 -0.687250
2019-11-04 15:18:51,123 train 550 1.102946e-02 -0.705672
2019-11-04 15:19:01,640 train 600 1.104829e-02 -0.732694
2019-11-04 15:19:12,196 train 650 1.106601e-02 -0.704517
2019-11-04 15:19:22,726 train 700 1.152048e-02 -0.800188
2019-11-04 15:19:33,238 train 750 1.238828e-02 -0.780252
2019-11-04 15:19:43,784 train 800 1.295345e-02 -0.774951
2019-11-04 15:19:54,329 train 850 1.339819e-02 -0.750344
2019-11-04 15:19:57,538 training loss; R2: 1.353744e-02 -0.744882
2019-11-04 15:19:58,125 valid 000 3.386860e-02 -0.536743
2019-11-04 15:20:08,409 valid 050 3.805720e-02 -1.313386
2019-11-04 15:20:17,514 validation loss; R2: 3.770038e-02 -1.450335
2019-11-04 15:20:17,626 epoch 98 lr 9.940875e-04
2019-11-04 15:20:18,375 train 000 2.127683e-02 -2.688968
2019-11-04 15:20:28,899 train 050 2.003278e-02 -0.367502
2019-11-04 15:20:39,419 train 100 1.979557e-02 -0.555801
2019-11-04 15:20:49,929 train 150 1.948443e-02 -0.614563
2019-11-04 15:21:00,385 train 200 1.928889e-02 -0.769977
2019-11-04 15:21:10,842 train 250 1.902207e-02 -0.770463
2019-11-04 15:21:21,372 train 300 1.883415e-02 -0.705889
2019-11-04 15:21:31,845 train 350 1.861671e-02 -0.695205
2019-11-04 15:21:42,391 train 400 1.859000e-02 -0.674191
2019-11-04 15:21:52,973 train 450 1.849799e-02 -0.707388
2019-11-04 15:22:03,469 train 500 1.835614e-02 -0.700182
2019-11-04 15:22:14,013 train 550 1.824897e-02 -0.677448
2019-11-04 15:22:24,603 train 600 1.809345e-02 -0.671788
2019-11-04 15:22:35,208 train 650 1.795028e-02 -0.666657
2019-11-04 15:22:45,794 train 700 1.779794e-02 -0.676500
2019-11-04 15:22:56,364 train 750 1.765312e-02 -0.685875
2019-11-04 15:23:06,941 train 800 1.756693e-02 -0.675720
2019-11-04 15:23:17,480 train 850 1.745851e-02 -0.678161
2019-11-04 15:23:20,704 training loss; R2: 1.742252e-02 -0.674251
2019-11-04 15:23:21,258 valid 000 5.723656e-02 -8.047572
2019-11-04 15:23:31,591 valid 050 6.394777e-02 -53.275123
2019-11-04 15:23:40,690 validation loss; R2: 6.378796e-02 -62.678311
2019-11-04 15:23:40,807 epoch 99 lr 9.939664e-04
2019-11-04 15:23:41,550 train 000 1.462466e-02 -0.391908
2019-11-04 15:23:52,494 train 050 1.558051e-02 -0.979730
2019-11-04 15:24:03,305 train 100 1.546166e-02 -0.725363
2019-11-04 15:24:13,932 train 150 1.533379e-02 -0.651120
2019-11-04 15:24:24,649 train 200 1.521946e-02 -0.665223
2019-11-04 15:24:35,302 train 250 1.507425e-02 -0.656262
2019-11-04 15:24:45,991 train 300 1.518001e-02 -0.648077
2019-11-04 15:24:56,598 train 350 1.514189e-02 -0.653756
2019-11-04 15:25:07,192 train 400 1.508826e-02 -0.644249
2019-11-04 15:25:17,770 train 450 1.500133e-02 -0.648553
2019-11-04 15:25:28,333 train 500 1.490767e-02 -0.925724
2019-11-04 15:25:38,899 train 550 1.486541e-02 -1.548330
2019-11-04 15:25:49,448 train 600 1.478387e-02 -1.462034
2019-11-04 15:26:00,110 train 650 1.473354e-02 -1.387732
2019-11-04 15:26:10,756 train 700 1.468902e-02 -1.331565
2019-11-04 15:26:21,425 train 750 1.460714e-02 -1.272053
2019-11-04 15:26:32,079 train 800 1.453241e-02 -1.245211
2019-11-04 15:26:42,728 train 850 1.446769e-02 -1.211641
2019-11-04 15:26:45,993 training loss; R2: 1.445641e-02 -1.209644
2019-11-04 15:26:46,571 valid 000 6.159777e-02 -10.484427
2019-11-04 15:26:56,944 valid 050 6.773710e-02 -12.650690
2019-11-04 15:27:05,994 validation loss; R2: 6.733699e-02 -12.602548
2019-11-04 15:27:06,118 epoch 100 lr 9.938442e-04
2019-11-04 15:27:06,813 train 000 1.860311e-02 0.060845
2019-11-04 15:27:17,442 train 050 1.366389e-02 -0.412380
2019-11-04 15:27:28,033 train 100 1.347795e-02 -0.488994
2019-11-04 15:27:38,562 train 150 1.343616e-02 -0.593555
2019-11-04 15:27:49,147 train 200 1.348107e-02 -0.608102
2019-11-04 15:27:59,783 train 250 1.343775e-02 -0.615875
2019-11-04 15:28:10,416 train 300 1.341996e-02 -0.652993
2019-11-04 15:28:20,944 train 350 1.340613e-02 -0.627706
2019-11-04 15:28:31,532 train 400 1.348851e-02 -0.658435
2019-11-04 15:28:42,209 train 450 1.353007e-02 -0.770746
2019-11-04 15:28:52,875 train 500 1.349719e-02 -0.755170
2019-11-04 15:29:03,417 train 550 1.345303e-02 -0.747231
2019-11-04 15:29:13,957 train 600 1.343734e-02 -0.750992
2019-11-04 15:29:24,586 train 650 1.338978e-02 -0.769612
2019-11-04 15:29:35,254 train 700 1.339278e-02 -0.786849
2019-11-04 15:29:45,905 train 750 1.337497e-02 -0.793182
2019-11-04 15:29:56,492 train 800 1.332752e-02 -0.797628
2019-11-04 15:30:07,015 train 850 1.329654e-02 -0.793485
2019-11-04 15:30:10,272 training loss; R2: 1.328862e-02 -0.789820
2019-11-04 15:30:10,818 valid 000 5.036180e-02 -2.077839
2019-11-04 15:30:21,132 valid 050 5.653544e-02 -4.870600
2019-11-04 15:30:30,199 validation loss; R2: 5.586102e-02 -5.500180
2019-11-04 15:30:30,310 epoch 101 lr 9.937207e-04
2019-11-04 15:30:31,046 train 000 1.161741e-02 0.161715
2019-11-04 15:30:41,637 train 050 1.263592e-02 -0.953542
2019-11-04 15:30:52,281 train 100 1.249254e-02 -0.803613
2019-11-04 15:31:02,881 train 150 1.255452e-02 -0.718772
2019-11-04 15:31:13,434 train 200 1.266608e-02 -0.665325
2019-11-04 15:31:24,038 train 250 1.265633e-02 -0.660437
2019-11-04 15:31:34,604 train 300 1.267938e-02 -0.637050
2019-11-04 15:31:45,107 train 350 1.266660e-02 -0.654520
2019-11-04 15:31:55,579 train 400 1.266779e-02 -0.647382
2019-11-04 15:32:06,158 train 450 1.265045e-02 -0.663338
2019-11-04 15:32:16,856 train 500 1.264802e-02 -0.669645
2019-11-04 15:32:27,506 train 550 1.261812e-02 -0.698775
2019-11-04 15:32:38,175 train 600 1.261064e-02 -0.679605
2019-11-04 15:32:48,802 train 650 1.260166e-02 -0.674572
2019-11-04 15:32:59,460 train 700 1.260219e-02 -0.671449
2019-11-04 15:33:10,154 train 750 1.260093e-02 -0.680534
2019-11-04 15:33:20,872 train 800 1.260329e-02 -0.666693
2019-11-04 15:33:31,576 train 850 1.257000e-02 -0.652404
2019-11-04 15:33:34,842 training loss; R2: 1.256789e-02 -0.652364
2019-11-04 15:33:35,380 valid 000 7.226440e-02 -1.493374
2019-11-04 15:33:45,588 valid 050 7.769886e-02 -3.655409
2019-11-04 15:33:54,609 validation loss; R2: 7.791427e-02 -3.773070
2019-11-04 15:33:54,719 epoch 102 lr 9.935960e-04
2019-11-04 15:33:55,492 train 000 1.322407e-02 -0.601350
2019-11-04 15:34:06,124 train 050 1.230956e-02 -0.733020
2019-11-04 15:34:16,762 train 100 1.238007e-02 -0.701387
2019-11-04 15:34:27,341 train 150 1.228588e-02 -1.233538
2019-11-04 15:34:37,998 train 200 1.232601e-02 -1.060334
2019-11-04 15:34:48,626 train 250 1.223858e-02 -0.950992
2019-11-04 15:34:59,156 train 300 1.221423e-02 -0.914902
2019-11-04 15:35:09,542 train 350 1.221262e-02 -0.872287
2019-11-04 15:35:20,092 train 400 1.221544e-02 -0.842761
2019-11-04 15:35:30,690 train 450 1.228455e-02 -0.831519
2019-11-04 15:35:41,228 train 500 1.229608e-02 -0.850262
2019-11-04 15:35:51,719 train 550 1.232705e-02 -0.854411
2019-11-04 15:36:02,262 train 600 1.233122e-02 -0.841914
2019-11-04 15:36:12,806 train 650 1.237528e-02 -1.467265
2019-11-04 15:36:23,320 train 700 1.235159e-02 -1.459052
2019-11-04 15:36:33,868 train 750 1.233331e-02 -1.441811
2019-11-04 15:36:44,427 train 800 1.231564e-02 -1.391748
2019-11-04 15:36:55,020 train 850 1.231937e-02 -1.343561
2019-11-04 15:36:58,296 training loss; R2: 1.232449e-02 -1.329418
2019-11-04 15:36:58,874 valid 000 3.488301e-02 -1.224325
2019-11-04 15:37:09,069 valid 050 3.963971e-02 -2.993061
2019-11-04 15:37:18,100 validation loss; R2: 3.939429e-02 -3.288848
2019-11-04 15:37:18,218 epoch 103 lr 9.934701e-04
2019-11-04 15:37:18,949 train 000 1.073470e-02 -0.041143
2019-11-04 15:37:29,603 train 050 1.239493e-02 -0.466322
2019-11-04 15:37:40,230 train 100 1.219857e-02 -0.557108
2019-11-04 15:37:50,832 train 150 1.215459e-02 -0.637316
2019-11-04 15:38:01,407 train 200 1.210233e-02 -0.635515
2019-11-04 15:38:11,936 train 250 1.218336e-02 -0.622488
2019-11-04 15:38:22,495 train 300 1.216752e-02 -0.706635
2019-11-04 15:38:32,988 train 350 1.216437e-02 -0.715511
2019-11-04 15:38:43,520 train 400 1.217401e-02 -0.707171
2019-11-04 15:38:54,001 train 450 1.212123e-02 -0.691821
2019-11-04 15:39:04,533 train 500 1.212974e-02 -0.693123
2019-11-04 15:39:15,033 train 550 1.215748e-02 -0.697221
2019-11-04 15:39:25,566 train 600 1.217290e-02 -0.684426
2019-11-04 15:39:36,107 train 650 1.217646e-02 -0.702432
2019-11-04 15:39:46,720 train 700 1.217950e-02 -0.692609
2019-11-04 15:39:57,341 train 750 1.218170e-02 -1.052260
2019-11-04 15:40:07,940 train 800 1.219305e-02 -1.085992
2019-11-04 15:40:18,527 train 850 1.219099e-02 -1.063437
2019-11-04 15:40:21,787 training loss; R2: 1.219567e-02 -1.065955
2019-11-04 15:40:22,317 valid 000 3.399955e-02 -1.309469
2019-11-04 15:40:32,515 valid 050 3.762461e-02 -2.379532
2019-11-04 15:40:41,471 validation loss; R2: 3.749135e-02 -2.512963
2019-11-04 15:40:41,585 epoch 104 lr 9.933430e-04
2019-11-04 15:40:42,334 train 000 1.030305e-02 -0.329976
2019-11-04 15:40:52,865 train 050 1.212830e-02 -0.457570
2019-11-04 15:41:03,417 train 100 1.210198e-02 -0.630660
2019-11-04 15:41:13,929 train 150 1.216840e-02 -0.625986
2019-11-04 15:41:24,459 train 200 1.214744e-02 -0.632882
2019-11-04 15:41:34,957 train 250 1.214736e-02 -0.695482
2019-11-04 15:41:45,472 train 300 1.209575e-02 -0.692992
2019-11-04 15:41:55,902 train 350 1.205819e-02 -0.698619
2019-11-04 15:42:06,357 train 400 1.204615e-02 -0.696823
2019-11-04 15:42:16,722 train 450 1.200760e-02 -0.693218
2019-11-04 15:42:27,253 train 500 1.206332e-02 -0.667969
2019-11-04 15:42:37,805 train 550 1.211045e-02 -0.650871
2019-11-04 15:42:48,284 train 600 1.210838e-02 -0.755296
2019-11-04 15:42:58,769 train 650 1.209236e-02 -0.734929
2019-11-04 15:43:09,258 train 700 1.208764e-02 -0.720441
2019-11-04 15:43:19,777 train 750 1.207995e-02 -0.769998
2019-11-04 15:43:30,379 train 800 1.205273e-02 -0.767680
2019-11-04 15:43:40,927 train 850 1.202793e-02 -0.745514
2019-11-04 15:43:44,215 training loss; R2: 1.202210e-02 -0.740560
2019-11-04 15:43:44,761 valid 000 3.339514e-02 -0.960990
2019-11-04 15:43:54,996 valid 050 3.832162e-02 -3.104561
2019-11-04 15:44:04,019 validation loss; R2: 3.806153e-02 -3.542637
2019-11-04 15:44:04,131 epoch 105 lr 9.932146e-04
2019-11-04 15:44:04,881 train 000 1.117384e-02 -0.602482
2019-11-04 15:44:15,440 train 050 1.216938e-02 -0.734623
2019-11-04 15:44:25,994 train 100 1.222516e-02 -0.796023
2019-11-04 15:44:36,547 train 150 1.218782e-02 -0.731312
2019-11-04 15:44:47,088 train 200 1.221045e-02 -0.895747
2019-11-04 15:44:57,626 train 250 1.209486e-02 -0.826236
2019-11-04 15:45:08,162 train 300 1.207553e-02 -0.803144
2019-11-04 15:45:18,657 train 350 1.207070e-02 -0.787736
2019-11-04 15:45:29,014 train 400 1.201239e-02 -0.791343
2019-11-04 15:45:39,419 train 450 1.198908e-02 -0.777319
2019-11-04 15:45:49,916 train 500 1.197486e-02 -0.786182
2019-11-04 15:46:00,423 train 550 1.200252e-02 -0.777906
2019-11-04 15:46:10,931 train 600 1.200643e-02 -0.750494
2019-11-04 15:46:21,435 train 650 1.199173e-02 -0.736636
2019-11-04 15:46:32,018 train 700 1.197840e-02 -0.728817
2019-11-04 15:46:42,524 train 750 1.199789e-02 -0.736840
2019-11-04 15:46:53,140 train 800 1.201885e-02 -0.728774
