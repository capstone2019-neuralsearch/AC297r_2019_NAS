2019-11-01 22:05:49,627 gpu device = 0
2019-11-01 22:05:49,628 args = Namespace(arch_learning_rate=0.0003, arch_weight_decay=0.001, batch_size=20, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.3, epochs=50, gpu=0, grad_clip=5, gz_regression=False, init_channels=16, layers=8, learning_rate=0.01, learning_rate_min=0.001, model_path='saved_models', momentum=0.9, report_freq=50, save='search-GZ_TEST-20191101-220549', seed=2, train_portion=0.5, unrolled=True, weight_decay=1e-06)
2019-11-01 22:05:53,164 param size = 2.229141MB
2019-11-01 22:05:53,177 epoch 0 lr 1.000000e-02
2019-11-01 22:05:53,178 genotype = Genotype(normal=[('dil_conv_5x5', 1), ('skip_connect', 0), ('sep_conv_5x5', 2), ('skip_connect', 1), ('avg_pool_3x3', 3), ('dil_conv_5x5', 1), ('sep_conv_3x3', 3), ('skip_connect', 0)], normal_concat=range(2, 6), reduce=[('sep_conv_3x3', 1), ('dil_conv_5x5', 0), ('sep_conv_5x5', 1), ('max_pool_3x3', 2), ('skip_connect', 1), ('sep_conv_5x5', 3), ('max_pool_3x3', 3), ('max_pool_3x3', 1)], reduce_concat=range(2, 6))
2019-11-01 22:05:53,180 
alphas_normal = Variable containing:
 0.1251  0.1250  0.1250  0.1251  0.1251  0.1250  0.1250  0.1249
 0.1250  0.1249  0.1250  0.1249  0.1251  0.1251  0.1247  0.1254
 0.1251  0.1251  0.1248  0.1250  0.1250  0.1249  0.1250  0.1250
 0.1250  0.1250  0.1250  0.1251  0.1250  0.1248  0.1250  0.1251
 0.1251  0.1251  0.1249  0.1250  0.1248  0.1253  0.1247  0.1251
 0.1251  0.1251  0.1250  0.1249  0.1248  0.1248  0.1250  0.1251
 0.1249  0.1250  0.1251  0.1249  0.1250  0.1250  0.1249  0.1252
 0.1252  0.1249  0.1250  0.1250  0.1250  0.1251  0.1250  0.1248
 0.1250  0.1250  0.1252  0.1251  0.1249  0.1251  0.1247  0.1250
 0.1250  0.1248  0.1249  0.1252  0.1250  0.1249  0.1250  0.1252
 0.1250  0.1252  0.1251  0.1249  0.1248  0.1249  0.1251  0.1250
 0.1250  0.1250  0.1250  0.1249  0.1251  0.1249  0.1250  0.1251
 0.1250  0.1251  0.1248  0.1249  0.1252  0.1250  0.1249  0.1249
 0.1249  0.1251  0.1250  0.1249  0.1250  0.1251  0.1250  0.1250
[torch.cuda.FloatTensor of size 14x8 (GPU 0)]

2019-11-01 22:05:53,182 
alphas_reduce = Variable containing:
 0.1249  0.1251  0.1250  0.1250  0.1249  0.1250  0.1249  0.1253
 0.1249  0.1250  0.1248  0.1251  0.1253  0.1251  0.1250  0.1249
 0.1251  0.1250  0.1250  0.1250  0.1250  0.1251  0.1251  0.1248
 0.1249  0.1250  0.1250  0.1250  0.1249  0.1252  0.1250  0.1250
 0.1249  0.1252  0.1251  0.1249  0.1247  0.1250  0.1252  0.1251
 0.1250  0.1248  0.1252  0.1250  0.1249  0.1251  0.1249  0.1251
 0.1251  0.1251  0.1250  0.1253  0.1247  0.1249  0.1250  0.1248
 0.1251  0.1251  0.1251  0.1251  0.1249  0.1248  0.1250  0.1250
 0.1250  0.1251  0.1250  0.1250  0.1251  0.1253  0.1248  0.1247
 0.1252  0.1247  0.1250  0.1251  0.1250  0.1250  0.1250  0.1250
 0.1250  0.1252  0.1249  0.1250  0.1250  0.1252  0.1249  0.1249
 0.1249  0.1250  0.1249  0.1250  0.1250  0.1251  0.1249  0.1251
 0.1249  0.1253  0.1247  0.1251  0.1249  0.1252  0.1251  0.1248
 0.1249  0.1252  0.1249  0.1250  0.1250  0.1249  0.1251  0.1251
[torch.cuda.FloatTensor of size 14x8 (GPU 0)]

2019-11-01 22:06:00,112 train 000 3.432354e-02 -3.629193
2019-11-01 22:08:43,510 train 050 3.913953e-02 -5.558475
2019-11-01 22:11:26,414 train 100 3.562775e-02 -5.036732
2019-11-01 22:14:10,732 train 150 3.394646e-02 -4.705849
2019-11-01 22:16:55,682 train 200 3.223404e-02 -4.574289
2019-11-01 22:19:40,157 train 250 3.095848e-02 -9.559211
2019-11-01 22:22:25,886 train 300 2.998344e-02 -8.643598
2019-11-01 22:25:09,975 train 350 2.918405e-02 -7.904489
2019-11-01 22:27:55,689 train 400 2.843966e-02 -7.472551
2019-11-01 22:30:41,042 train 450 2.785577e-02 -7.262919
2019-11-01 22:33:24,385 train 500 2.727199e-02 -6.826945
2019-11-01 22:36:09,508 train 550 2.670502e-02 -6.453506
2019-11-01 22:38:53,746 train 600 2.623478e-02 -6.220124
2019-11-01 22:41:38,203 train 650 2.582364e-02 -5.934798
2019-11-01 22:44:22,898 train 700 2.544498e-02 -5.675841
2019-11-01 22:47:07,422 train 750 2.515833e-02 -5.509372
2019-11-01 22:49:53,231 train 800 2.487947e-02 -5.309964
2019-11-01 22:52:38,071 train 850 2.461359e-02 -5.286018
2019-11-01 22:55:22,285 train 900 2.437949e-02 -34.028954
2019-11-01 22:58:07,373 train 950 2.410984e-02 -32.470753
2019-11-01 23:00:55,195 train 1000 2.385415e-02 -30.961208
2019-11-01 23:03:42,049 train 1050 2.364394e-02 -31.535757
2019-11-01 23:06:27,225 train 1100 2.341055e-02 -30.200330
2019-11-01 23:09:12,605 train 1150 2.322392e-02 -28.991777
2019-11-01 23:11:57,242 train 1200 2.305038e-02 -30.734373
2019-11-01 23:14:42,079 train 1250 2.285239e-02 -29.574403
2019-11-01 23:17:26,739 train 1300 2.269893e-02 -28.560049
2019-11-01 23:20:10,907 train 1350 2.253640e-02 -27.577765
2019-11-01 23:22:53,844 train 1400 2.237149e-02 -26.674712
2019-11-01 23:25:38,070 train 1450 2.219031e-02 -27.901427
2019-11-01 23:28:23,540 train 1500 2.204596e-02 -27.043402
2019-11-01 23:30:34,089 training loss; accuracy or R2: 2.195099e-02 -26.417381
2019-11-01 23:30:34,557 valid 000 1.617447e-02 -0.919160
2019-11-01 23:30:43,328 valid 050 1.726802e-02 -1.898522
2019-11-01 23:30:52,086 valid 100 1.676424e-02 -1.987091
2019-11-01 23:31:00,879 valid 150 1.673896e-02 -1.967291
2019-11-01 23:31:09,627 valid 200 1.669284e-02 -25.976404
2019-11-01 23:31:18,412 valid 250 1.664441e-02 -21.180170
2019-11-01 23:31:27,165 valid 300 1.668751e-02 -19.077701
2019-11-01 23:31:35,929 valid 350 1.668533e-02 -16.583090
2019-11-01 23:31:44,658 valid 400 1.662275e-02 -14.732674
2019-11-01 23:31:53,388 valid 450 1.666599e-02 -14.400885
2019-11-01 23:32:02,147 valid 500 1.673952e-02 -13.170449
2019-11-01 23:32:10,921 valid 550 1.684843e-02 -12.146304
2019-11-01 23:32:19,703 valid 600 1.684286e-02 -11.249489
2019-11-01 23:32:28,409 valid 650 1.680452e-02 -10.540704
2019-11-01 23:32:37,097 valid 700 1.681388e-02 -9.937386
2019-11-01 23:32:45,839 valid 750 1.686265e-02 -9.478209
2019-11-01 23:32:54,579 valid 800 1.688338e-02 -9.026319
2019-11-01 23:33:03,330 valid 850 1.688095e-02 -8.624139
2019-11-01 23:33:12,159 valid 900 1.689237e-02 -8.283904
2019-11-01 23:33:20,994 valid 950 1.688235e-02 -7.969217
2019-11-01 23:33:29,817 valid 1000 1.686508e-02 -7.661587
2019-11-01 23:33:38,604 valid 1050 1.684112e-02 -7.412342
2019-11-01 23:33:47,404 valid 1100 1.685035e-02 -7.128629
2019-11-01 23:33:56,175 valid 1150 1.684337e-02 -6.913128
2019-11-01 23:34:04,936 valid 1200 1.685215e-02 -7.064294
2019-11-01 23:34:13,754 valid 1250 1.685971e-02 -9.070509
2019-11-01 23:34:22,542 valid 1300 1.684400e-02 -8.791865
2019-11-01 23:34:31,302 valid 1350 1.681486e-02 -8.524997
2019-11-01 23:34:40,082 valid 1400 1.681417e-02 -8.482303
2019-11-01 23:34:48,846 valid 1450 1.681312e-02 -8.292752
2019-11-01 23:34:57,613 valid 1500 1.681213e-02 -8.103827
2019-11-01 23:35:04,439 validation loss; accuracy or R2: 1.681368e-02 -7.955365
2019-11-01 23:35:04,564 epoch 1 lr 9.991120e-03
2019-11-01 23:35:04,565 genotype = Genotype(normal=[('dil_conv_5x5', 0), ('sep_conv_5x5', 1), ('max_pool_3x3', 2), ('dil_conv_5x5', 1), ('sep_conv_5x5', 0), ('max_pool_3x3', 2), ('dil_conv_5x5', 1), ('max_pool_3x3', 4)], normal_concat=range(2, 6), reduce=[('skip_connect', 1), ('dil_conv_3x3', 0), ('sep_conv_5x5', 1), ('max_pool_3x3', 2), ('max_pool_3x3', 3), ('skip_connect', 1), ('dil_conv_5x5', 2), ('dil_conv_5x5', 3)], reduce_concat=range(2, 6))
2019-11-01 23:35:04,567 
alphas_normal = Variable containing:
 0.1258  0.1234  0.1225  0.1225  0.1259  0.1250  0.1269  0.1280
 0.1234  0.1246  0.1246  0.1238  0.1250  0.1279  0.1236  0.1270
 0.1257  0.1244  0.1229  0.1225  0.1265  0.1248  0.1269  0.1263
 0.1238  0.1258  0.1235  0.1232  0.1243  0.1269  0.1255  0.1270
 0.1243  0.1275  0.1253  0.1241  0.1225  0.1273  0.1238  0.1252
 0.1238  0.1247  0.1232  0.1236  0.1260  0.1282  0.1239  0.1266
 0.1241  0.1260  0.1255  0.1251  0.1255  0.1259  0.1239  0.1240
 0.1235  0.1272  0.1256  0.1250  0.1255  0.1253  0.1231  0.1247
 0.1241  0.1268  0.1258  0.1257  0.1265  0.1245  0.1221  0.1245
 0.1253  0.1259  0.1258  0.1262  0.1246  0.1253  0.1230  0.1241
 0.1246  0.1247  0.1239  0.1237  0.1252  0.1249  0.1243  0.1286
 0.1240  0.1271  0.1257  0.1248  0.1242  0.1249  0.1237  0.1256
 0.1233  0.1269  0.1257  0.1250  0.1244  0.1268  0.1249  0.1229
 0.1230  0.1276  0.1264  0.1265  0.1224  0.1239  0.1231  0.1271
[torch.cuda.FloatTensor of size 14x8 (GPU 0)]

2019-11-01 23:35:04,568 
alphas_reduce = Variable containing:
 0.1262  0.1268  0.1253  0.1217  0.1238  0.1240  0.1272  0.1249
 0.1239  0.1229  0.1218  0.1275  0.1262  0.1257  0.1251  0.1268
 0.1257  0.1254  0.1251  0.1245  0.1247  0.1250  0.1245  0.1250
 0.1242  0.1237  0.1230  0.1258  0.1251  0.1279  0.1253  0.1251
 0.1238  0.1262  0.1235  0.1239  0.1254  0.1261  0.1262  0.1250
 0.1245  0.1249  0.1241  0.1250  0.1253  0.1250  0.1257  0.1255
 0.1250  0.1241  0.1232  0.1268  0.1259  0.1250  0.1260  0.1238
 0.1257  0.1255  0.1230  0.1243  0.1267  0.1260  0.1242  0.1247
 0.1251  0.1269  0.1249  0.1247  0.1243  0.1245  0.1233  0.1263
 0.1256  0.1254  0.1245  0.1245  0.1252  0.1265  0.1239  0.1244
 0.1255  0.1244  0.1240  0.1257  0.1254  0.1262  0.1239  0.1250
 0.1258  0.1248  0.1231  0.1246  0.1264  0.1245  0.1239  0.1270
 0.1243  0.1262  0.1248  0.1244  0.1260  0.1237  0.1236  0.1270
 0.1254  0.1254  0.1243  0.1245  0.1233  0.1268  0.1237  0.1267
[torch.cuda.FloatTensor of size 14x8 (GPU 0)]

2019-11-01 23:35:08,241 train 000 1.716403e-02 -3.793270
2019-11-01 23:37:53,400 train 050 1.768935e-02 -3.596338
2019-11-01 23:40:37,972 train 100 1.741415e-02 -2.985505
2019-11-01 23:43:23,283 train 150 1.738253e-02 -150.035600
2019-11-01 23:46:08,419 train 200 1.751876e-02 -113.468647
2019-11-01 23:48:55,271 train 250 1.731001e-02 -91.229682
2019-11-01 23:51:40,872 train 300 1.721657e-02 -76.396950
2019-11-01 23:54:27,154 train 350 1.714444e-02 -65.820184
2019-11-01 23:57:11,744 train 400 1.705405e-02 -57.856200
2019-11-01 23:59:55,978 train 450 1.706251e-02 -51.635169
2019-11-02 00:02:40,179 train 500 1.700084e-02 -46.707996
2019-11-02 00:05:24,362 train 550 1.696209e-02 -111.172083
2019-11-02 00:08:10,511 train 600 1.691287e-02 -102.102164
2019-11-02 00:10:56,570 train 650 1.686400e-02 -94.495360
2019-11-02 00:13:41,718 train 700 1.683674e-02 -87.965893
2019-11-02 00:16:25,301 train 750 1.682135e-02 -82.368252
2019-11-02 00:19:10,726 train 800 1.679118e-02 -77.348324
2019-11-02 00:21:56,127 train 850 1.671959e-02 -72.920319
2019-11-02 00:24:41,792 train 900 1.670974e-02 -69.001676
2019-11-02 00:27:28,100 train 950 1.670444e-02 -65.478459
2019-11-02 00:30:14,661 train 1000 1.667831e-02 -62.293577
2019-11-02 00:33:00,239 train 1050 1.663323e-02 -59.421022
2019-11-02 00:35:45,550 train 1100 1.660486e-02 -56.849971
2019-11-02 00:38:31,107 train 1150 1.656413e-02 -54.437545
2019-11-02 00:41:17,972 train 1200 1.651459e-02 -52.244204
2019-11-02 00:44:03,545 train 1250 1.647100e-02 -50.329636
2019-11-02 00:46:47,540 train 1300 1.645603e-02 -48.466688
2019-11-02 00:49:31,902 train 1350 1.639952e-02 -46.746581
2019-11-02 00:52:17,276 train 1400 1.635120e-02 -45.159153
2019-11-02 00:55:02,485 train 1450 1.634005e-02 -43.757562
2019-11-02 00:57:47,491 train 1500 1.629910e-02 -42.415046
2019-11-02 00:59:56,707 training loss; accuracy or R2: 1.629581e-02 -41.530966
2019-11-02 00:59:57,202 valid 000 1.371372e-02 -1.693996
2019-11-02 01:00:06,052 valid 050 1.599034e-02 -4.336396
2019-11-02 01:00:14,864 valid 100 1.591206e-02 -3.592689
2019-11-02 01:00:23,652 valid 150 1.564221e-02 -3.142987
2019-11-02 01:00:32,458 valid 200 1.558304e-02 -2.976635
2019-11-02 01:00:41,251 valid 250 1.563194e-02 -2.718388
2019-11-02 01:00:50,045 valid 300 1.556736e-02 -2.629013
2019-11-02 01:00:58,841 valid 350 1.555961e-02 -2.512282
2019-11-02 01:01:07,587 valid 400 1.556080e-02 -2.512653
2019-11-02 01:01:16,364 valid 450 1.562667e-02 -2.507207
2019-11-02 01:01:25,168 valid 500 1.564387e-02 -2.417422
2019-11-02 01:01:33,927 valid 550 1.566078e-02 -2.391149
2019-11-02 01:01:42,668 valid 600 1.565744e-02 -2.407897
2019-11-02 01:01:51,433 valid 650 1.561808e-02 -2.387649
2019-11-02 01:02:00,185 valid 700 1.558325e-02 -2.352868
2019-11-02 01:02:08,964 valid 750 1.561666e-02 -2.594801
2019-11-02 01:02:17,718 valid 800 1.559608e-02 -2.602056
2019-11-02 01:02:26,493 valid 850 1.559906e-02 -2.582080
2019-11-02 01:02:35,259 valid 900 1.561002e-02 -2.570989
2019-11-02 01:02:44,012 valid 950 1.561831e-02 -2.557435
2019-11-02 01:02:52,795 valid 1000 1.560492e-02 -2.612828
2019-11-02 01:03:01,549 valid 1050 1.559797e-02 -2.603727
2019-11-02 01:03:10,255 valid 1100 1.559257e-02 -2.580451
2019-11-02 01:03:19,025 valid 1150 1.559526e-02 -2.570850
2019-11-02 01:03:27,778 valid 1200 1.560568e-02 -2.537220
2019-11-02 01:03:36,529 valid 1250 1.560604e-02 -2.520746
2019-11-02 01:03:45,293 valid 1300 1.561478e-02 -2.495124
2019-11-02 01:03:54,047 valid 1350 1.559315e-02 -2.471453
2019-11-02 01:04:02,839 valid 1400 1.556876e-02 -2.519598
2019-11-02 01:04:11,599 valid 1450 1.557203e-02 -2.500882
2019-11-02 01:04:20,342 valid 1500 1.557641e-02 -2.488139
2019-11-02 01:04:27,114 validation loss; accuracy or R2: 1.556145e-02 -2.507323
2019-11-02 01:04:27,250 epoch 2 lr 9.964516e-03
2019-11-02 01:04:27,251 genotype = Genotype(normal=[('sep_conv_5x5', 0), ('sep_conv_3x3', 1), ('sep_conv_5x5', 2), ('dil_conv_5x5', 0), ('sep_conv_5x5', 0), ('sep_conv_5x5', 1), ('dil_conv_5x5', 1), ('sep_conv_5x5', 0)], normal_concat=range(2, 6), reduce=[('dil_conv_5x5', 1), ('dil_conv_3x3', 0), ('sep_conv_5x5', 1), ('sep_conv_5x5', 0), ('dil_conv_5x5', 3), ('dil_conv_5x5', 2), ('sep_conv_5x5', 1), ('sep_conv_3x3', 0)], reduce_concat=range(2, 6))
2019-11-02 01:04:27,252 
alphas_normal = Variable containing:
 0.1249  0.1246  0.1224  0.1230  0.1262  0.1299  0.1241  0.1248
 0.1235  0.1246  0.1233  0.1231  0.1285  0.1267  0.1237  0.1267
 0.1263  0.1240  0.1223  0.1227  0.1250  0.1268  0.1255  0.1274
 0.1245  0.1250  0.1229  0.1238  0.1270  0.1270  0.1235  0.1263
 0.1246  0.1267  0.1233  0.1231  0.1232  0.1275  0.1254  0.1262
 0.1244  0.1258  0.1243  0.1246  0.1222  0.1287  0.1243  0.1256
 0.1252  0.1254  0.1242  0.1244  0.1245  0.1276  0.1242  0.1246
 0.1239  0.1269  0.1248  0.1247  0.1274  0.1255  0.1229  0.1239
 0.1238  0.1270  0.1256  0.1253  0.1257  0.1243  0.1238  0.1246
 0.1255  0.1242  0.1234  0.1237  0.1236  0.1274  0.1254  0.1267
 0.1232  0.1256  0.1243  0.1243  0.1255  0.1241  0.1245  0.1285
 0.1253  0.1264  0.1248  0.1248  0.1253  0.1251  0.1234  0.1249
 0.1248  0.1251  0.1244  0.1245  0.1249  0.1262  0.1250  0.1250
 0.1249  0.1257  0.1245  0.1241  0.1251  0.1245  0.1239  0.1273
[torch.cuda.FloatTensor of size 14x8 (GPU 0)]

2019-11-02 01:04:27,254 
alphas_reduce = Variable containing:
 0.1265  0.1237  0.1233  0.1230  0.1251  0.1250  0.1272  0.1262
 0.1240  0.1226  0.1218  0.1284  0.1250  0.1246  0.1246  0.1290
 0.1256  0.1235  0.1227  0.1247  0.1268  0.1270  0.1250  0.1248
 0.1246  0.1233  0.1228  0.1254  0.1258  0.1271  0.1261  0.1249
 0.1245  0.1259  0.1236  0.1240  0.1256  0.1263  0.1246  0.1255
 0.1250  0.1248  0.1245  0.1263  0.1245  0.1253  0.1254  0.1241
 0.1254  0.1240  0.1238  0.1260  0.1251  0.1256  0.1254  0.1247
 0.1252  0.1244  0.1236  0.1248  0.1254  0.1258  0.1241  0.1268
 0.1247  0.1252  0.1249  0.1250  0.1248  0.1241  0.1245  0.1268
 0.1255  0.1245  0.1245  0.1242  0.1267  0.1254  0.1244  0.1247
 0.1252  0.1246  0.1243  0.1245  0.1256  0.1268  0.1242  0.1248
 0.1245  0.1251  0.1244  0.1247  0.1247  0.1254  0.1250  0.1262
 0.1244  0.1257  0.1256  0.1252  0.1249  0.1260  0.1238  0.1243
 0.1249  0.1250  0.1246  0.1250  0.1252  0.1265  0.1234  0.1254
[torch.cuda.FloatTensor of size 14x8 (GPU 0)]

2019-11-02 01:04:30,824 train 000 2.327620e-02 -1.427577
2019-11-02 01:07:15,200 train 050 1.543168e-02 -2.702168
2019-11-02 01:10:00,153 train 100 1.551942e-02 -2.510189
2019-11-02 01:12:44,009 train 150 1.537464e-02 -2.233538
2019-11-02 01:15:28,457 train 200 1.521782e-02 -3.210566
2019-11-02 01:18:13,855 train 250 1.514286e-02 -2.924829
2019-11-02 01:20:58,763 train 300 1.504829e-02 -2.765106
2019-11-02 01:23:44,049 train 350 1.496041e-02 -2.643671
2019-11-02 01:26:28,953 train 400 1.492538e-02 -10.016368
2019-11-02 01:29:13,761 train 450 1.491597e-02 -9.105869
2019-11-02 01:31:58,824 train 500 1.495235e-02 -8.364390
2019-11-02 01:34:43,568 train 550 1.491427e-02 -7.944659
2019-11-02 01:37:29,437 train 600 1.491336e-02 -7.460153
2019-11-02 01:40:13,907 train 650 1.488759e-02 -7.445211
2019-11-02 01:42:57,830 train 700 1.487108e-02 -7.055382
2019-11-02 01:45:42,598 train 750 1.487911e-02 -6.703241
2019-11-02 01:48:27,203 train 800 1.488656e-02 -6.363780
2019-11-02 01:51:11,219 train 850 1.486736e-02 -6.082918
2019-11-02 01:53:55,822 train 900 1.486758e-02 -5.887488
2019-11-02 01:56:42,913 train 950 1.485989e-02 -5.664407
2019-11-02 01:59:29,963 train 1000 1.485147e-02 -5.478953
2019-11-02 02:02:14,932 train 1050 1.486697e-02 -5.295124
2019-11-02 02:04:59,770 train 1100 1.486925e-02 -5.169225
2019-11-02 02:07:44,724 train 1150 1.485131e-02 -5.780769
2019-11-02 02:10:30,654 train 1200 1.482798e-02 -6.018444
2019-11-02 02:13:15,692 train 1250 1.480677e-02 -5.862382
2019-11-02 02:16:00,871 train 1300 1.478104e-02 -5.687448
2019-11-02 02:18:46,075 train 1350 1.476091e-02 -5.545852
2019-11-02 02:21:30,588 train 1400 1.474567e-02 -5.520895
2019-11-02 02:24:13,961 train 1450 1.473462e-02 -5.387243
2019-11-02 02:26:58,060 train 1500 1.470345e-02 -5.869256
2019-11-02 02:29:06,122 training loss; accuracy or R2: 1.467948e-02 -5.755410
2019-11-02 02:29:06,625 valid 000 1.655170e-02 -0.918440
2019-11-02 02:29:15,407 valid 050 1.576597e-02 -2.865715
2019-11-02 02:29:24,172 valid 100 1.527529e-02 -3.227718
2019-11-02 02:29:32,913 valid 150 1.523127e-02 -2.824804
2019-11-02 02:29:41,638 valid 200 1.523289e-02 -2.741961
2019-11-02 02:29:50,387 valid 250 1.520199e-02 -2.573688
2019-11-02 02:29:59,167 valid 300 1.511736e-02 -2.494358
2019-11-02 02:30:07,907 valid 350 1.518438e-02 -2.430963
2019-11-02 02:30:16,675 valid 400 1.510919e-02 -2.523895
2019-11-02 02:30:25,396 valid 450 1.504611e-02 -2.629635
2019-11-02 02:30:34,068 valid 500 1.504185e-02 -2.565690
2019-11-02 02:30:42,766 valid 550 1.499375e-02 -2.514914
2019-11-02 02:30:51,469 valid 600 1.493965e-02 -2.612130
2019-11-02 02:31:00,235 valid 650 1.493221e-02 -2.754818
2019-11-02 02:31:08,985 valid 700 1.493184e-02 -2.679357
2019-11-02 02:31:17,750 valid 750 1.490927e-02 -2.649092
2019-11-02 02:31:26,522 valid 800 1.495631e-02 -2.589981
2019-11-02 02:31:35,297 valid 850 1.494809e-02 -2.638960
2019-11-02 02:31:44,039 valid 900 1.499299e-02 -2.631238
2019-11-02 02:31:52,830 valid 950 1.500915e-02 -2.674112
2019-11-02 02:32:01,611 valid 1000 1.503136e-02 -16.430039
2019-11-02 02:32:10,361 valid 1050 1.505824e-02 -16.599791
2019-11-02 02:32:19,137 valid 1100 1.506556e-02 -15.998954
2019-11-02 02:32:27,907 valid 1150 1.504328e-02 -15.442426
2019-11-02 02:32:36,701 valid 1200 1.508856e-02 -14.899834
2019-11-02 02:32:45,477 valid 1250 1.509806e-02 -14.377266
2019-11-02 02:32:54,224 valid 1300 1.511181e-02 -13.912071
2019-11-02 02:33:02,992 valid 1350 1.510285e-02 -14.953182
2019-11-02 02:33:11,778 valid 1400 1.511175e-02 -14.486579
2019-11-02 02:33:20,544 valid 1450 1.509067e-02 -14.062480
2019-11-02 02:33:29,350 valid 1500 1.510999e-02 -13.685279
2019-11-02 02:33:36,165 validation loss; accuracy or R2: 1.513173e-02 -13.393667
2019-11-02 02:33:36,296 epoch 3 lr 9.920293e-03
2019-11-02 02:33:36,297 genotype = Genotype(normal=[('sep_conv_5x5', 1), ('sep_conv_5x5', 0), ('sep_conv_5x5', 1), ('dil_conv_5x5', 0), ('sep_conv_3x3', 2), ('max_pool_3x3', 3), ('dil_conv_5x5', 1), ('max_pool_3x3', 2)], normal_concat=range(2, 6), reduce=[('dil_conv_5x5', 1), ('dil_conv_5x5', 0), ('sep_conv_3x3', 1), ('dil_conv_3x3', 2), ('dil_conv_5x5', 3), ('dil_conv_5x5', 2), ('sep_conv_5x5', 2), ('sep_conv_3x3', 3)], reduce_concat=range(2, 6))
2019-11-02 02:33:36,299 
alphas_normal = Variable containing:
 0.1251  0.1238  0.1211  0.1219  0.1278  0.1292  0.1257  0.1255
 0.1238  0.1247  0.1235  0.1234  0.1251  0.1293  0.1240  0.1261
 0.1251  0.1239  0.1225  0.1231  0.1270  0.1271  0.1241  0.1272
 0.1246  0.1251  0.1234  0.1240  0.1263  0.1273  0.1252  0.1243
 0.1238  0.1271  0.1247  0.1251  0.1251  0.1246  0.1253  0.1244
 0.1249  0.1253  0.1243  0.1247  0.1237  0.1265  0.1248  0.1259
 0.1247  0.1255  0.1249  0.1245  0.1252  0.1268  0.1250  0.1233
 0.1235  0.1270  0.1248  0.1246  0.1277  0.1257  0.1224  0.1243
 0.1237  0.1269  0.1259  0.1254  0.1253  0.1258  0.1227  0.1242
 0.1245  0.1256  0.1251  0.1252  0.1239  0.1269  0.1235  0.1253
 0.1255  0.1250  0.1249  0.1250  0.1246  0.1239  0.1238  0.1274
 0.1242  0.1273  0.1264  0.1257  0.1236  0.1260  0.1240  0.1226
 0.1236  0.1261  0.1251  0.1249  0.1242  0.1271  0.1251  0.1240
 0.1236  0.1269  0.1260  0.1261  0.1242  0.1232  0.1236  0.1264
[torch.cuda.FloatTensor of size 14x8 (GPU 0)]

2019-11-02 02:33:36,300 
alphas_reduce = Variable containing:
 0.1250  0.1253  0.1245  0.1211  0.1243  0.1262  0.1266  0.1271
 0.1251  0.1227  0.1223  0.1276  0.1237  0.1261  0.1247  0.1278
 0.1257  0.1250  0.1246  0.1249  0.1251  0.1254  0.1235  0.1257
 0.1253  0.1237  0.1234  0.1245  0.1272  0.1261  0.1243  0.1255
 0.1246  0.1251  0.1235  0.1247  0.1236  0.1257  0.1270  0.1259
 0.1257  0.1248  0.1241  0.1250  0.1256  0.1249  0.1249  0.1250
 0.1250  0.1255  0.1251  0.1261  0.1250  0.1240  0.1245  0.1249
 0.1248  0.1254  0.1235  0.1248  0.1254  0.1245  0.1250  0.1266
 0.1245  0.1256  0.1241  0.1243  0.1255  0.1238  0.1253  0.1269
 0.1254  0.1251  0.1250  0.1249  0.1250  0.1258  0.1243  0.1246
 0.1247  0.1257  0.1256  0.1245  0.1245  0.1257  0.1238  0.1255
 0.1246  0.1252  0.1242  0.1249  0.1241  0.1281  0.1236  0.1254
 0.1248  0.1256  0.1247  0.1247  0.1264  0.1252  0.1242  0.1245
 0.1244  0.1263  0.1250  0.1252  0.1236  0.1256  0.1244  0.1255
[torch.cuda.FloatTensor of size 14x8 (GPU 0)]

2019-11-02 02:33:39,880 train 000 1.107489e-02 -4.774249
2019-11-02 02:36:24,746 train 050 1.377544e-02 -2.069971
2019-11-02 02:39:09,987 train 100 1.418339e-02 -1.955494
2019-11-02 02:41:55,637 train 150 1.412798e-02 -2.052735
2019-11-02 02:44:40,744 train 200 1.429338e-02 -1.936264
2019-11-02 02:47:25,738 train 250 1.420383e-02 -1.947764
2019-11-02 02:50:10,712 train 300 1.411218e-02 -2.053831
2019-11-02 02:52:56,744 train 350 1.399888e-02 -2.164949
2019-11-02 02:55:42,347 train 400 1.396332e-02 -2.150744
2019-11-02 02:58:28,473 train 450 1.386262e-02 -2.080565
2019-11-02 03:01:13,781 train 500 1.386578e-02 -2.018473
2019-11-02 03:03:58,633 train 550 1.380628e-02 -1.939021
2019-11-02 03:06:43,702 train 600 1.376097e-02 -1.896315
2019-11-02 03:09:29,003 train 650 1.373640e-02 -1.947562
2019-11-02 03:12:14,528 train 700 1.374942e-02 -1.936672
2019-11-02 03:14:58,847 train 750 1.369457e-02 -1.925325
2019-11-02 03:17:44,346 train 800 1.373754e-02 -1.883540
2019-11-02 03:20:30,008 train 850 1.372949e-02 -1.862097
2019-11-02 03:23:16,855 train 900 1.371221e-02 -2.503125
2019-11-02 03:26:05,330 train 950 1.371735e-02 -2.496188
2019-11-02 03:28:52,292 train 1000 1.370589e-02 -3.769629
2019-11-02 03:31:37,428 train 1050 1.372381e-02 -3.707483
2019-11-02 03:34:21,417 train 1100 1.373094e-02 -3.781736
2019-11-02 03:37:07,686 train 1150 1.371267e-02 -3.684330
2019-11-02 03:39:53,497 train 1200 1.372925e-02 -3.578795
2019-11-02 03:42:39,788 train 1250 1.373109e-02 -4.156515
2019-11-02 03:45:26,064 train 1300 1.370257e-02 -6.422713
2019-11-02 03:48:11,344 train 1350 1.367892e-02 -6.279996
2019-11-02 03:50:57,265 train 1400 1.368427e-02 -6.141775
2019-11-02 03:53:43,070 train 1450 1.366322e-02 -7.100873
2019-11-02 03:56:27,714 train 1500 1.365367e-02 -6.907012
2019-11-02 03:58:36,060 training loss; accuracy or R2: 1.364443e-02 -6.831462
2019-11-02 03:58:36,620 valid 000 9.557386e-03 -4.477152
2019-11-02 03:58:45,364 valid 050 1.222460e-02 -2.248928
2019-11-02 03:58:54,114 valid 100 1.237972e-02 -80.366128
2019-11-02 03:59:02,874 valid 150 1.242509e-02 -54.277145
2019-11-02 03:59:11,610 valid 200 1.239226e-02 -41.133933
2019-11-02 03:59:20,377 valid 250 1.243990e-02 -33.212342
2019-11-02 03:59:29,107 valid 300 1.251073e-02 -27.959011
2019-11-02 03:59:37,901 valid 350 1.262777e-02 -24.225239
2019-11-02 03:59:46,666 valid 400 1.258804e-02 -21.842703
2019-11-02 03:59:55,429 valid 450 1.263200e-02 -19.556030
2019-11-02 04:00:04,178 valid 500 1.271339e-02 -17.733574
2019-11-02 04:00:12,958 valid 550 1.272940e-02 -16.265421
2019-11-02 04:00:21,715 valid 600 1.270809e-02 -15.025898
2019-11-02 04:00:30,584 valid 650 1.268929e-02 -13.960280
2019-11-02 04:00:39,412 valid 700 1.272345e-02 -13.053737
2019-11-02 04:00:48,232 valid 750 1.274147e-02 -12.271577
2019-11-02 04:00:57,026 valid 800 1.272524e-02 -11.591120
2019-11-02 04:01:05,841 valid 850 1.274173e-02 -10.983722
2019-11-02 04:01:14,654 valid 900 1.271561e-02 -10.434453
2019-11-02 04:01:23,479 valid 950 1.268157e-02 -10.201717
2019-11-02 04:01:32,299 valid 1000 1.268229e-02 -9.754717
2019-11-02 04:01:41,094 valid 1050 1.268047e-02 -9.358443
2019-11-02 04:01:49,899 valid 1100 1.268152e-02 -9.029412
2019-11-02 04:01:58,710 valid 1150 1.266034e-02 -8.696519
2019-11-02 04:02:07,561 valid 1200 1.265885e-02 -8.372973
2019-11-02 04:02:16,406 valid 1250 1.266243e-02 -8.122818
2019-11-02 04:02:25,252 valid 1300 1.265818e-02 -7.853579
2019-11-02 04:02:34,112 valid 1350 1.267828e-02 -7.606414
2019-11-02 04:02:42,967 valid 1400 1.265351e-02 -7.371834
2019-11-02 04:02:51,848 valid 1450 1.265303e-02 -7.186390
2019-11-02 04:03:00,642 valid 1500 1.265334e-02 -6.991408
2019-11-02 04:03:07,477 validation loss; accuracy or R2: 1.265264e-02 -6.913828
2019-11-02 04:03:07,609 epoch 4 lr 9.858624e-03
2019-11-02 04:03:07,610 genotype = Genotype(normal=[('sep_conv_3x3', 0), ('sep_conv_5x5', 1), ('dil_conv_5x5', 0), ('sep_conv_5x5', 1), ('sep_conv_5x5', 0), ('sep_conv_5x5', 2), ('sep_conv_5x5', 0), ('dil_conv_5x5', 1)], normal_concat=range(2, 6), reduce=[('skip_connect', 1), ('sep_conv_5x5', 0), ('dil_conv_3x3', 1), ('sep_conv_3x3', 2), ('dil_conv_5x5', 2), ('sep_conv_5x5', 3), ('sep_conv_5x5', 2), ('sep_conv_3x3', 3)], reduce_concat=range(2, 6))
2019-11-02 04:03:07,611 
alphas_normal = Variable containing:
 0.1251  0.1228  0.1203  0.1213  0.1281  0.1281  0.1270  0.1272
 0.1240  0.1263  0.1243  0.1248  0.1263  0.1281  0.1211  0.1251
 0.1251  0.1236  0.1226  0.1235  0.1278  0.1254  0.1237  0.1282
 0.1250  0.1251  0.1233  0.1238  0.1251  0.1279  0.1232  0.1266
 0.1253  0.1264  0.1248  0.1256  0.1240  0.1228  0.1259  0.1251
 0.1247  0.1247  0.1251  0.1253  0.1235  0.1268  0.1233  0.1266
 0.1240  0.1265  0.1260  0.1262  0.1241  0.1261  0.1238  0.1233
 0.1243  0.1259  0.1250  0.1256  0.1261  0.1267  0.1237  0.1226
 0.1245  0.1263  0.1257  0.1259  0.1250  0.1261  0.1232  0.1233
 0.1246  0.1243  0.1244  0.1243  0.1245  0.1289  0.1238  0.1252
 0.1252  0.1244  0.1244  0.1244  0.1235  0.1235  0.1264  0.1283
 0.1257  0.1247  0.1245  0.1249  0.1249  0.1271  0.1248  0.1235
 0.1253  0.1250  0.1248  0.1249  0.1248  0.1258  0.1246  0.1248
 0.1247  0.1251  0.1248  0.1250  0.1264  0.1254  0.1222  0.1264
[torch.cuda.FloatTensor of size 14x8 (GPU 0)]

2019-11-02 04:03:07,613 
alphas_reduce = Variable containing:
 0.1258  0.1249  0.1249  0.1246  0.1238  0.1271  0.1237  0.1252
 0.1239  0.1223  0.1215  0.1286  0.1244  0.1257  0.1253  0.1283
 0.1251  0.1250  0.1248  0.1244  0.1262  0.1252  0.1243  0.1250
 0.1245  0.1229  0.1223  0.1250  0.1266  0.1262  0.1268  0.1258
 0.1249  0.1247  0.1237  0.1247  0.1265  0.1256  0.1250  0.1248
 0.1257  0.1242  0.1235  0.1259  0.1257  0.1247  0.1245  0.1259
 0.1254  0.1241  0.1236  0.1258  0.1251  0.1256  0.1246  0.1257
 0.1254  0.1243  0.1230  0.1247  0.1258  0.1256  0.1246  0.1266
 0.1248  0.1244  0.1236  0.1239  0.1256  0.1265  0.1256  0.1255
 0.1255  0.1246  0.1238  0.1242  0.1267  0.1254  0.1244  0.1254
 0.1254  0.1237  0.1235  0.1250  0.1262  0.1259  0.1245  0.1259
 0.1247  0.1240  0.1228  0.1241  0.1256  0.1282  0.1248  0.1257
 0.1245  0.1249  0.1238  0.1244  0.1276  0.1249  0.1258  0.1240
 0.1251  0.1248  0.1238  0.1242  0.1250  0.1273  0.1234  0.1263
[torch.cuda.FloatTensor of size 14x8 (GPU 0)]

2019-11-02 04:03:11,281 train 000 1.231503e-02 -1.530728
2019-11-02 04:05:55,323 train 050 1.355152e-02 -1.240763
2019-11-02 04:08:39,834 train 100 1.334313e-02 -1.321905
2019-11-02 04:11:24,540 train 150 1.321217e-02 -2.143833
2019-11-02 04:14:09,331 train 200 1.335463e-02 -2.209077
2019-11-02 04:16:54,250 train 250 1.326569e-02 -1.993364
2019-11-02 04:19:40,399 train 300 1.315804e-02 -1.972182
2019-11-02 04:22:25,498 train 350 1.310963e-02 -2.042546
2019-11-02 04:25:10,578 train 400 1.323782e-02 -2.028732
2019-11-02 04:27:55,765 train 450 1.321545e-02 -1.965780
2019-11-02 04:30:40,743 train 500 1.321654e-02 -1.906628
2019-11-02 04:33:25,377 train 550 1.323033e-02 -2.199513
2019-11-02 04:36:10,954 train 600 1.320887e-02 -2.155132
2019-11-02 04:38:55,805 train 650 1.313506e-02 -2.220607
2019-11-02 04:41:40,751 train 700 1.312534e-02 -2.447831
2019-11-02 04:44:26,086 train 750 1.309566e-02 -2.357455
2019-11-02 04:47:10,340 train 800 1.309890e-02 -3.017517
2019-11-02 04:49:55,339 train 850 1.308219e-02 -3.879257
2019-11-02 04:52:42,249 train 900 1.307931e-02 -3.732524
2019-11-02 04:55:30,421 train 950 1.307819e-02 -3.625388
2019-11-02 04:58:15,649 train 1000 1.307457e-02 -7.517270
2019-11-02 05:01:00,924 train 1050 1.304335e-02 -7.273750
2019-11-02 05:03:45,573 train 1100 1.306627e-02 -7.046690
2019-11-02 05:06:30,724 train 1150 1.305298e-02 -6.963895
2019-11-02 05:09:15,932 train 1200 1.304408e-02 -6.805439
2019-11-02 05:12:00,489 train 1250 1.303490e-02 -6.586680
2019-11-02 05:14:45,563 train 1300 1.302583e-02 -6.394980
2019-11-02 05:17:30,145 train 1350 1.299127e-02 -6.220628
2019-11-02 05:20:13,638 train 1400 1.296631e-02 -6.067081
2019-11-02 05:22:58,648 train 1450 1.296937e-02 -5.958045
2019-11-02 05:25:42,607 train 1500 1.297055e-02 -5.954385
2019-11-02 05:27:51,857 training loss; accuracy or R2: 1.296581e-02 -5.911581
2019-11-02 05:27:52,358 valid 000 1.313816e-02 -3.400874
2019-11-02 05:28:01,324 valid 050 1.183008e-02 -1.135843
2019-11-02 05:28:10,133 valid 100 1.237666e-02 -1.461059
2019-11-02 05:28:18,980 valid 150 1.225483e-02 -1.997836
2019-11-02 05:28:27,819 valid 200 1.234028e-02 -1.843950
2019-11-02 05:28:36,690 valid 250 1.241055e-02 -1.744402
2019-11-02 05:28:45,523 valid 300 1.243658e-02 -1.750161
2019-11-02 05:28:54,385 valid 350 1.241720e-02 -1.741761
2019-11-02 05:29:03,246 valid 400 1.238932e-02 -1.685624
2019-11-02 05:29:12,044 valid 450 1.237677e-02 -1.664139
2019-11-02 05:29:20,851 valid 500 1.239332e-02 -1.679984
2019-11-02 05:29:29,685 valid 550 1.240288e-02 -1.864291
2019-11-02 05:29:38,470 valid 600 1.232810e-02 -1.904796
2019-11-02 05:29:47,323 valid 650 1.234061e-02 -1.859193
2019-11-02 05:29:56,096 valid 700 1.233797e-02 -19.212249
2019-11-02 05:30:04,865 valid 750 1.236777e-02 -18.010475
2019-11-02 05:30:13,616 valid 800 1.238465e-02 -17.178930
2019-11-02 05:30:22,369 valid 850 1.237895e-02 -16.536124
2019-11-02 05:30:31,128 valid 900 1.241840e-02 -16.010068
2019-11-02 05:30:39,938 valid 950 1.244966e-02 -15.231536
2019-11-02 05:30:48,718 valid 1000 1.243283e-02 -14.537463
2019-11-02 05:30:57,513 valid 1050 1.241249e-02 -14.140207
2019-11-02 05:31:06,275 valid 1100 1.241949e-02 -13.553253
2019-11-02 05:31:15,075 valid 1150 1.241833e-02 -13.020262
2019-11-02 05:31:23,861 valid 1200 1.243988e-02 -12.533014
2019-11-02 05:31:32,597 valid 1250 1.243062e-02 -19.183549
2019-11-02 05:31:41,384 valid 1300 1.242818e-02 -18.497030
2019-11-02 05:31:50,181 valid 1350 1.241111e-02 -17.852191
2019-11-02 05:31:58,926 valid 1400 1.241370e-02 -17.255897
2019-11-02 05:32:07,745 valid 1450 1.239920e-02 -16.703513
2019-11-02 05:32:16,563 valid 1500 1.238595e-02 -16.225090
2019-11-02 05:32:23,463 validation loss; accuracy or R2: 1.238488e-02 -15.868410
2019-11-02 05:32:23,603 epoch 5 lr 9.779754e-03
2019-11-02 05:32:23,604 genotype = Genotype(normal=[('sep_conv_5x5', 0), ('sep_conv_5x5', 1), ('sep_conv_3x3', 0), ('sep_conv_5x5', 1), ('max_pool_3x3', 2), ('max_pool_3x3', 3), ('dil_conv_5x5', 1), ('sep_conv_5x5', 4)], normal_concat=range(2, 6), reduce=[('dil_conv_5x5', 1), ('sep_conv_5x5', 0), ('sep_conv_5x5', 2), ('sep_conv_3x3', 0), ('sep_conv_5x5', 2), ('dil_conv_5x5', 1), ('sep_conv_5x5', 1), ('dil_conv_5x5', 2)], reduce_concat=range(2, 6))
2019-11-02 05:32:23,605 
alphas_normal = Variable containing:
 0.1256  0.1228  0.1214  0.1224  0.1261  0.1287  0.1270  0.1259
 0.1251  0.1254  0.1232  0.1238  0.1264  0.1283  0.1238  0.1240
 0.1256  0.1233  0.1225  0.1230  0.1281  0.1268  0.1240  0.1266
 0.1240  0.1241  0.1228  0.1231  0.1265  0.1272  0.1259  0.1263
 0.1244  0.1251  0.1234  0.1240  0.1258  0.1252  0.1265  0.1256
 0.1239  0.1255  0.1257  0.1257  0.1239  0.1249  0.1242  0.1261
 0.1252  0.1259  0.1253  0.1255  0.1244  0.1250  0.1253  0.1234
 0.1255  0.1271  0.1260  0.1259  0.1239  0.1253  0.1227  0.1235
 0.1236  0.1268  0.1263  0.1259  0.1239  0.1242  0.1247  0.1247
 0.1252  0.1245  0.1249  0.1250  0.1254  0.1258  0.1246  0.1247
 0.1242  0.1243  0.1244  0.1248  0.1240  0.1258  0.1245  0.1280
 0.1247  0.1252  0.1254  0.1257  0.1247  0.1253  0.1247  0.1243
 0.1240  0.1253  0.1256  0.1253  0.1249  0.1250  0.1242  0.1258
 0.1244  0.1248  0.1251  0.1253  0.1265  0.1266  0.1228  0.1245
[torch.cuda.FloatTensor of size 14x8 (GPU 0)]

2019-11-02 05:32:23,607 
alphas_reduce = Variable containing:
 0.1258  0.1256  0.1250  0.1239  0.1242  0.1264  0.1250  0.1241
 0.1243  0.1221  0.1219  0.1274  0.1254  0.1263  0.1245  0.1282
 0.1247  0.1248  0.1243  0.1242  0.1267  0.1261  0.1250  0.1243
 0.1253  0.1243  0.1240  0.1264  0.1247  0.1252  0.1257  0.1243
 0.1243  0.1251  0.1245  0.1242  0.1246  0.1270  0.1255  0.1248
 0.1248  0.1245  0.1243  0.1260  0.1245  0.1259  0.1249  0.1250
 0.1255  0.1230  0.1228  0.1256  0.1256  0.1257  0.1250  0.1269
 0.1255  0.1236  0.1231  0.1242  0.1257  0.1276  0.1243  0.1260
 0.1251  0.1246  0.1244  0.1252  0.1259  0.1248  0.1247  0.1252
 0.1244  0.1249  0.1249  0.1251  0.1253  0.1256  0.1250  0.1248
 0.1249  0.1252  0.1248  0.1248  0.1245  0.1265  0.1245  0.1249
 0.1251  0.1242  0.1247  0.1252  0.1246  0.1257  0.1247  0.1259
 0.1251  0.1256  0.1253  0.1254  0.1253  0.1249  0.1259  0.1226
 0.1251  0.1253  0.1254  0.1258  0.1236  0.1253  0.1246  0.1248
[torch.cuda.FloatTensor of size 14x8 (GPU 0)]

2019-11-02 05:32:27,224 train 000 1.287249e-02 -1.012327
2019-11-02 05:35:12,139 train 050 1.259369e-02 -1.324682
2019-11-02 05:37:56,768 train 100 1.264778e-02 -1.514720
2019-11-02 05:40:42,071 train 150 1.256408e-02 -7.402890
2019-11-02 05:43:27,343 train 200 1.241403e-02 -5.861305
2019-11-02 05:46:12,695 train 250 1.239907e-02 -5.027801
2019-11-02 05:48:55,924 train 300 1.249131e-02 -4.417993
2019-11-02 05:51:40,209 train 350 1.247928e-02 -3.967109
2019-11-02 05:54:25,641 train 400 1.255295e-02 -3.745965
2019-11-02 05:57:11,148 train 450 1.257160e-02 -6.639498
2019-11-02 05:59:56,618 train 500 1.257462e-02 -6.133318
2019-11-02 06:02:41,227 train 550 1.251355e-02 -5.889081
2019-11-02 06:05:26,573 train 600 1.251176e-02 -5.513230
2019-11-02 06:08:11,106 train 650 1.251955e-02 -5.195637
2019-11-02 06:10:56,055 train 700 1.252305e-02 -4.924138
2019-11-02 06:13:40,558 train 750 1.254831e-02 -4.723783
2019-11-02 06:16:25,965 train 800 1.253666e-02 -4.532539
2019-11-02 06:19:11,194 train 850 1.257089e-02 -5.058276
2019-11-02 06:21:58,668 train 900 1.256729e-02 -4.860632
2019-11-02 06:24:45,555 train 950 1.255057e-02 -4.680787
2019-11-02 06:27:31,364 train 1000 1.254008e-02 -4.512913
2019-11-02 06:30:17,266 train 1050 1.254381e-02 -6.692949
2019-11-02 06:33:02,948 train 1100 1.254276e-02 -6.461958
2019-11-02 06:35:48,723 train 1150 1.255715e-02 -6.244617
2019-11-02 06:38:33,391 train 1200 1.254666e-02 -6.060370
2019-11-02 06:41:18,715 train 1250 1.256650e-02 -5.890443
2019-11-02 06:44:02,784 train 1300 1.256207e-02 -5.743612
2019-11-02 06:46:48,592 train 1350 1.256311e-02 -5.690488
2019-11-02 06:49:33,967 train 1400 1.255012e-02 -6.092505
2019-11-02 06:52:19,640 train 1450 1.253142e-02 -5.929442
2019-11-02 06:55:06,108 train 1500 1.252789e-02 -5.780911
2019-11-02 06:57:14,777 training loss; accuracy or R2: 1.252459e-02 -5.677482
2019-11-02 06:57:15,237 valid 000 1.288703e-02 -1.736954
2019-11-02 06:57:24,107 valid 050 1.149178e-02 -1.324270
2019-11-02 06:57:32,942 valid 100 1.164545e-02 -1.247330
2019-11-02 06:57:41,773 valid 150 1.182346e-02 -1.493697
2019-11-02 06:57:50,649 valid 200 1.192489e-02 -1.531524
2019-11-02 06:57:59,504 valid 250 1.194102e-02 -1.463945
2019-11-02 06:58:08,350 valid 300 1.192593e-02 -1.432560
2019-11-02 06:58:17,228 valid 350 1.190993e-02 -1.367577
2019-11-02 06:58:26,050 valid 400 1.186624e-02 -1.353129
2019-11-02 06:58:34,904 valid 450 1.185578e-02 -12.824678
2019-11-02 06:58:43,742 valid 500 1.186342e-02 -12.025731
2019-11-02 06:58:52,595 valid 550 1.184474e-02 -11.052363
2019-11-02 06:59:01,428 valid 600 1.184908e-02 -10.241727
2019-11-02 06:59:10,279 valid 650 1.186402e-02 -19.220683
2019-11-02 06:59:19,148 valid 700 1.185098e-02 -17.936092
2019-11-02 06:59:27,953 valid 750 1.183873e-02 -16.845824
2019-11-02 06:59:36,786 valid 800 1.182094e-02 -15.847161
2019-11-02 06:59:45,610 valid 850 1.180340e-02 -14.989530
2019-11-02 06:59:54,445 valid 900 1.176609e-02 -14.481098
2019-11-02 07:00:03,282 valid 950 1.178797e-02 -13.798509
2019-11-02 07:00:12,098 valid 1000 1.178460e-02 -13.161351
2019-11-02 07:00:20,913 valid 1050 1.179004e-02 -12.591977
2019-11-02 07:00:29,739 valid 1100 1.179762e-02 -12.071565
2019-11-02 07:00:38,508 valid 1150 1.179902e-02 -11.606096
2019-11-02 07:00:47,307 valid 1200 1.180012e-02 -11.180248
2019-11-02 07:00:56,101 valid 1250 1.178345e-02 -10.774199
2019-11-02 07:01:04,887 valid 1300 1.177309e-02 -10.428784
2019-11-02 07:01:13,729 valid 1350 1.175902e-02 -10.085840
2019-11-02 07:01:22,518 valid 1400 1.174644e-02 -9.777082
2019-11-02 07:01:31,346 valid 1450 1.174395e-02 -9.477030
2019-11-02 07:01:40,158 valid 1500 1.175497e-02 -9.379037
2019-11-02 07:01:47,014 validation loss; accuracy or R2: 1.175938e-02 -9.190274
2019-11-02 07:01:47,150 epoch 6 lr 9.683994e-03
2019-11-02 07:01:47,151 genotype = Genotype(normal=[('dil_conv_5x5', 0), ('sep_conv_5x5', 1), ('sep_conv_5x5', 1), ('sep_conv_5x5', 2), ('max_pool_3x3', 3), ('max_pool_3x3', 2), ('sep_conv_3x3', 4), ('dil_conv_5x5', 1)], normal_concat=range(2, 6), reduce=[('dil_conv_5x5', 1), ('max_pool_3x3', 0), ('sep_conv_3x3', 1), ('dil_conv_5x5', 0), ('dil_conv_5x5', 2), ('dil_conv_5x5', 1), ('avg_pool_3x3', 4), ('sep_conv_3x3', 0)], reduce_concat=range(2, 6))
2019-11-02 07:01:47,153 
alphas_normal = Variable containing:
 0.1260  0.1237  0.1220  0.1235  0.1257  0.1256  0.1257  0.1279
 0.1236  0.1263  0.1252  0.1253  0.1246  0.1271  0.1236  0.1243
 0.1237  0.1256  0.1252  0.1255  0.1260  0.1238  0.1241  0.1263
 0.1246  0.1242  0.1234  0.1235  0.1256  0.1269  0.1258  0.1260
 0.1241  0.1254  0.1246  0.1246  0.1252  0.1264  0.1258  0.1238
 0.1243  0.1253  0.1258  0.1258  0.1243  0.1243  0.1244  0.1260
 0.1247  0.1255  0.1254  0.1251  0.1237  0.1261  0.1259  0.1237
 0.1250  0.1264  0.1259  0.1256  0.1245  0.1262  0.1225  0.1238
 0.1238  0.1267  0.1260  0.1254  0.1243  0.1242  0.1238  0.1259
 0.1248  0.1248  0.1244  0.1248  0.1249  0.1260  0.1242  0.1260
 0.1239  0.1251  0.1258  0.1259  0.1250  0.1244  0.1232  0.1268
 0.1255  0.1251  0.1252  0.1253  0.1243  0.1260  0.1249  0.1237
 0.1240  0.1257  0.1256  0.1259  0.1263  0.1244  0.1240  0.1242
 0.1242  0.1251  0.1249  0.1253  0.1274  0.1247  0.1239  0.1246
[torch.cuda.FloatTensor of size 14x8 (GPU 0)]

2019-11-02 07:01:47,154 
alphas_reduce = Variable containing:
 0.1260  0.1255  0.1248  0.1252  0.1229  0.1253  0.1251  0.1251
 0.1244  0.1223  0.1222  0.1265  0.1245  0.1252  0.1267  0.1281
 0.1248  0.1246  0.1244  0.1237  0.1248  0.1255  0.1253  0.1269
 0.1253  0.1252  0.1253  0.1238  0.1271  0.1256  0.1229  0.1248
 0.1245  0.1256  0.1253  0.1251  0.1240  0.1255  0.1247  0.1252
 0.1257  0.1231  0.1234  0.1253  0.1251  0.1258  0.1258  0.1256
 0.1250  0.1238  0.1240  0.1236  0.1247  0.1265  0.1258  0.1267
 0.1256  0.1245  0.1241  0.1251  0.1244  0.1250  0.1241  0.1272
 0.1256  0.1243  0.1241  0.1245  0.1244  0.1252  0.1263  0.1256
 0.1248  0.1250  0.1249  0.1250  0.1261  0.1247  0.1253  0.1242
 0.1247  0.1254  0.1258  0.1252  0.1245  0.1256  0.1239  0.1249
 0.1246  0.1257  0.1260  0.1255  0.1245  0.1247  0.1242  0.1248
 0.1238  0.1250  0.1250  0.1247  0.1259  0.1253  0.1247  0.1256
 0.1242  0.1261  0.1266  0.1264  0.1241  0.1245  0.1238  0.1243
[torch.cuda.FloatTensor of size 14x8 (GPU 0)]

2019-11-02 07:01:50,717 train 000 1.204712e-02 -0.040503
2019-11-02 07:04:35,657 train 050 1.214929e-02 -16.759762
2019-11-02 07:07:19,803 train 100 1.258521e-02 -742.790584
2019-11-02 07:10:04,325 train 150 1.244195e-02 -505.550127
2019-11-02 07:12:48,881 train 200 1.242808e-02 -380.391956
2019-11-02 07:15:32,616 train 250 1.239321e-02 -305.348422
2019-11-02 07:18:17,345 train 300 1.245579e-02 -256.771357
2019-11-02 07:21:02,429 train 350 1.241930e-02 -220.423943
2019-11-02 07:23:47,820 train 400 1.239396e-02 -193.159316
2019-11-02 07:26:32,845 train 450 1.231563e-02 -171.930513
2019-11-02 07:29:17,296 train 500 1.228185e-02 -154.912257
2019-11-02 07:32:02,219 train 550 1.228375e-02 -140.976319
2019-11-02 07:34:47,656 train 600 1.230017e-02 -129.882079
2019-11-02 07:37:31,935 train 650 1.230500e-02 -120.033474
2019-11-02 07:40:16,944 train 700 1.231576e-02 -111.552981
2019-11-02 07:43:02,105 train 750 1.231361e-02 -104.234131
2019-11-02 07:45:47,356 train 800 1.229609e-02 -97.965851
2019-11-02 07:48:31,634 train 850 1.228361e-02 -92.327188
2019-11-02 07:51:19,230 train 900 1.226331e-02 -87.683234
2019-11-02 07:54:03,739 train 950 1.227397e-02 -83.154820
2019-11-02 07:56:48,708 train 1000 1.226758e-02 -79.100668
2019-11-02 07:59:34,042 train 1050 1.226531e-02 -75.531138
2019-11-02 08:02:19,221 train 1100 1.226346e-02 -72.196476
2019-11-02 08:05:03,958 train 1150 1.224328e-02 -69.132722
2019-11-02 08:07:48,922 train 1200 1.221911e-02 -66.318325
2019-11-02 08:10:34,335 train 1250 1.221033e-02 -63.746927
2019-11-02 08:13:20,052 train 1300 1.222856e-02 -61.379237
2019-11-02 08:16:04,038 train 1350 1.222455e-02 -59.145770
2019-11-02 08:18:47,662 train 1400 1.221238e-02 -57.201108
2019-11-02 08:21:32,434 train 1450 1.220051e-02 -55.469954
2019-11-02 08:24:17,175 train 1500 1.218840e-02 -53.680093
2019-11-02 08:26:25,244 training loss; accuracy or R2: 1.218635e-02 -52.484314
2019-11-02 08:26:25,731 valid 000 1.167708e-02 -0.858858
2019-11-02 08:26:34,568 valid 050 1.203605e-02 -1.425179
2019-11-02 08:26:43,339 valid 100 1.195660e-02 -1.589821
2019-11-02 08:26:52,122 valid 150 1.184239e-02 -1.500426
2019-11-02 08:27:00,902 valid 200 1.173755e-02 -1.843374
2019-11-02 08:27:09,698 valid 250 1.168090e-02 -1.995896
2019-11-02 08:27:18,471 valid 300 1.170112e-02 -1.884745
2019-11-02 08:27:27,264 valid 350 1.169171e-02 -1.803095
2019-11-02 08:27:36,035 valid 400 1.173239e-02 -12.532812
2019-11-02 08:27:44,810 valid 450 1.166075e-02 -11.314171
2019-11-02 08:27:53,565 valid 500 1.164661e-02 -10.326784
2019-11-02 08:28:02,297 valid 550 1.167651e-02 -9.579599
2019-11-02 08:28:11,056 valid 600 1.165630e-02 -8.925673
2019-11-02 08:28:19,863 valid 650 1.165885e-02 -8.362660
2019-11-02 08:28:28,573 valid 700 1.163654e-02 -7.913780
2019-11-02 08:28:37,481 valid 750 1.163351e-02 -7.498485
2019-11-02 08:28:46,269 valid 800 1.162621e-02 -7.124241
2019-11-02 08:28:55,049 valid 850 1.158944e-02 -6.791395
2019-11-02 08:29:03,856 valid 900 1.160883e-02 -6.498254
2019-11-02 08:29:12,737 valid 950 1.161288e-02 -6.268394
2019-11-02 08:29:21,518 valid 1000 1.160721e-02 -6.076373
2019-11-02 08:29:30,348 valid 1050 1.162500e-02 -5.867661
2019-11-02 08:29:39,193 valid 1100 1.163536e-02 -5.688425
2019-11-02 08:29:48,001 valid 1150 1.164421e-02 -5.553125
2019-11-02 08:29:56,813 valid 1200 1.166073e-02 -5.417203
2019-11-02 08:30:05,640 valid 1250 1.167049e-02 -5.255453
2019-11-02 08:30:14,429 valid 1300 1.168835e-02 -5.120187
2019-11-02 08:30:23,269 valid 1350 1.168574e-02 -4.987406
2019-11-02 08:30:32,048 valid 1400 1.168346e-02 -4.909496
2019-11-02 08:30:40,886 valid 1450 1.168490e-02 -4.806921
2019-11-02 08:30:49,763 valid 1500 1.167774e-02 -4.713462
2019-11-02 08:30:56,677 validation loss; accuracy or R2: 1.167624e-02 -4.629546
2019-11-02 08:30:56,807 epoch 7 lr 9.571722e-03
2019-11-02 08:30:56,808 genotype = Genotype(normal=[('dil_conv_5x5', 0), ('sep_conv_5x5', 1), ('dil_conv_5x5', 0), ('sep_conv_5x5', 2), ('sep_conv_5x5', 0), ('max_pool_3x3', 1), ('sep_conv_5x5', 0), ('avg_pool_3x3', 1)], normal_concat=range(2, 6), reduce=[('dil_conv_5x5', 1), ('skip_connect', 0), ('dil_conv_5x5', 0), ('sep_conv_5x5', 2), ('dil_conv_5x5', 2), ('dil_conv_5x5', 3), ('sep_conv_5x5', 3), ('sep_conv_5x5', 1)], reduce_concat=range(2, 6))
2019-11-02 08:30:56,810 
alphas_normal = Variable containing:
 0.1267  0.1211  0.1208  0.1215  0.1278  0.1272  0.1271  0.1279
 0.1231  0.1262  0.1256  0.1255  0.1257  0.1273  0.1226  0.1240
 0.1259  0.1229  0.1232  0.1235  0.1272  0.1228  0.1262  0.1283
 0.1234  0.1241  0.1241  0.1245  0.1251  0.1264  0.1257  0.1267
 0.1245  0.1240  0.1235  0.1239  0.1251  0.1279  0.1259  0.1251
 0.1267  0.1227  0.1236  0.1235  0.1238  0.1272  0.1259  0.1266
 0.1236  0.1269  0.1264  0.1262  0.1239  0.1255  0.1244  0.1232
 0.1241  0.1258  0.1252  0.1247  0.1257  0.1247  0.1252  0.1247
 0.1234  0.1265  0.1259  0.1253  0.1239  0.1245  0.1250  0.1254
 0.1249  0.1234  0.1239  0.1240  0.1244  0.1286  0.1242  0.1266
 0.1242  0.1261  0.1268  0.1263  0.1238  0.1242  0.1234  0.1252
 0.1250  0.1244  0.1244  0.1248  0.1256  0.1256  0.1263  0.1238
 0.1232  0.1260  0.1262  0.1262  0.1248  0.1250  0.1247  0.1238
 0.1241  0.1251  0.1249  0.1255  0.1257  0.1250  0.1241  0.1255
[torch.cuda.FloatTensor of size 14x8 (GPU 0)]

2019-11-02 08:30:56,811 
alphas_reduce = Variable containing:
 0.1259  0.1244  0.1247  0.1257  0.1248  0.1256  0.1245  0.1244
 0.1243  0.1221  0.1228  0.1261  0.1255  0.1243  0.1259  0.1290
 0.1246  0.1249  0.1248  0.1244  0.1250  0.1251  0.1248  0.1263
 0.1249  0.1241  0.1244  0.1251  0.1259  0.1258  0.1245  0.1252
 0.1246  0.1254  0.1249  0.1248  0.1247  0.1261  0.1239  0.1255
 0.1254  0.1245  0.1246  0.1259  0.1236  0.1258  0.1249  0.1252
 0.1253  0.1242  0.1246  0.1238  0.1249  0.1260  0.1252  0.1260
 0.1254  0.1245  0.1244  0.1248  0.1247  0.1258  0.1240  0.1263
 0.1247  0.1253  0.1251  0.1246  0.1238  0.1249  0.1255  0.1261
 0.1248  0.1248  0.1246  0.1259  0.1249  0.1256  0.1253  0.1242
 0.1250  0.1245  0.1250  0.1244  0.1250  0.1266  0.1246  0.1250
 0.1249  0.1251  0.1251  0.1253  0.1243  0.1253  0.1251  0.1247
 0.1247  0.1245  0.1240  0.1240  0.1256  0.1266  0.1251  0.1255
 0.1250  0.1256  0.1257  0.1256  0.1244  0.1247  0.1243  0.1247
[torch.cuda.FloatTensor of size 14x8 (GPU 0)]

2019-11-02 08:31:00,407 train 000 1.126497e-02 -0.120366
2019-11-02 08:33:45,428 train 050 1.196577e-02 -1.391253
2019-11-02 08:36:31,108 train 100 1.190669e-02 -1.451304
2019-11-02 08:39:16,129 train 150 1.179010e-02 -1.365194
2019-11-02 08:42:01,059 train 200 1.204676e-02 -20.071651
2019-11-02 08:44:46,812 train 250 1.194388e-02 -19.110362
2019-11-02 08:47:31,711 train 300 1.192834e-02 -16.262241
2019-11-02 08:50:16,587 train 350 1.197025e-02 -14.166147
2019-11-02 08:53:01,998 train 400 1.200091e-02 -12.600864
2019-11-02 08:55:47,651 train 450 1.198635e-02 -11.461491
2019-11-02 08:58:32,714 train 500 1.194049e-02 -10.544158
2019-11-02 09:01:18,983 train 550 1.192987e-02 -9.867175
2019-11-02 09:04:05,404 train 600 1.189608e-02 -9.324800
2019-11-02 09:06:50,830 train 650 1.188340e-02 -8.736033
2019-11-02 09:09:36,491 train 700 1.186731e-02 -8.206315
2019-11-02 09:12:22,588 train 750 1.186383e-02 -7.752544
2019-11-02 09:15:08,657 train 800 1.185467e-02 -7.335854
2019-11-02 09:17:56,232 train 850 1.185969e-02 -6.984107
2019-11-02 09:20:44,289 train 900 1.187057e-02 -6.670923
2019-11-02 09:23:30,022 train 950 1.185871e-02 -6.503787
2019-11-02 09:26:13,995 train 1000 1.185023e-02 -6.252298
2019-11-02 09:28:59,329 train 1050 1.186363e-02 -6.034308
2019-11-02 09:31:44,028 train 1100 1.186141e-02 -5.807539
2019-11-02 09:34:26,946 train 1150 1.186420e-02 -5.611672
2019-11-02 09:37:05,663 train 1200 1.186761e-02 -5.437671
2019-11-02 09:39:42,818 train 1250 1.187864e-02 -5.292983
2019-11-02 09:42:18,455 train 1300 1.189358e-02 -5.155322
2019-11-02 09:44:56,720 train 1350 1.190410e-02 -5.006439
2019-11-02 09:47:35,991 train 1400 1.189412e-02 -4.877374
2019-11-02 09:50:14,718 train 1450 1.187983e-02 -4.818900
2019-11-02 09:52:53,320 train 1500 1.186497e-02 -4.700736
2019-11-02 09:54:56,882 training loss; accuracy or R2: 1.186048e-02 -5.290481
2019-11-02 09:54:57,354 valid 000 1.030472e-02 -0.077462
2019-11-02 09:55:05,592 valid 050 1.214255e-02 -1.507075
2019-11-02 09:55:13,791 valid 100 1.214775e-02 -1.774933
2019-11-02 09:55:21,991 valid 150 1.221824e-02 -1.784374
2019-11-02 09:55:30,241 valid 200 1.233371e-02 -1.820294
2019-11-02 09:55:38,463 valid 250 1.231195e-02 -1.820568
2019-11-02 09:55:46,675 valid 300 1.229176e-02 -1.912720
2019-11-02 09:55:54,925 valid 350 1.228268e-02 -2.167833
2019-11-02 09:56:03,245 valid 400 1.232515e-02 -2.119516
2019-11-02 09:56:11,536 valid 450 1.232570e-02 -2.118744
2019-11-02 09:56:19,801 valid 500 1.232320e-02 -2.346847
2019-11-02 09:56:28,035 valid 550 1.229029e-02 -2.490958
2019-11-02 09:56:36,253 valid 600 1.225977e-02 -2.474107
2019-11-02 09:56:44,502 valid 650 1.225452e-02 -2.468726
2019-11-02 09:56:52,713 valid 700 1.221736e-02 -2.482551
2019-11-02 09:57:00,932 valid 750 1.223368e-02 -2.464820
2019-11-02 09:57:09,167 valid 800 1.226541e-02 -2.485395
2019-11-02 09:57:17,363 valid 850 1.223983e-02 -2.813265
2019-11-02 09:57:25,574 valid 900 1.224904e-02 -2.752926
2019-11-02 09:57:33,748 valid 950 1.225790e-02 -2.724747
2019-11-02 09:57:41,927 valid 1000 1.227268e-02 -2.665768
2019-11-02 09:57:50,092 valid 1050 1.226892e-02 -2.620356
2019-11-02 09:57:58,277 valid 1100 1.226802e-02 -2.605598
2019-11-02 09:58:06,480 valid 1150 1.225520e-02 -2.583676
2019-11-02 09:58:14,680 valid 1200 1.223172e-02 -2.690742
2019-11-02 09:58:22,866 valid 1250 1.223538e-02 -2.651436
2019-11-02 09:58:31,057 valid 1300 1.225304e-02 -6.841265
2019-11-02 09:58:39,261 valid 1350 1.225611e-02 -6.658187
2019-11-02 09:58:47,463 valid 1400 1.227095e-02 -6.501707
2019-11-02 09:58:55,666 valid 1450 1.227135e-02 -13.250883
2019-11-02 09:59:03,901 valid 1500 1.229192e-02 -12.873281
2019-11-02 09:59:10,270 validation loss; accuracy or R2: 1.228127e-02 -12.609257
2019-11-02 09:59:10,437 epoch 8 lr 9.443380e-03
2019-11-02 09:59:10,437 genotype = Genotype(normal=[('dil_conv_5x5', 0), ('sep_conv_5x5', 1), ('dil_conv_5x5', 0), ('sep_conv_5x5', 1), ('max_pool_3x3', 2), ('sep_conv_5x5', 0), ('sep_conv_5x5', 2), ('sep_conv_5x5', 4)], normal_concat=range(2, 6), reduce=[('skip_connect', 1), ('sep_conv_5x5', 0), ('dil_conv_5x5', 0), ('sep_conv_5x5', 2), ('dil_conv_5x5', 3), ('sep_conv_5x5', 1), ('sep_conv_3x3', 3), ('sep_conv_5x5', 2)], reduce_concat=range(2, 6))
2019-11-02 09:59:10,439 
alphas_normal = Variable containing:
 0.1254  0.1220  0.1207  0.1218  0.1266  0.1269  0.1270  0.1296
 0.1244  0.1261  0.1241  0.1249  0.1255  0.1263  0.1253  0.1233
 0.1265  0.1229  0.1225  0.1237  0.1280  0.1246  0.1228  0.1290
 0.1234  0.1252  0.1245  0.1248  0.1247  0.1281  0.1244  0.1250
 0.1251  0.1247  0.1240  0.1252  0.1243  0.1263  0.1232  0.1272
 0.1252  0.1249  0.1242  0.1247  0.1242  0.1272  0.1237  0.1261
 0.1239  0.1270  0.1253  0.1256  0.1250  0.1263  0.1248  0.1221
 0.1244  0.1275  0.1261  0.1254  0.1251  0.1247  0.1239  0.1229
 0.1252  0.1269  0.1257  0.1261  0.1244  0.1241  0.1244  0.1231
 0.1257  0.1237  0.1239  0.1246  0.1255  0.1263  0.1247  0.1255
 0.1257  0.1253  0.1251  0.1252  0.1232  0.1246  0.1250  0.1258
 0.1247  0.1244  0.1239  0.1247  0.1261  0.1273  0.1254  0.1234
 0.1255  0.1245  0.1245  0.1248  0.1249  0.1259  0.1258  0.1241
 0.1247  0.1237  0.1236  0.1240  0.1265  0.1267  0.1252  0.1256
[torch.cuda.FloatTensor of size 14x8 (GPU 0)]

2019-11-02 09:59:10,440 
alphas_reduce = Variable containing:
 0.1253  0.1266  0.1262  0.1235  0.1236  0.1268  0.1252  0.1228
 0.1245  0.1226  0.1228  0.1281  0.1242  0.1241  0.1260  0.1278
 0.1243  0.1264  0.1259  0.1235  0.1259  0.1239  0.1236  0.1266
 0.1255  0.1250  0.1247  0.1251  0.1240  0.1255  0.1255  0.1247
 0.1250  0.1256  0.1257  0.1256  0.1251  0.1260  0.1237  0.1233
 0.1252  0.1250  0.1246  0.1242  0.1249  0.1250  0.1255  0.1257
 0.1243  0.1244  0.1241  0.1244  0.1260  0.1266  0.1254  0.1249
 0.1247  0.1247  0.1241  0.1248  0.1259  0.1264  0.1245  0.1250
 0.1248  0.1252  0.1242  0.1246  0.1240  0.1263  0.1243  0.1266
 0.1254  0.1242  0.1237  0.1259  0.1261  0.1248  0.1245  0.1255
 0.1252  0.1249  0.1246  0.1256  0.1259  0.1251  0.1238  0.1249
 0.1254  0.1241  0.1242  0.1250  0.1253  0.1275  0.1235  0.1250
 0.1255  0.1239  0.1229  0.1237  0.1277  0.1257  0.1257  0.1248
 0.1255  0.1240  0.1238  0.1239  0.1250  0.1264  0.1247  0.1266
[torch.cuda.FloatTensor of size 14x8 (GPU 0)]

2019-11-02 09:59:13,977 train 000 1.115227e-02 -0.703868
2019-11-02 10:01:52,428 train 050 1.138783e-02 -10.874981
2019-11-02 10:04:31,864 train 100 1.145921e-02 -10.596999
2019-11-02 10:07:12,962 train 150 1.155032e-02 -7.592793
2019-11-02 10:09:53,859 train 200 1.159621e-02 -6.209012
2019-11-02 10:12:36,057 train 250 1.170929e-02 -5.369672
2019-11-02 10:15:16,780 train 300 1.165750e-02 -4.641242
2019-11-02 10:17:58,511 train 350 1.162669e-02 -4.138418
2019-11-02 10:20:40,907 train 400 1.162662e-02 -4.014652
2019-11-02 10:23:22,415 train 450 1.168809e-02 -3.744334
2019-11-02 10:26:02,500 train 500 1.167959e-02 -3.499909
2019-11-02 10:28:44,786 train 550 1.166058e-02 -3.325411
2019-11-02 10:31:31,641 train 600 1.166057e-02 -4.194577
2019-11-02 10:34:15,920 train 650 1.163058e-02 -3.984386
2019-11-02 10:37:01,701 train 700 1.159487e-02 -3.790280
2019-11-02 10:39:47,212 train 750 1.159371e-02 -3.634476
2019-11-02 10:42:31,326 train 800 1.159305e-02 -3.499372
2019-11-02 10:45:16,252 train 850 1.157292e-02 -3.395932
2019-11-02 10:48:00,563 train 900 1.155774e-02 -3.268310
2019-11-02 10:50:45,679 train 950 1.156474e-02 -4.134226
2019-11-02 10:53:30,101 train 1000 1.157732e-02 -4.016419
2019-11-02 10:56:15,432 train 1050 1.157682e-02 -3.903981
2019-11-02 10:59:01,030 train 1100 1.157315e-02 -3.792469
2019-11-02 11:01:46,501 train 1150 1.158039e-02 -6.006400
2019-11-02 11:04:32,066 train 1200 1.159387e-02 -5.818718
2019-11-02 11:07:16,438 train 1250 1.159166e-02 -5.659633
2019-11-02 11:10:00,891 train 1300 1.159356e-02 -5.511044
2019-11-02 11:12:45,579 train 1350 1.160305e-02 -5.392700
2019-11-02 11:15:30,374 train 1400 1.160466e-02 -5.276917
2019-11-02 11:18:14,484 train 1450 1.161645e-02 -5.142920
2019-11-02 11:20:59,076 train 1500 1.161304e-02 -5.107388
2019-11-02 11:23:07,223 training loss; accuracy or R2: 1.162362e-02 -5.015012
2019-11-02 11:23:07,729 valid 000 1.150504e-02 -1.623306
2019-11-02 11:23:16,598 valid 050 1.093772e-02 -1.514741
2019-11-02 11:23:25,165 valid 100 1.098996e-02 -2.747557
2019-11-02 11:23:33,613 valid 150 1.096105e-02 -2.644479
2019-11-02 11:23:42,139 valid 200 1.106633e-02 -2.260366
2019-11-02 11:23:50,592 valid 250 1.098425e-02 -2.140595
2019-11-02 11:23:59,005 valid 300 1.101611e-02 -2.048938
2019-11-02 11:24:07,394 valid 350 1.105160e-02 -1.952151
2019-11-02 11:24:15,740 valid 400 1.104233e-02 -1.872850
2019-11-02 11:24:24,190 valid 450 1.107593e-02 -1.822669
2019-11-02 11:24:32,585 valid 500 1.106706e-02 -1.822938
2019-11-02 11:24:40,948 valid 550 1.108065e-02 -1.737502
2019-11-02 11:24:49,324 valid 600 1.106845e-02 -1.717715
2019-11-02 11:24:57,634 valid 650 1.103410e-02 -1.753296
2019-11-02 11:25:05,980 valid 700 1.099227e-02 -1.716250
2019-11-02 11:25:14,291 valid 750 1.100340e-02 -1.678059
2019-11-02 11:25:22,620 valid 800 1.099160e-02 -1.641152
2019-11-02 11:25:30,920 valid 850 1.099006e-02 -1.612405
2019-11-02 11:25:39,280 valid 900 1.097037e-02 -1.597411
2019-11-02 11:25:47,630 valid 950 1.098240e-02 -1.625077
2019-11-02 11:25:55,985 valid 1000 1.096262e-02 -1.600410
2019-11-02 11:26:04,320 valid 1050 1.096473e-02 -1.596038
2019-11-02 11:26:12,646 valid 1100 1.097801e-02 -1.619208
2019-11-02 11:26:20,973 valid 1150 1.100340e-02 -1.747101
2019-11-02 11:26:29,309 valid 1200 1.099728e-02 -1.713308
2019-11-02 11:26:37,610 valid 1250 1.098513e-02 -1.694345
2019-11-02 11:26:45,918 valid 1300 1.099129e-02 -1.669993
2019-11-02 11:26:54,203 valid 1350 1.098758e-02 -1.694866
2019-11-02 11:27:02,518 valid 1400 1.100001e-02 -1.696389
2019-11-02 11:27:10,834 valid 1450 1.098634e-02 -1.674157
2019-11-02 11:27:19,124 valid 1500 1.099656e-02 -1.662580
2019-11-02 11:27:25,545 validation loss; accuracy or R2: 1.099775e-02 -1.644925
2019-11-02 11:27:25,678 epoch 9 lr 9.299476e-03
2019-11-02 11:27:25,679 genotype = Genotype(normal=[('dil_conv_5x5', 1), ('dil_conv_5x5', 0), ('sep_conv_3x3', 0), ('sep_conv_5x5', 1), ('dil_conv_5x5', 0), ('sep_conv_5x5', 1), ('dil_conv_5x5', 1), ('sep_conv_5x5', 3)], normal_concat=range(2, 6), reduce=[('dil_conv_5x5', 1), ('dil_conv_3x3', 0), ('sep_conv_5x5', 2), ('sep_conv_5x5', 1), ('sep_conv_5x5', 1), ('dil_conv_3x3', 3), ('sep_conv_5x5', 3), ('sep_conv_5x5', 2)], reduce_concat=range(2, 6))
2019-11-02 11:27:25,680 
alphas_normal = Variable containing:
 0.1240  0.1249  0.1223  0.1232  0.1259  0.1257  0.1270  0.1271
 0.1252  0.1260  0.1238  0.1243  0.1254  0.1235  0.1234  0.1284
 0.1250  0.1248  0.1236  0.1247  0.1277  0.1241  0.1236  0.1267
 0.1242  0.1254  0.1240  0.1241  0.1243  0.1268  0.1245  0.1267
 0.1232  0.1266  0.1253  0.1252  0.1245  0.1241  0.1257  0.1254
 0.1246  0.1250  0.1246  0.1248  0.1254  0.1254  0.1224  0.1279
 0.1250  0.1242  0.1232  0.1237  0.1267  0.1274  0.1262  0.1237
 0.1253  0.1263  0.1257  0.1250  0.1249  0.1258  0.1228  0.1243
 0.1239  0.1253  0.1246  0.1242  0.1255  0.1248  0.1260  0.1257
 0.1250  0.1238  0.1235  0.1242  0.1250  0.1269  0.1253  0.1264
 0.1249  0.1248  0.1244  0.1244  0.1233  0.1245  0.1258  0.1279
 0.1252  0.1248  0.1240  0.1243  0.1241  0.1259  0.1256  0.1260
 0.1250  0.1248  0.1242  0.1242  0.1246  0.1272  0.1246  0.1252
 0.1240  0.1251  0.1249  0.1250  0.1245  0.1252  0.1247  0.1266
[torch.cuda.FloatTensor of size 14x8 (GPU 0)]

2019-11-02 11:27:25,682 
alphas_reduce = Variable containing:
 0.1257  0.1255  0.1253  0.1250  0.1243  0.1251  0.1271  0.1220
 0.1248  0.1222  0.1224  0.1276  0.1244  0.1252  0.1255  0.1280
 0.1245  0.1251  0.1252  0.1243  0.1261  0.1239  0.1256  0.1253
 0.1259  0.1231  0.1228  0.1243  0.1263  0.1269  0.1265  0.1243
 0.1256  0.1248  0.1247  0.1251  0.1243  0.1271  0.1238  0.1246
 0.1249  0.1238  0.1235  0.1263  0.1248  0.1255  0.1251  0.1262
 0.1248  0.1247  0.1243  0.1253  0.1246  0.1271  0.1240  0.1252
 0.1259  0.1250  0.1236  0.1251  0.1247  0.1264  0.1229  0.1264
 0.1249  0.1243  0.1241  0.1244  0.1240  0.1253  0.1266  0.1265
 0.1248  0.1250  0.1244  0.1253  0.1261  0.1263  0.1239  0.1242
 0.1250  0.1245  0.1242  0.1258  0.1254  0.1250  0.1248  0.1253
 0.1248  0.1248  0.1241  0.1244  0.1248  0.1264  0.1251  0.1256
 0.1238  0.1248  0.1247  0.1245  0.1256  0.1270  0.1242  0.1254
 0.1246  0.1258  0.1254  0.1254  0.1235  0.1252  0.1241  0.1259
[torch.cuda.FloatTensor of size 14x8 (GPU 0)]

2019-11-02 11:27:29,192 train 000 1.077424e-02 0.194356
