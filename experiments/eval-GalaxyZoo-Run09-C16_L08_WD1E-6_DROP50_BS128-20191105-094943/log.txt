2019-11-05 09:49:43,380 gpu device = 1
2019-11-05 09:49:43,380 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=128, cutout=False, cutout_length=16, data='../data', dataset='GalaxyZoo', drop_path_prob=0.5, epochs=200, fc1_size=1024, fc2_size=1024, gpu=1, grad_clip=5, init_channels=16, layers=8, learning_rate=0.0001, model_path='saved_models', momentum=0.9, optimizer='Adam', random=False, report_freq=50, save='eval-GalaxyZoo-Run09-C16_L08_WD1E-6_DROP50_BS128-20191105-094943', seed=0, val_portion=0.1, weight_decay=1e-06)
2019-11-05 09:49:46,860 param size = 1.618229MB
2019-11-05 09:49:46,865 epoch 0 lr 1.000000e-04
2019-11-05 09:49:49,928 train 000 4.523887e-02 -6.753380
2019-11-05 09:50:09,955 train 050 3.137790e-02 -1.104869
2019-11-05 09:50:30,331 train 100 2.971638e-02 -0.801486
2019-11-05 09:50:50,679 train 150 2.909188e-02 -0.662082
2019-11-05 09:51:11,028 train 200 2.867371e-02 -0.582470
2019-11-05 09:51:31,530 train 250 2.843200e-02 -0.527811
2019-11-05 09:51:52,315 train 300 2.824722e-02 -0.474980
2019-11-05 09:52:12,598 train 350 2.809559e-02 -0.441524
2019-11-05 09:52:33,100 train 400 2.801292e-02 -0.419129
2019-11-05 09:52:47,068 training loss; R2: 2.793933e-02 -0.405041
2019-11-05 09:52:48,061 valid 000 2.720110e-02 -0.342685
2019-11-05 09:53:07,507 validation loss; R2: 2.706210e-02 -0.297938
2019-11-05 09:53:07,544 epoch 1 lr 1.000000e-04
2019-11-05 09:53:08,837 train 000 2.839835e-02 -0.250140
2019-11-05 09:53:29,646 train 050 2.723042e-02 -0.266988
2019-11-05 09:53:50,653 train 100 2.724754e-02 -0.243511
2019-11-05 09:54:11,266 train 150 2.731580e-02 -0.232116
2019-11-05 09:54:32,140 train 200 2.723149e-02 -0.234138
2019-11-05 09:54:52,732 train 250 2.725369e-02 -0.232344
2019-11-05 09:55:13,367 train 300 2.721976e-02 -0.236949
2019-11-05 09:55:34,220 train 350 2.719415e-02 -0.232824
2019-11-05 09:55:55,215 train 400 2.716757e-02 -0.234431
2019-11-05 09:56:08,304 training loss; R2: 2.712480e-02 -0.231383
2019-11-05 09:56:09,362 valid 000 2.723689e-02 -0.049017
2019-11-05 09:56:28,671 validation loss; R2: 2.694035e-02 -0.269970
2019-11-05 09:56:28,704 epoch 2 lr 1.000000e-04
2019-11-05 09:56:29,947 train 000 2.828915e-02 -0.293290
2019-11-05 09:56:50,705 train 050 2.727361e-02 -0.239759
2019-11-05 09:57:11,332 train 100 2.710067e-02 -0.213784
2019-11-05 09:57:31,848 train 150 2.697546e-02 -0.240207
2019-11-05 09:57:52,445 train 200 2.698581e-02 -0.230959
2019-11-05 09:58:13,068 train 250 2.700391e-02 -0.240226
2019-11-05 09:58:33,618 train 300 2.693866e-02 -0.246414
2019-11-05 09:58:54,212 train 350 2.695479e-02 -0.241053
2019-11-05 09:59:14,823 train 400 2.695230e-02 -0.245965
2019-11-05 09:59:27,960 training loss; R2: 2.696928e-02 -0.245537
2019-11-05 09:59:29,009 valid 000 2.403773e-02 -0.331550
2019-11-05 09:59:48,255 validation loss; R2: 2.694078e-02 -0.227204
2019-11-05 09:59:48,291 epoch 3 lr 1.000000e-04
2019-11-05 09:59:49,564 train 000 2.638521e-02 -0.378597
2019-11-05 10:00:10,263 train 050 2.655260e-02 -0.243077
2019-11-05 10:00:31,028 train 100 2.670581e-02 -0.231492
2019-11-05 10:00:51,096 train 150 2.679551e-02 -0.225664
2019-11-05 10:01:11,701 train 200 2.674984e-02 -0.229828
2019-11-05 10:01:32,275 train 250 2.672570e-02 -0.229338
2019-11-05 10:01:52,900 train 300 2.666314e-02 -0.223476
2019-11-05 10:02:13,616 train 350 2.659961e-02 -0.225938
2019-11-05 10:02:34,308 train 400 2.646067e-02 -0.232356
2019-11-05 10:02:47,518 training loss; R2: 2.639064e-02 -0.235003
2019-11-05 10:02:48,533 valid 000 2.577969e-02 -0.061085
2019-11-05 10:03:07,672 validation loss; R2: 2.564031e-02 -0.139742
2019-11-05 10:03:07,699 epoch 4 lr 1.000000e-04
2019-11-05 10:03:08,900 train 000 2.597223e-02 -0.082366
2019-11-05 10:03:29,861 train 050 2.582222e-02 -0.268998
2019-11-05 10:03:50,402 train 100 2.563123e-02 -0.257295
2019-11-05 10:04:10,884 train 150 2.564728e-02 -0.238209
2019-11-05 10:04:31,307 train 200 2.567124e-02 -0.263499
2019-11-05 10:04:51,761 train 250 2.557501e-02 -0.270721
2019-11-05 10:05:12,383 train 300 2.555847e-02 -0.265061
2019-11-05 10:05:32,909 train 350 2.552903e-02 -0.267387
2019-11-05 10:05:53,441 train 400 2.547454e-02 -0.262074
2019-11-05 10:06:06,583 training loss; R2: 2.546881e-02 -0.254229
2019-11-05 10:06:07,607 valid 000 2.637252e-02 -0.139445
2019-11-05 10:06:26,668 validation loss; R2: 2.548054e-02 -0.149930
2019-11-05 10:06:26,709 epoch 5 lr 1.000000e-04
2019-11-05 10:06:27,933 train 000 2.507707e-02 -0.252881
2019-11-05 10:06:48,423 train 050 2.520520e-02 -0.252942
2019-11-05 10:07:08,976 train 100 2.512282e-02 -0.246894
2019-11-05 10:07:29,410 train 150 2.515772e-02 -0.249002
2019-11-05 10:07:50,001 train 200 2.511760e-02 -0.250226
2019-11-05 10:08:10,679 train 250 2.506836e-02 -0.250947
2019-11-05 10:08:31,335 train 300 2.505600e-02 -0.254267
2019-11-05 10:08:52,066 train 350 2.508982e-02 -0.255137
2019-11-05 10:09:12,744 train 400 2.503876e-02 -0.257325
2019-11-05 10:09:25,933 training loss; R2: 2.503265e-02 -0.260612
2019-11-05 10:09:27,058 valid 000 2.282466e-02 -0.046920
2019-11-05 10:09:46,403 validation loss; R2: 2.379248e-02 -0.183866
2019-11-05 10:09:46,438 epoch 6 lr 1.000000e-04
2019-11-05 10:09:47,652 train 000 2.685056e-02 -0.198627
2019-11-05 10:10:08,345 train 050 2.474740e-02 -0.296231
2019-11-05 10:10:28,926 train 100 2.490848e-02 -0.288389
2019-11-05 10:10:49,600 train 150 2.502004e-02 -0.286435
2019-11-05 10:11:10,235 train 200 2.499678e-02 -0.277638
2019-11-05 10:11:30,813 train 250 2.497379e-02 -0.271167
2019-11-05 10:11:51,398 train 300 2.493459e-02 -0.266181
2019-11-05 10:12:11,919 train 350 2.492199e-02 -0.268899
2019-11-05 10:12:32,427 train 400 2.484653e-02 -0.273910
2019-11-05 10:12:45,513 training loss; R2: 2.484802e-02 -0.276171
2019-11-05 10:12:46,516 valid 000 2.300600e-02 -0.151858
2019-11-05 10:13:06,186 validation loss; R2: 2.364372e-02 -0.217427
2019-11-05 10:13:06,218 epoch 7 lr 1.000000e-04
2019-11-05 10:13:07,446 train 000 2.718838e-02 -0.077108
2019-11-05 10:13:27,962 train 050 2.504471e-02 -0.237375
2019-11-05 10:13:48,530 train 100 2.492427e-02 -0.257534
2019-11-05 10:14:09,037 train 150 2.480769e-02 -0.261329
2019-11-05 10:14:29,554 train 200 2.471999e-02 -0.281767
2019-11-05 10:14:49,892 train 250 2.469634e-02 -0.276260
2019-11-05 10:15:10,401 train 300 2.466202e-02 -0.276584
2019-11-05 10:15:31,116 train 350 2.464674e-02 -0.270853
2019-11-05 10:15:51,415 train 400 2.461123e-02 -0.283552
2019-11-05 10:16:04,532 training loss; R2: 2.460808e-02 -0.284652
2019-11-05 10:16:05,554 valid 000 2.223912e-02 -0.531917
2019-11-05 10:16:24,836 validation loss; R2: 2.326490e-02 -0.175523
2019-11-05 10:16:24,870 epoch 8 lr 1.000000e-04
2019-11-05 10:16:26,139 train 000 2.183958e-02 -0.048900
2019-11-05 10:16:47,226 train 050 2.461856e-02 -0.296313
2019-11-05 10:17:08,807 train 100 2.459223e-02 -0.279185
2019-11-05 10:17:29,561 train 150 2.460412e-02 -0.305351
2019-11-05 10:17:50,231 train 200 2.456070e-02 -0.286487
2019-11-05 10:18:10,836 train 250 2.457946e-02 -0.285085
2019-11-05 10:18:31,435 train 300 2.446613e-02 -0.295546
2019-11-05 10:18:52,068 train 350 2.443888e-02 -0.291929
2019-11-05 10:19:12,599 train 400 2.445221e-02 -0.295120
2019-11-05 10:19:25,709 training loss; R2: 2.445594e-02 -0.291444
2019-11-05 10:19:26,776 valid 000 2.350571e-02 -0.158311
2019-11-05 10:19:45,879 validation loss; R2: 2.355981e-02 -0.285543
2019-11-05 10:19:45,915 epoch 9 lr 1.000000e-04
2019-11-05 10:19:47,129 train 000 2.858788e-02 -0.008282
2019-11-05 10:20:07,765 train 050 2.410543e-02 -0.339408
2019-11-05 10:20:28,353 train 100 2.421615e-02 -0.310744
2019-11-05 10:20:49,033 train 150 2.432751e-02 -0.309563
2019-11-05 10:21:09,553 train 200 2.424328e-02 -0.295193
2019-11-05 10:21:29,998 train 250 2.418623e-02 -0.293787
2019-11-05 10:21:50,431 train 300 2.413372e-02 -0.296035
2019-11-05 10:22:10,933 train 350 2.414140e-02 -0.289169
2019-11-05 10:22:31,416 train 400 2.412252e-02 -0.281821
2019-11-05 10:22:44,466 training loss; R2: 2.411033e-02 -0.286667
2019-11-05 10:22:45,480 valid 000 2.292915e-02 -0.057499
2019-11-05 10:23:04,636 validation loss; R2: 2.344106e-02 -0.261259
2019-11-05 10:23:04,676 epoch 10 lr 1.000000e-04
2019-11-05 10:23:05,926 train 000 2.616035e-02 -0.120924
2019-11-05 10:23:26,449 train 050 2.360218e-02 -0.256300
2019-11-05 10:23:47,083 train 100 2.361675e-02 -0.254532
2019-11-05 10:24:07,608 train 150 2.363982e-02 -0.298627
2019-11-05 10:24:28,277 train 200 2.376641e-02 -0.294714
2019-11-05 10:24:48,887 train 250 2.373907e-02 -0.280964
2019-11-05 10:25:09,502 train 300 2.371782e-02 -0.283413
2019-11-05 10:25:30,119 train 350 2.372720e-02 -0.287167
2019-11-05 10:25:50,683 train 400 2.370537e-02 -0.284305
2019-11-05 10:26:03,834 training loss; R2: 2.370527e-02 -0.281924
2019-11-05 10:26:04,874 valid 000 2.508744e-02 -0.153275
2019-11-05 10:26:23,899 validation loss; R2: 2.389536e-02 -0.152165
2019-11-05 10:26:23,931 epoch 11 lr 1.000000e-04
2019-11-05 10:26:25,113 train 000 2.366938e-02 -0.243055
2019-11-05 10:26:45,628 train 050 2.338507e-02 -0.213855
2019-11-05 10:27:06,090 train 100 2.344879e-02 -0.259248
2019-11-05 10:27:26,808 train 150 2.344778e-02 -0.268290
2019-11-05 10:27:47,692 train 200 2.345456e-02 -0.264875
2019-11-05 10:28:08,338 train 250 2.347381e-02 -0.272600
2019-11-05 10:28:29,199 train 300 2.337150e-02 -0.274279
2019-11-05 10:28:49,994 train 350 2.339116e-02 -0.269753
2019-11-05 10:29:10,632 train 400 2.338146e-02 -0.267923
2019-11-05 10:29:23,671 training loss; R2: 2.336357e-02 -0.264932
2019-11-05 10:29:24,659 valid 000 2.475278e-02 -0.114317
2019-11-05 10:29:43,724 validation loss; R2: 2.396953e-02 -0.269628
2019-11-05 10:29:43,754 epoch 12 lr 1.000000e-04
2019-11-05 10:29:44,953 train 000 2.309550e-02 -0.513975
2019-11-05 10:30:05,776 train 050 2.345325e-02 -0.259382
2019-11-05 10:30:26,489 train 100 2.326949e-02 -0.252681
2019-11-05 10:30:46,782 train 150 2.315454e-02 -0.266322
2019-11-05 10:31:07,419 train 200 2.319021e-02 -0.288327
2019-11-05 10:31:27,950 train 250 2.323053e-02 -0.276709
2019-11-05 10:31:48,456 train 300 2.327658e-02 -0.274894
2019-11-05 10:32:08,887 train 350 2.320620e-02 -0.277884
2019-11-05 10:32:29,333 train 400 2.315825e-02 -0.269309
2019-11-05 10:32:42,450 training loss; R2: 2.313239e-02 -0.268445
2019-11-05 10:32:43,443 valid 000 2.189947e-02 0.091479
2019-11-05 10:33:02,322 validation loss; R2: 2.267684e-02 -0.148452
2019-11-05 10:33:02,353 epoch 13 lr 1.000000e-04
2019-11-05 10:33:03,549 train 000 1.906864e-02 -0.026378
2019-11-05 10:33:24,245 train 050 2.310944e-02 -0.260037
2019-11-05 10:33:44,838 train 100 2.303736e-02 -0.275705
2019-11-05 10:34:05,111 train 150 2.307220e-02 -0.275043
2019-11-05 10:34:25,692 train 200 2.304818e-02 -0.293784
2019-11-05 10:34:46,414 train 250 2.298883e-02 -0.285816
2019-11-05 10:35:07,175 train 300 2.301405e-02 -0.286863
2019-11-05 10:35:27,894 train 350 2.297388e-02 -0.281274
2019-11-05 10:35:48,588 train 400 2.300663e-02 -0.275468
2019-11-05 10:36:01,764 training loss; R2: 2.298791e-02 -0.279998
2019-11-05 10:36:02,799 valid 000 2.267468e-02 -0.395094
2019-11-05 10:36:21,852 validation loss; R2: 2.295089e-02 -0.259481
2019-11-05 10:36:21,899 epoch 14 lr 1.000000e-04
2019-11-05 10:36:23,075 train 000 1.989496e-02 -2.157670
2019-11-05 10:36:43,911 train 050 2.283197e-02 -0.258149
2019-11-05 10:37:04,620 train 100 2.278298e-02 -0.374296
2019-11-05 10:37:25,161 train 150 2.284228e-02 -0.327561
2019-11-05 10:37:46,156 train 200 2.293291e-02 -0.314159
2019-11-05 10:38:07,582 train 250 2.295574e-02 -0.296384
2019-11-05 10:38:28,760 train 300 2.290086e-02 -0.276142
2019-11-05 10:38:49,819 train 350 2.288293e-02 -0.282153
2019-11-05 10:39:10,672 train 400 2.283578e-02 -0.271737
2019-11-05 10:39:23,901 training loss; R2: 2.278352e-02 -0.284271
2019-11-05 10:39:24,899 valid 000 2.299913e-02 -0.935212
2019-11-05 10:39:44,050 validation loss; R2: 2.337710e-02 -0.322787
2019-11-05 10:39:44,085 epoch 15 lr 1.000000e-04
2019-11-05 10:39:45,278 train 000 1.822419e-02 -0.024562
2019-11-05 10:40:06,279 train 050 2.286317e-02 -0.352497
2019-11-05 10:40:27,107 train 100 2.263915e-02 -0.295899
2019-11-05 10:40:46,962 train 150 2.258719e-02 -0.284224
2019-11-05 10:41:07,276 train 200 2.254950e-02 -0.303778
2019-11-05 10:41:27,963 train 250 2.257861e-02 -0.287910
2019-11-05 10:41:48,665 train 300 2.259763e-02 -0.294384
2019-11-05 10:42:09,335 train 350 2.262408e-02 -0.287627
2019-11-05 10:42:30,056 train 400 2.264378e-02 -0.287513
2019-11-05 10:42:43,237 training loss; R2: 2.262031e-02 -0.293445
2019-11-05 10:42:44,283 valid 000 2.174594e-02 -0.070152
2019-11-05 10:43:03,231 validation loss; R2: 2.279641e-02 -0.309259
2019-11-05 10:43:03,264 epoch 16 lr 1.000000e-04
2019-11-05 10:43:04,457 train 000 2.553793e-02 -0.550940
2019-11-05 10:43:25,253 train 050 2.276486e-02 -0.208135
2019-11-05 10:43:45,883 train 100 2.244453e-02 -0.284679
2019-11-05 10:44:05,964 train 150 2.250527e-02 -0.277306
2019-11-05 10:44:26,656 train 200 2.254931e-02 -0.264365
2019-11-05 10:44:47,196 train 250 2.247902e-02 -0.271702
2019-11-05 10:45:07,748 train 300 2.253503e-02 -0.268512
2019-11-05 10:45:28,304 train 350 2.254352e-02 -0.260369
2019-11-05 10:45:48,865 train 400 2.254312e-02 -0.262660
2019-11-05 10:46:01,929 training loss; R2: 2.254944e-02 -0.261289
2019-11-05 10:46:02,905 valid 000 2.205981e-02 0.009321
2019-11-05 10:46:21,904 validation loss; R2: 2.234604e-02 -0.243258
2019-11-05 10:46:21,938 epoch 17 lr 1.000000e-04
2019-11-05 10:46:23,115 train 000 2.270089e-02 -0.080473
2019-11-05 10:46:43,909 train 050 2.196198e-02 -0.280455
2019-11-05 10:47:04,644 train 100 2.220873e-02 -0.256082
2019-11-05 10:47:25,375 train 150 2.229519e-02 -0.273121
2019-11-05 10:47:46,161 train 200 2.227230e-02 -0.263647
2019-11-05 10:48:06,644 train 250 2.229094e-02 -0.256981
2019-11-05 10:48:27,085 train 300 2.227917e-02 -0.264215
2019-11-05 10:48:47,570 train 350 2.230919e-02 -0.263279
2019-11-05 10:49:08,055 train 400 2.235191e-02 -0.264603
2019-11-05 10:49:21,092 training loss; R2: 2.236314e-02 -0.265286
2019-11-05 10:49:22,114 valid 000 2.315586e-02 0.055611
2019-11-05 10:49:41,134 validation loss; R2: 2.219062e-02 -0.185764
2019-11-05 10:49:41,168 epoch 18 lr 1.000000e-04
2019-11-05 10:49:42,368 train 000 2.211385e-02 -0.265435
2019-11-05 10:50:02,771 train 050 2.222996e-02 -0.214721
2019-11-05 10:50:23,023 train 100 2.234432e-02 -0.236074
2019-11-05 10:50:42,924 train 150 2.232108e-02 -0.277481
2019-11-05 10:51:03,376 train 200 2.234777e-02 -0.272730
2019-11-05 10:51:23,813 train 250 2.230507e-02 -0.312425
2019-11-05 10:51:44,230 train 300 2.228951e-02 -0.330726
2019-11-05 10:52:04,703 train 350 2.235440e-02 -0.313953
2019-11-05 10:52:25,332 train 400 2.232369e-02 -0.303391
2019-11-05 10:52:38,563 training loss; R2: 2.234187e-02 -0.306786
2019-11-05 10:52:39,597 valid 000 2.227371e-02 -0.050787
2019-11-05 10:52:58,550 validation loss; R2: 2.176285e-02 -0.252500
2019-11-05 10:52:58,583 epoch 19 lr 1.000000e-04
2019-11-05 10:52:59,784 train 000 2.356381e-02 -0.358630
2019-11-05 10:53:20,587 train 050 2.237381e-02 -0.365827
2019-11-05 10:53:41,228 train 100 2.244021e-02 -0.294381
2019-11-05 10:54:02,041 train 150 2.240279e-02 -0.272451
2019-11-05 10:54:22,829 train 200 2.238673e-02 -0.271986
2019-11-05 10:54:43,422 train 250 2.232684e-02 -0.272062
2019-11-05 10:55:03,972 train 300 2.230792e-02 -0.275651
2019-11-05 10:55:24,492 train 350 2.227746e-02 -0.276171
2019-11-05 10:55:45,055 train 400 2.228420e-02 -0.285693
2019-11-05 10:55:58,187 training loss; R2: 2.229015e-02 -0.290548
2019-11-05 10:55:59,168 valid 000 2.282867e-02 -0.121728
2019-11-05 10:56:18,138 validation loss; R2: 2.313950e-02 -0.222987
2019-11-05 10:56:18,178 epoch 20 lr 1.000000e-04
2019-11-05 10:56:19,329 train 000 2.160469e-02 -0.213469
2019-11-05 10:56:40,081 train 050 2.194542e-02 -0.191387
2019-11-05 10:57:00,707 train 100 2.202443e-02 -0.218238
2019-11-05 10:57:21,336 train 150 2.223427e-02 -0.229754
2019-11-05 10:57:41,939 train 200 2.221741e-02 -0.229693
2019-11-05 10:58:02,379 train 250 2.228705e-02 -0.247778
2019-11-05 10:58:22,901 train 300 2.227333e-02 -0.258457
2019-11-05 10:58:43,545 train 350 2.228990e-02 -0.272331
2019-11-05 10:59:04,487 train 400 2.229545e-02 -0.281469
2019-11-05 10:59:17,661 training loss; R2: 2.231530e-02 -0.277934
2019-11-05 10:59:18,677 valid 000 2.240432e-02 -0.136267
2019-11-05 10:59:37,965 validation loss; R2: 2.328334e-02 -0.242742
2019-11-05 10:59:38,008 epoch 21 lr 1.000000e-04
2019-11-05 10:59:39,227 train 000 2.215014e-02 -0.002199
2019-11-05 10:59:59,741 train 050 2.234521e-02 -0.230896
2019-11-05 11:00:20,461 train 100 2.231211e-02 -0.226363
2019-11-05 11:00:40,904 train 150 2.233041e-02 -0.245574
2019-11-05 11:01:01,379 train 200 2.224506e-02 -0.236757
2019-11-05 11:01:22,112 train 250 2.223049e-02 -0.255782
2019-11-05 11:01:42,717 train 300 2.221859e-02 -0.268091
2019-11-05 11:02:03,584 train 350 2.216197e-02 -0.256795
2019-11-05 11:02:24,293 train 400 2.219082e-02 -0.263652
2019-11-05 11:02:37,506 training loss; R2: 2.220239e-02 -0.264685
2019-11-05 11:02:38,551 valid 000 2.183401e-02 -0.093520
2019-11-05 11:02:57,634 validation loss; R2: 2.268232e-02 -0.192137
2019-11-05 11:02:57,664 epoch 22 lr 1.000000e-04
2019-11-05 11:02:58,789 train 000 2.146725e-02 -0.063278
2019-11-05 11:03:19,769 train 050 2.181418e-02 -0.265243
2019-11-05 11:03:40,631 train 100 2.185996e-02 -0.251422
2019-11-05 11:04:01,379 train 150 2.200137e-02 -0.282837
2019-11-05 11:04:21,899 train 200 2.206476e-02 -0.263057
2019-11-05 11:04:42,409 train 250 2.203150e-02 -0.270474
2019-11-05 11:05:02,926 train 300 2.204409e-02 -0.306786
2019-11-05 11:05:23,389 train 350 2.205281e-02 -0.298556
2019-11-05 11:05:43,932 train 400 2.201471e-02 -0.295227
2019-11-05 11:05:57,021 training loss; R2: 2.199462e-02 -0.294218
2019-11-05 11:05:58,097 valid 000 2.154384e-02 -0.260458
2019-11-05 11:06:17,221 validation loss; R2: 2.301127e-02 -0.151811
2019-11-05 11:06:17,254 epoch 23 lr 1.000000e-04
2019-11-05 11:06:18,480 train 000 2.301236e-02 -0.062215
2019-11-05 11:06:39,146 train 050 2.224800e-02 -0.272158
2019-11-05 11:06:59,853 train 100 2.229821e-02 -0.249983
2019-11-05 11:07:20,344 train 150 2.221803e-02 -0.288074
2019-11-05 11:07:40,862 train 200 2.224004e-02 -0.269377
2019-11-05 11:08:01,292 train 250 2.216846e-02 -0.263644
2019-11-05 11:08:21,712 train 300 2.209185e-02 -0.279821
2019-11-05 11:08:42,309 train 350 2.208770e-02 -0.289599
2019-11-05 11:09:02,979 train 400 2.208764e-02 -0.285246
2019-11-05 11:09:16,149 training loss; R2: 2.208750e-02 -0.285785
2019-11-05 11:09:17,220 valid 000 2.376187e-02 -0.157690
2019-11-05 11:09:36,273 validation loss; R2: 2.223601e-02 -0.129354
2019-11-05 11:09:36,310 epoch 24 lr 1.000000e-04
2019-11-05 11:09:37,531 train 000 2.304982e-02 -0.057795
2019-11-05 11:09:58,360 train 050 2.197008e-02 -0.209262
2019-11-05 11:10:19,186 train 100 2.205031e-02 -0.238345
2019-11-05 11:10:39,733 train 150 2.208205e-02 -0.272484
2019-11-05 11:11:00,344 train 200 2.198076e-02 -0.275259
2019-11-05 11:11:21,038 train 250 2.197578e-02 -0.280418
2019-11-05 11:11:41,710 train 300 2.190122e-02 -0.278812
2019-11-05 11:12:02,280 train 350 2.188274e-02 -0.273951
2019-11-05 11:12:22,797 train 400 2.191917e-02 -0.271468
2019-11-05 11:12:35,966 training loss; R2: 2.194894e-02 -0.262987
2019-11-05 11:12:36,989 valid 000 2.447918e-02 -0.844111
2019-11-05 11:12:55,920 validation loss; R2: 2.217161e-02 -0.235922
2019-11-05 11:12:55,958 epoch 25 lr 1.000000e-04
2019-11-05 11:12:57,123 train 000 1.995169e-02 -0.053200
2019-11-05 11:13:17,726 train 050 2.191012e-02 -0.236208
2019-11-05 11:13:38,161 train 100 2.183132e-02 -0.250945
2019-11-05 11:13:58,440 train 150 2.181729e-02 -0.267192
2019-11-05 11:14:19,273 train 200 2.187070e-02 -0.262551
2019-11-05 11:14:39,982 train 250 2.197549e-02 -0.292984
2019-11-05 11:15:00,652 train 300 2.193792e-02 -0.293535
2019-11-05 11:15:21,264 train 350 2.192632e-02 -0.306901
2019-11-05 11:15:41,906 train 400 2.191382e-02 -0.301238
2019-11-05 11:15:55,173 training loss; R2: 2.190558e-02 -0.298682
2019-11-05 11:15:56,208 valid 000 2.631429e-02 -0.234195
2019-11-05 11:16:15,470 validation loss; R2: 2.551740e-02 -0.154889
2019-11-05 11:16:15,511 epoch 26 lr 1.000000e-04
2019-11-05 11:16:16,748 train 000 2.136077e-02 -0.231590
2019-11-05 11:16:37,693 train 050 2.202645e-02 -0.290538
2019-11-05 11:16:58,419 train 100 2.219548e-02 -0.280254
2019-11-05 11:17:18,392 train 150 2.203684e-02 -0.278729
2019-11-05 11:17:38,689 train 200 2.208136e-02 -0.262909
2019-11-05 11:17:59,305 train 250 2.202829e-02 -0.271437
2019-11-05 11:18:19,903 train 300 2.195757e-02 -0.279415
2019-11-05 11:18:40,487 train 350 2.193743e-02 -0.276385
2019-11-05 11:19:00,991 train 400 2.196684e-02 -0.269614
2019-11-05 11:19:14,068 training loss; R2: 2.195543e-02 -0.270660
2019-11-05 11:19:15,123 valid 000 2.150410e-02 -0.749320
2019-11-05 11:19:34,060 validation loss; R2: 2.246865e-02 -0.224294
2019-11-05 11:19:34,095 epoch 27 lr 1.000000e-04
2019-11-05 11:19:35,312 train 000 2.209619e-02 -0.123706
2019-11-05 11:19:56,084 train 050 2.207070e-02 -0.260435
2019-11-05 11:20:16,785 train 100 2.201286e-02 -0.308214
2019-11-05 11:20:36,775 train 150 2.193853e-02 -0.329814
2019-11-05 11:20:57,609 train 200 2.197608e-02 -0.342474
2019-11-05 11:21:18,601 train 250 2.191441e-02 -0.319536
2019-11-05 11:21:39,221 train 300 2.192350e-02 -0.310687
2019-11-05 11:21:59,849 train 350 2.197974e-02 -0.303657
2019-11-05 11:22:20,466 train 400 2.199025e-02 -0.296258
2019-11-05 11:22:33,593 training loss; R2: 2.200141e-02 -0.294785
2019-11-05 11:22:34,613 valid 000 2.472245e-02 -0.185998
2019-11-05 11:22:53,656 validation loss; R2: 2.336209e-02 -0.144987
2019-11-05 11:22:53,703 epoch 28 lr 1.000000e-04
2019-11-05 11:22:54,892 train 000 2.042204e-02 -0.132438
2019-11-05 11:23:15,527 train 050 2.210579e-02 -0.230740
2019-11-05 11:23:36,144 train 100 2.223865e-02 -0.263977
2019-11-05 11:23:56,427 train 150 2.220426e-02 -0.243282
2019-11-05 11:24:16,910 train 200 2.220846e-02 -0.243588
2019-11-05 11:24:37,532 train 250 2.219282e-02 -0.258106
2019-11-05 11:24:58,172 train 300 2.214817e-02 -0.266294
2019-11-05 11:25:18,800 train 350 2.211116e-02 -0.262448
2019-11-05 11:25:39,437 train 400 2.217569e-02 -0.264800
2019-11-05 11:25:52,579 training loss; R2: 2.216817e-02 -0.265318
2019-11-05 11:25:53,561 valid 000 2.098479e-02 -0.135814
2019-11-05 11:26:12,580 validation loss; R2: 2.194868e-02 -0.170897
2019-11-05 11:26:12,613 epoch 29 lr 1.000000e-04
2019-11-05 11:26:13,901 train 000 1.989850e-02 -0.232907
2019-11-05 11:26:34,476 train 050 2.204376e-02 -0.195825
2019-11-05 11:26:55,127 train 100 2.209087e-02 -0.203390
2019-11-05 11:27:15,748 train 150 2.214842e-02 -0.229006
2019-11-05 11:27:36,352 train 200 2.210108e-02 -0.253914
2019-11-05 11:27:56,816 train 250 2.215281e-02 -0.256095
2019-11-05 11:28:17,275 train 300 2.222459e-02 -0.261544
2019-11-05 11:28:37,798 train 350 2.219485e-02 -0.260915
2019-11-05 11:28:58,302 train 400 2.220011e-02 -0.262977
2019-11-05 11:29:11,412 training loss; R2: 2.220550e-02 -0.264921
2019-11-05 11:29:12,442 valid 000 2.139447e-02 -0.199274
2019-11-05 11:29:31,683 validation loss; R2: 2.213436e-02 -0.211249
2019-11-05 11:29:31,718 epoch 30 lr 1.000000e-04
2019-11-05 11:29:32,946 train 000 1.995156e-02 -0.064425
2019-11-05 11:29:53,566 train 050 2.229433e-02 -0.209845
2019-11-05 11:30:14,069 train 100 2.224784e-02 -0.217134
2019-11-05 11:30:34,545 train 150 2.238134e-02 -0.235569
2019-11-05 11:30:55,171 train 200 2.229515e-02 -0.240546
2019-11-05 11:31:16,044 train 250 2.224450e-02 -0.233812
2019-11-05 11:31:36,886 train 300 2.223509e-02 -0.253231
2019-11-05 11:31:57,685 train 350 2.231870e-02 -0.251136
2019-11-05 11:32:18,554 train 400 2.226569e-02 -0.257371
2019-11-05 11:32:31,869 training loss; R2: 2.223736e-02 -0.264834
2019-11-05 11:32:32,834 valid 000 2.274473e-02 -0.407377
2019-11-05 11:32:51,991 validation loss; R2: 2.198213e-02 -0.183815
2019-11-05 11:32:52,018 epoch 31 lr 1.000000e-04
2019-11-05 11:32:53,213 train 000 2.542744e-02 -0.353279
2019-11-05 11:33:14,142 train 050 2.205079e-02 -0.253859
2019-11-05 11:33:34,924 train 100 2.213535e-02 -0.236230
2019-11-05 11:33:55,413 train 150 2.214282e-02 -0.312491
2019-11-05 11:34:15,926 train 200 2.211140e-02 -0.303329
2019-11-05 11:34:36,421 train 250 2.213670e-02 -0.303656
2019-11-05 11:34:56,821 train 300 2.215691e-02 -0.285231
2019-11-05 11:35:17,365 train 350 2.211540e-02 -0.282858
2019-11-05 11:35:37,949 train 400 2.211750e-02 -0.285899
2019-11-05 11:35:51,047 training loss; R2: 2.213374e-02 -0.278362
2019-11-05 11:35:52,093 valid 000 2.149900e-02 -0.127513
2019-11-05 11:36:11,230 validation loss; R2: 2.138255e-02 -0.250625
2019-11-05 11:36:11,268 epoch 32 lr 1.000000e-04
2019-11-05 11:36:12,509 train 000 2.224963e-02 0.010092
2019-11-05 11:36:33,037 train 050 2.206573e-02 -0.236646
2019-11-05 11:36:53,618 train 100 2.189752e-02 -0.270914
2019-11-05 11:37:14,295 train 150 2.197381e-02 -0.281965
2019-11-05 11:37:34,679 train 200 2.199956e-02 -0.278855
2019-11-05 11:37:55,045 train 250 2.200971e-02 -0.284358
2019-11-05 11:38:15,338 train 300 2.200173e-02 -0.274702
2019-11-05 11:38:35,774 train 350 2.200508e-02 -0.275282
2019-11-05 11:38:56,261 train 400 2.203448e-02 -0.275315
2019-11-05 11:39:09,284 training loss; R2: 2.205148e-02 -0.294431
2019-11-05 11:39:10,335 valid 000 2.147116e-02 -0.344817
2019-11-05 11:39:29,572 validation loss; R2: 2.129059e-02 -0.241472
2019-11-05 11:39:29,613 epoch 33 lr 1.000000e-04
2019-11-05 11:39:30,859 train 000 2.224002e-02 -0.054346
2019-11-05 11:39:51,492 train 050 2.202910e-02 -0.631416
2019-11-05 11:40:12,020 train 100 2.213875e-02 -0.451259
2019-11-05 11:40:32,567 train 150 2.206487e-02 -0.396447
2019-11-05 11:40:52,917 train 200 2.210763e-02 -0.348216
2019-11-05 11:41:13,524 train 250 2.201664e-02 -0.353585
2019-11-05 11:41:34,250 train 300 2.204154e-02 -0.343849
2019-11-05 11:41:54,833 train 350 2.202876e-02 -0.332690
2019-11-05 11:42:15,465 train 400 2.204064e-02 -0.323569
2019-11-05 11:42:28,602 training loss; R2: 2.203449e-02 -0.319620
2019-11-05 11:42:29,631 valid 000 2.224354e-02 0.056641
2019-11-05 11:42:48,946 validation loss; R2: 2.269791e-02 -0.188362
2019-11-05 11:42:48,980 epoch 34 lr 1.000000e-04
2019-11-05 11:42:50,220 train 000 2.160103e-02 -0.414366
2019-11-05 11:43:10,841 train 050 2.186846e-02 -0.223452
2019-11-05 11:43:31,481 train 100 2.186688e-02 -0.260666
2019-11-05 11:43:51,676 train 150 2.193555e-02 -0.260744
2019-11-05 11:44:12,338 train 200 2.194692e-02 -0.269984
2019-11-05 11:44:32,623 train 250 2.186137e-02 -0.264872
2019-11-05 11:44:53,257 train 300 2.193250e-02 -1.028281
2019-11-05 11:45:14,010 train 350 2.195375e-02 -0.928961
2019-11-05 11:45:34,593 train 400 2.193842e-02 -0.868559
2019-11-05 11:45:47,629 training loss; R2: 2.196606e-02 -0.821253
2019-11-05 11:45:48,702 valid 000 2.005419e-02 -0.303710
2019-11-05 11:46:07,803 validation loss; R2: 2.105690e-02 -0.241338
2019-11-05 11:46:07,835 epoch 35 lr 1.000000e-04
2019-11-05 11:46:08,987 train 000 2.032240e-02 -0.628668
2019-11-05 11:46:29,896 train 050 2.199866e-02 -0.244293
2019-11-05 11:46:50,635 train 100 2.206967e-02 -0.229162
2019-11-05 11:47:10,695 train 150 2.212231e-02 -0.230334
2019-11-05 11:47:31,111 train 200 2.202442e-02 -0.240386
2019-11-05 11:47:51,801 train 250 2.201753e-02 -0.272717
2019-11-05 11:48:12,504 train 300 2.191537e-02 -0.273437
2019-11-05 11:48:33,221 train 350 2.190032e-02 -0.277459
2019-11-05 11:48:53,935 train 400 2.192193e-02 -0.272230
2019-11-05 11:49:07,127 training loss; R2: 2.192804e-02 -0.273611
2019-11-05 11:49:08,144 valid 000 2.461881e-02 -0.343249
2019-11-05 11:49:27,149 validation loss; R2: 2.210093e-02 -0.235068
2019-11-05 11:49:27,193 epoch 36 lr 1.000000e-04
2019-11-05 11:49:28,413 train 000 2.049764e-02 -0.281482
2019-11-05 11:49:49,219 train 050 2.179425e-02 -0.260235
2019-11-05 11:50:09,892 train 100 2.184702e-02 -0.242183
2019-11-05 11:50:29,836 train 150 2.193275e-02 -0.268739
2019-11-05 11:50:50,480 train 200 2.185668e-02 -0.279402
2019-11-05 11:51:11,438 train 250 2.186651e-02 -0.276538
2019-11-05 11:51:32,471 train 300 2.182573e-02 -0.273390
2019-11-05 11:51:53,272 train 350 2.184543e-02 -0.273500
2019-11-05 11:52:13,989 train 400 2.181382e-02 -0.274543
2019-11-05 11:52:27,238 training loss; R2: 2.181718e-02 -0.272295
2019-11-05 11:52:28,220 valid 000 2.378676e-02 -0.028710
2019-11-05 11:52:47,405 validation loss; R2: 2.285505e-02 -0.118647
2019-11-05 11:52:47,446 epoch 37 lr 1.000000e-04
2019-11-05 11:52:48,656 train 000 1.978112e-02 0.000728
2019-11-05 11:53:09,587 train 050 2.119711e-02 -0.180487
2019-11-05 11:53:30,417 train 100 2.142185e-02 -0.231246
2019-11-05 11:53:51,167 train 150 2.165816e-02 -0.227909
2019-11-05 11:54:12,019 train 200 2.171362e-02 -0.243364
2019-11-05 11:54:32,982 train 250 2.174728e-02 -0.257666
2019-11-05 11:54:53,899 train 300 2.182106e-02 -0.269522
2019-11-05 11:55:14,717 train 350 2.179079e-02 -0.258096
2019-11-05 11:55:35,549 train 400 2.180484e-02 -0.252197
2019-11-05 11:55:48,792 training loss; R2: 2.180372e-02 -0.252885
2019-11-05 11:55:49,859 valid 000 2.208238e-02 -0.005848
2019-11-05 11:56:09,065 validation loss; R2: 2.068225e-02 -0.319004
2019-11-05 11:56:09,099 epoch 38 lr 1.000000e-04
2019-11-05 11:56:10,245 train 000 2.058370e-02 -0.992704
2019-11-05 11:56:31,294 train 050 2.157488e-02 -0.370925
2019-11-05 11:56:52,095 train 100 2.175846e-02 -0.338120
2019-11-05 11:57:12,752 train 150 2.174519e-02 -0.314316
2019-11-05 11:57:33,453 train 200 2.177523e-02 -0.321543
2019-11-05 11:57:54,056 train 250 2.176688e-02 -0.307718
2019-11-05 11:58:14,576 train 300 2.172804e-02 -0.302860
2019-11-05 11:58:35,061 train 350 2.181572e-02 -0.280482
2019-11-05 11:58:55,565 train 400 2.177867e-02 -0.282612
2019-11-05 11:59:08,684 training loss; R2: 2.180257e-02 -0.273570
2019-11-05 11:59:09,724 valid 000 2.329094e-02 -0.019092
2019-11-05 11:59:28,859 validation loss; R2: 2.246487e-02 -0.278735
2019-11-05 11:59:28,902 epoch 39 lr 1.000000e-04
2019-11-05 11:59:30,102 train 000 2.157841e-02 -0.085893
2019-11-05 11:59:50,915 train 050 2.177925e-02 -0.321168
2019-11-05 12:00:11,641 train 100 2.156453e-02 -0.256695
2019-11-05 12:00:32,293 train 150 2.167237e-02 -0.248716
2019-11-05 12:00:52,941 train 200 2.178655e-02 -0.254027
2019-11-05 12:01:13,584 train 250 2.181225e-02 -0.270098
2019-11-05 12:01:34,211 train 300 2.173205e-02 -0.265703
2019-11-05 12:01:54,885 train 350 2.175432e-02 -0.273607
2019-11-05 12:02:15,494 train 400 2.172433e-02 -0.269666
2019-11-05 12:02:28,600 training loss; R2: 2.172588e-02 -0.271031
2019-11-05 12:02:29,609 valid 000 2.148754e-02 -0.867780
2019-11-05 12:02:48,689 validation loss; R2: 2.118917e-02 -0.312058
2019-11-05 12:02:48,727 epoch 40 lr 1.000000e-04
2019-11-05 12:02:49,982 train 000 2.075079e-02 -0.226495
2019-11-05 12:03:10,502 train 050 2.150425e-02 -0.356267
2019-11-05 12:03:30,917 train 100 2.180540e-02 -0.357107
2019-11-05 12:03:51,304 train 150 2.177606e-02 -0.314776
2019-11-05 12:04:12,010 train 200 2.174480e-02 -0.293409
2019-11-05 12:04:32,657 train 250 2.168666e-02 -0.283891
2019-11-05 12:04:53,271 train 300 2.169665e-02 -0.277005
2019-11-05 12:05:13,885 train 350 2.168832e-02 -0.263950
2019-11-05 12:05:34,460 train 400 2.169576e-02 -0.260272
2019-11-05 12:05:47,563 training loss; R2: 2.168791e-02 -0.256425
2019-11-05 12:05:48,577 valid 000 2.143922e-02 -0.369763
2019-11-05 12:06:07,813 validation loss; R2: 2.171152e-02 -0.266989
2019-11-05 12:06:07,842 epoch 41 lr 1.000000e-04
2019-11-05 12:06:08,995 train 000 2.326277e-02 -0.611996
2019-11-05 12:06:29,831 train 050 2.165032e-02 -0.331825
2019-11-05 12:06:50,492 train 100 2.160189e-02 -0.286109
2019-11-05 12:07:10,357 train 150 2.162495e-02 -0.300505
2019-11-05 12:07:30,985 train 200 2.166887e-02 -0.301245
2019-11-05 12:07:51,661 train 250 2.164033e-02 -0.297856
2019-11-05 12:08:12,331 train 300 2.162336e-02 -0.298376
2019-11-05 12:08:32,959 train 350 2.158155e-02 -0.294553
2019-11-05 12:08:53,558 train 400 2.158487e-02 -0.287398
2019-11-05 12:09:06,678 training loss; R2: 2.160518e-02 -0.284038
2019-11-05 12:09:07,677 valid 000 2.088853e-02 -0.635983
2019-11-05 12:09:26,612 validation loss; R2: 2.165212e-02 -0.206594
2019-11-05 12:09:26,658 epoch 42 lr 1.000000e-04
2019-11-05 12:09:27,805 train 000 2.173787e-02 -0.736293
2019-11-05 12:09:48,572 train 050 2.154128e-02 -0.325406
2019-11-05 12:10:08,955 train 100 2.149314e-02 -0.275273
2019-11-05 12:10:29,099 train 150 2.158115e-02 -0.278028
2019-11-05 12:10:49,883 train 200 2.161057e-02 -0.255923
2019-11-05 12:11:10,645 train 250 2.155674e-02 -0.252946
2019-11-05 12:11:31,378 train 300 2.151463e-02 -0.262034
2019-11-05 12:11:52,151 train 350 2.150641e-02 -0.273425
2019-11-05 12:12:12,895 train 400 2.154639e-02 -0.277005
2019-11-05 12:12:26,152 training loss; R2: 2.154446e-02 -0.276790
2019-11-05 12:12:27,182 valid 000 2.035058e-02 -0.053842
2019-11-05 12:12:46,178 validation loss; R2: 2.077796e-02 -0.364313
2019-11-05 12:12:46,209 epoch 43 lr 1.000000e-04
2019-11-05 12:12:47,429 train 000 1.880395e-02 -0.320844
2019-11-05 12:13:08,506 train 050 2.141920e-02 -0.325788
2019-11-05 12:13:29,276 train 100 2.151099e-02 -0.278272
2019-11-05 12:13:49,595 train 150 2.162736e-02 -0.275327
2019-11-05 12:14:10,285 train 200 2.157753e-02 -0.272245
2019-11-05 12:14:31,182 train 250 2.159631e-02 -0.263085
2019-11-05 12:14:52,200 train 300 2.156454e-02 -0.265361
2019-11-05 12:15:13,237 train 350 2.156705e-02 -0.264561
2019-11-05 12:15:33,926 train 400 2.157833e-02 -0.266213
2019-11-05 12:15:47,322 training loss; R2: 2.157123e-02 -0.260998
2019-11-05 12:15:48,290 valid 000 2.497271e-02 -0.106850
2019-11-05 12:16:07,339 validation loss; R2: 2.394890e-02 -0.163540
2019-11-05 12:16:07,372 epoch 44 lr 1.000000e-04
2019-11-05 12:16:08,566 train 000 2.196475e-02 -0.272312
2019-11-05 12:16:29,409 train 050 2.176883e-02 -0.274362
2019-11-05 12:16:50,000 train 100 2.171797e-02 -0.317834
2019-11-05 12:17:09,828 train 150 2.180167e-02 -0.299281
2019-11-05 12:17:30,415 train 200 2.168609e-02 -0.289561
2019-11-05 12:17:51,023 train 250 2.160137e-02 -0.295386
2019-11-05 12:18:11,600 train 300 2.159048e-02 -0.285622
2019-11-05 12:18:32,220 train 350 2.153409e-02 -0.286145
2019-11-05 12:18:52,826 train 400 2.153238e-02 -0.283827
2019-11-05 12:19:06,035 training loss; R2: 2.153757e-02 -0.279686
2019-11-05 12:19:06,998 valid 000 2.173633e-02 0.003459
2019-11-05 12:19:25,898 validation loss; R2: 2.180237e-02 -0.179139
2019-11-05 12:19:25,945 epoch 45 lr 1.000000e-04
2019-11-05 12:19:27,106 train 000 2.140157e-02 -0.107806
2019-11-05 12:19:47,837 train 050 2.159319e-02 -0.270643
2019-11-05 12:20:08,360 train 100 2.149533e-02 -0.354553
2019-11-05 12:20:28,428 train 150 2.156665e-02 -0.322702
2019-11-05 12:20:49,087 train 200 2.156998e-02 -0.298147
2019-11-05 12:21:09,770 train 250 2.163978e-02 -0.317238
2019-11-05 12:21:30,424 train 300 2.156885e-02 -0.318513
2019-11-05 12:21:51,072 train 350 2.152920e-02 -0.313045
2019-11-05 12:22:11,686 train 400 2.150896e-02 -0.312417
2019-11-05 12:22:24,819 training loss; R2: 2.146106e-02 -0.310834
2019-11-05 12:22:25,786 valid 000 1.968075e-02 0.006558
2019-11-05 12:22:44,702 validation loss; R2: 2.092954e-02 -0.210976
2019-11-05 12:22:44,735 epoch 46 lr 1.000000e-04
2019-11-05 12:22:45,910 train 000 2.038314e-02 -0.053085
2019-11-05 12:23:06,649 train 050 2.126138e-02 -0.238590
2019-11-05 12:23:27,354 train 100 2.144189e-02 -0.253026
2019-11-05 12:23:47,926 train 150 2.141326e-02 -0.256632
2019-11-05 12:24:08,444 train 200 2.139156e-02 -0.280511
2019-11-05 12:24:28,968 train 250 2.137551e-02 -0.321841
2019-11-05 12:24:49,471 train 300 2.135462e-02 -0.318890
2019-11-05 12:25:09,997 train 350 2.136420e-02 -0.311557
2019-11-05 12:25:30,589 train 400 2.142343e-02 -0.297007
2019-11-05 12:25:43,641 training loss; R2: 2.139805e-02 -0.289644
2019-11-05 12:25:44,600 valid 000 2.221848e-02 -0.383673
2019-11-05 12:26:03,581 validation loss; R2: 2.130126e-02 -0.165360
2019-11-05 12:26:03,620 epoch 47 lr 1.000000e-04
2019-11-05 12:26:04,810 train 000 2.191461e-02 -0.092571
2019-11-05 12:26:25,632 train 050 2.141010e-02 -0.290750
2019-11-05 12:26:46,297 train 100 2.135395e-02 -0.297533
2019-11-05 12:27:06,411 train 150 2.138328e-02 -0.274067
2019-11-05 12:27:26,967 train 200 2.134796e-02 -0.263647
2019-11-05 12:27:47,627 train 250 2.134675e-02 -0.267995
2019-11-05 12:28:08,282 train 300 2.142627e-02 -0.271006
2019-11-05 12:28:28,875 train 350 2.141986e-02 -0.264112
2019-11-05 12:28:49,411 train 400 2.138668e-02 -0.266274
2019-11-05 12:29:02,503 training loss; R2: 2.136994e-02 -0.273013
2019-11-05 12:29:03,442 valid 000 2.208247e-02 -0.088730
2019-11-05 12:29:22,248 validation loss; R2: 2.153993e-02 -0.211790
2019-11-05 12:29:22,280 epoch 48 lr 1.000000e-04
2019-11-05 12:29:23,468 train 000 1.955307e-02 -0.540350
2019-11-05 12:29:44,058 train 050 2.125705e-02 -0.233825
2019-11-05 12:30:04,677 train 100 2.127366e-02 -0.260055
2019-11-05 12:30:25,137 train 150 2.138810e-02 -0.290707
2019-11-05 12:30:45,867 train 200 2.137740e-02 -0.266920
2019-11-05 12:31:06,723 train 250 2.135592e-02 -0.311598
2019-11-05 12:31:27,416 train 300 2.135972e-02 -0.294637
2019-11-05 12:31:48,023 train 350 2.132877e-02 -0.287833
2019-11-05 12:32:08,642 train 400 2.133700e-02 -0.286106
2019-11-05 12:32:21,866 training loss; R2: 2.131244e-02 -0.289749
2019-11-05 12:32:22,906 valid 000 2.303297e-02 -0.099925
2019-11-05 12:32:41,991 validation loss; R2: 2.266957e-02 -0.158386
2019-11-05 12:32:42,025 epoch 49 lr 1.000000e-04
2019-11-05 12:32:43,167 train 000 2.289726e-02 0.017081
2019-11-05 12:33:03,980 train 050 2.135303e-02 -0.246979
2019-11-05 12:33:24,767 train 100 2.118390e-02 -0.235759
2019-11-05 12:33:45,442 train 150 2.115433e-02 -0.262038
2019-11-05 12:34:06,092 train 200 2.115619e-02 -0.258534
2019-11-05 12:34:26,636 train 250 2.118546e-02 -0.260391
2019-11-05 12:34:47,194 train 300 2.118467e-02 -0.276610
2019-11-05 12:35:07,883 train 350 2.119339e-02 -0.274646
2019-11-05 12:35:28,592 train 400 2.117973e-02 -0.281688
2019-11-05 12:35:41,763 training loss; R2: 2.121476e-02 -0.276905
2019-11-05 12:35:42,775 valid 000 1.995777e-02 -0.053771
2019-11-05 12:36:01,680 validation loss; R2: 2.128665e-02 -0.240860
2019-11-05 12:36:01,726 epoch 50 lr 1.000000e-04
2019-11-05 12:36:02,857 train 000 1.947368e-02 -0.064445
2019-11-05 12:36:23,677 train 050 2.067691e-02 -0.361782
2019-11-05 12:36:44,336 train 100 2.096273e-02 -0.283072
2019-11-05 12:37:03,957 train 150 2.101711e-02 -0.306720
2019-11-05 12:37:24,244 train 200 2.116972e-02 -0.290292
2019-11-05 12:37:44,879 train 250 2.120013e-02 -0.275162
2019-11-05 12:38:05,794 train 300 2.119779e-02 -0.281253
2019-11-05 12:38:26,576 train 350 2.119089e-02 -0.276319
2019-11-05 12:38:47,422 train 400 2.115908e-02 -0.278954
2019-11-05 12:39:00,746 training loss; R2: 2.116052e-02 -0.271568
2019-11-05 12:39:01,783 valid 000 1.918214e-02 -0.553329
2019-11-05 12:39:20,899 validation loss; R2: 2.013765e-02 -0.174483
2019-11-05 12:39:20,930 epoch 51 lr 1.000000e-04
2019-11-05 12:39:22,121 train 000 2.193048e-02 -0.026501
2019-11-05 12:39:43,053 train 050 2.118278e-02 -0.305151
2019-11-05 12:40:03,736 train 100 2.134020e-02 -0.262861
2019-11-05 12:40:24,194 train 150 2.127519e-02 -0.279150
2019-11-05 12:40:44,711 train 200 2.126023e-02 -0.288970
2019-11-05 12:41:05,185 train 250 2.125088e-02 -0.298791
2019-11-05 12:41:25,694 train 300 2.121967e-02 -0.301294
2019-11-05 12:41:46,164 train 350 2.118930e-02 -0.289731
2019-11-05 12:42:06,703 train 400 2.121866e-02 -0.283047
2019-11-05 12:42:19,800 training loss; R2: 2.124176e-02 -0.282791
2019-11-05 12:42:20,810 valid 000 2.346597e-02 -0.054941
2019-11-05 12:42:39,857 validation loss; R2: 2.124788e-02 -0.113763
2019-11-05 12:42:39,893 epoch 52 lr 1.000000e-04
2019-11-05 12:42:41,163 train 000 2.200903e-02 -0.166734
2019-11-05 12:43:01,737 train 050 2.087938e-02 -0.295658
2019-11-05 12:43:22,311 train 100 2.120601e-02 -0.304695
2019-11-05 12:43:42,913 train 150 2.128599e-02 -0.296107
2019-11-05 12:44:03,426 train 200 2.124701e-02 -0.290704
2019-11-05 12:44:23,878 train 250 2.122122e-02 -0.295600
2019-11-05 12:44:44,376 train 300 2.118047e-02 -0.284654
2019-11-05 12:45:04,876 train 350 2.119856e-02 -0.273767
2019-11-05 12:45:25,435 train 400 2.123130e-02 -0.271716
2019-11-05 12:45:38,546 training loss; R2: 2.122502e-02 -0.275080
2019-11-05 12:45:39,606 valid 000 1.997902e-02 0.036917
2019-11-05 12:45:58,908 validation loss; R2: 2.066940e-02 -0.127726
2019-11-05 12:45:58,945 epoch 53 lr 1.000000e-04
2019-11-05 12:46:00,188 train 000 2.126842e-02 -0.281612
2019-11-05 12:46:20,948 train 050 2.110157e-02 -0.265610
2019-11-05 12:46:41,492 train 100 2.117614e-02 -0.286133
2019-11-05 12:47:01,659 train 150 2.126146e-02 -0.298331
2019-11-05 12:47:22,250 train 200 2.117174e-02 -0.280863
2019-11-05 12:47:43,026 train 250 2.119973e-02 -0.273219
2019-11-05 12:48:03,778 train 300 2.122709e-02 -0.269958
2019-11-05 12:48:24,422 train 350 2.117118e-02 -0.281075
2019-11-05 12:48:45,000 train 400 2.112255e-02 -0.303981
2019-11-05 12:48:58,119 training loss; R2: 2.112017e-02 -0.294879
2019-11-05 12:48:59,132 valid 000 1.936910e-02 -0.105763
2019-11-05 12:49:18,160 validation loss; R2: 2.137447e-02 -0.174184
2019-11-05 12:49:18,191 epoch 54 lr 1.000000e-04
2019-11-05 12:49:19,385 train 000 2.135079e-02 -0.360189
2019-11-05 12:49:40,370 train 050 2.142793e-02 -0.350561
2019-11-05 12:50:01,177 train 100 2.111343e-02 -0.320432
2019-11-05 12:50:21,779 train 150 2.101873e-02 -0.287615
2019-11-05 12:50:42,471 train 200 2.111087e-02 -0.273208
2019-11-05 12:51:03,094 train 250 2.105626e-02 -0.270794
2019-11-05 12:51:23,719 train 300 2.109847e-02 -0.276090
2019-11-05 12:51:44,296 train 350 2.108692e-02 -0.278427
2019-11-05 12:52:04,767 train 400 2.114185e-02 -0.266232
2019-11-05 12:52:17,840 training loss; R2: 2.116126e-02 -0.267657
2019-11-05 12:52:18,909 valid 000 2.095314e-02 -0.777187
2019-11-05 12:52:37,928 validation loss; R2: 2.168169e-02 -0.173590
2019-11-05 12:52:37,975 epoch 55 lr 1.000000e-04
2019-11-05 12:52:39,238 train 000 2.309350e-02 -0.072682
2019-11-05 12:52:59,758 train 050 2.108720e-02 -0.270270
2019-11-05 12:53:20,484 train 100 2.089412e-02 -0.321632
2019-11-05 12:53:41,171 train 150 2.095673e-02 -0.326813
2019-11-05 12:54:01,815 train 200 2.102953e-02 -0.313584
