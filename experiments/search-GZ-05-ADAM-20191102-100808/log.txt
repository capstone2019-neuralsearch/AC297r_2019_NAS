2019-11-02 10:08:08,366 gpu device = 1
2019-11-02 10:08:08,367 args = Namespace(arch_learning_rate=0.01, arch_weight_decay=1e-06, batch_size=20, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.3, epochs=50, gpu=1, grad_clip=5, gz_regression=False, init_channels=16, layers=8, learning_rate=0.001, learning_rate_min=0.0001, model_path='saved_models', momentum=0.9, optimizer='Adam', report_freq=50, save='search-GZ_ADAM-20191102-100808', seed=2, train_portion=0.8, unrolled=True, weight_decay=1e-08)
2019-11-02 10:08:11,887 param size = 2.229141MB
2019-11-02 10:08:11,899 epoch 0 lr 1.000000e-03
2019-11-02 10:08:11,900 genotype = Genotype(normal=[('dil_conv_5x5', 1), ('skip_connect', 0), ('sep_conv_5x5', 2), ('skip_connect', 1), ('avg_pool_3x3', 3), ('dil_conv_5x5', 1), ('sep_conv_3x3', 3), ('skip_connect', 0)], normal_concat=range(2, 6), reduce=[('sep_conv_3x3', 1), ('dil_conv_5x5', 0), ('sep_conv_5x5', 1), ('max_pool_3x3', 2), ('skip_connect', 1), ('sep_conv_5x5', 3), ('max_pool_3x3', 3), ('max_pool_3x3', 1)], reduce_concat=range(2, 6))
2019-11-02 10:08:11,902 
alphas_normal = Variable containing:
 0.1251  0.1250  0.1250  0.1251  0.1251  0.1250  0.1250  0.1249
 0.1250  0.1249  0.1250  0.1249  0.1251  0.1251  0.1247  0.1254
 0.1251  0.1251  0.1248  0.1250  0.1250  0.1249  0.1250  0.1250
 0.1250  0.1250  0.1250  0.1251  0.1250  0.1248  0.1250  0.1251
 0.1251  0.1251  0.1249  0.1250  0.1248  0.1253  0.1247  0.1251
 0.1251  0.1251  0.1250  0.1249  0.1248  0.1248  0.1250  0.1251
 0.1249  0.1250  0.1251  0.1249  0.1250  0.1250  0.1249  0.1252
 0.1252  0.1249  0.1250  0.1250  0.1250  0.1251  0.1250  0.1248
 0.1250  0.1250  0.1252  0.1251  0.1249  0.1251  0.1247  0.1250
 0.1250  0.1248  0.1249  0.1252  0.1250  0.1249  0.1250  0.1252
 0.1250  0.1252  0.1251  0.1249  0.1248  0.1249  0.1251  0.1250
 0.1250  0.1250  0.1250  0.1249  0.1251  0.1249  0.1250  0.1251
 0.1250  0.1251  0.1248  0.1249  0.1252  0.1250  0.1249  0.1249
 0.1249  0.1251  0.1250  0.1249  0.1250  0.1251  0.1250  0.1250
[torch.cuda.FloatTensor of size 14x8 (GPU 1)]

2019-11-02 10:08:11,903 
alphas_reduce = Variable containing:
 0.1249  0.1251  0.1250  0.1250  0.1249  0.1250  0.1249  0.1253
 0.1249  0.1250  0.1248  0.1251  0.1253  0.1251  0.1250  0.1249
 0.1251  0.1250  0.1250  0.1250  0.1250  0.1251  0.1251  0.1248
 0.1249  0.1250  0.1250  0.1250  0.1249  0.1252  0.1250  0.1250
 0.1249  0.1252  0.1251  0.1249  0.1247  0.1250  0.1252  0.1251
 0.1250  0.1248  0.1252  0.1250  0.1249  0.1251  0.1249  0.1251
 0.1251  0.1251  0.1250  0.1253  0.1247  0.1249  0.1250  0.1248
 0.1251  0.1251  0.1251  0.1251  0.1249  0.1248  0.1250  0.1250
 0.1250  0.1251  0.1250  0.1250  0.1251  0.1253  0.1248  0.1247
 0.1252  0.1247  0.1250  0.1251  0.1250  0.1250  0.1250  0.1250
 0.1250  0.1252  0.1249  0.1250  0.1250  0.1252  0.1249  0.1249
 0.1249  0.1250  0.1249  0.1250  0.1250  0.1251  0.1249  0.1251
 0.1249  0.1253  0.1247  0.1251  0.1249  0.1252  0.1251  0.1248
 0.1249  0.1252  0.1249  0.1250  0.1250  0.1249  0.1251  0.1251
[torch.cuda.FloatTensor of size 14x8 (GPU 1)]

2019-11-02 10:08:19,682 train 000 4.419178e-02 -2.379490
2019-11-02 10:11:05,382 train 050 3.015388e-02 -3.141805
2019-11-02 10:13:54,065 train 100 2.647501e-02 -2.347320
2019-11-02 10:16:39,292 train 150 2.460857e-02 -2.536220
2019-11-02 10:19:27,360 train 200 2.384011e-02 -2.402087
2019-11-02 10:22:11,877 train 250 2.330425e-02 -2.139885
2019-11-02 10:25:00,758 train 300 2.287150e-02 -1.926925
2019-11-02 10:27:46,230 train 350 2.243532e-02 -1.794916
2019-11-02 10:30:33,763 train 400 2.205314e-02 -1.750326
2019-11-02 10:33:17,765 train 450 2.177818e-02 -1.688490
2019-11-02 10:36:03,397 train 500 2.141466e-02 -1.608400
2019-11-02 10:38:51,269 train 550 2.111031e-02 -1.546274
2019-11-02 10:41:35,408 train 600 2.087636e-02 -1.526103
2019-11-02 10:44:20,477 train 650 2.058371e-02 -1.585413
2019-11-02 10:47:06,771 train 700 2.031091e-02 -1.830888
2019-11-02 10:49:52,703 train 750 2.004511e-02 -1.901772
2019-11-02 10:52:38,771 train 800 1.978822e-02 -1.878557
2019-11-02 10:55:25,806 train 850 1.954808e-02 -1.853618
2019-11-02 10:58:11,369 train 900 1.933420e-02 -1.814039
2019-11-02 11:00:56,706 train 950 1.911277e-02 -1.794134
2019-11-02 11:03:42,736 train 1000 1.889504e-02 -1.756714
2019-11-02 11:06:27,260 train 1050 1.873186e-02 -1.731163
2019-11-02 11:09:11,500 train 1100 1.854399e-02 -1.700741
2019-11-02 11:11:56,156 train 1150 1.838398e-02 -1.673799
2019-11-02 11:14:40,871 train 1200 1.821192e-02 -1.948812
2019-11-02 11:17:25,165 train 1250 1.805261e-02 -2.239824
2019-11-02 11:20:09,566 train 1300 1.791401e-02 -2.184010
2019-11-02 11:22:54,416 train 1350 1.777249e-02 -2.553505
2019-11-02 11:25:39,512 train 1400 1.763045e-02 -2.502738
2019-11-02 11:28:23,251 train 1450 1.749384e-02 -2.453707
2019-11-02 11:31:05,591 train 1500 1.738182e-02 -2.420641
2019-11-02 11:33:48,250 train 1550 1.727361e-02 -2.392119
2019-11-02 11:36:27,618 train 1600 1.714646e-02 -2.354285
2019-11-02 11:39:05,651 train 1650 1.702405e-02 -2.324391
2019-11-02 11:41:43,406 train 1700 1.691304e-02 -2.286060
2019-11-02 11:44:27,400 train 1750 1.682880e-02 -2.256241
2019-11-02 11:47:05,210 train 1800 1.673210e-02 -2.235946
2019-11-02 11:49:47,907 train 1850 1.664042e-02 -2.209084
2019-11-02 11:52:39,062 train 1900 1.654611e-02 -2.470578
2019-11-02 11:55:30,446 train 1950 1.646123e-02 -2.431498
2019-11-02 11:58:21,826 train 2000 1.637058e-02 -2.395311
2019-11-02 12:01:13,674 train 2050 1.629213e-02 -2.420066
2019-11-02 12:04:04,829 train 2100 1.620507e-02 -2.385357
2019-11-02 12:06:41,997 train 2150 1.612538e-02 -2.372616
2019-11-02 12:09:19,043 train 2200 1.605050e-02 -2.347529
2019-11-02 12:12:06,780 train 2250 1.598482e-02 -2.319619
2019-11-02 12:14:57,873 train 2300 1.591972e-02 -2.296811
2019-11-02 12:17:46,813 train 2350 1.586136e-02 -2.262450
2019-11-02 12:20:21,307 train 2400 1.579473e-02 -2.235119
2019-11-02 12:22:57,134 train 2450 1.574046e-02 -2.214764
2019-11-02 12:23:40,318 training loss; R2: 1.572329e-02 -2.215063
2019-11-02 12:23:40,785 valid 000 1.023551e-02 -1.747869
2019-11-02 12:23:48,781 valid 050 1.143905e-02 -4.558279
2019-11-02 12:23:56,709 valid 100 1.167077e-02 -3.134309
2019-11-02 12:24:04,615 valid 150 1.163402e-02 -2.655500
2019-11-02 12:24:12,531 valid 200 1.167861e-02 -2.281672
2019-11-02 12:24:20,442 valid 250 1.171217e-02 -2.155294
2019-11-02 12:24:28,355 valid 300 1.171594e-02 -2.170295
2019-11-02 12:24:36,258 valid 350 1.182620e-02 -2.118902
2019-11-02 12:24:44,159 valid 400 1.187873e-02 -1.991359
2019-11-02 12:24:52,095 valid 450 1.181630e-02 -1.936224
2019-11-02 12:25:00,011 valid 500 1.182708e-02 -1.851529
2019-11-02 12:25:07,924 valid 550 1.183218e-02 -1.788027
2019-11-02 12:25:15,840 valid 600 1.180692e-02 -1.760216
2019-11-02 12:25:19,171 validation loss; R2: 1.181388e-02 -1.755483
2019-11-02 12:25:19,288 epoch 1 lr 9.991120e-04
2019-11-02 12:25:19,288 genotype = Genotype(normal=[('dil_conv_5x5', 1), ('max_pool_3x3', 0), ('dil_conv_5x5', 0), ('skip_connect', 2), ('max_pool_3x3', 0), ('dil_conv_3x3', 3), ('max_pool_3x3', 0), ('dil_conv_5x5', 1)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 0), ('max_pool_3x3', 1), ('max_pool_3x3', 0), ('max_pool_3x3', 2), ('max_pool_3x3', 3), ('skip_connect', 1), ('max_pool_3x3', 3), ('max_pool_3x3', 2)], reduce_concat=range(2, 6))
2019-11-02 12:25:19,290 
alphas_normal = Variable containing:
 0.1395  0.3027  0.0490  0.1160  0.0363  0.0494  0.0926  0.2143
 0.1726  0.0828  0.0399  0.0653  0.0424  0.0926  0.1102  0.3941
 0.1341  0.2062  0.0642  0.1391  0.0377  0.0750  0.0984  0.2452
 0.4607  0.0850  0.0490  0.0915  0.0812  0.0251  0.0887  0.1187
 0.3689  0.0648  0.0606  0.1345  0.0667  0.1235  0.0724  0.1086
 0.1220  0.4874  0.0357  0.0580  0.0618  0.0277  0.1140  0.0934
 0.2204  0.1561  0.0542  0.1014  0.1577  0.0640  0.0913  0.1549
 0.3226  0.0722  0.0445  0.0714  0.1712  0.0357  0.1023  0.1800
 0.5209  0.0636  0.0326  0.0465  0.0511  0.0475  0.1940  0.0437
 0.1443  0.3244  0.0679  0.1315  0.0533  0.0440  0.1205  0.1141
 0.5394  0.0477  0.0344  0.0458  0.0584  0.0634  0.0861  0.1249
 0.4920  0.0737  0.0497  0.1172  0.0490  0.0425  0.0669  0.1090
 0.5889  0.0627  0.0356  0.0510  0.0775  0.0391  0.0704  0.0749
 0.5280  0.0715  0.0349  0.0573  0.0524  0.0823  0.0806  0.0931
[torch.cuda.FloatTensor of size 14x8 (GPU 1)]

2019-11-02 12:25:19,291 
alphas_reduce = Variable containing:
 0.0354  0.6706  0.0306  0.0907  0.0325  0.0754  0.0233  0.0415
 0.5304  0.2356  0.0383  0.0409  0.0317  0.0412  0.0440  0.0379
 0.0497  0.6921  0.0337  0.0883  0.0305  0.0415  0.0322  0.0320
 0.1935  0.1042  0.0471  0.1673  0.1820  0.0738  0.1639  0.0681
 0.0478  0.6106  0.0253  0.0391  0.0383  0.0423  0.0695  0.1271
 0.1838  0.2073  0.0684  0.1317  0.0727  0.0729  0.1931  0.0700
 0.1219  0.0840  0.0474  0.4685  0.0923  0.0677  0.0380  0.0802
 0.0774  0.3193  0.0312  0.0469  0.0327  0.0563  0.0732  0.3630
 0.0584  0.5502  0.0483  0.0564  0.0458  0.0606  0.0510  0.1293
 0.2031  0.1212  0.0731  0.1116  0.1146  0.0938  0.1551  0.1275
 0.3561  0.0930  0.0852  0.1009  0.1849  0.0634  0.0578  0.0587
 0.0992  0.4961  0.0362  0.0528  0.0512  0.0599  0.0503  0.1543
 0.0587  0.5548  0.0468  0.0495  0.0771  0.0535  0.0791  0.0804
 0.1251  0.4771  0.0636  0.0646  0.0557  0.0856  0.0362  0.0921
[torch.cuda.FloatTensor of size 14x8 (GPU 1)]

2019-11-02 12:25:23,887 train 000 1.504180e-02 -0.969091
2019-11-02 12:27:58,642 train 050 1.261879e-02 -2.197964
2019-11-02 12:30:33,641 train 100 1.254084e-02 -2.055876
2019-11-02 12:33:10,190 train 150 1.274277e-02 -1.870880
2019-11-02 12:35:46,721 train 200 1.256829e-02 -1.619159
2019-11-02 12:38:21,797 train 250 1.265873e-02 -1.544867
2019-11-02 12:40:57,257 train 300 1.262071e-02 -1.539311
2019-11-02 12:43:34,664 train 350 1.267956e-02 -1.856691
2019-11-02 12:46:11,850 train 400 1.264706e-02 -1.726763
2019-11-02 12:48:48,836 train 450 1.259369e-02 -1.679015
2019-11-02 12:51:25,530 train 500 1.256771e-02 -1.611525
2019-11-02 12:54:02,602 train 550 1.252702e-02 -1.552568
2019-11-02 12:56:39,784 train 600 1.254352e-02 -1.514744
2019-11-02 12:59:16,021 train 650 1.249851e-02 -1.539636
2019-11-02 13:01:52,409 train 700 1.247196e-02 -1.486235
2019-11-02 13:04:31,417 train 750 1.241303e-02 -1.443490
2019-11-02 13:07:08,669 train 800 1.239349e-02 -1.544494
2019-11-02 13:09:46,434 train 850 1.236419e-02 -1.524099
2019-11-02 13:12:24,323 train 900 1.234820e-02 -1.502553
2019-11-02 13:15:03,097 train 950 1.235276e-02 -1.557928
2019-11-02 13:17:40,194 train 1000 1.237582e-02 -1.644964
2019-11-02 13:20:31,746 train 1050 1.236454e-02 -1.631848
2019-11-02 13:23:22,631 train 1100 1.232669e-02 -1.985184
2019-11-02 13:26:11,388 train 1150 1.233092e-02 -1.962677
2019-11-02 13:28:48,861 train 1200 1.232661e-02 -7.682946
2019-11-02 13:31:26,373 train 1250 1.232119e-02 -7.417098
2019-11-02 13:34:12,976 train 1300 1.231587e-02 -7.214954
2019-11-02 13:37:04,230 train 1350 1.229601e-02 -6.985683
2019-11-02 13:39:53,810 train 1400 1.226591e-02 -6.990275
2019-11-02 13:42:31,230 train 1450 1.225124e-02 -6.787619
2019-11-02 13:45:09,224 train 1500 1.224292e-02 -6.607280
2019-11-02 13:47:55,254 train 1550 1.223392e-02 -6.425107
2019-11-02 13:50:46,591 train 1600 1.222412e-02 -6.258137
2019-11-02 13:53:38,228 train 1650 1.221525e-02 -6.136629
2019-11-02 13:56:29,997 train 1700 1.222943e-02 -7.484154
2019-11-02 13:59:21,383 train 1750 1.223338e-02 -7.312405
2019-11-02 14:02:03,937 train 1800 1.222978e-02 -7.144360
2019-11-02 14:04:41,559 train 1850 1.222884e-02 -6.990021
2019-11-02 14:07:18,864 train 1900 1.222625e-02 -7.330754
2019-11-02 14:09:54,408 train 1950 1.222820e-02 -7.177913
2019-11-02 14:12:29,748 train 2000 1.220674e-02 -7.025087
2019-11-02 14:15:05,114 train 2050 1.219393e-02 -6.877931
2019-11-02 14:17:40,689 train 2100 1.217271e-02 -6.751325
2019-11-02 14:20:16,324 train 2150 1.217445e-02 -6.621727
2019-11-02 14:22:54,380 train 2200 1.216094e-02 -6.541115
2019-11-02 14:25:33,899 train 2250 1.213840e-02 -6.437079
2019-11-02 14:28:25,527 train 2300 1.211927e-02 -6.320282
2019-11-02 14:31:16,796 train 2350 1.210314e-02 -6.309688
2019-11-02 14:34:08,225 train 2400 1.209257e-02 -6.313678
2019-11-02 14:36:59,290 train 2450 1.208755e-02 -6.206196
2019-11-02 14:37:43,555 training loss; R2: 1.208619e-02 -6.186582
2019-11-02 14:37:44,125 valid 000 1.248584e-02 -0.790815
2019-11-02 14:37:53,983 valid 050 1.130395e-02 -1.159464
2019-11-02 14:38:03,809 valid 100 1.145328e-02 -1.414306
2019-11-02 14:38:13,656 valid 150 1.168213e-02 -1.379281
2019-11-02 14:38:23,497 valid 200 1.160959e-02 -21.674187
2019-11-02 14:38:33,337 valid 250 1.171071e-02 -18.657972
2019-11-02 14:38:43,143 valid 300 1.163265e-02 -15.770621
2019-11-02 14:38:52,951 valid 350 1.166867e-02 -13.722613
2019-11-02 14:39:02,772 valid 400 1.161474e-02 -12.165332
2019-11-02 14:39:12,606 valid 450 1.162517e-02 -10.943695
2019-11-02 14:39:22,435 valid 500 1.166076e-02 -9.973615
2019-11-02 14:39:32,249 valid 550 1.162537e-02 -9.166529
2019-11-02 14:39:42,059 valid 600 1.157240e-02 -8.603897
2019-11-02 14:39:44,979 validation loss; R2: 1.158207e-02 -8.425454
2019-11-02 14:39:45,159 epoch 2 lr 9.964516e-04
2019-11-02 14:39:45,160 genotype = Genotype(normal=[('dil_conv_5x5', 1), ('max_pool_3x3', 0), ('dil_conv_5x5', 0), ('skip_connect', 1), ('max_pool_3x3', 0), ('sep_conv_3x3', 1), ('max_pool_3x3', 0), ('skip_connect', 2)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 0), ('max_pool_3x3', 1), ('max_pool_3x3', 2), ('max_pool_3x3', 0), ('dil_conv_5x5', 2), ('max_pool_3x3', 3), ('max_pool_3x3', 3), ('max_pool_3x3', 4)], reduce_concat=range(2, 6))
2019-11-02 14:39:45,162 
alphas_normal = Variable containing:
 0.1118  0.3275  0.0288  0.0713  0.1132  0.0467  0.1367  0.1640
 0.1700  0.1114  0.0444  0.0717  0.0945  0.0761  0.0889  0.3429
 0.1359  0.2369  0.0422  0.1092  0.0580  0.0644  0.0982  0.2552
 0.4499  0.0952  0.0793  0.1496  0.0882  0.0163  0.0668  0.0546
 0.5533  0.0388  0.0466  0.1033  0.0427  0.0453  0.0583  0.1117
 0.0924  0.5565  0.0219  0.0301  0.0377  0.0188  0.1007  0.1419
 0.2917  0.1595  0.0634  0.1048  0.1778  0.0609  0.0496  0.0923
 0.4944  0.0494  0.0386  0.0571  0.0951  0.0348  0.1138  0.1167
 0.6863  0.0292  0.0215  0.0259  0.0345  0.0298  0.1424  0.0305
 0.0929  0.4266  0.0467  0.1028  0.0423  0.0475  0.1102  0.1311
 0.5765  0.0502  0.0501  0.0537  0.0496  0.0415  0.1041  0.0741
 0.4273  0.1031  0.0642  0.1419  0.0522  0.0402  0.0540  0.1171
 0.8092  0.0245  0.0167  0.0218  0.0439  0.0170  0.0272  0.0397
 0.4706  0.0717  0.0322  0.0518  0.0371  0.0971  0.1324  0.1072
[torch.cuda.FloatTensor of size 14x8 (GPU 1)]

2019-11-02 14:39:45,164 
alphas_reduce = Variable containing:
 0.0371  0.6513  0.0225  0.0966  0.0291  0.0838  0.0220  0.0576
 0.6487  0.1555  0.0336  0.0254  0.0180  0.0573  0.0295  0.0322
 0.0756  0.6195  0.0325  0.0924  0.0329  0.0374  0.0645  0.0451
 0.2407  0.0967  0.0790  0.1224  0.1646  0.0526  0.1631  0.0812
 0.0396  0.6543  0.0257  0.0323  0.0225  0.0342  0.0451  0.1463
 0.2900  0.1205  0.0647  0.0610  0.0891  0.0537  0.2292  0.0917
 0.3587  0.0599  0.0625  0.2574  0.0664  0.0625  0.0404  0.0922
 0.0508  0.1313  0.0263  0.0325  0.0260  0.0389  0.0428  0.6513
 0.0606  0.6088  0.0467  0.0425  0.0299  0.0515  0.0517  0.1084
 0.1654  0.0610  0.0519  0.0714  0.1631  0.0824  0.2342  0.1706
 0.4917  0.0664  0.0835  0.0727  0.1123  0.0467  0.0803  0.0465
 0.1204  0.4514  0.0379  0.0488  0.0564  0.0744  0.0549  0.1558
 0.0620  0.5148  0.0404  0.0369  0.0582  0.0832  0.1293  0.0752
 0.0686  0.4650  0.0540  0.0428  0.0653  0.1156  0.0433  0.1454
[torch.cuda.FloatTensor of size 14x8 (GPU 1)]

2019-11-02 14:39:48,857 train 000 1.659630e-02 -1.996399
2019-11-02 14:42:39,921 train 050 1.151315e-02 -1.036057
2019-11-02 14:45:30,781 train 100 1.175176e-02 -1.023329
2019-11-02 14:48:11,706 train 150 1.169082e-02 -1.048201
2019-11-02 14:50:49,545 train 200 1.160507e-02 -1.194696
2019-11-02 14:53:33,519 train 250 1.158406e-02 -1.212486
2019-11-02 14:56:17,041 train 300 1.156144e-02 -1.232087
2019-11-02 14:59:00,821 train 350 1.152274e-02 -1.414357
2019-11-02 15:01:44,346 train 400 1.156229e-02 -1.402830
2019-11-02 15:04:25,885 train 450 1.152893e-02 -1.361082
2019-11-02 15:07:03,378 train 500 1.151304e-02 -1.649277
2019-11-02 15:09:40,137 train 550 1.153787e-02 -1.642517
2019-11-02 15:12:17,534 train 600 1.150546e-02 -1.572981
2019-11-02 15:14:55,058 train 650 1.145124e-02 -1.525291
2019-11-02 15:17:32,330 train 700 1.144123e-02 -1.504820
2019-11-02 15:20:09,761 train 750 1.145637e-02 -1.489755
2019-11-02 15:22:47,292 train 800 1.144132e-02 -1.462937
2019-11-02 15:25:24,412 train 850 1.145123e-02 -1.437143
2019-11-02 15:28:06,208 train 900 1.143670e-02 -1.423785
2019-11-02 15:30:58,116 train 950 1.146025e-02 -1.442183
2019-11-02 15:33:49,835 train 1000 1.144239e-02 -1.433049
2019-11-02 15:36:42,203 train 1050 1.142289e-02 -2.012353
2019-11-02 15:39:34,059 train 1100 1.144221e-02 -2.002738
2019-11-02 15:42:25,510 train 1150 1.144325e-02 -1.981098
2019-11-02 15:45:16,823 train 1200 1.143695e-02 -1.938205
2019-11-02 15:48:00,954 train 1250 1.142443e-02 -1.923407
2019-11-02 15:50:38,236 train 1300 1.143470e-02 -1.896666
2019-11-02 15:53:20,747 train 1350 1.142770e-02 -1.858148
2019-11-02 15:56:12,635 train 1400 1.142445e-02 -1.977304
2019-11-02 15:59:04,474 train 1450 1.141978e-02 -1.946209
2019-11-02 16:01:48,870 train 1500 1.141762e-02 -1.910098
2019-11-02 16:04:28,607 train 1550 1.142565e-02 -1.886894
2019-11-02 16:07:06,540 train 1600 1.142576e-02 -1.869304
2019-11-02 16:09:44,101 train 1650 1.141700e-02 -1.842384
2019-11-02 16:12:22,206 train 1700 1.141106e-02 -1.957660
2019-11-02 16:14:59,342 train 1750 1.140995e-02 -1.937119
2019-11-02 16:17:35,081 train 1800 1.140699e-02 -1.909725
2019-11-02 16:20:11,939 train 1850 1.140074e-02 -1.885608
2019-11-02 16:22:55,729 train 1900 1.139411e-02 -1.859984
2019-11-02 16:25:37,873 train 1950 1.139413e-02 -1.849435
2019-11-02 16:28:28,227 train 2000 1.138975e-02 -1.831207
2019-11-02 16:31:04,871 train 2050 1.139333e-02 -1.809351
2019-11-02 16:33:42,727 train 2100 1.140157e-02 -1.795317
2019-11-02 16:36:20,036 train 2150 1.141353e-02 -1.776148
2019-11-02 16:38:58,018 train 2200 1.140934e-02 -1.764490
2019-11-02 16:41:35,817 train 2250 1.141150e-02 -3.054585
2019-11-02 16:44:13,204 train 2300 1.140102e-02 -3.011720
2019-11-02 16:46:50,552 train 2350 1.138586e-02 -2.962895
2019-11-02 16:49:32,477 train 2400 1.137240e-02 -2.932909
2019-11-02 16:52:18,252 train 2450 1.136774e-02 -2.904929
2019-11-02 16:52:59,187 training loss; R2: 1.137090e-02 -2.896137
2019-11-02 16:52:59,676 valid 000 1.104914e-02 -1.917254
2019-11-02 16:53:07,811 valid 050 1.092263e-02 -4.428843
2019-11-02 16:53:15,905 valid 100 1.086899e-02 -2.812407
2019-11-02 16:53:24,023 valid 150 1.099121e-02 -2.310272
2019-11-02 16:53:32,104 valid 200 1.088225e-02 -2.007698
2019-11-02 16:53:40,221 valid 250 1.092890e-02 -1.783797
2019-11-02 16:53:48,346 valid 300 1.095361e-02 -1.686609
2019-11-02 16:53:56,451 valid 350 1.101958e-02 -1.575016
2019-11-02 16:54:04,558 valid 400 1.109551e-02 -1.489554
2019-11-02 16:54:12,678 valid 450 1.107883e-02 -11.488406
2019-11-02 16:54:20,782 valid 500 1.102925e-02 -10.468450
2019-11-02 16:54:28,871 valid 550 1.097411e-02 -9.681624
2019-11-02 16:54:36,937 valid 600 1.100285e-02 -8.987688
2019-11-02 16:54:39,331 validation loss; R2: 1.099846e-02 -8.804477
2019-11-02 16:54:39,507 epoch 3 lr 9.920293e-04
2019-11-02 16:54:39,507 genotype = Genotype(normal=[('dil_conv_5x5', 1), ('max_pool_3x3', 0), ('max_pool_3x3', 0), ('skip_connect', 1), ('max_pool_3x3', 0), ('max_pool_3x3', 1), ('max_pool_3x3', 0), ('skip_connect', 2)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 0), ('max_pool_3x3', 1), ('max_pool_3x3', 2), ('max_pool_3x3', 0), ('dil_conv_5x5', 2), ('max_pool_3x3', 3), ('max_pool_3x3', 4), ('max_pool_3x3', 3)], reduce_concat=range(2, 6))
2019-11-02 16:54:39,509 
alphas_normal = Variable containing:
 0.0764  0.3526  0.0343  0.0943  0.1160  0.0303  0.1212  0.1749
 0.2259  0.1219  0.0400  0.0654  0.0529  0.0548  0.0711  0.3679
 0.1215  0.3053  0.0409  0.1335  0.0461  0.0293  0.1144  0.2091
 0.4084  0.0769  0.1003  0.2143  0.0547  0.0125  0.0519  0.0812
 0.6715  0.0270  0.0409  0.0726  0.0287  0.0286  0.0437  0.0871
 0.0559  0.6501  0.0221  0.0286  0.0284  0.0198  0.0735  0.1216
 0.2546  0.1779  0.0596  0.0912  0.1554  0.0804  0.0369  0.1439
 0.5070  0.0378  0.0416  0.0549  0.0607  0.0167  0.1741  0.1072
 0.7213  0.0206  0.0213  0.0270  0.0122  0.0316  0.1305  0.0353
 0.0676  0.5071  0.0369  0.0880  0.0398  0.0437  0.1092  0.1079
 0.6592  0.0502  0.0484  0.0520  0.0328  0.0394  0.0678  0.0502
 0.5017  0.0953  0.0626  0.1195  0.0394  0.0261  0.0362  0.1192
 0.8849  0.0126  0.0114  0.0144  0.0276  0.0144  0.0153  0.0193
 0.6179  0.0675  0.0289  0.0411  0.0221  0.0438  0.0898  0.0887
[torch.cuda.FloatTensor of size 14x8 (GPU 1)]

2019-11-02 16:54:39,510 
alphas_reduce = Variable containing:
 0.0355  0.6973  0.0189  0.0755  0.0233  0.0810  0.0163  0.0521
 0.7076  0.1410  0.0177  0.0175  0.0173  0.0572  0.0196  0.0222
 0.0788  0.4688  0.0343  0.1221  0.0430  0.0810  0.1150  0.0569
 0.3280  0.0922  0.0586  0.1393  0.0892  0.0326  0.1736  0.0865
 0.0432  0.5564  0.0220  0.0340  0.0223  0.0401  0.0369  0.2452
 0.3231  0.0931  0.0838  0.0455  0.0560  0.0552  0.2504  0.0928
 0.3905  0.0479  0.0454  0.1821  0.0513  0.0975  0.0353  0.1500
 0.0371  0.0581  0.0228  0.0287  0.0204  0.0494  0.0398  0.7436
 0.0796  0.5125  0.0626  0.0703  0.0227  0.0715  0.0641  0.1167
 0.1056  0.0453  0.0428  0.0520  0.1602  0.1381  0.3003  0.1558
 0.6390  0.0394  0.0443  0.0609  0.0825  0.0434  0.0548  0.0357
 0.1223  0.4195  0.0362  0.0446  0.0659  0.0944  0.0643  0.1528
 0.0749  0.4252  0.0459  0.0457  0.0571  0.0678  0.2251  0.0584
 0.0607  0.4720  0.0609  0.0502  0.0604  0.1175  0.0377  0.1405
[torch.cuda.FloatTensor of size 14x8 (GPU 1)]

2019-11-02 16:54:42,877 train 000 9.583564e-03 -0.807857
2019-11-02 16:57:20,284 train 050 1.118990e-02 -0.912483
2019-11-02 16:59:58,125 train 100 1.115806e-02 -0.966662
2019-11-02 17:02:36,037 train 150 1.117416e-02 -1.064738
2019-11-02 17:05:13,250 train 200 1.102185e-02 -1.081379
2019-11-02 17:07:50,769 train 250 1.098911e-02 -1.027233
2019-11-02 17:10:31,255 train 300 1.092858e-02 -1.045927
2019-11-02 17:13:11,508 train 350 1.090627e-02 -1.086334
2019-11-02 17:15:48,612 train 400 1.092535e-02 -1.091009
2019-11-02 17:18:25,453 train 450 1.089440e-02 -1.100854
2019-11-02 17:21:02,961 train 500 1.092792e-02 -1.262121
2019-11-02 17:23:40,897 train 550 1.087519e-02 -1.310987
2019-11-02 17:26:18,396 train 600 1.087886e-02 -1.280652
2019-11-02 17:28:55,512 train 650 1.087525e-02 -1.308594
2019-11-02 17:31:41,607 train 700 1.089932e-02 -1.321182
2019-11-02 17:34:19,996 train 750 1.087931e-02 -1.322273
2019-11-02 17:37:07,981 train 800 1.089415e-02 -3.441609
2019-11-02 17:39:48,531 train 850 1.091350e-02 -3.313003
2019-11-02 17:42:36,875 train 900 1.091041e-02 -3.321821
2019-11-02 17:45:16,684 train 950 1.091036e-02 -3.212084
2019-11-02 17:47:56,569 train 1000 1.093887e-02 -3.130874
2019-11-02 17:50:38,868 train 1050 1.092214e-02 -3.045542
2019-11-02 17:53:16,494 train 1100 1.095389e-02 -2.952136
2019-11-02 17:55:53,995 train 1150 1.097398e-02 -2.900138
2019-11-02 17:58:31,372 train 1200 1.094769e-02 -2.828612
2019-11-02 18:01:09,857 train 1250 1.094112e-02 -2.765390
2019-11-02 18:03:48,156 train 1300 1.093103e-02 -2.703362
2019-11-02 18:06:26,551 train 1350 1.091968e-02 -2.653141
2019-11-02 18:09:03,784 train 1400 1.091857e-02 -2.598584
2019-11-02 18:11:42,112 train 1450 1.091725e-02 -2.543541
2019-11-02 18:14:20,479 train 1500 1.091674e-02 -2.501611
2019-11-02 18:16:58,573 train 1550 1.093181e-02 -2.472713
2019-11-02 18:19:42,359 train 1600 1.093914e-02 -2.428283
2019-11-02 18:22:28,059 train 1650 1.093678e-02 -2.410424
2019-11-02 18:25:20,309 train 1700 1.092012e-02 -3.837205
2019-11-02 18:28:00,132 train 1750 1.092996e-02 -3.752499
2019-11-02 18:30:45,022 train 1800 1.092555e-02 -3.689758
2019-11-02 18:33:31,992 train 1850 1.091059e-02 -3.614822
2019-11-02 18:36:17,626 train 1900 1.091357e-02 -4.114239
2019-11-02 18:39:05,013 train 1950 1.089630e-02 -4.041587
2019-11-02 18:42:00,102 train 2000 1.089320e-02 -3.974317
2019-11-02 18:44:46,577 train 2050 1.089896e-02 -3.907556
2019-11-02 18:47:28,605 train 2100 1.089226e-02 -3.843815
2019-11-02 18:50:12,184 train 2150 1.088632e-02 -3.790179
2019-11-02 18:52:55,848 train 2200 1.086891e-02 -3.725096
2019-11-02 18:55:44,884 train 2250 1.086641e-02 -4.156714
2019-11-02 18:58:45,123 train 2300 1.087040e-02 -4.084540
2019-11-02 19:01:30,785 train 2350 1.086940e-02 -4.023070
2019-11-02 19:04:14,518 train 2400 1.087425e-02 -3.996211
2019-11-02 19:07:05,657 train 2450 1.086877e-02 -4.026761
2019-11-02 19:07:52,792 training loss; R2: 1.087384e-02 -4.010150
2019-11-02 19:07:53,335 valid 000 1.201727e-02 -7.852598
2019-11-02 19:08:01,935 valid 050 1.149446e-02 -1.431651
2019-11-02 19:08:10,566 valid 100 1.106962e-02 -1.626406
2019-11-02 19:08:19,085 valid 150 1.119419e-02 -1.601299
2019-11-02 19:08:27,518 valid 200 1.117817e-02 -2.050635
2019-11-02 19:08:35,874 valid 250 1.116802e-02 -1.879416
2019-11-02 19:08:44,228 valid 300 1.114415e-02 -11.594513
2019-11-02 19:08:52,563 valid 350 1.109563e-02 -10.228660
2019-11-02 19:09:00,909 valid 400 1.110823e-02 -9.073974
2019-11-02 19:09:09,313 valid 450 1.103681e-02 -8.248022
2019-11-02 19:09:17,726 valid 500 1.104994e-02 -7.932812
2019-11-02 19:09:26,077 valid 550 1.107858e-02 -7.321043
2019-11-02 19:09:34,458 valid 600 1.110144e-02 -6.841273
2019-11-02 19:09:36,980 validation loss; R2: 1.108856e-02 -6.778519
2019-11-02 19:09:37,105 epoch 4 lr 9.858624e-04
2019-11-02 19:09:37,106 genotype = Genotype(normal=[('max_pool_3x3', 0), ('dil_conv_5x5', 1), ('max_pool_3x3', 0), ('skip_connect', 1), ('max_pool_3x3', 0), ('max_pool_3x3', 1), ('max_pool_3x3', 0), ('dil_conv_3x3', 4)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 0), ('max_pool_3x3', 1), ('max_pool_3x3', 2), ('max_pool_3x3', 0), ('dil_conv_5x5', 2), ('max_pool_3x3', 3), ('max_pool_3x3', 4), ('max_pool_3x3', 3)], reduce_concat=range(2, 6))
2019-11-02 19:09:37,108 
alphas_normal = Variable containing:
 0.0487  0.3890  0.0327  0.0774  0.1286  0.0294  0.1294  0.1647
 0.2702  0.1382  0.0332  0.0507  0.0381  0.0497  0.0683  0.3517
 0.1063  0.3167  0.0422  0.1380  0.0590  0.0200  0.0872  0.2307
 0.4812  0.0658  0.0876  0.2144  0.0336  0.0237  0.0364  0.0573
 0.6912  0.0319  0.0499  0.0661  0.0145  0.0189  0.0368  0.0907
 0.0671  0.6768  0.0230  0.0252  0.0274  0.0173  0.0631  0.1001
 0.2889  0.2176  0.0627  0.0938  0.0670  0.1528  0.0261  0.0911
 0.5164  0.0283  0.0335  0.0366  0.0419  0.0184  0.2108  0.1141
 0.7513  0.0217  0.0229  0.0232  0.0123  0.0285  0.0922  0.0478
 0.0682  0.5238  0.0336  0.0656  0.0501  0.0295  0.1039  0.1253
 0.6378  0.0472  0.0429  0.0431  0.0357  0.0388  0.0731  0.0814
 0.6001  0.0955  0.0518  0.0769  0.0211  0.0353  0.0338  0.0854
 0.8462  0.0234  0.0168  0.0195  0.0348  0.0249  0.0137  0.0208
 0.4345  0.0902  0.0320  0.0345  0.0185  0.0667  0.1898  0.1338
[torch.cuda.FloatTensor of size 14x8 (GPU 1)]

2019-11-02 19:09:37,109 
alphas_reduce = Variable containing:
 0.0300  0.7517  0.0200  0.0358  0.0298  0.0628  0.0229  0.0470
 0.6887  0.1449  0.0228  0.0192  0.0144  0.0723  0.0141  0.0235
 0.0745  0.5143  0.0484  0.0870  0.0412  0.0689  0.0984  0.0673
 0.5444  0.0774  0.0540  0.0899  0.0474  0.0191  0.1044  0.0633
 0.0439  0.5196  0.0281  0.0394  0.0271  0.0373  0.0208  0.2838
 0.3471  0.0901  0.0898  0.0436  0.0856  0.0658  0.2079  0.0701
 0.6068  0.0394  0.0376  0.1004  0.0360  0.0632  0.0227  0.0939
 0.0288  0.0352  0.0206  0.0250  0.0176  0.0247  0.0241  0.8240
 0.0797  0.5564  0.0652  0.0663  0.0184  0.0551  0.0663  0.0926
 0.1214  0.0561  0.0683  0.0514  0.1730  0.1241  0.2665  0.1394
 0.6825  0.0327  0.0489  0.0568  0.0771  0.0379  0.0455  0.0185
 0.1667  0.3372  0.0408  0.0478  0.0801  0.0653  0.0798  0.1823
 0.0819  0.3798  0.0517  0.0439  0.0507  0.0480  0.2660  0.0781
 0.0450  0.5279  0.0441  0.0337  0.0493  0.0857  0.0337  0.1806
[torch.cuda.FloatTensor of size 14x8 (GPU 1)]

2019-11-02 19:09:40,717 train 000 7.016147e-03 -0.347106
2019-11-02 19:12:26,936 train 050 1.000654e-02 -1.725451
2019-11-02 19:15:12,228 train 100 1.017946e-02 -1.472875
2019-11-02 19:17:58,648 train 150 1.030518e-02 -1.327905
2019-11-02 19:20:43,807 train 200 1.035452e-02 -1.472742
2019-11-02 19:23:28,894 train 250 1.041455e-02 -1.514942
2019-11-02 19:26:14,425 train 300 1.046154e-02 -1.462080
2019-11-02 19:28:59,585 train 350 1.041357e-02 -1.426358
2019-11-02 19:31:45,714 train 400 1.044006e-02 -1.370927
2019-11-02 19:34:32,423 train 450 1.054292e-02 -1.370338
2019-11-02 19:37:17,522 train 500 1.054528e-02 -1.341856
2019-11-02 19:40:03,836 train 550 1.053776e-02 -1.363355
2019-11-02 19:42:52,638 train 600 1.049218e-02 -1.344668
2019-11-02 19:45:38,392 train 650 1.055233e-02 -1.341897
2019-11-02 19:48:24,473 train 700 1.060436e-02 -1.336655
2019-11-02 19:51:09,980 train 750 1.061812e-02 -1.317585
2019-11-02 19:53:55,821 train 800 1.060315e-02 -1.300804
2019-11-02 19:56:40,803 train 850 1.059596e-02 -1.296865
2019-11-02 19:59:27,474 train 900 1.059065e-02 -1.263951
2019-11-02 20:02:14,193 train 950 1.060514e-02 -1.262626
2019-11-02 20:05:01,658 train 1000 1.061783e-02 -1.814246
2019-11-02 20:07:44,581 train 1050 1.062633e-02 -1.848762
2019-11-02 20:10:36,527 train 1100 1.062489e-02 -1.873236
2019-11-02 20:13:31,173 train 1150 1.061762e-02 -1.832934
2019-11-02 20:16:16,210 train 1200 1.062452e-02 -1.893736
2019-11-02 20:19:04,587 train 1250 1.063469e-02 -2.384332
2019-11-02 20:21:48,728 train 1300 1.063472e-02 -2.325625
2019-11-02 20:24:40,039 train 1350 1.062518e-02 -2.280326
2019-11-02 20:27:27,599 train 1400 1.060739e-02 -2.234200
2019-11-02 20:30:11,954 train 1450 1.061447e-02 -2.192184
2019-11-02 20:33:01,054 train 1500 1.061256e-02 -2.216344
2019-11-02 20:35:43,534 train 1550 1.061528e-02 -2.174888
2019-11-02 20:38:25,903 train 1600 1.061957e-02 -2.200076
2019-11-02 20:41:15,901 train 1650 1.061986e-02 -2.170966
2019-11-02 20:43:59,566 train 1700 1.061555e-02 -2.139796
2019-11-02 20:46:46,324 train 1750 1.062082e-02 -2.102953
2019-11-02 20:49:32,974 train 1800 1.063744e-02 -2.069561
2019-11-02 20:52:20,366 train 1850 1.063837e-02 -2.037348
2019-11-02 20:55:05,003 train 1900 1.062521e-02 -2.018982
2019-11-02 20:57:49,937 train 1950 1.064056e-02 -2.006010
2019-11-02 21:00:35,426 train 2000 1.063438e-02 -1.997779
2019-11-02 21:03:19,916 train 2050 1.064099e-02 -1.974027
2019-11-02 21:06:03,612 train 2100 1.063159e-02 -1.978096
2019-11-02 21:08:48,493 train 2150 1.062222e-02 -1.967372
2019-11-02 21:11:33,052 train 2200 1.061261e-02 -1.953183
2019-11-02 21:14:18,037 train 2250 1.062003e-02 -1.962789
2019-11-02 21:17:02,445 train 2300 1.061411e-02 -1.969360
2019-11-02 21:19:46,351 train 2350 1.061715e-02 -1.951031
2019-11-02 21:22:35,161 train 2400 1.060838e-02 -2.022913
2019-11-02 21:25:23,734 train 2450 1.060306e-02 -2.008530
2019-11-02 21:26:06,316 training loss; R2: 1.060624e-02 -2.003325
2019-11-02 21:26:06,774 valid 000 1.277826e-02 -0.163499
2019-11-02 21:26:15,389 valid 050 1.145771e-02 -1.388803
2019-11-02 21:26:23,996 valid 100 1.139668e-02 -1.528468
2019-11-02 21:26:32,595 valid 150 1.140761e-02 -1.636057
2019-11-02 21:26:41,167 valid 200 1.146427e-02 -1.686049
2019-11-02 21:26:49,724 valid 250 1.145928e-02 -1.665559
2019-11-02 21:26:58,350 valid 300 1.154003e-02 -1.809148
2019-11-02 21:27:07,005 valid 350 1.142556e-02 -1.858623
2019-11-02 21:27:15,641 valid 400 1.145200e-02 -1.854198
2019-11-02 21:27:24,258 valid 450 1.143829e-02 -1.825234
2019-11-02 21:27:32,852 valid 500 1.142927e-02 -1.748988
2019-11-02 21:27:41,498 valid 550 1.141413e-02 -1.753761
2019-11-02 21:27:50,100 valid 600 1.141088e-02 -1.755072
2019-11-02 21:27:52,655 validation loss; R2: 1.141111e-02 -1.769671
2019-11-02 21:27:52,783 epoch 5 lr 9.779754e-04
2019-11-02 21:27:52,784 genotype = Genotype(normal=[('max_pool_3x3', 0), ('dil_conv_5x5', 1), ('max_pool_3x3', 0), ('skip_connect', 1), ('max_pool_3x3', 0), ('dil_conv_3x3', 2), ('max_pool_3x3', 0), ('dil_conv_3x3', 4)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 0), ('max_pool_3x3', 1), ('max_pool_3x3', 0), ('max_pool_3x3', 2), ('dil_conv_5x5', 2), ('max_pool_3x3', 3), ('max_pool_3x3', 4), ('max_pool_3x3', 3)], reduce_concat=range(2, 6))
2019-11-02 21:27:52,786 
alphas_normal = Variable containing:
 0.0528  0.3510  0.0417  0.0987  0.1919  0.0287  0.1126  0.1226
 0.2571  0.2022  0.0388  0.0603  0.0354  0.0524  0.0355  0.3182
 0.1548  0.2618  0.0411  0.1405  0.0751  0.0140  0.0891  0.2237
 0.4353  0.0492  0.0793  0.2602  0.0283  0.0438  0.0332  0.0708
 0.7030  0.0234  0.0390  0.0486  0.0245  0.0267  0.0565  0.0782
 0.0734  0.5871  0.0304  0.0440  0.0533  0.0247  0.0742  0.1130
 0.3022  0.2014  0.0663  0.0870  0.0555  0.1551  0.0236  0.1089
 0.5391  0.0171  0.0210  0.0257  0.0563  0.0188  0.2112  0.1107
 0.7502  0.0141  0.0171  0.0178  0.0109  0.0420  0.0787  0.0692
 0.0618  0.5122  0.0518  0.0776  0.0506  0.0192  0.0869  0.1398
 0.5173  0.0507  0.0805  0.0888  0.0223  0.0706  0.1034  0.0665
 0.5309  0.0978  0.0598  0.0767  0.0205  0.0421  0.0389  0.1333
 0.7855  0.0291  0.0308  0.0324  0.0348  0.0465  0.0167  0.0242
 0.4202  0.0913  0.0352  0.0378  0.0169  0.0842  0.2087  0.1058
[torch.cuda.FloatTensor of size 14x8 (GPU 1)]

2019-11-02 21:27:52,787 
alphas_reduce = Variable containing:
 0.0312  0.7855  0.0168  0.0258  0.0290  0.0448  0.0186  0.0482
 0.6989  0.1237  0.0225  0.0147  0.0137  0.0905  0.0142  0.0218
 0.0679  0.4073  0.0548  0.1895  0.0437  0.0867  0.0933  0.0568
 0.5629  0.0723  0.0680  0.0712  0.0511  0.0191  0.0899  0.0654
 0.0512  0.3905  0.0343  0.0482  0.0260  0.0547  0.0182  0.3769
 0.2372  0.0847  0.1424  0.0432  0.1569  0.0777  0.2024  0.0555
 0.6698  0.0333  0.0341  0.0668  0.0352  0.0508  0.0274  0.0826
 0.0178  0.0183  0.0151  0.0169  0.0116  0.0188  0.0150  0.8866
 0.1537  0.4652  0.0782  0.0655  0.0255  0.0576  0.0768  0.0775
 0.1058  0.0613  0.0826  0.0735  0.1441  0.1480  0.2445  0.1402
 0.6832  0.0282  0.0385  0.0453  0.0811  0.0390  0.0528  0.0318
 0.1871  0.2775  0.0437  0.0566  0.0738  0.0710  0.0800  0.2103
 0.1052  0.3260  0.0607  0.0505  0.0884  0.0435  0.2350  0.0908
 0.0550  0.3719  0.0558  0.0445  0.0505  0.0771  0.0450  0.3002
[torch.cuda.FloatTensor of size 14x8 (GPU 1)]

2019-11-02 21:27:56,460 train 000 1.118160e-02 -1.318638
2019-11-02 21:30:42,957 train 050 1.040276e-02 -1.254848
2019-11-02 21:33:28,815 train 100 1.045328e-02 -1.186326
2019-11-02 21:36:13,110 train 150 1.031226e-02 -1.124854
2019-11-02 21:38:56,945 train 200 1.031532e-02 -1.171926
2019-11-02 21:41:40,622 train 250 1.029517e-02 -1.143585
2019-11-02 21:44:24,573 train 300 1.032079e-02 -1.212025
2019-11-02 21:47:08,503 train 350 1.035676e-02 -1.416362
2019-11-02 21:49:55,459 train 400 1.037278e-02 -1.434509
2019-11-02 21:52:43,056 train 450 1.040222e-02 -1.484910
2019-11-02 21:55:27,227 train 500 1.038356e-02 -1.702654
2019-11-02 21:58:11,751 train 550 1.034288e-02 -1.671651
2019-11-02 22:00:57,382 train 600 1.032388e-02 -1.617675
2019-11-02 22:03:42,485 train 650 1.032060e-02 -1.648404
2019-11-02 22:06:28,278 train 700 1.030911e-02 -1.615715
2019-11-02 22:09:11,511 train 750 1.029628e-02 -3.592434
2019-11-02 22:11:58,342 train 800 1.030293e-02 -3.523298
2019-11-02 22:14:42,801 train 850 1.032903e-02 -3.364632
2019-11-02 22:17:26,396 train 900 1.034485e-02 -3.238669
2019-11-02 22:20:11,637 train 950 1.033513e-02 -3.142515
2019-11-02 22:22:55,035 train 1000 1.032323e-02 -3.032063
2019-11-02 22:25:40,207 train 1050 1.033648e-02 -2.928812
2019-11-02 22:28:25,541 train 1100 1.036330e-02 -2.845154
2019-11-02 22:31:11,859 train 1150 1.034099e-02 -2.782345
2019-11-02 22:33:56,741 train 1200 1.032410e-02 -2.702047
2019-11-02 22:36:40,700 train 1250 1.029894e-02 -2.799164
2019-11-02 22:39:26,128 train 1300 1.032126e-02 -2.743924
2019-11-02 22:42:09,612 train 1350 1.031405e-02 -2.682751
2019-11-02 22:44:53,522 train 1400 1.030475e-02 -2.628805
2019-11-02 22:47:37,099 train 1450 1.030867e-02 -2.568637
2019-11-02 22:50:22,308 train 1500 1.030414e-02 -2.529646
2019-11-02 22:53:06,455 train 1550 1.029653e-02 -2.486283
2019-11-02 22:55:55,015 train 1600 1.029217e-02 -2.439395
2019-11-02 22:58:40,081 train 1650 1.031075e-02 -3.021707
2019-11-02 23:01:29,474 train 1700 1.031318e-02 -2.966001
2019-11-02 23:04:13,939 train 1750 1.030610e-02 -2.912229
2019-11-02 23:07:03,343 train 1800 1.029318e-02 -2.903458
2019-11-02 23:09:50,502 train 1850 1.027559e-02 -2.856895
2019-11-02 23:12:37,701 train 1900 1.026652e-02 -2.808131
2019-11-02 23:15:22,868 train 1950 1.026571e-02 -2.770559
2019-11-02 23:18:06,200 train 2000 1.027695e-02 -2.728403
2019-11-02 23:20:52,091 train 2050 1.027782e-02 -4.823189
2019-11-02 23:23:38,367 train 2100 1.028484e-02 -4.730304
2019-11-02 23:26:22,893 train 2150 1.027732e-02 -5.046897
2019-11-02 23:29:07,576 train 2200 1.026675e-02 -4.957289
2019-11-02 23:31:52,032 train 2250 1.026270e-02 -4.878788
2019-11-02 23:34:36,735 train 2300 1.026553e-02 -4.804580
2019-11-02 23:37:21,393 train 2350 1.025845e-02 -4.719729
2019-11-02 23:40:06,465 train 2400 1.025760e-02 -5.224524
2019-11-02 23:42:57,626 train 2450 1.026925e-02 -5.137235
2019-11-02 23:43:39,525 training loss; R2: 1.027258e-02 -5.125091
2019-11-02 23:43:40,012 valid 000 1.236130e-02 -0.873550
2019-11-02 23:43:48,574 valid 050 1.154841e-02 -4.106761
2019-11-02 23:43:57,161 valid 100 1.130860e-02 -3.901965
2019-11-02 23:44:05,699 valid 150 1.153602e-02 -3.506948
2019-11-02 23:44:14,259 valid 200 1.156407e-02 -3.630542
2019-11-02 23:44:22,797 valid 250 1.162609e-02 -3.571885
2019-11-02 23:44:31,335 valid 300 1.172696e-02 -3.855656
2019-11-02 23:44:39,902 valid 350 1.171863e-02 -3.780381
2019-11-02 23:44:48,474 valid 400 1.172824e-02 -38.129811
2019-11-02 23:44:57,040 valid 450 1.173178e-02 -34.786832
2019-11-02 23:45:05,597 valid 500 1.171761e-02 -96.416026
2019-11-02 23:45:14,183 valid 550 1.177159e-02 -87.993548
2019-11-02 23:45:22,736 valid 600 1.181105e-02 -80.941648
2019-11-02 23:45:25,286 validation loss; R2: 1.179742e-02 -79.054114
2019-11-02 23:45:25,415 epoch 6 lr 9.683994e-04
2019-11-02 23:45:25,416 genotype = Genotype(normal=[('max_pool_3x3', 0), ('dil_conv_5x5', 1), ('skip_connect', 1), ('max_pool_3x3', 0), ('max_pool_3x3', 0), ('dil_conv_3x3', 2), ('max_pool_3x3', 0), ('dil_conv_3x3', 4)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 0), ('max_pool_3x3', 1), ('max_pool_3x3', 0), ('dil_conv_5x5', 2), ('dil_conv_5x5', 2), ('max_pool_3x3', 3), ('dil_conv_5x5', 4), ('max_pool_3x3', 3)], reduce_concat=range(2, 6))
2019-11-02 23:45:25,417 
alphas_normal = Variable containing:
 0.0436  0.3657  0.0418  0.0990  0.2204  0.0198  0.1017  0.1080
 0.2970  0.2130  0.0366  0.0540  0.0300  0.0604  0.0435  0.2655
 0.1837  0.2588  0.0379  0.1363  0.0799  0.0154  0.0416  0.2463
 0.4453  0.0409  0.0774  0.3415  0.0141  0.0187  0.0205  0.0416
 0.6579  0.0151  0.0229  0.0278  0.0276  0.0302  0.0817  0.1368
 0.0969  0.5990  0.0205  0.0318  0.0482  0.0223  0.0861  0.0951
 0.3762  0.1832  0.0462  0.0624  0.0557  0.1508  0.0239  0.1017
 0.5811  0.0166  0.0171  0.0220  0.0369  0.0156  0.2215  0.0891
 0.8294  0.0126  0.0112  0.0146  0.0093  0.0212  0.0450  0.0566
 0.0549  0.5237  0.0348  0.0543  0.0597  0.0147  0.0867  0.1713
 0.5126  0.0622  0.0837  0.0880  0.0212  0.0550  0.1026  0.0748
 0.6155  0.0858  0.0434  0.0472  0.0178  0.0366  0.0303  0.1234
 0.8495  0.0286  0.0202  0.0246  0.0246  0.0239  0.0151  0.0135
 0.4769  0.0930  0.0311  0.0402  0.0160  0.0487  0.1626  0.1314
[torch.cuda.FloatTensor of size 14x8 (GPU 1)]

2019-11-02 23:45:25,419 
alphas_reduce = Variable containing:
 0.0301  0.7808  0.0206  0.0154  0.0251  0.0707  0.0179  0.0396
 0.7534  0.0996  0.0171  0.0147  0.0154  0.0789  0.0089  0.0121
 0.0748  0.4545  0.0633  0.1700  0.0246  0.0713  0.0710  0.0706
 0.6357  0.0486  0.0472  0.0964  0.0360  0.0171  0.0624  0.0567
 0.0458  0.3282  0.0271  0.0391  0.0448  0.0959  0.0164  0.4026
 0.1947  0.0897  0.1883  0.0403  0.1407  0.0510  0.2381  0.0573
 0.5945  0.0360  0.0335  0.1010  0.0531  0.0814  0.0312  0.0692
 0.0378  0.0309  0.0270  0.0321  0.0181  0.0262  0.0238  0.8040
 0.1083  0.4757  0.0951  0.0691  0.0221  0.0636  0.0567  0.1093
 0.1313  0.0768  0.0998  0.0695  0.1284  0.0944  0.2971  0.1028
 0.5328  0.0522  0.0723  0.0516  0.1599  0.0502  0.0511  0.0299
 0.1293  0.2938  0.0481  0.0638  0.0693  0.0644  0.0705  0.2608
 0.0870  0.3302  0.0886  0.0650  0.0759  0.0469  0.1796  0.1267
 0.0482  0.3554  0.0454  0.0409  0.0308  0.0432  0.0286  0.4076
[torch.cuda.FloatTensor of size 14x8 (GPU 1)]

2019-11-02 23:45:28,907 train 000 8.763866e-03 -5.357848
2019-11-02 23:48:14,351 train 050 1.013473e-02 -22.941437
2019-11-02 23:50:59,153 train 100 1.003340e-02 -12.208487
2019-11-02 23:53:44,406 train 150 1.016108e-02 -8.554549
2019-11-02 23:56:31,514 train 200 1.020261e-02 -6.617046
2019-11-02 23:59:16,620 train 250 1.019882e-02 -5.739169
2019-11-03 00:02:04,422 train 300 1.021746e-02 -4.994804
2019-11-03 00:04:49,525 train 350 1.025917e-02 -4.464348
2019-11-03 00:07:34,687 train 400 1.022066e-02 -4.073166
2019-11-03 00:10:17,261 train 450 1.022473e-02 -4.661193
2019-11-03 00:13:02,374 train 500 1.022787e-02 -4.398425
2019-11-03 00:15:49,328 train 550 1.022018e-02 -4.171884
2019-11-03 00:18:30,821 train 600 1.019035e-02 -3.907099
2019-11-03 00:21:19,744 train 650 1.016853e-02 -3.679112
2019-11-03 00:24:04,211 train 700 1.016686e-02 -3.481474
2019-11-03 00:26:50,108 train 750 1.017561e-02 -3.358470
2019-11-03 00:29:34,105 train 800 1.017521e-02 -3.225036
2019-11-03 00:32:20,432 train 850 1.016702e-02 -3.101824
2019-11-03 00:35:05,242 train 900 1.013676e-02 -3.014733
2019-11-03 00:37:50,404 train 950 1.014067e-02 -2.913147
2019-11-03 00:40:33,593 train 1000 1.014005e-02 -2.913582
2019-11-03 00:43:16,478 train 1050 1.012913e-02 -2.834288
2019-11-03 00:46:06,799 train 1100 1.011983e-02 -2.750492
2019-11-03 00:48:53,774 train 1150 1.009498e-02 -2.671341
2019-11-03 00:51:37,719 train 1200 1.012048e-02 -2.612522
2019-11-03 00:54:23,535 train 1250 1.011635e-02 -2.581304
2019-11-03 00:57:09,156 train 1300 1.011704e-02 -2.532466
2019-11-03 00:59:53,516 train 1350 1.011812e-02 -2.483009
2019-11-03 01:02:37,428 train 1400 1.009789e-02 -2.574052
2019-11-03 01:05:22,710 train 1450 1.009682e-02 -2.544614
2019-11-03 01:08:07,802 train 1500 1.009479e-02 -2.506526
2019-11-03 01:10:55,664 train 1550 1.008615e-02 -2.469967
2019-11-03 01:13:40,914 train 1600 1.008413e-02 -2.430352
2019-11-03 01:16:25,338 train 1650 1.007522e-02 -2.395658
2019-11-03 01:19:12,084 train 1700 1.008167e-02 -2.452985
2019-11-03 01:21:58,290 train 1750 1.009422e-02 -2.508995
2019-11-03 01:24:40,478 train 1800 1.010088e-02 -8.753302
2019-11-03 01:27:25,272 train 1850 1.009362e-02 -8.540663
2019-11-03 01:30:10,668 train 1900 1.009347e-02 -8.376084
2019-11-03 01:32:54,664 train 1950 1.009084e-02 -8.193272
2019-11-03 01:35:46,905 train 2000 1.008486e-02 -8.010798
2019-11-03 01:38:31,797 train 2050 1.008789e-02 -7.836932
2019-11-03 01:41:19,983 train 2100 1.007648e-02 -7.678446
2019-11-03 01:44:04,519 train 2150 1.007790e-02 -7.523645
2019-11-03 01:46:48,545 train 2200 1.006624e-02 -7.379550
2019-11-03 01:49:35,626 train 2250 1.006776e-02 -7.242775
2019-11-03 01:52:21,366 train 2300 1.006572e-02 -7.118455
2019-11-03 01:55:06,941 train 2350 1.006582e-02 -6.985975
2019-11-03 01:57:55,065 train 2400 1.006536e-02 -6.863782
2019-11-03 01:00:40,071 train 2450 1.007076e-02 -6.775925
2019-11-03 01:01:22,626 training loss; R2: 1.007165e-02 -6.750009
2019-11-03 01:01:23,134 valid 000 9.664024e-03 -2.125750
2019-11-03 01:01:31,734 valid 050 9.729038e-03 -2.142288
2019-11-03 01:01:40,392 valid 100 9.659728e-03 -16.526035
2019-11-03 01:01:48,996 valid 150 9.588520e-03 -11.416885
2019-11-03 01:01:57,563 valid 200 9.526357e-03 -8.841539
2019-11-03 01:02:06,118 valid 250 9.622418e-03 -7.259140
2019-11-03 01:02:14,618 valid 300 9.687378e-03 -6.221753
2019-11-03 01:02:23,139 valid 350 9.635737e-03 -5.517580
2019-11-03 01:02:31,739 valid 400 9.674303e-03 -4.998346
2019-11-03 01:02:40,320 valid 450 9.637981e-03 -4.573557
2019-11-03 01:02:48,894 valid 500 9.618012e-03 -4.197270
2019-11-03 01:02:57,517 valid 550 9.615061e-03 -3.940982
2019-11-03 01:03:06,111 valid 600 9.604977e-03 -8.436051
2019-11-03 01:03:08,672 validation loss; R2: 9.605599e-03 -10.266369
2019-11-03 01:03:08,802 epoch 7 lr 9.571722e-04
2019-11-03 01:03:08,803 genotype = Genotype(normal=[('max_pool_3x3', 0), ('dil_conv_5x5', 1), ('skip_connect', 1), ('max_pool_3x3', 0), ('max_pool_3x3', 0), ('dil_conv_3x3', 2), ('max_pool_3x3', 0), ('dil_conv_3x3', 4)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 0), ('sep_conv_5x5', 1), ('max_pool_3x3', 0), ('dil_conv_5x5', 2), ('dil_conv_5x5', 2), ('max_pool_3x3', 3), ('dil_conv_5x5', 4), ('dil_conv_5x5', 2)], reduce_concat=range(2, 6))
2019-11-03 01:03:08,805 
alphas_normal = Variable containing:
 0.0563  0.3272  0.0342  0.0856  0.2211  0.0356  0.1397  0.1004
 0.2672  0.2178  0.0339  0.0577  0.0402  0.0504  0.0602  0.2727
 0.2760  0.2386  0.0366  0.1378  0.0379  0.0110  0.0394  0.2228
 0.4072  0.0302  0.0581  0.3509  0.0150  0.0341  0.0296  0.0750
 0.7269  0.0224  0.0336  0.0530  0.0171  0.0219  0.0482  0.0770
 0.0780  0.6812  0.0317  0.0502  0.0274  0.0137  0.0525  0.0654
 0.4126  0.1795  0.0452  0.0679  0.0719  0.0996  0.0266  0.0968
 0.6327  0.0170  0.0218  0.0336  0.0362  0.0104  0.1965  0.0518
 0.8353  0.0121  0.0115  0.0167  0.0080  0.0288  0.0381  0.0495
 0.0499  0.5327  0.0356  0.0482  0.0456  0.0158  0.0658  0.2065
 0.4927  0.0531  0.0548  0.0681  0.0694  0.0675  0.0959  0.0984
 0.6713  0.0870  0.0396  0.0498  0.0212  0.0264  0.0202  0.0846
 0.9150  0.0162  0.0112  0.0137  0.0120  0.0149  0.0091  0.0079
 0.4847  0.0913  0.0307  0.0393  0.0152  0.0532  0.1737  0.1119
[torch.cuda.FloatTensor of size 14x8 (GPU 1)]

2019-11-03 01:03:08,806 
alphas_reduce = Variable containing:
 0.0341  0.7896  0.0211  0.0215  0.0297  0.0414  0.0191  0.0434
 0.6569  0.1146  0.0252  0.0130  0.0252  0.1298  0.0125  0.0228
 0.0989  0.4953  0.0585  0.1563  0.0352  0.0484  0.0607  0.0468
 0.6353  0.0561  0.0616  0.0585  0.0397  0.0180  0.0538  0.0771
 0.0447  0.3230  0.0302  0.0389  0.0412  0.0971  0.0173  0.4076
 0.3292  0.0883  0.1199  0.0503  0.1352  0.0574  0.1718  0.0479
 0.6562  0.0507  0.0430  0.0569  0.0503  0.0488  0.0274  0.0667
 0.0298  0.0308  0.0222  0.0246  0.0198  0.0229  0.0242  0.8257
 0.1127  0.4757  0.0843  0.0560  0.0371  0.0603  0.0893  0.0846
 0.1693  0.0758  0.0688  0.0962  0.1060  0.1752  0.2015  0.1072
 0.5316  0.0477  0.0600  0.0552  0.0925  0.1078  0.0691  0.0359
 0.0888  0.2216  0.0426  0.0535  0.0620  0.1116  0.0676  0.3524
 0.0786  0.3159  0.0606  0.0585  0.0882  0.0405  0.2696  0.0881
 0.0360  0.3358  0.0407  0.0391  0.0366  0.0491  0.0294  0.4332
[torch.cuda.FloatTensor of size 14x8 (GPU 1)]

2019-11-03 01:03:12,349 train 000 5.906247e-03 -0.071583
2019-11-03 01:05:57,176 train 050 1.000205e-02 -2.314234
2019-11-03 01:08:42,735 train 100 9.695217e-03 -1.506639
2019-11-03 01:11:28,330 train 150 9.743560e-03 -1.349265
2019-11-03 01:14:14,000 train 200 9.819371e-03 -1.204474
2019-11-03 01:16:58,187 train 250 9.820313e-03 -1.177432
2019-11-03 01:19:52,873 train 300 9.788344e-03 -1.233790
2019-11-03 01:22:37,602 train 350 9.799231e-03 -1.416140
2019-11-03 01:25:29,278 train 400 9.873399e-03 -1.384930
2019-11-03 01:28:15,143 train 450 9.921193e-03 -1.314917
2019-11-03 01:31:00,523 train 500 9.893918e-03 -1.281721
2019-11-03 01:33:43,808 train 550 9.874663e-03 -1.257644
2019-11-03 01:36:28,940 train 600 9.875666e-03 -1.272586
2019-11-03 01:39:13,387 train 650 9.848736e-03 -1.321229
2019-11-03 01:41:58,200 train 700 9.886939e-03 -1.292647
2019-11-03 01:44:47,149 train 750 9.868653e-03 -1.279113
2019-11-03 01:47:32,407 train 800 9.881146e-03 -1.277903
2019-11-03 01:50:18,591 train 850 9.860922e-03 -1.801272
2019-11-03 01:53:02,064 train 900 9.861946e-03 -1.799416
2019-11-03 01:55:47,135 train 950 9.857826e-03 -1.767474
2019-11-03 01:58:31,331 train 1000 9.835896e-03 -1.749488
2019-11-03 02:01:16,110 train 1050 9.826364e-03 -1.728346
2019-11-03 02:04:01,520 train 1100 9.816414e-03 -1.713549
2019-11-03 02:06:45,877 train 1150 9.817778e-03 -1.678885
2019-11-03 02:09:30,782 train 1200 9.816029e-03 -1.645570
2019-11-03 02:12:19,785 train 1250 9.829841e-03 -1.771028
2019-11-03 02:15:05,655 train 1300 9.846944e-03 -1.734889
2019-11-03 02:17:48,489 train 1350 9.860959e-03 -1.700697
2019-11-03 02:20:36,542 train 1400 9.844275e-03 -1.682402
2019-11-03 02:23:23,312 train 1450 9.856029e-03 -1.681943
2019-11-03 02:26:08,805 train 1500 9.864618e-03 -1.650026
2019-11-03 02:28:52,485 train 1550 9.861405e-03 -1.649633
2019-11-03 02:31:36,084 train 1600 9.855251e-03 -1.660606
2019-11-03 02:34:22,759 train 1650 9.856565e-03 -1.645902
2019-11-03 02:37:08,379 train 1700 9.872705e-03 -1.635565
2019-11-03 02:39:54,200 train 1750 9.865738e-03 -1.610965
2019-11-03 02:42:38,135 train 1800 9.866963e-03 -2.067438
2019-11-03 02:45:26,150 train 1850 9.866619e-03 -2.094575
2019-11-03 02:48:11,978 train 1900 9.876351e-03 -2.086532
2019-11-03 02:50:57,278 train 1950 9.893137e-03 -2.058962
2019-11-03 02:53:43,630 train 2000 9.900518e-03 -2.037840
2019-11-03 02:56:31,075 train 2050 9.911565e-03 -2.042301
2019-11-03 02:59:14,856 train 2100 9.904073e-03 -2.030559
2019-11-03 03:02:01,623 train 2150 9.908125e-03 -2.044834
2019-11-03 03:04:46,569 train 2200 9.896103e-03 -2.020319
2019-11-03 03:07:30,683 train 2250 9.896086e-03 -2.003212
2019-11-03 03:10:15,091 train 2300 9.890970e-03 -1.988131
2019-11-03 03:12:59,349 train 2350 9.880262e-03 -1.982870
2019-11-03 03:15:44,734 train 2400 9.863585e-03 -1.957272
2019-11-03 03:18:30,965 train 2450 9.859944e-03 -1.944944
2019-11-03 03:19:16,649 training loss; R2: 9.860404e-03 -1.939464
2019-11-03 03:19:17,107 valid 000 1.254768e-02 -3.327270
2019-11-03 03:19:25,643 valid 050 1.144117e-02 -72.191004
2019-11-03 03:19:34,123 valid 100 1.110064e-02 -36.768902
2019-11-03 03:19:42,676 valid 150 1.100763e-02 -24.772020
2019-11-03 03:19:51,242 valid 200 1.084455e-02 -18.806795
2019-11-03 03:19:59,758 valid 250 1.090438e-02 -15.200105
2019-11-03 03:20:08,277 valid 300 1.090157e-02 -14.220881
2019-11-03 03:20:16,797 valid 350 1.090575e-02 -12.324135
2019-11-03 03:20:25,307 valid 400 1.091509e-02 -12.162282
2019-11-03 03:20:33,848 valid 450 1.088537e-02 -10.891247
2019-11-03 03:20:42,425 valid 500 1.086546e-02 -9.903132
2019-11-03 03:20:50,961 valid 550 1.080618e-02 -9.124169
2019-11-03 03:20:59,515 valid 600 1.077769e-02 -8.439471
2019-11-03 03:21:02,053 validation loss; R2: 1.075606e-02 -8.261572
2019-11-03 03:21:02,183 epoch 8 lr 9.443380e-04
2019-11-03 03:21:02,184 genotype = Genotype(normal=[('dil_conv_5x5', 1), ('max_pool_3x3', 0), ('skip_connect', 1), ('dil_conv_5x5', 0), ('max_pool_3x3', 0), ('dil_conv_3x3', 2), ('max_pool_3x3', 0), ('dil_conv_5x5', 1)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 0), ('sep_conv_5x5', 1), ('max_pool_3x3', 0), ('dil_conv_5x5', 2), ('dil_conv_5x5', 2), ('max_pool_3x3', 3), ('dil_conv_5x5', 4), ('dil_conv_5x5', 2)], reduce_concat=range(2, 6))
2019-11-03 03:21:02,186 
alphas_normal = Variable containing:
 0.0458  0.3051  0.0286  0.0636  0.2029  0.0374  0.1839  0.1326
 0.2530  0.1942  0.0282  0.0453  0.0239  0.0670  0.0801  0.3083
 0.2096  0.2631  0.0349  0.1139  0.0410  0.0110  0.0295  0.2969
 0.4154  0.0388  0.0533  0.3448  0.0175  0.0365  0.0318  0.0619
 0.5806  0.0337  0.0578  0.0784  0.0217  0.0227  0.0463  0.1587
 0.0710  0.7164  0.0233  0.0396  0.0311  0.0181  0.0459  0.0546
 0.3372  0.1509  0.0458  0.0658  0.0424  0.1628  0.0231  0.1720
 0.4800  0.0173  0.0276  0.0296  0.0416  0.0134  0.3069  0.0836
 0.8059  0.0153  0.0182  0.0208  0.0110  0.0328  0.0300  0.0660
 0.0429  0.5204  0.0249  0.0355  0.0298  0.0170  0.0368  0.2926
 0.4994  0.0385  0.0335  0.0433  0.0573  0.0685  0.0907  0.1688
 0.5676  0.1281  0.0487  0.0496  0.0253  0.0291  0.0327  0.1188
 0.8827  0.0225  0.0145  0.0161  0.0191  0.0169  0.0158  0.0124
 0.4444  0.1423  0.0386  0.0415  0.0204  0.0579  0.1543  0.1007
[torch.cuda.FloatTensor of size 14x8 (GPU 1)]

2019-11-03 03:21:02,187 
alphas_reduce = Variable containing:
 0.0329  0.7402  0.0210  0.0223  0.0488  0.0670  0.0306  0.0373
 0.6388  0.1131  0.0269  0.0134  0.0243  0.1550  0.0103  0.0182
 0.0792  0.4033  0.0804  0.2135  0.0558  0.0624  0.0404  0.0649
 0.5903  0.0668  0.0786  0.0753  0.0285  0.0182  0.0520  0.0903
 0.0527  0.3064  0.0412  0.0583  0.0522  0.1420  0.0312  0.3161
 0.2512  0.0767  0.1679  0.0553  0.1051  0.0565  0.2547  0.0326
 0.6711  0.0521  0.0509  0.0382  0.0340  0.0532  0.0231  0.0773
 0.0307  0.0317  0.0285  0.0312  0.0179  0.0441  0.0277  0.7881
 0.1401  0.4701  0.1243  0.0526  0.0278  0.0623  0.0719  0.0509
 0.1847  0.0661  0.1014  0.1112  0.0916  0.1836  0.1942  0.0672
 0.5479  0.0380  0.0533  0.0534  0.0940  0.1349  0.0443  0.0342
 0.1177  0.1586  0.0462  0.0647  0.0439  0.1131  0.0925  0.3633
 0.1089  0.3435  0.0858  0.0607  0.1097  0.0526  0.1622  0.0765
 0.0285  0.3527  0.0345  0.0296  0.0301  0.0372  0.0254  0.4619
[torch.cuda.FloatTensor of size 14x8 (GPU 1)]

2019-11-03 03:21:05,730 train 000 8.602708e-03 -0.299580
2019-11-03 03:23:53,242 train 050 9.956198e-03 -1.403852
2019-11-03 03:26:43,230 train 100 9.759171e-03 -1.216201
2019-11-03 03:29:26,217 train 150 9.753237e-03 -1.313900
2019-11-03 03:32:16,739 train 200 9.750311e-03 -1.213988
2019-11-03 03:35:03,115 train 250 9.782548e-03 -1.418194
2019-11-03 03:37:47,790 train 300 9.724533e-03 -1.355135
2019-11-03 03:40:32,361 train 350 9.746001e-03 -1.507264
2019-11-03 03:43:17,609 train 400 9.763250e-03 -1.505886
2019-11-03 03:46:01,669 train 450 9.767148e-03 -1.427614
2019-11-03 03:48:46,684 train 500 9.726161e-03 -1.706950
2019-11-03 03:51:30,226 train 550 9.725637e-03 -5.936311
2019-11-03 03:54:15,728 train 600 9.736640e-03 -5.546848
2019-11-03 03:57:03,108 train 650 9.731924e-03 -5.202657
2019-11-03 03:59:48,782 train 700 9.715343e-03 -4.940769
2019-11-03 04:02:37,865 train 750 9.713061e-03 -4.698347
2019-11-03 04:05:21,783 train 800 9.670001e-03 -4.462855
2019-11-03 04:08:06,194 train 850 9.677327e-03 -4.479862
2019-11-03 04:10:52,530 train 900 9.669487e-03 -4.909409
2019-11-03 04:13:38,487 train 950 9.649363e-03 -4.934694
2019-11-03 04:16:31,177 train 1000 9.677598e-03 -4.750207
2019-11-03 04:19:17,202 train 1050 9.677489e-03 -4.579239
2019-11-03 04:21:59,963 train 1100 9.688039e-03 -4.426121
2019-11-03 04:24:51,275 train 1150 9.689164e-03 -4.267727
2019-11-03 04:27:38,335 train 1200 9.703146e-03 -4.136408
2019-11-03 04:30:24,288 train 1250 9.719933e-03 -4.006947
2019-11-03 04:33:12,639 train 1300 9.728209e-03 -3.894665
2019-11-03 04:35:58,222 train 1350 9.717773e-03 -3.793191
2019-11-03 04:38:47,044 train 1400 9.716437e-03 -3.678533
2019-11-03 04:41:31,690 train 1450 9.720181e-03 -3.596767
2019-11-03 04:44:19,176 train 1500 9.707759e-03 -8.058906
2019-11-03 04:47:05,323 train 1550 9.703340e-03 -7.833999
2019-11-03 04:49:49,132 train 1600 9.697249e-03 -7.616471
2019-11-03 04:52:36,556 train 1650 9.690541e-03 -7.423032
2019-11-03 04:55:22,371 train 1700 9.684859e-03 -7.316445
2019-11-03 04:58:09,585 train 1750 9.690540e-03 -7.155369
2019-11-03 05:00:55,320 train 1800 9.688106e-03 -6.987139
2019-11-03 05:03:42,545 train 1850 9.687227e-03 -6.927345
2019-11-03 05:06:30,196 train 1900 9.685522e-03 -6.766973
2019-11-03 05:09:15,238 train 1950 9.688749e-03 -6.625482
2019-11-03 05:11:58,611 train 2000 9.683183e-03 -6.489811
2019-11-03 05:14:41,687 train 2050 9.692628e-03 -6.361123
2019-11-03 05:17:26,307 train 2100 9.695267e-03 -6.815213
2019-11-03 05:20:15,448 train 2150 9.692759e-03 -6.715670
2019-11-03 05:23:03,241 train 2200 9.689369e-03 -6.782635
2019-11-03 05:25:49,617 train 2250 9.689380e-03 -6.653346
2019-11-03 05:28:34,809 train 2300 9.679671e-03 -6.529154
2019-11-03 05:31:19,520 train 2350 9.676334e-03 -6.408657
2019-11-03 05:34:07,567 train 2400 9.679532e-03 -6.299804
2019-11-03 05:36:56,981 train 2450 9.670974e-03 -6.592594
2019-11-03 05:37:39,832 training loss; R2: 9.673042e-03 -6.568795
2019-11-03 05:37:40,317 valid 000 1.104793e-02 -1.426103
2019-11-03 05:37:48,962 valid 050 9.040070e-03 -1.559482
2019-11-03 05:37:57,523 valid 100 9.121917e-03 -1.594126
2019-11-03 05:38:06,067 valid 150 9.119741e-03 -1.382030
2019-11-03 05:38:14,703 valid 200 9.101542e-03 -26.241682
2019-11-03 05:38:23,398 valid 250 9.029968e-03 -21.265803
2019-11-03 05:38:32,001 valid 300 9.176345e-03 -17.911494
2019-11-03 05:38:40,586 valid 350 9.143437e-03 -15.550710
2019-11-03 05:38:49,172 valid 400 9.146514e-03 -13.726393
2019-11-03 05:38:57,819 valid 450 9.161771e-03 -37.913944
2019-11-03 05:39:06,349 valid 500 9.170832e-03 -34.255323
2019-11-03 05:39:14,884 valid 550 9.158987e-03 -31.413464
2019-11-03 05:39:23,437 valid 600 9.173350e-03 -28.843764
2019-11-03 05:39:25,972 validation loss; R2: 9.179751e-03 -28.202871
2019-11-03 05:39:26,119 epoch 9 lr 9.299476e-04
2019-11-03 05:39:26,120 genotype = Genotype(normal=[('max_pool_3x3', 0), ('max_pool_3x3', 1), ('skip_connect', 1), ('max_pool_3x3', 0), ('max_pool_3x3', 0), ('dil_conv_3x3', 2), ('max_pool_3x3', 0), ('dil_conv_3x3', 4)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 0), ('sep_conv_5x5', 1), ('dil_conv_5x5', 2), ('max_pool_3x3', 0), ('dil_conv_5x5', 2), ('max_pool_3x3', 3), ('dil_conv_5x5', 4), ('dil_conv_5x5', 2)], reduce_concat=range(2, 6))
2019-11-03 05:39:26,122 
alphas_normal = Variable containing:
 0.0606  0.3777  0.0327  0.0697  0.1991  0.0272  0.1649  0.0680
 0.3465  0.2327  0.0278  0.0389  0.0174  0.0469  0.0600  0.2298
 0.1590  0.3103  0.0587  0.2098  0.0425  0.0099  0.0330  0.1769
 0.3141  0.0346  0.0642  0.4556  0.0153  0.0265  0.0258  0.0638
 0.6672  0.0202  0.0404  0.0438  0.0165  0.0149  0.0484  0.1485
 0.0694  0.7415  0.0239  0.0345  0.0454  0.0176  0.0379  0.0297
 0.3601  0.1939  0.0428  0.0600  0.0266  0.1371  0.0305  0.1490
 0.3951  0.0240  0.0304  0.0326  0.0300  0.0266  0.3712  0.0901
 0.7509  0.0243  0.0195  0.0250  0.0177  0.0449  0.0238  0.0938
 0.0436  0.5584  0.0364  0.0489  0.0352  0.0107  0.0337  0.2332
 0.5617  0.0455  0.0316  0.0360  0.0523  0.0638  0.0903  0.1188
 0.5529  0.1125  0.0407  0.0403  0.0247  0.0454  0.0399  0.1435
 0.8894  0.0233  0.0151  0.0161  0.0127  0.0184  0.0114  0.0137
 0.3465  0.1387  0.0295  0.0296  0.0157  0.0817  0.2579  0.1005
[torch.cuda.FloatTensor of size 14x8 (GPU 1)]

2019-11-03 05:39:26,123 
alphas_reduce = Variable containing:
 0.0259  0.7774  0.0227  0.0178  0.0357  0.0692  0.0220  0.0292
 0.6188  0.1157  0.0329  0.0152  0.0195  0.1564  0.0161  0.0255
 0.0812  0.3669  0.1160  0.2421  0.0773  0.0304  0.0272  0.0588
 0.5817  0.0680  0.0753  0.0695  0.0266  0.0196  0.0720  0.0873
 0.0402  0.2228  0.0418  0.0445  0.0421  0.1678  0.0218  0.4189
 0.2348  0.0607  0.2004  0.0403  0.1002  0.0459  0.2858  0.0320
 0.5423  0.0669  0.0626  0.0624  0.0432  0.0609  0.0363  0.1254
 0.0345  0.0300  0.0342  0.0338  0.0215  0.0500  0.0251  0.7710
 0.1242  0.4126  0.1571  0.0577  0.0292  0.0547  0.0921  0.0724
 0.1715  0.0580  0.1014  0.0728  0.0982  0.2240  0.2143  0.0599
 0.5932  0.0531  0.0696  0.0522  0.0614  0.0908  0.0479  0.0319
 0.1713  0.1461  0.0547  0.0773  0.0401  0.1030  0.0747  0.3328
 0.1354  0.2743  0.0853  0.0568  0.0833  0.0443  0.2095  0.1111
 0.0535  0.3199  0.0463  0.0316  0.0551  0.0537  0.0446  0.3954
[torch.cuda.FloatTensor of size 14x8 (GPU 1)]

2019-11-03 05:39:29,629 train 000 9.777096e-03 -2.601303
2019-11-03 05:42:11,691 train 050 9.215606e-03 -1.825793
2019-11-03 05:44:55,533 train 100 9.328124e-03 -1.479150
2019-11-03 05:47:39,376 train 150 9.402573e-03 -1.439238
2019-11-03 05:50:27,073 train 200 9.370872e-03 -1.470767
2019-11-03 05:53:16,197 train 250 9.404828e-03 -1.290252
2019-11-03 05:56:03,691 train 300 9.386876e-03 -1.277194
2019-11-03 05:58:46,965 train 350 9.408744e-03 -1.298576
2019-11-03 06:01:34,645 train 400 9.435859e-03 -1.358094
2019-11-03 06:04:18,467 train 450 9.455214e-03 -1.331239
2019-11-03 06:07:03,829 train 500 9.460842e-03 -1.319856
2019-11-03 06:09:48,396 train 550 9.453960e-03 -1.269723
2019-11-03 06:12:36,252 train 600 9.461779e-03 -1.262555
2019-11-03 06:15:22,387 train 650 9.460296e-03 -1.251612
2019-11-03 06:18:11,425 train 700 9.502466e-03 -1.226703
2019-11-03 06:20:56,609 train 750 9.508961e-03 -1.241067
2019-11-03 06:23:41,764 train 800 9.492105e-03 -1.335864
2019-11-03 06:26:31,757 train 850 9.520584e-03 -2.329544
2019-11-03 06:29:16,205 train 900 9.494543e-03 -113.967067
2019-11-03 06:32:08,652 train 950 9.494679e-03 -108.056497
2019-11-03 06:34:56,845 train 1000 9.493997e-03 -103.226910
2019-11-03 06:37:42,719 train 1050 9.478943e-03 -98.361547
2019-11-03 06:40:31,656 train 1100 9.505586e-03 -93.944965
2019-11-03 06:43:19,265 train 1150 9.501673e-03 -89.932109
2019-11-03 06:46:05,022 train 1200 9.516449e-03 -86.816365
2019-11-03 06:48:51,359 train 1250 9.500301e-03 -83.384507
2019-11-03 06:51:35,529 train 1300 9.505483e-03 -80.230160
2019-11-03 06:54:20,487 train 1350 9.509033e-03 -77.317509
2019-11-03 06:57:07,970 train 1400 9.520543e-03 -74.714826
2019-11-03 06:59:56,326 train 1450 9.512640e-03 -72.170002
2019-11-03 07:02:40,333 train 1500 9.510859e-03 -69.815233
2019-11-03 07:05:24,775 train 1550 9.510820e-03 -67.592265
2019-11-03 07:08:07,978 train 1600 9.521134e-03 -65.510988
2019-11-03 07:10:54,277 train 1650 9.513304e-03 -63.561152
2019-11-03 07:13:37,497 train 1700 9.516951e-03 -61.725124
2019-11-03 07:16:23,827 train 1750 9.513185e-03 -59.986317
2019-11-03 07:19:10,746 train 1800 9.526620e-03 -58.481341
2019-11-03 07:21:55,662 train 1850 9.522921e-03 -56.961103
2019-11-03 07:24:37,690 train 1900 9.528018e-03 -55.489025
2019-11-03 07:27:27,252 train 1950 9.534205e-03 -54.093575
2019-11-03 07:30:13,365 train 2000 9.532381e-03 -52.771578
2019-11-03 07:33:00,842 train 2050 9.536087e-03 -51.514800
2019-11-03 07:35:55,363 train 2100 9.538061e-03 -50.319568
2019-11-03 07:38:41,969 train 2150 9.546715e-03 -49.170014
2019-11-03 07:41:29,283 train 2200 9.544670e-03 -48.094545
2019-11-03 07:44:17,350 train 2250 9.538036e-03 -47.059684
2019-11-03 07:47:04,530 train 2300 9.528245e-03 -46.051495
2019-11-03 07:49:48,965 train 2350 9.522662e-03 -45.096152
2019-11-03 07:52:38,386 train 2400 9.514900e-03 -45.388847
2019-11-03 07:55:22,375 train 2450 9.507281e-03 -44.491737
2019-11-03 07:56:04,957 training loss; R2: 9.507607e-03 -44.281625
2019-11-03 07:56:05,518 valid 000 7.848044e-03 -1.341252
2019-11-03 07:56:15,317 valid 050 9.152136e-03 -1.199812
2019-11-03 07:56:23,949 valid 100 9.147363e-03 -5.309810
2019-11-03 07:56:32,527 valid 150 9.240401e-03 -4.052113
2019-11-03 07:56:41,112 valid 200 9.196876e-03 -3.352735
2019-11-03 07:56:49,559 valid 250 9.180433e-03 -12.899547
2019-11-03 07:56:58,085 valid 300 9.162001e-03 -10.988128
2019-11-03 07:57:06,548 valid 350 9.149976e-03 -9.578885
2019-11-03 07:57:15,360 valid 400 9.158319e-03 -8.577992
2019-11-03 07:57:24,135 valid 450 9.171541e-03 -7.907333
2019-11-03 07:57:32,763 valid 500 9.170709e-03 -7.238216
2019-11-03 07:57:41,392 valid 550 9.131931e-03 -6.775614
2019-11-03 07:57:49,966 valid 600 9.124082e-03 -6.292545
2019-11-03 07:57:52,509 validation loss; R2: 9.117709e-03 -6.165112
2019-11-03 07:57:52,637 epoch 10 lr 9.140576e-04
2019-11-03 07:57:52,638 genotype = Genotype(normal=[('max_pool_3x3', 0), ('dil_conv_5x5', 1), ('skip_connect', 1), ('max_pool_3x3', 0), ('max_pool_3x3', 0), ('dil_conv_3x3', 2), ('max_pool_3x3', 0), ('dil_conv_3x3', 4)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 0), ('sep_conv_5x5', 1), ('dil_conv_5x5', 2), ('max_pool_3x3', 0), ('dil_conv_5x5', 2), ('max_pool_3x3', 3), ('dil_conv_5x5', 4), ('max_pool_3x3', 3)], reduce_concat=range(2, 6))
2019-11-03 07:57:52,640 
alphas_normal = Variable containing:
 0.0606  0.3158  0.0313  0.0562  0.2387  0.0460  0.1644  0.0870
 0.3003  0.2083  0.0275  0.0379  0.0177  0.0820  0.0539  0.2724
 0.0941  0.3652  0.0472  0.1486  0.0572  0.0094  0.0330  0.2452
 0.3929  0.0291  0.0464  0.3978  0.0166  0.0245  0.0304  0.0623
 0.6005  0.0343  0.0566  0.0500  0.0224  0.0196  0.0470  0.1695
 0.0893  0.7159  0.0330  0.0361  0.0494  0.0186  0.0388  0.0191
 0.4695  0.1303  0.0271  0.0373  0.0215  0.1264  0.0276  0.1602
 0.4685  0.0214  0.0256  0.0279  0.0370  0.0239  0.2947  0.1011
 0.7391  0.0171  0.0135  0.0155  0.0119  0.0524  0.0283  0.1223
 0.0360  0.4724  0.0374  0.0450  0.0351  0.0174  0.0219  0.3349
 0.5856  0.0511  0.0404  0.0440  0.0326  0.0758  0.0855  0.0850
 0.5619  0.1083  0.0389  0.0332  0.0274  0.0469  0.0453  0.1379
 0.8291  0.0364  0.0223  0.0226  0.0258  0.0247  0.0172  0.0219
 0.3089  0.1773  0.0311  0.0274  0.0271  0.0881  0.1977  0.1424
[torch.cuda.FloatTensor of size 14x8 (GPU 1)]

2019-11-03 07:57:52,641 
alphas_reduce = Variable containing:
 0.0328  0.7195  0.0323  0.0157  0.0534  0.0905  0.0297  0.0262
 0.6694  0.0722  0.0291  0.0105  0.0186  0.1689  0.0109  0.0205
 0.0692  0.3430  0.1705  0.1605  0.0994  0.0528  0.0301  0.0745
 0.6779  0.0522  0.0584  0.0560  0.0345  0.0263  0.0319  0.0630
 0.0477  0.2445  0.0443  0.0494  0.0251  0.2075  0.0307  0.3508
 0.3109  0.0673  0.1718  0.0434  0.0858  0.0482  0.2461  0.0266
 0.6192  0.0617  0.0417  0.0902  0.0360  0.0449  0.0306  0.0758
 0.0267  0.0298  0.0257  0.0263  0.0177  0.0541  0.0233  0.7964
 0.1147  0.5477  0.1069  0.0408  0.0289  0.0480  0.0533  0.0598
 0.1002  0.0527  0.0907  0.0799  0.0937  0.2989  0.2438  0.0401
 0.5771  0.0381  0.0522  0.0633  0.0641  0.1113  0.0500  0.0440
 0.1294  0.1542  0.0485  0.0673  0.0409  0.0667  0.0915  0.4015
 0.1171  0.4108  0.0875  0.0576  0.0685  0.0342  0.1087  0.1155
 0.0415  0.3285  0.0386  0.0335  0.0359  0.0379  0.0320  0.4521
[torch.cuda.FloatTensor of size 14x8 (GPU 1)]

2019-11-03 07:57:56,346 train 000 9.661479e-03 0.114058
2019-11-03 08:00:42,309 train 050 9.221513e-03 -1.596572
2019-11-03 08:03:28,437 train 100 9.436590e-03 -1.155701
2019-11-03 08:06:14,459 train 150 9.474868e-03 -3.035061
2019-11-03 08:08:58,645 train 200 9.537879e-03 -3.833717
2019-11-03 08:11:44,401 train 250 9.559462e-03 -3.269829
2019-11-03 08:14:27,677 train 300 9.527548e-03 -2.892450
2019-11-03 08:17:14,901 train 350 9.590039e-03 -2.582649
2019-11-03 08:20:04,557 train 400 9.550360e-03 -2.423732
2019-11-03 08:22:49,343 train 450 9.541102e-03 -2.261679
2019-11-03 08:25:35,763 train 500 9.512026e-03 -2.213654
2019-11-03 08:28:23,506 train 550 9.506707e-03 -2.113286
2019-11-03 08:31:12,744 train 600 9.478993e-03 -2.027570
2019-11-03 08:34:00,523 train 650 9.483457e-03 -2.375132
2019-11-03 08:36:45,615 train 700 9.492563e-03 -2.287935
2019-11-03 08:39:30,221 train 750 9.471188e-03 -2.237615
2019-11-03 08:42:19,752 train 800 9.467717e-03 -2.180612
2019-11-03 08:45:04,568 train 850 9.477171e-03 -2.124668
2019-11-03 08:47:54,475 train 900 9.474666e-03 -2.457362
2019-11-03 08:50:39,111 train 950 9.473686e-03 -2.503821
2019-11-03 08:53:28,560 train 1000 9.455804e-03 -2.438803
2019-11-03 08:56:12,788 train 1050 9.442919e-03 -2.375867
2019-11-03 08:58:58,458 train 1100 9.440325e-03 -2.327998
2019-11-03 09:01:42,974 train 1150 9.465952e-03 -2.259457
2019-11-03 09:04:30,617 train 1200 9.461354e-03 -2.211260
2019-11-03 09:07:21,085 train 1250 9.446880e-03 -2.161534
2019-11-03 09:10:06,038 train 1300 9.452320e-03 -2.116012
2019-11-03 09:12:53,559 train 1350 9.462902e-03 -2.069999
2019-11-03 09:15:41,378 train 1400 9.463626e-03 -2.124364
2019-11-03 09:18:26,614 train 1450 9.449457e-03 -2.088571
2019-11-03 09:21:13,902 train 1500 9.445933e-03 -3.674543
2019-11-03 09:23:59,081 train 1550 9.441458e-03 -3.612166
2019-11-03 09:26:41,906 train 1600 9.430915e-03 -3.522843
