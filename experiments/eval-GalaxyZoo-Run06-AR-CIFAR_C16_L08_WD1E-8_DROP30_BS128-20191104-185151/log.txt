2019-11-04 18:51:51,681 gpu device = 2
2019-11-04 18:51:51,681 args = Namespace(arch='CIFAR_10', auxiliary=False, auxiliary_weight=0.4, batch_size=128, cutout=False, cutout_length=16, data='../data', dataset='GalaxyZoo', drop_path_prob=0.3, epochs=200, fc1_size=1024, fc2_size=1024, gpu=2, grad_clip=5, init_channels=16, layers=8, learning_rate=0.001, model_path='saved_models', momentum=0.9, optimizer='Adam', random=False, report_freq=50, save='eval-GalaxyZoo-Run06-AR-CIFAR_C16_L08_WD1E-8_DROP30_BS128-20191104-185151', seed=0, val_portion=0.1, weight_decay=1e-08)
2019-11-04 18:51:54,966 param size = 1.594229MB
2019-11-04 18:51:54,970 epoch 0 lr 1.000000e-03
2019-11-04 18:51:57,964 train 000 4.310625e-02 -1.506304
2019-11-04 18:52:17,129 train 050 2.821257e-02 -0.413824
2019-11-04 18:52:37,095 train 100 2.632497e-02 -0.353354
2019-11-04 18:52:57,087 train 150 2.530892e-02 -0.404891
2019-11-04 18:53:17,067 train 200 2.450622e-02 -0.412491
2019-11-04 18:53:37,432 train 250 2.390699e-02 -0.393028
2019-11-04 18:53:57,753 train 300 2.338223e-02 -0.377070
2019-11-04 18:54:18,080 train 350 2.296717e-02 -0.374939
2019-11-04 18:54:38,455 train 400 2.271227e-02 -0.387789
2019-11-04 18:54:52,229 training loss; R2: 2.255193e-02 -0.390280
2019-11-04 18:54:53,206 valid 000 2.416843e-02 -0.487931
2019-11-04 18:55:12,402 validation loss; R2: 2.126867e-02 -0.739199
2019-11-04 18:55:12,436 epoch 1 lr 1.000000e-03
2019-11-04 18:55:13,720 train 000 1.958323e-02 -2.682873
2019-11-04 18:55:33,844 train 050 2.047981e-02 -0.395691
2019-11-04 18:55:53,980 train 100 2.028015e-02 -0.362645
2019-11-04 18:56:14,341 train 150 2.011601e-02 -0.361174
2019-11-04 18:56:34,859 train 200 2.000565e-02 -0.343352
2019-11-04 18:56:54,804 train 250 1.993890e-02 -0.326442
2019-11-04 18:57:15,006 train 300 1.991476e-02 -0.321019
2019-11-04 18:57:35,394 train 350 1.984509e-02 -0.340973
2019-11-04 18:57:55,743 train 400 2.043199e-02 -0.430234
2019-11-04 18:58:08,708 training loss; R2: 2.118632e-02 -0.490187
2019-11-04 18:58:09,708 valid 000 2.656556e-02 -2.131479
2019-11-04 18:58:28,783 validation loss; R2: 2.767291e-02 -0.779241
2019-11-04 18:58:28,824 epoch 2 lr 1.000000e-03
2019-11-04 18:58:29,968 train 000 2.826930e-02 -0.499721
2019-11-04 18:58:50,616 train 050 2.520618e-02 -0.494297
2019-11-04 18:59:11,127 train 100 2.530051e-02 -0.547632
2019-11-04 18:59:31,885 train 150 2.559423e-02 -0.502732
2019-11-04 18:59:52,815 train 200 2.542257e-02 -0.452121
2019-11-04 19:00:13,571 train 250 2.531927e-02 -0.423005
2019-11-04 19:00:34,070 train 300 2.518093e-02 -0.404632
2019-11-04 19:00:54,788 train 350 2.521482e-02 -0.388243
2019-11-04 19:01:15,818 train 400 2.509015e-02 -0.377400
2019-11-04 19:01:29,032 training loss; R2: 2.503270e-02 -0.363338
2019-11-04 19:01:30,033 valid 000 2.300943e-02 -0.904982
2019-11-04 19:01:48,911 validation loss; R2: 2.382004e-02 -0.432783
2019-11-04 19:01:48,951 epoch 3 lr 1.000000e-03
2019-11-04 19:01:50,135 train 000 2.243037e-02 -0.631147
2019-11-04 19:02:10,174 train 050 2.459564e-02 -0.290378
2019-11-04 19:02:30,462 train 100 2.433290e-02 -0.315709
2019-11-04 19:02:50,857 train 150 2.430881e-02 -0.293230
2019-11-04 19:03:11,076 train 200 2.426018e-02 -0.291559
2019-11-04 19:03:31,027 train 250 2.413393e-02 -0.292070
2019-11-04 19:03:51,303 train 300 2.415768e-02 -0.290556
2019-11-04 19:04:11,556 train 350 2.419033e-02 -0.285810
2019-11-04 19:04:31,536 train 400 2.415744e-02 -0.289788
2019-11-04 19:04:44,505 training loss; R2: 2.412978e-02 -0.286135
2019-11-04 19:04:45,479 valid 000 3.236874e-02 -0.127266
2019-11-04 19:05:04,443 validation loss; R2: 2.905722e-02 -0.132358
2019-11-04 19:05:04,495 epoch 4 lr 1.000000e-03
2019-11-04 19:05:05,669 train 000 2.686569e-02 -0.242520
2019-11-04 19:05:26,176 train 050 2.560116e-02 -0.519755
2019-11-04 19:05:46,612 train 100 2.510060e-02 -0.392406
2019-11-04 19:06:07,007 train 150 2.472770e-02 -0.380216
2019-11-04 19:06:27,197 train 200 2.457425e-02 -0.349150
2019-11-04 19:06:46,911 train 250 2.460179e-02 -0.363947
2019-11-04 19:07:06,832 train 300 2.455618e-02 -0.346905
2019-11-04 19:07:26,807 train 350 2.445543e-02 -0.342457
2019-11-04 19:07:46,940 train 400 2.443669e-02 -0.338479
2019-11-04 19:07:59,644 training loss; R2: 2.470220e-02 -0.399600
2019-11-04 19:08:00,610 valid 000 8.650658e-02 -2.043693
2019-11-04 19:08:18,977 validation loss; R2: 8.774637e-02 -2.485252
2019-11-04 19:08:19,021 epoch 5 lr 1.000000e-03
2019-11-04 19:08:20,118 train 000 2.609974e-02 -0.259679
2019-11-04 19:08:40,193 train 050 5.015322e-02 -4.353742
2019-11-04 19:09:00,202 train 100 4.257434e-02 -2.938083
2019-11-04 19:09:20,124 train 150 4.333174e-02 -2.882975
2019-11-04 19:09:40,036 train 200 4.104947e-02 -2.595700
2019-11-04 19:10:00,576 train 250 3.883775e-02 -2.269943
2019-11-04 19:10:21,088 train 300 3.805848e-02 -2.156334
2019-11-04 19:10:41,242 train 350 4.380996e-02 -4.969570
2019-11-04 19:11:01,487 train 400 5.121421e-02 -6.226455
2019-11-04 19:11:14,561 training loss; R2: 5.486961e-02 -6.896596
2019-11-04 19:11:15,526 valid 000 8.422879e-02 -58.463530
2019-11-04 19:11:34,581 validation loss; R2: 8.482933e-02 -25.611325
2019-11-04 19:11:34,624 epoch 6 lr 1.000000e-03
2019-11-04 19:11:35,809 train 000 9.539952e-02 -20.101076
2019-11-04 19:11:56,318 train 050 1.021004e-01 -18.810443
2019-11-04 19:12:16,685 train 100 1.019374e-01 -20.444694
2019-11-04 19:12:37,069 train 150 1.026995e-01 -21.114094
2019-11-04 19:12:57,414 train 200 1.034642e-01 -19.867224
2019-11-04 19:13:17,611 train 250 1.036872e-01 -19.295669
2019-11-04 19:13:37,969 train 300 1.043894e-01 -19.180225
2019-11-04 19:13:58,337 train 350 1.040310e-01 -19.265758
2019-11-04 19:14:18,484 train 400 1.038116e-01 -19.444317
2019-11-04 19:14:31,300 training loss; R2: 1.037597e-01 -19.495074
2019-11-04 19:14:32,247 valid 000 8.411676e-02 -10.741094
2019-11-04 19:14:50,954 validation loss; R2: 8.482933e-02 -24.825845
2019-11-04 19:14:50,990 epoch 7 lr 1.000000e-03
2019-11-04 19:14:52,134 train 000 1.021112e-01 -23.170989
2019-11-04 19:15:12,451 train 050 1.029442e-01 -28.275560
2019-11-04 19:15:32,552 train 100 1.030481e-01 -23.783763
2019-11-04 19:15:52,666 train 150 1.022945e-01 -22.775684
2019-11-04 19:16:12,904 train 200 1.019620e-01 -21.646216
2019-11-04 19:16:32,402 train 250 1.019118e-01 -21.434579
2019-11-04 19:16:52,069 train 300 1.022335e-01 -21.621224
2019-11-04 19:17:11,971 train 350 1.022274e-01 -21.390891
2019-11-04 19:17:31,915 train 400 1.020488e-01 -21.363417
2019-11-04 19:17:44,652 training loss; R2: 1.019815e-01 -21.480896
2019-11-04 19:17:45,572 valid 000 8.558455e-02 -28.415297
2019-11-04 19:18:03,743 validation loss; R2: 8.482933e-02 -25.948512
2019-11-04 19:18:03,790 epoch 8 lr 1.000000e-03
2019-11-04 19:18:04,851 train 000 1.006641e-01 -19.933691
2019-11-04 19:18:24,654 train 050 1.012091e-01 -25.597130
2019-11-04 19:18:44,408 train 100 1.010286e-01 -26.307898
2019-11-04 19:19:04,420 train 150 1.018435e-01 -27.880848
2019-11-04 19:19:24,436 train 200 1.006859e-01 -28.560883
2019-11-04 19:19:44,325 train 250 9.897032e-02 -28.182886
2019-11-04 19:20:04,217 train 300 9.792991e-02 -27.557059
2019-11-04 19:20:24,275 train 350 9.731571e-02 -27.290464
2019-11-04 19:20:44,262 train 400 9.661523e-02 -27.322221
2019-11-04 19:20:57,123 training loss; R2: 9.618580e-02 -27.124069
2019-11-04 19:20:58,135 valid 000 8.598258e-02 -23.361162
2019-11-04 19:21:16,474 validation loss; R2: 8.482933e-02 -24.654352
2019-11-04 19:21:16,510 epoch 9 lr 1.000000e-03
2019-11-04 19:21:17,598 train 000 8.906989e-02 -20.366369
2019-11-04 19:21:37,579 train 050 9.124402e-02 -25.452916
2019-11-04 19:21:57,567 train 100 9.115123e-02 -26.035857
2019-11-04 19:22:17,522 train 150 9.093800e-02 -25.652754
2019-11-04 19:22:37,598 train 200 9.074462e-02 -25.415922
2019-11-04 19:22:57,836 train 250 9.082836e-02 -25.028368
2019-11-04 19:23:18,441 train 300 9.093916e-02 -25.130940
2019-11-04 19:23:38,911 train 350 9.100008e-02 -25.145183
2019-11-04 19:23:59,275 train 400 9.100906e-02 -25.534191
2019-11-04 19:24:12,297 training loss; R2: 9.094547e-02 -25.654467
2019-11-04 19:24:13,314 valid 000 8.590014e-02 -11.407475
2019-11-04 19:24:32,162 validation loss; R2: 8.482933e-02 -24.646891
2019-11-04 19:24:32,193 epoch 10 lr 1.000000e-03
2019-11-04 19:24:33,312 train 000 9.242946e-02 -37.937628
2019-11-04 19:24:53,320 train 050 9.086995e-02 -28.941716
2019-11-04 19:25:13,653 train 100 9.035382e-02 -26.716399
2019-11-04 19:25:34,023 train 150 9.042821e-02 -26.297582
2019-11-04 19:25:54,293 train 200 9.061540e-02 -25.708558
2019-11-04 19:26:14,527 train 250 9.079967e-02 -25.949719
2019-11-04 19:26:34,714 train 300 9.087598e-02 -25.915937
2019-11-04 19:26:54,900 train 350 9.099554e-02 -26.469387
2019-11-04 19:27:15,112 train 400 9.081492e-02 -26.160908
2019-11-04 19:27:28,112 training loss; R2: 9.077117e-02 -26.274357
2019-11-04 19:27:29,097 valid 000 8.189634e-02 -30.806736
2019-11-04 19:27:48,339 validation loss; R2: 8.482933e-02 -25.368639
2019-11-04 19:27:48,376 epoch 11 lr 1.000000e-03
2019-11-04 19:27:49,543 train 000 9.439619e-02 -11.801088
2019-11-04 19:28:10,267 train 050 9.063924e-02 -25.752785
2019-11-04 19:28:30,793 train 100 9.017200e-02 -25.465452
2019-11-04 19:28:51,305 train 150 9.012131e-02 -25.386089
2019-11-04 19:29:11,861 train 200 9.019918e-02 -25.691629
2019-11-04 19:29:32,421 train 250 9.048601e-02 -25.501584
2019-11-04 19:29:52,887 train 300 9.060774e-02 -25.352481
2019-11-04 19:30:13,449 train 350 9.055216e-02 -25.234876
2019-11-04 19:30:33,969 train 400 9.042870e-02 -25.312580
2019-11-04 19:30:46,998 training loss; R2: 9.059450e-02 -25.159383
2019-11-04 19:30:47,996 valid 000 8.088659e-02 -15.083965
2019-11-04 19:31:06,833 validation loss; R2: 8.482933e-02 -24.836333
2019-11-04 19:31:06,869 epoch 12 lr 1.000000e-03
2019-11-04 19:31:08,028 train 000 9.412359e-02 -18.919701
2019-11-04 19:31:28,410 train 050 9.312473e-02 -26.700631
2019-11-04 19:31:48,483 train 100 9.390128e-02 -27.093025
2019-11-04 19:32:08,804 train 150 9.320196e-02 -26.284683
2019-11-04 19:32:29,134 train 200 9.358081e-02 -27.031292
2019-11-04 19:32:49,409 train 250 9.365269e-02 -27.141638
2019-11-04 19:33:09,680 train 300 9.378391e-02 -27.012002
2019-11-04 19:33:29,911 train 350 9.384533e-02 -27.210992
2019-11-04 19:33:50,030 train 400 9.387515e-02 -27.109461
2019-11-04 19:34:02,918 training loss; R2: 9.402197e-02 -27.112782
2019-11-04 19:34:03,944 valid 000 8.762725e-02 -8.082305
2019-11-04 19:34:23,107 validation loss; R2: 8.482933e-02 -24.146762
2019-11-04 19:34:23,145 epoch 13 lr 1.000000e-03
2019-11-04 19:34:24,309 train 000 9.338587e-02 -39.112368
2019-11-04 19:34:44,816 train 050 9.522965e-02 -28.335108
2019-11-04 19:35:05,308 train 100 9.524229e-02 -27.723887
2019-11-04 19:35:25,757 train 150 9.505005e-02 -27.626557
2019-11-04 19:35:46,207 train 200 9.423503e-02 -26.990018
2019-11-04 19:36:06,200 train 250 9.362289e-02 -26.475639
2019-11-04 19:36:26,190 train 300 9.290311e-02 -26.608991
2019-11-04 19:36:46,291 train 350 9.243157e-02 -26.391698
2019-11-04 19:37:06,499 train 400 9.205746e-02 -26.222102
2019-11-04 19:37:19,361 training loss; R2: 9.182524e-02 -26.267763
2019-11-04 19:37:20,343 valid 000 8.714406e-02 -29.621138
2019-11-04 19:37:38,573 validation loss; R2: 8.482933e-02 -24.926411
2019-11-04 19:37:38,616 epoch 14 lr 1.000000e-03
2019-11-04 19:37:39,720 train 000 9.010340e-02 -13.731389
2019-11-04 19:37:59,813 train 050 8.880837e-02 -25.699853
2019-11-04 19:38:19,731 train 100 8.929110e-02 -25.730964
2019-11-04 19:38:39,749 train 150 8.915465e-02 -25.241725
2019-11-04 19:38:59,758 train 200 8.878364e-02 -24.930001
2019-11-04 19:39:19,882 train 250 8.852775e-02 -24.548566
2019-11-04 19:39:40,238 train 300 8.852853e-02 -24.125604
2019-11-04 19:40:00,587 train 350 8.850681e-02 -24.064920
2019-11-04 19:40:20,898 train 400 8.859327e-02 -24.237888
2019-11-04 19:40:33,871 training loss; R2: 8.855720e-02 -24.190378
2019-11-04 19:40:34,831 valid 000 8.898231e-02 -29.638384
2019-11-04 19:40:53,773 validation loss; R2: 8.482933e-02 -24.644772
2019-11-04 19:40:53,809 epoch 15 lr 1.000000e-03
2019-11-04 19:40:54,980 train 000 9.001146e-02 -7.918344
2019-11-04 19:41:15,362 train 050 8.848546e-02 -23.196602
2019-11-04 19:41:35,717 train 100 8.837371e-02 -23.493128
2019-11-04 19:41:56,071 train 150 8.840558e-02 -22.796318
2019-11-04 19:42:16,394 train 200 8.835657e-02 -22.737113
2019-11-04 19:42:36,727 train 250 8.822394e-02 -22.697866
2019-11-04 19:42:56,707 train 300 8.807916e-02 -22.951449
2019-11-04 19:43:17,042 train 350 8.806645e-02 -22.900152
2019-11-04 19:43:37,611 train 400 8.813221e-02 -23.359628
2019-11-04 19:43:50,685 training loss; R2: 8.808367e-02 -23.448848
2019-11-04 19:43:51,770 valid 000 8.283070e-02 -24.809313
2019-11-04 19:44:10,881 validation loss; R2: 8.482933e-02 -24.189998
2019-11-04 19:44:10,916 epoch 16 lr 1.000000e-03
2019-11-04 19:44:12,088 train 000 9.318146e-02 -18.000693
2019-11-04 19:44:32,640 train 050 8.772601e-02 -22.134967
2019-11-04 19:44:53,141 train 100 8.828078e-02 -22.266896
2019-11-04 19:45:13,656 train 150 8.792118e-02 -22.946682
2019-11-04 19:45:34,127 train 200 8.796424e-02 -22.990769
2019-11-04 19:45:54,649 train 250 8.783081e-02 -22.900341
2019-11-04 19:46:14,914 train 300 8.790960e-02 -23.457345
2019-11-04 19:46:34,893 train 350 8.794629e-02 -23.771945
2019-11-04 19:46:54,900 train 400 8.788424e-02 -23.939466
2019-11-04 19:47:07,649 training loss; R2: 8.792656e-02 -24.059315
2019-11-04 19:47:08,611 valid 000 9.081703e-02 -28.534288
2019-11-04 19:47:27,138 validation loss; R2: 8.482933e-02 -25.557320
2019-11-04 19:47:27,173 epoch 17 lr 1.000000e-03
2019-11-04 19:47:28,316 train 000 9.074119e-02 -18.142112
2019-11-04 19:47:48,302 train 050 8.757371e-02 -23.616351
2019-11-04 19:48:08,344 train 100 8.741266e-02 -22.405011
2019-11-04 19:48:28,393 train 150 8.776041e-02 -23.839197
2019-11-04 19:48:48,450 train 200 8.763444e-02 -23.265694
2019-11-04 19:49:08,569 train 250 8.754657e-02 -23.796187
2019-11-04 19:49:28,800 train 300 8.769381e-02 -23.455386
2019-11-04 19:49:49,272 train 350 8.776564e-02 -23.657336
2019-11-04 19:50:09,889 train 400 8.784705e-02 -23.863091
2019-11-04 19:50:22,668 training loss; R2: 8.781252e-02 -24.263352
2019-11-04 19:50:23,628 valid 000 8.485854e-02 -8.454858
2019-11-04 19:50:42,542 validation loss; R2: 8.482933e-02 -25.848189
2019-11-04 19:50:42,584 epoch 18 lr 1.000000e-03
2019-11-04 19:50:43,712 train 000 8.899756e-02 -8.258435
2019-11-04 19:51:04,170 train 050 8.751378e-02 -24.773953
2019-11-04 19:51:24,561 train 100 8.748863e-02 -25.239754
2019-11-04 19:51:44,859 train 150 8.735764e-02 -25.373961
2019-11-04 19:52:05,139 train 200 8.730484e-02 -25.061519
2019-11-04 19:52:25,365 train 250 8.718256e-02 -24.648170
2019-11-04 19:52:45,598 train 300 8.710034e-02 -24.894793
2019-11-04 19:53:05,770 train 350 8.703643e-02 -24.794479
2019-11-04 19:53:25,852 train 400 8.703843e-02 -24.554858
2019-11-04 19:53:38,701 training loss; R2: 8.702832e-02 -24.463834
2019-11-04 19:53:39,701 valid 000 8.670431e-02 -16.020658
2019-11-04 19:53:58,615 validation loss; R2: 8.482933e-02 -24.567186
2019-11-04 19:53:58,655 epoch 19 lr 1.000000e-03
2019-11-04 19:53:59,785 train 000 8.621712e-02 -25.106644
2019-11-04 19:54:20,234 train 050 8.664136e-02 -23.768612
2019-11-04 19:54:40,595 train 100 8.676299e-02 -23.957577
2019-11-04 19:55:01,019 train 150 8.709857e-02 -24.959779
2019-11-04 19:55:21,439 train 200 8.696059e-02 -24.687244
2019-11-04 19:55:41,715 train 250 8.682407e-02 -24.913820
2019-11-04 19:56:01,356 train 300 8.684100e-02 -24.584325
2019-11-04 19:56:21,316 train 350 8.698464e-02 -24.435668
2019-11-04 19:56:41,161 train 400 8.700062e-02 -24.541250
2019-11-04 19:56:53,978 training loss; R2: 8.703952e-02 -24.505205
2019-11-04 19:56:54,964 valid 000 8.105060e-02 -25.458197
2019-11-04 19:57:13,154 validation loss; R2: 8.482933e-02 -24.701695
2019-11-04 19:57:13,207 epoch 20 lr 1.000000e-03
2019-11-04 19:57:14,331 train 000 8.560534e-02 -25.775505
2019-11-04 19:57:34,242 train 050 8.702409e-02 -25.174311
2019-11-04 19:57:54,377 train 100 8.709346e-02 -24.074521
2019-11-04 19:58:14,635 train 150 8.716256e-02 -24.589224
2019-11-04 19:58:34,995 train 200 8.721439e-02 -24.100886
2019-11-04 19:58:54,965 train 250 8.704064e-02 -23.520868
2019-11-04 19:59:15,310 train 300 8.716053e-02 -24.091250
2019-11-04 19:59:35,581 train 350 8.728964e-02 -24.096145
2019-11-04 19:59:55,583 train 400 8.751831e-02 -24.159569
2019-11-04 20:00:08,415 training loss; R2: 8.761555e-02 -23.722418
2019-11-04 20:00:09,348 valid 000 7.802203e-02 -19.455866
2019-11-04 20:00:27,656 validation loss; R2: 8.482933e-02 -24.907078
2019-11-04 20:00:27,707 epoch 21 lr 1.000000e-03
2019-11-04 20:00:28,798 train 000 9.116299e-02 -16.892054
2019-11-04 20:00:48,885 train 050 8.913230e-02 -23.532535
2019-11-04 20:01:08,934 train 100 8.962922e-02 -23.387035
2019-11-04 20:01:28,886 train 150 8.958110e-02 -23.028835
2019-11-04 20:01:48,788 train 200 8.953283e-02 -22.243577
2019-11-04 20:02:08,727 train 250 8.968590e-02 -22.516850
2019-11-04 20:02:29,062 train 300 8.986989e-02 -22.386329
2019-11-04 20:02:49,537 train 350 8.987887e-02 -22.358902
2019-11-04 20:03:10,021 train 400 8.995720e-02 -22.595453
2019-11-04 20:03:23,003 training loss; R2: 8.992093e-02 -22.854746
2019-11-04 20:03:24,076 valid 000 8.629213e-02 -24.791677
2019-11-04 20:03:42,903 validation loss; R2: 8.482933e-02 -23.942088
2019-11-04 20:03:42,938 epoch 22 lr 1.000000e-03
2019-11-04 20:03:44,120 train 000 8.462095e-02 -30.293960
2019-11-04 20:04:04,498 train 050 8.757550e-02 -24.179816
2019-11-04 20:04:24,885 train 100 8.786801e-02 -23.335853
2019-11-04 20:04:45,237 train 150 8.784531e-02 -24.035293
2019-11-04 20:05:05,411 train 200 8.772561e-02 -24.017978
2019-11-04 20:05:25,599 train 250 8.790556e-02 -23.982768
2019-11-04 20:05:45,719 train 300 8.792248e-02 -23.953635
2019-11-04 20:06:05,694 train 350 8.799733e-02 -23.785425
2019-11-04 20:06:25,564 train 400 8.801693e-02 -23.533326
2019-11-04 20:06:38,378 training loss; R2: 8.805773e-02 -23.451500
2019-11-04 20:06:39,292 valid 000 8.588605e-02 -4.867508
2019-11-04 20:06:57,613 validation loss; R2: 8.482933e-02 -24.796419
2019-11-04 20:06:57,657 epoch 23 lr 1.000000e-03
2019-11-04 20:06:58,746 train 000 8.837985e-02 -11.105044
2019-11-04 20:07:18,843 train 050 8.796511e-02 -23.835787
2019-11-04 20:07:38,969 train 100 8.823687e-02 -23.450686
2019-11-04 20:07:59,114 train 150 8.802273e-02 -22.658699
2019-11-04 20:08:19,150 train 200 8.788790e-02 -22.848001
2019-11-04 20:08:39,198 train 250 8.803275e-02 -23.468555
2019-11-04 20:08:59,312 train 300 8.792591e-02 -23.174810
2019-11-04 20:09:19,853 train 350 8.798947e-02 -23.137731
2019-11-04 20:09:39,925 train 400 8.808942e-02 -22.998031
2019-11-04 20:09:52,814 training loss; R2: 8.822012e-02 -22.983388
2019-11-04 20:09:53,812 valid 000 8.331998e-02 -40.484554
2019-11-04 20:10:12,690 validation loss; R2: 8.482933e-02 -24.577335
2019-11-04 20:10:12,727 epoch 24 lr 1.000000e-03
2019-11-04 20:10:13,860 train 000 8.503035e-02 -35.362807
2019-11-04 20:10:34,526 train 050 8.915465e-02 -24.750970
2019-11-04 20:10:55,101 train 100 8.855156e-02 -23.365300
2019-11-04 20:11:15,553 train 150 8.867407e-02 -23.627227
2019-11-04 20:11:35,838 train 200 8.881241e-02 -23.280675
2019-11-04 20:11:56,376 train 250 8.875622e-02 -23.062205
2019-11-04 20:12:16,877 train 300 8.867446e-02 -23.089611
2019-11-04 20:12:37,437 train 350 8.874592e-02 -23.106401
2019-11-04 20:12:57,939 train 400 8.871012e-02 -23.044504
2019-11-04 20:13:11,005 training loss; R2: 8.866757e-02 -23.047126
2019-11-04 20:13:12,011 valid 000 8.494236e-02 -29.730777
2019-11-04 20:13:30,977 validation loss; R2: 8.482933e-02 -24.848196
2019-11-04 20:13:31,018 epoch 25 lr 1.000000e-03
2019-11-04 20:13:32,191 train 000 8.571357e-02 -11.852685
2019-11-04 20:13:52,701 train 050 8.868521e-02 -22.275762
2019-11-04 20:14:13,211 train 100 8.868166e-02 -22.879165
2019-11-04 20:14:33,654 train 150 8.887189e-02 -23.789177
2019-11-04 20:14:54,075 train 200 8.878155e-02 -23.001013
2019-11-04 20:15:14,477 train 250 8.870385e-02 -23.141160
2019-11-04 20:15:35,077 train 300 8.865540e-02 -23.297318
2019-11-04 20:15:55,497 train 350 8.859858e-02 -23.050656
2019-11-04 20:16:15,935 train 400 8.859633e-02 -22.920351
2019-11-04 20:16:28,967 training loss; R2: 8.855464e-02 -23.030108
2019-11-04 20:16:29,921 valid 000 8.255968e-02 -5.019501
2019-11-04 20:16:48,691 validation loss; R2: 8.482933e-02 -23.432633
2019-11-04 20:16:48,730 epoch 26 lr 1.000000e-03
2019-11-04 20:16:49,921 train 000 8.864050e-02 -19.089863
2019-11-04 20:17:10,347 train 050 8.791639e-02 -23.164960
2019-11-04 20:17:30,867 train 100 8.839389e-02 -23.840492
2019-11-04 20:17:51,361 train 150 8.822235e-02 -23.384347
2019-11-04 20:18:11,873 train 200 8.849068e-02 -23.329118
2019-11-04 20:18:32,370 train 250 8.849293e-02 -23.592698
2019-11-04 20:18:52,884 train 300 8.861382e-02 -23.220906
2019-11-04 20:19:13,367 train 350 8.855775e-02 -23.466282
2019-11-04 20:19:33,865 train 400 8.851593e-02 -23.408647
2019-11-04 20:19:46,933 training loss; R2: 8.849962e-02 -23.379983
2019-11-04 20:19:47,936 valid 000 8.220623e-02 -20.291464
2019-11-04 20:20:07,133 validation loss; R2: 8.482933e-02 -24.065297
2019-11-04 20:20:07,170 epoch 27 lr 1.000000e-03
2019-11-04 20:20:08,307 train 000 8.814534e-02 -17.099814
2019-11-04 20:20:28,722 train 050 8.888348e-02 -22.909698
2019-11-04 20:20:48,941 train 100 8.853766e-02 -23.051284
2019-11-04 20:21:09,277 train 150 8.849387e-02 -23.565961
2019-11-04 20:21:29,582 train 200 8.842130e-02 -23.335074
2019-11-04 20:21:49,625 train 250 8.870212e-02 -23.140997
2019-11-04 20:22:09,790 train 300 8.871637e-02 -22.998424
2019-11-04 20:22:29,987 train 350 8.883720e-02 -22.719309
2019-11-04 20:22:50,118 train 400 8.890661e-02 -22.749503
2019-11-04 20:23:03,017 training loss; R2: 8.895592e-02 -22.559634
2019-11-04 20:23:04,001 valid 000 8.303487e-02 -9.262826
2019-11-04 20:23:22,573 validation loss; R2: 8.482933e-02 -23.958400
2019-11-04 20:23:22,603 epoch 28 lr 1.000000e-03
2019-11-04 20:23:23,807 train 000 9.031922e-02 -21.437553
2019-11-04 20:23:43,888 train 050 8.924186e-02 -23.919350
2019-11-04 20:24:03,972 train 100 8.891289e-02 -24.213352
2019-11-04 20:24:24,252 train 150 8.909069e-02 -23.693489
2019-11-04 20:24:44,475 train 200 8.921520e-02 -23.472373
2019-11-04 20:25:04,664 train 250 8.937984e-02 -22.948874
2019-11-04 20:25:24,920 train 300 8.931588e-02 -22.767190
2019-11-04 20:25:45,173 train 350 8.915607e-02 -22.747522
2019-11-04 20:26:05,321 train 400 8.916356e-02 -22.646981
2019-11-04 20:26:18,180 training loss; R2: 8.906283e-02 -22.660519
2019-11-04 20:26:19,173 valid 000 8.639143e-02 -7.968034
2019-11-04 20:26:38,140 validation loss; R2: 8.482933e-02 -25.084280
2019-11-04 20:26:38,171 epoch 29 lr 1.000000e-03
2019-11-04 20:26:39,340 train 000 8.163915e-02 -36.839589
2019-11-04 20:26:59,927 train 050 8.847032e-02 -22.757964
2019-11-04 20:27:20,324 train 100 9.268193e-02 -22.501943
2019-11-04 20:27:40,818 train 150 9.360481e-02 -21.506766
2019-11-04 20:28:01,196 train 200 9.301289e-02 -21.885111
2019-11-04 20:28:21,631 train 250 9.263872e-02 -21.836599
2019-11-04 20:28:42,099 train 300 9.278380e-02 -21.520990
2019-11-04 20:29:02,629 train 350 9.314188e-02 -21.375178
2019-11-04 20:29:23,170 train 400 9.365803e-02 -21.221271
2019-11-04 20:29:36,200 training loss; R2: 9.391942e-02 -21.179875
2019-11-04 20:29:37,244 valid 000 8.280358e-02 -44.256477
2019-11-04 20:29:56,214 validation loss; R2: 8.482933e-02 -25.685067
2019-11-04 20:29:56,246 epoch 30 lr 1.000000e-03
2019-11-04 20:29:57,414 train 000 9.870364e-02 -23.266672
2019-11-04 20:30:18,019 train 050 9.875067e-02 -20.091947
2019-11-04 20:30:38,608 train 100 9.927370e-02 -19.508957
2019-11-04 20:30:59,304 train 150 9.904327e-02 -19.110630
2019-11-04 20:31:19,894 train 200 9.910750e-02 -18.981980
2019-11-04 20:31:40,364 train 250 9.931099e-02 -18.787061
