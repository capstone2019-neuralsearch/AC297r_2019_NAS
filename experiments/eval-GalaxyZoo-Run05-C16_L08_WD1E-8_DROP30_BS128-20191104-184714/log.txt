2019-11-04 18:47:14,459 gpu device = 1
2019-11-04 18:47:14,459 args = Namespace(arch='DATASET', auxiliary=False, auxiliary_weight=0.4, batch_size=128, cutout=False, cutout_length=16, data='../data', dataset='GalaxyZoo', drop_path_prob=0.3, epochs=200, fc1_size=1024, fc2_size=1024, gpu=1, grad_clip=5, init_channels=16, layers=8, learning_rate=0.0001, model_path='saved_models', momentum=0.9, optimizer='Adam', random=False, report_freq=50, save='eval-GalaxyZoo-Run05-C16_L08_WD1E-8_DROP30_BS128-20191104-184714', seed=0, val_portion=0.1, weight_decay=1e-08)
2019-11-04 18:47:17,641 param size = 1.553237MB
2019-11-04 18:47:17,645 epoch 0 lr 1.000000e-04
2019-11-04 18:47:20,755 train 000 3.921444e-02 -3.353761
2019-11-04 18:47:39,200 train 050 2.920919e-02 -0.610127
2019-11-04 18:47:58,587 train 100 2.816874e-02 -0.500477
2019-11-04 18:48:17,792 train 150 2.746305e-02 -0.455893
2019-11-04 18:48:36,868 train 200 2.700671e-02 -0.435658
2019-11-04 18:48:56,228 train 250 2.666868e-02 -0.421668
2019-11-04 18:49:15,300 train 300 2.636166e-02 -0.410558
2019-11-04 18:49:34,400 train 350 2.616545e-02 -0.403961
2019-11-04 18:49:53,476 train 400 2.598903e-02 -0.398392
2019-11-04 18:50:06,485 training loss; R2: 2.590945e-02 -0.397158
2019-11-04 18:50:07,451 valid 000 2.199072e-02 -0.425958
2019-11-04 18:50:25,239 validation loss; R2: 2.317432e-02 -0.661871
2019-11-04 18:50:25,264 epoch 1 lr 1.000000e-04
2019-11-04 18:50:26,487 train 000 2.455266e-02 -0.209650
2019-11-04 18:50:45,743 train 050 2.457117e-02 -0.239495
2019-11-04 18:51:05,970 train 100 2.453454e-02 -0.322222
2019-11-04 18:51:26,236 train 150 2.431694e-02 -0.340508
2019-11-04 18:51:45,887 train 200 2.428413e-02 -0.374076
2019-11-04 18:52:05,926 train 250 2.417699e-02 -0.396353
2019-11-04 18:52:26,512 train 300 2.412155e-02 -0.381806
2019-11-04 18:52:47,088 train 350 2.402911e-02 -0.384922
2019-11-04 18:53:07,592 train 400 2.395713e-02 -0.362990
2019-11-04 18:53:20,669 training loss; R2: 2.392807e-02 -0.354065
2019-11-04 18:53:21,772 valid 000 2.533502e-02 -2.083379
2019-11-04 18:53:40,852 validation loss; R2: 2.471464e-02 -1.143407
2019-11-04 18:53:40,879 epoch 2 lr 1.000000e-04
2019-11-04 18:53:42,024 train 000 2.318936e-02 -0.251041
2019-11-04 18:54:02,252 train 050 2.319375e-02 -0.266039
2019-11-04 18:54:22,298 train 100 2.312321e-02 -0.298644
2019-11-04 18:54:42,528 train 150 2.297837e-02 -0.276912
2019-11-04 18:55:02,586 train 200 2.291017e-02 -0.284820
2019-11-04 18:55:22,976 train 250 2.285547e-02 -0.293928
2019-11-04 18:55:43,031 train 300 2.285238e-02 -0.288719
2019-11-04 18:56:03,274 train 350 2.279194e-02 -0.308943
2019-11-04 18:56:23,737 train 400 2.277390e-02 -0.311131
2019-11-04 18:56:36,692 training loss; R2: 2.275321e-02 -0.314862
2019-11-04 18:56:37,684 valid 000 2.185518e-02 -0.437997
2019-11-04 18:56:56,664 validation loss; R2: 2.080216e-02 -1.336509
2019-11-04 18:56:56,690 epoch 3 lr 1.000000e-04
2019-11-04 18:56:57,845 train 000 2.183495e-02 0.043892
2019-11-04 18:57:18,435 train 050 2.246623e-02 -0.285040
2019-11-04 18:57:38,789 train 100 2.214403e-02 -0.366216
2019-11-04 18:57:59,125 train 150 2.209247e-02 -0.325859
2019-11-04 18:58:19,212 train 200 2.197352e-02 -0.310604
2019-11-04 18:58:38,903 train 250 2.197279e-02 -0.300643
2019-11-04 18:58:58,836 train 300 2.194595e-02 -0.291025
2019-11-04 18:59:18,763 train 350 2.192707e-02 -0.291849
2019-11-04 18:59:38,709 train 400 2.189072e-02 -0.291885
2019-11-04 18:59:51,480 training loss; R2: 2.188272e-02 -0.283526
2019-11-04 18:59:52,416 valid 000 2.080733e-02 -0.488373
2019-11-04 19:00:10,900 validation loss; R2: 2.079774e-02 -1.052470
2019-11-04 19:00:10,928 epoch 4 lr 1.000000e-04
2019-11-04 19:00:12,039 train 000 2.395296e-02 -0.276965
2019-11-04 19:00:32,123 train 050 2.156101e-02 -0.291703
2019-11-04 19:00:52,181 train 100 2.157681e-02 -0.286723
2019-11-04 19:01:12,116 train 150 2.146767e-02 -0.289356
2019-11-04 19:01:32,186 train 200 2.149140e-02 -0.289971
2019-11-04 19:01:52,697 train 250 2.142697e-02 -0.278996
2019-11-04 19:02:13,402 train 300 2.145516e-02 -0.277456
2019-11-04 19:02:33,951 train 350 2.142760e-02 -0.284446
2019-11-04 19:02:54,407 train 400 2.135618e-02 -0.290592
2019-11-04 19:03:07,434 training loss; R2: 2.132535e-02 -0.286575
2019-11-04 19:03:08,439 valid 000 2.008304e-02 -0.599997
2019-11-04 19:03:27,451 validation loss; R2: 1.959587e-02 -0.593101
2019-11-04 19:03:27,484 epoch 5 lr 1.000000e-04
2019-11-04 19:03:28,641 train 000 2.072494e-02 -0.383697
2019-11-04 19:03:48,580 train 050 2.084461e-02 -0.309088
2019-11-04 19:04:09,014 train 100 2.094387e-02 -0.295834
2019-11-04 19:04:29,025 train 150 2.095010e-02 -0.276596
2019-11-04 19:04:49,434 train 200 2.093545e-02 -0.270127
2019-11-04 19:05:09,506 train 250 2.088657e-02 -0.273746
2019-11-04 19:05:29,934 train 300 2.081249e-02 -0.278984
2019-11-04 19:05:50,092 train 350 2.075119e-02 -0.272866
2019-11-04 19:06:10,187 train 400 2.071249e-02 -0.286850
2019-11-04 19:06:23,134 training loss; R2: 2.069471e-02 -0.278726
2019-11-04 19:06:24,144 valid 000 2.047043e-02 -0.191799
2019-11-04 19:06:43,337 validation loss; R2: 1.930048e-02 -0.626612
2019-11-04 19:06:43,359 epoch 6 lr 1.000000e-04
2019-11-04 19:06:44,468 train 000 1.839931e-02 -0.358198
2019-11-04 19:07:05,086 train 050 2.022442e-02 -0.249804
2019-11-04 19:07:25,410 train 100 2.021860e-02 -0.310358
2019-11-04 19:07:45,845 train 150 2.026901e-02 -0.322696
2019-11-04 19:08:06,197 train 200 2.017864e-02 -0.294681
2019-11-04 19:08:26,614 train 250 2.014099e-02 -0.275684
2019-11-04 19:08:47,065 train 300 2.013145e-02 -0.279839
2019-11-04 19:09:07,521 train 350 2.016373e-02 -0.279788
2019-11-04 19:09:28,076 train 400 2.013821e-02 -0.281547
2019-11-04 19:09:41,141 training loss; R2: 2.014328e-02 -0.285985
2019-11-04 19:09:42,180 valid 000 2.813381e-02 -0.254758
2019-11-04 19:10:01,148 validation loss; R2: 2.386939e-02 -1.883193
2019-11-04 19:10:01,180 epoch 7 lr 1.000000e-04
2019-11-04 19:10:02,376 train 000 2.236244e-02 -0.221947
2019-11-04 19:10:22,871 train 050 1.961792e-02 -0.299444
2019-11-04 19:10:43,091 train 100 1.982922e-02 -0.312002
2019-11-04 19:11:03,431 train 150 1.984974e-02 -0.397851
2019-11-04 19:11:23,616 train 200 1.979671e-02 -0.360546
2019-11-04 19:11:43,842 train 250 1.979117e-02 -0.351426
2019-11-04 19:12:04,125 train 300 1.973848e-02 -0.337491
2019-11-04 19:12:24,515 train 350 1.969482e-02 -0.320410
2019-11-04 19:12:44,906 train 400 1.962988e-02 -0.318166
2019-11-04 19:12:57,914 training loss; R2: 1.962729e-02 -0.315582
2019-11-04 19:12:58,917 valid 000 1.710098e-02 -0.719075
2019-11-04 19:13:17,926 validation loss; R2: 1.895273e-02 -1.062147
2019-11-04 19:13:17,955 epoch 8 lr 1.000000e-04
2019-11-04 19:13:19,108 train 000 1.818511e-02 -0.033479
2019-11-04 19:13:39,369 train 050 1.945263e-02 -0.247321
2019-11-04 19:13:59,570 train 100 1.910915e-02 -0.260147
2019-11-04 19:14:19,811 train 150 1.901092e-02 -0.294375
2019-11-04 19:14:39,975 train 200 1.903024e-02 -0.279220
2019-11-04 19:15:00,195 train 250 1.908773e-02 -0.266350
2019-11-04 19:15:20,490 train 300 1.912756e-02 -0.260490
2019-11-04 19:15:40,598 train 350 1.911718e-02 -0.260587
2019-11-04 19:16:00,715 train 400 1.907062e-02 -0.263778
2019-11-04 19:16:13,605 training loss; R2: 1.905448e-02 -0.276501
2019-11-04 19:16:14,603 valid 000 1.913413e-02 -2.002649
2019-11-04 19:16:33,906 validation loss; R2: 1.867564e-02 -1.120551
2019-11-04 19:16:33,935 epoch 9 lr 1.000000e-04
2019-11-04 19:16:35,048 train 000 1.822780e-02 -0.014968
2019-11-04 19:16:55,899 train 050 1.901260e-02 -0.217691
2019-11-04 19:17:16,552 train 100 1.896134e-02 -0.259567
2019-11-04 19:17:37,117 train 150 1.891173e-02 -0.316530
2019-11-04 19:17:57,678 train 200 1.893148e-02 -0.300955
2019-11-04 19:18:18,230 train 250 1.880566e-02 -0.300528
2019-11-04 19:18:38,783 train 300 1.872540e-02 -0.299541
2019-11-04 19:18:59,312 train 350 1.871225e-02 -0.312919
2019-11-04 19:19:19,828 train 400 1.871230e-02 -0.307903
2019-11-04 19:19:32,939 training loss; R2: 1.866772e-02 -0.310308
2019-11-04 19:19:33,960 valid 000 1.735332e-02 -0.304102
2019-11-04 19:19:53,203 validation loss; R2: 1.743767e-02 -0.887966
2019-11-04 19:19:53,231 epoch 10 lr 1.000000e-04
2019-11-04 19:19:54,391 train 000 2.060056e-02 -0.801925
2019-11-04 19:20:14,943 train 050 1.847499e-02 -0.508687
2019-11-04 19:20:35,438 train 100 1.849077e-02 -0.401552
2019-11-04 19:20:55,943 train 150 1.847583e-02 -0.353992
2019-11-04 19:21:16,314 train 200 1.847301e-02 -0.335743
2019-11-04 19:21:36,876 train 250 1.844218e-02 -0.320834
2019-11-04 19:21:57,452 train 300 1.845021e-02 -0.310297
2019-11-04 19:22:18,018 train 350 1.842436e-02 -0.311941
2019-11-04 19:22:38,499 train 400 1.840361e-02 -0.301769
2019-11-04 19:22:51,521 training loss; R2: 1.837197e-02 -0.299139
2019-11-04 19:22:52,545 valid 000 1.653675e-02 -1.178939
2019-11-04 19:23:11,534 validation loss; R2: 1.879752e-02 -1.835229
2019-11-04 19:23:11,562 epoch 11 lr 1.000000e-04
2019-11-04 19:23:12,739 train 000 1.596336e-02 -0.513981
2019-11-04 19:23:32,966 train 050 1.818595e-02 -0.291083
2019-11-04 19:23:53,237 train 100 1.818882e-02 -0.348029
2019-11-04 19:24:13,465 train 150 1.819161e-02 -0.311017
2019-11-04 19:24:33,454 train 200 1.814815e-02 -0.299430
2019-11-04 19:24:53,728 train 250 1.811185e-02 -0.301557
2019-11-04 19:25:14,116 train 300 1.809139e-02 -0.319951
2019-11-04 19:25:34,267 train 350 1.810636e-02 -0.313460
2019-11-04 19:25:54,392 train 400 1.812078e-02 -0.319774
2019-11-04 19:26:07,415 training loss; R2: 1.812387e-02 -0.322860
2019-11-04 19:26:08,467 valid 000 2.041891e-02 -1.101451
2019-11-04 19:26:27,294 validation loss; R2: 1.837959e-02 -1.201790
2019-11-04 19:26:27,322 epoch 12 lr 1.000000e-04
2019-11-04 19:26:28,489 train 000 1.753833e-02 -0.007056
2019-11-04 19:26:49,119 train 050 1.753181e-02 -0.162385
2019-11-04 19:27:09,622 train 100 1.772925e-02 -0.222345
2019-11-04 19:27:29,984 train 150 1.774994e-02 -0.271265
2019-11-04 19:27:49,545 train 200 1.777637e-02 -0.263857
2019-11-04 19:28:09,189 train 250 1.781458e-02 -0.278665
2019-11-04 19:28:29,228 train 300 1.778479e-02 -0.326491
2019-11-04 19:28:49,234 train 350 1.781755e-02 -0.320018
2019-11-04 19:29:09,277 train 400 1.783290e-02 -0.319718
2019-11-04 19:29:21,988 training loss; R2: 1.782177e-02 -0.322810
2019-11-04 19:29:22,924 valid 000 1.841346e-02 -1.197517
2019-11-04 19:29:41,265 validation loss; R2: 1.829935e-02 -1.579772
2019-11-04 19:29:41,294 epoch 13 lr 1.000000e-04
2019-11-04 19:29:42,429 train 000 1.662675e-02 -0.136562
2019-11-04 19:30:02,430 train 050 1.773159e-02 -0.348261
2019-11-04 19:30:22,443 train 100 1.770128e-02 -0.310297
2019-11-04 19:30:42,498 train 150 1.765033e-02 -0.304169
2019-11-04 19:31:02,948 train 200 1.765387e-02 -0.313440
2019-11-04 19:31:23,392 train 250 1.761405e-02 -0.289745
2019-11-04 19:31:43,873 train 300 1.762727e-02 -0.347222
2019-11-04 19:32:04,201 train 350 1.757970e-02 -0.361712
2019-11-04 19:32:24,506 train 400 1.757317e-02 -0.349500
2019-11-04 19:32:37,457 training loss; R2: 1.759757e-02 -0.347332
2019-11-04 19:32:38,518 valid 000 1.759249e-02 0.109294
2019-11-04 19:32:57,154 validation loss; R2: 1.665112e-02 -0.788793
2019-11-04 19:32:57,182 epoch 14 lr 1.000000e-04
2019-11-04 19:32:58,376 train 000 1.817537e-02 -0.562430
2019-11-04 19:33:18,662 train 050 1.721908e-02 -0.251545
2019-11-04 19:33:38,814 train 100 1.727578e-02 -0.258561
2019-11-04 19:33:59,178 train 150 1.733109e-02 -0.287234
2019-11-04 19:34:19,219 train 200 1.741580e-02 -0.306202
2019-11-04 19:34:39,646 train 250 1.741216e-02 -0.328301
2019-11-04 19:34:59,922 train 300 1.734040e-02 -0.319827
2019-11-04 19:35:19,977 train 350 1.735056e-02 -0.324953
2019-11-04 19:35:40,225 train 400 1.740076e-02 -0.320233
2019-11-04 19:35:53,167 training loss; R2: 1.738401e-02 -0.311887
2019-11-04 19:35:54,212 valid 000 1.620106e-02 -0.073441
2019-11-04 19:36:13,196 validation loss; R2: 1.511671e-02 -0.435288
2019-11-04 19:36:13,223 epoch 15 lr 1.000000e-04
2019-11-04 19:36:14,330 train 000 1.678153e-02 -0.058844
2019-11-04 19:36:34,957 train 050 1.740893e-02 -0.351825
2019-11-04 19:36:55,564 train 100 1.738340e-02 -0.336398
2019-11-04 19:37:16,065 train 150 1.735049e-02 -0.323548
2019-11-04 19:37:36,540 train 200 1.733018e-02 -0.306573
2019-11-04 19:37:57,263 train 250 1.734121e-02 -0.308197
2019-11-04 19:38:17,814 train 300 1.729528e-02 -0.313957
2019-11-04 19:38:38,329 train 350 1.727323e-02 -0.308100
2019-11-04 19:38:58,869 train 400 1.724978e-02 -0.312003
2019-11-04 19:39:11,983 training loss; R2: 1.724298e-02 -0.329658
2019-11-04 19:39:13,037 valid 000 1.533285e-02 -2.697879
2019-11-04 19:39:32,237 validation loss; R2: 1.552766e-02 -0.666586
2019-11-04 19:39:32,266 epoch 16 lr 1.000000e-04
2019-11-04 19:39:33,457 train 000 1.500667e-02 -0.158472
2019-11-04 19:39:53,883 train 050 1.742730e-02 -0.365082
2019-11-04 19:40:14,155 train 100 1.746007e-02 -0.304843
2019-11-04 19:40:34,458 train 150 1.736214e-02 -0.329583
2019-11-04 19:40:54,552 train 200 1.732768e-02 -0.341333
2019-11-04 19:41:14,883 train 250 1.731062e-02 -0.333562
2019-11-04 19:41:35,247 train 300 1.725794e-02 -0.336211
2019-11-04 19:41:55,614 train 350 1.724321e-02 -0.340188
2019-11-04 19:42:15,928 train 400 1.719282e-02 -0.351645
2019-11-04 19:42:28,943 training loss; R2: 1.719113e-02 -0.345879
2019-11-04 19:42:30,049 valid 000 1.648341e-02 -7.190414
2019-11-04 19:42:48,923 validation loss; R2: 1.701120e-02 -1.623232
2019-11-04 19:42:48,949 epoch 17 lr 1.000000e-04
2019-11-04 19:42:50,131 train 000 1.768868e-02 -0.124184
2019-11-04 19:43:10,863 train 050 1.711713e-02 -0.194827
2019-11-04 19:43:31,199 train 100 1.716565e-02 -0.231641
2019-11-04 19:43:51,600 train 150 1.707482e-02 -0.287087
2019-11-04 19:44:11,875 train 200 1.701406e-02 -0.311469
2019-11-04 19:44:32,304 train 250 1.702755e-02 -0.287578
2019-11-04 19:44:52,678 train 300 1.699923e-02 -0.283496
2019-11-04 19:45:13,073 train 350 1.700313e-02 -0.283286
2019-11-04 19:45:33,591 train 400 1.699140e-02 -0.282324
2019-11-04 19:45:46,711 training loss; R2: 1.701862e-02 -0.286142
2019-11-04 19:45:47,726 valid 000 1.820907e-02 -1.260442
2019-11-04 19:46:06,774 validation loss; R2: 1.732779e-02 -1.185037
2019-11-04 19:46:06,806 epoch 18 lr 1.000000e-04
2019-11-04 19:46:07,976 train 000 1.756238e-02 -0.265795
2019-11-04 19:46:29,093 train 050 1.702015e-02 -0.274617
2019-11-04 19:46:49,984 train 100 1.705785e-02 -0.251947
2019-11-04 19:47:10,715 train 150 1.712406e-02 -0.289763
2019-11-04 19:47:31,748 train 200 1.718752e-02 -0.282446
2019-11-04 19:47:52,678 train 250 1.721925e-02 -0.327890
2019-11-04 19:48:13,369 train 300 1.723607e-02 -0.314618
2019-11-04 19:48:33,880 train 350 1.722461e-02 -0.320649
2019-11-04 19:48:54,422 train 400 1.721808e-02 -0.325586
2019-11-04 19:49:07,707 training loss; R2: 1.723177e-02 -0.330901
2019-11-04 19:49:08,785 valid 000 1.801147e-02 -1.220275
2019-11-04 19:49:27,474 validation loss; R2: 1.540483e-02 -0.812755
2019-11-04 19:49:27,502 epoch 19 lr 1.000000e-04
2019-11-04 19:49:28,709 train 000 1.961907e-02 -0.299064
2019-11-04 19:49:48,846 train 050 1.760080e-02 -0.527721
2019-11-04 19:50:09,206 train 100 1.754149e-02 -0.403182
2019-11-04 19:50:29,472 train 150 1.756592e-02 -0.433823
2019-11-04 19:50:49,556 train 200 1.757461e-02 -0.381840
2019-11-04 19:51:09,908 train 250 1.752627e-02 -0.362710
2019-11-04 19:51:30,238 train 300 1.750047e-02 -0.353517
2019-11-04 19:51:50,436 train 350 1.755467e-02 -0.354081
2019-11-04 19:52:10,561 train 400 1.750516e-02 -0.336427
2019-11-04 19:52:23,344 training loss; R2: 1.748931e-02 -0.335582
2019-11-04 19:52:24,391 valid 000 1.553133e-02 -0.038671
2019-11-04 19:52:43,728 validation loss; R2: 1.506102e-02 -0.625495
2019-11-04 19:52:43,755 epoch 20 lr 1.000000e-04
2019-11-04 19:52:44,949 train 000 1.734898e-02 -0.189852
2019-11-04 19:53:05,221 train 050 1.751404e-02 -0.516224
2019-11-04 19:53:25,499 train 100 1.734831e-02 -0.419169
2019-11-04 19:53:45,796 train 150 1.743751e-02 -0.359100
2019-11-04 19:54:05,635 train 200 1.739377e-02 -0.346463
2019-11-04 19:54:26,041 train 250 1.742724e-02 -0.333800
2019-11-04 19:54:45,907 train 300 1.737676e-02 -0.344257
2019-11-04 19:55:06,272 train 350 1.735235e-02 -0.348770
2019-11-04 19:55:26,479 train 400 1.730281e-02 -0.343079
2019-11-04 19:55:39,412 training loss; R2: 1.728127e-02 -0.335339
2019-11-04 19:55:40,449 valid 000 1.434576e-02 -1.341551
2019-11-04 19:55:59,655 validation loss; R2: 1.503252e-02 -0.491591
2019-11-04 19:55:59,693 epoch 21 lr 1.000000e-04
2019-11-04 19:56:00,867 train 000 1.481233e-02 -0.300876
2019-11-04 19:56:21,473 train 050 1.707785e-02 -0.498210
2019-11-04 19:56:42,217 train 100 1.717232e-02 -0.421226
2019-11-04 19:57:02,761 train 150 1.713354e-02 -0.372559
2019-11-04 19:57:23,342 train 200 1.715212e-02 -0.340819
2019-11-04 19:57:43,915 train 250 1.709899e-02 -0.323881
2019-11-04 19:58:04,443 train 300 1.708963e-02 -0.313676
2019-11-04 19:58:24,965 train 350 1.703007e-02 -0.299292
2019-11-04 19:58:45,432 train 400 1.700722e-02 -0.306398
2019-11-04 19:58:58,574 training loss; R2: 1.700924e-02 -0.312147
2019-11-04 19:58:59,627 valid 000 1.446007e-02 -0.280814
2019-11-04 19:59:18,758 validation loss; R2: 1.477862e-02 -0.630338
2019-11-04 19:59:18,787 epoch 22 lr 1.000000e-04
2019-11-04 19:59:19,979 train 000 1.658832e-02 0.073017
2019-11-04 19:59:40,629 train 050 1.680076e-02 -0.323768
2019-11-04 20:00:01,319 train 100 1.676752e-02 -0.351244
2019-11-04 20:00:21,821 train 150 1.674620e-02 -0.351056
2019-11-04 20:00:42,377 train 200 1.679736e-02 -0.310983
2019-11-04 20:01:02,910 train 250 1.674672e-02 -0.305538
2019-11-04 20:01:23,509 train 300 1.671398e-02 -0.301493
2019-11-04 20:01:44,126 train 350 1.669583e-02 -0.330737
2019-11-04 20:02:04,725 train 400 1.669473e-02 -0.323106
2019-11-04 20:02:17,883 training loss; R2: 1.671897e-02 -0.322578
2019-11-04 20:02:18,964 valid 000 1.369562e-02 -0.077058
2019-11-04 20:02:38,044 validation loss; R2: 1.433636e-02 -0.459228
2019-11-04 20:02:38,073 epoch 23 lr 1.000000e-04
2019-11-04 20:02:39,261 train 000 1.512944e-02 -0.067554
2019-11-04 20:02:59,633 train 050 1.659200e-02 -0.349661
2019-11-04 20:03:19,750 train 100 1.672434e-02 -0.368771
2019-11-04 20:03:40,053 train 150 1.671139e-02 -0.366071
2019-11-04 20:04:00,280 train 200 1.664970e-02 -0.364795
2019-11-04 20:04:20,628 train 250 1.664143e-02 -0.351601
2019-11-04 20:04:40,874 train 300 1.663441e-02 -0.328149
2019-11-04 20:05:01,057 train 350 1.663773e-02 -0.332187
2019-11-04 20:05:21,251 train 400 1.663429e-02 -0.342302
2019-11-04 20:05:34,130 training loss; R2: 1.660084e-02 -0.339352
2019-11-04 20:05:35,132 valid 000 1.378419e-02 -0.337523
2019-11-04 20:05:53,783 validation loss; R2: 1.438676e-02 -0.614586
2019-11-04 20:05:53,810 epoch 24 lr 1.000000e-04
2019-11-04 20:05:55,024 train 000 1.670047e-02 -0.418779
2019-11-04 20:06:15,819 train 050 1.674625e-02 -0.346734
2019-11-04 20:06:36,456 train 100 1.653426e-02 -0.339849
2019-11-04 20:06:56,968 train 150 1.649497e-02 -0.335332
2019-11-04 20:07:17,441 train 200 1.648186e-02 -0.354200
2019-11-04 20:07:37,782 train 250 1.648236e-02 -0.347848
2019-11-04 20:07:58,380 train 300 1.642538e-02 -0.356575
2019-11-04 20:08:18,804 train 350 1.644303e-02 -0.339399
2019-11-04 20:08:39,517 train 400 1.646382e-02 -0.339902
2019-11-04 20:08:52,623 training loss; R2: 1.645181e-02 -0.347805
2019-11-04 20:08:53,658 valid 000 1.319484e-02 -0.853089
2019-11-04 20:09:12,483 validation loss; R2: 1.479152e-02 -0.888882
2019-11-04 20:09:12,510 epoch 25 lr 1.000000e-04
2019-11-04 20:09:13,687 train 000 1.775294e-02 -0.241098
2019-11-04 20:09:34,422 train 050 1.618194e-02 -0.311764
2019-11-04 20:09:54,836 train 100 1.626707e-02 -0.317500
2019-11-04 20:10:15,144 train 150 1.622685e-02 -0.307667
2019-11-04 20:10:35,370 train 200 1.630443e-02 -0.296855
2019-11-04 20:10:55,480 train 250 1.630084e-02 -0.348753
2019-11-04 20:11:15,456 train 300 1.632163e-02 -0.351810
2019-11-04 20:11:35,504 train 350 1.633061e-02 -0.338790
2019-11-04 20:11:55,526 train 400 1.633645e-02 -0.358082
2019-11-04 20:12:08,215 training loss; R2: 1.632999e-02 -0.361669
2019-11-04 20:12:09,223 valid 000 1.498114e-02 0.117714
2019-11-04 20:12:27,610 validation loss; R2: 1.467649e-02 -0.914952
2019-11-04 20:12:27,644 epoch 26 lr 1.000000e-04
2019-11-04 20:12:28,776 train 000 1.852199e-02 -0.988733
2019-11-04 20:12:48,961 train 050 1.621454e-02 -0.250986
2019-11-04 20:13:09,038 train 100 1.617025e-02 -0.282014
2019-11-04 20:13:28,772 train 150 1.618978e-02 -0.395005
2019-11-04 20:13:48,734 train 200 1.618646e-02 -0.380676
2019-11-04 20:14:08,743 train 250 1.615971e-02 -0.363646
2019-11-04 20:14:28,709 train 300 1.614937e-02 -0.356931
2019-11-04 20:14:48,734 train 350 1.619528e-02 -0.339876
2019-11-04 20:15:08,684 train 400 1.620466e-02 -0.335871
2019-11-04 20:15:21,443 training loss; R2: 1.620198e-02 -0.339006
2019-11-04 20:15:22,391 valid 000 1.468542e-02 -0.091216
2019-11-04 20:15:40,678 validation loss; R2: 1.320932e-02 -0.623254
2019-11-04 20:15:40,705 epoch 27 lr 1.000000e-04
2019-11-04 20:15:41,825 train 000 1.552744e-02 -0.756894
2019-11-04 20:16:01,852 train 050 1.599351e-02 -0.280365
2019-11-04 20:16:21,902 train 100 1.613391e-02 -0.291667
2019-11-04 20:16:41,787 train 150 1.611691e-02 -0.309439
2019-11-04 20:17:01,922 train 200 1.611256e-02 -0.305351
2019-11-04 20:17:21,979 train 250 1.614441e-02 -0.310439
2019-11-04 20:17:42,005 train 300 1.616134e-02 -0.310506
2019-11-04 20:18:02,052 train 350 1.609998e-02 -0.302592
2019-11-04 20:18:22,142 train 400 1.607158e-02 -0.330898
2019-11-04 20:18:34,957 training loss; R2: 1.607888e-02 -0.323685
2019-11-04 20:18:35,897 valid 000 1.371438e-02 -2.760906
2019-11-04 20:18:54,321 validation loss; R2: 1.351794e-02 -0.758239
2019-11-04 20:18:54,349 epoch 28 lr 1.000000e-04
2019-11-04 20:18:55,465 train 000 1.620581e-02 -0.063509
2019-11-04 20:19:15,741 train 050 1.588924e-02 -0.301238
2019-11-04 20:19:35,956 train 100 1.596650e-02 -0.303884
2019-11-04 20:19:56,334 train 150 1.589972e-02 -0.301063
2019-11-04 20:20:16,749 train 200 1.594525e-02 -0.262417
2019-11-04 20:20:37,224 train 250 1.595097e-02 -0.273158
2019-11-04 20:20:57,761 train 300 1.597345e-02 -0.274200
2019-11-04 20:21:18,243 train 350 1.597683e-02 -0.289373
2019-11-04 20:21:38,332 train 400 1.595768e-02 -0.300641
2019-11-04 20:21:51,284 training loss; R2: 1.593660e-02 -0.305890
2019-11-04 20:21:52,342 valid 000 1.349644e-02 -1.527140
2019-11-04 20:22:11,239 validation loss; R2: 1.453687e-02 -1.317480
2019-11-04 20:22:11,266 epoch 29 lr 1.000000e-04
2019-11-04 20:22:12,417 train 000 1.654080e-02 -0.259971
2019-11-04 20:22:32,778 train 050 1.598464e-02 -0.318160
2019-11-04 20:22:52,988 train 100 1.588381e-02 -0.310035
2019-11-04 20:23:13,117 train 150 1.592354e-02 -0.303763
2019-11-04 20:23:33,446 train 200 1.594483e-02 -0.275383
2019-11-04 20:23:53,817 train 250 1.587964e-02 -0.262856
2019-11-04 20:24:14,156 train 300 1.590498e-02 -0.276319
2019-11-04 20:24:34,595 train 350 1.587627e-02 -0.285167
2019-11-04 20:24:55,007 train 400 1.585455e-02 -0.296851
2019-11-04 20:25:07,973 training loss; R2: 1.582253e-02 -0.306458
2019-11-04 20:25:08,990 valid 000 1.238860e-02 0.085559
2019-11-04 20:25:27,965 validation loss; R2: 1.337133e-02 -0.894333
2019-11-04 20:25:28,005 epoch 30 lr 1.000000e-04
2019-11-04 20:25:29,137 train 000 1.721522e-02 -0.051945
2019-11-04 20:25:49,591 train 050 1.587331e-02 -0.359613
2019-11-04 20:26:10,014 train 100 1.572547e-02 -0.365017
2019-11-04 20:26:30,034 train 150 1.565138e-02 -0.340055
2019-11-04 20:26:49,776 train 200 1.567490e-02 -0.308552
2019-11-04 20:27:09,770 train 250 1.562881e-02 -0.302179
2019-11-04 20:27:29,929 train 300 1.565411e-02 -0.303232
2019-11-04 20:27:50,276 train 350 1.566089e-02 -0.305247
2019-11-04 20:28:10,367 train 400 1.570309e-02 -0.305567
2019-11-04 20:28:23,163 training loss; R2: 1.569329e-02 -0.299536
2019-11-04 20:28:24,096 valid 000 1.384201e-02 0.065507
2019-11-04 20:28:42,529 validation loss; R2: 1.330139e-02 -1.109887
2019-11-04 20:28:42,555 epoch 31 lr 1.000000e-04
2019-11-04 20:28:43,632 train 000 1.458809e-02 -0.951907
2019-11-04 20:29:03,731 train 050 1.559977e-02 -0.289125
2019-11-04 20:29:23,647 train 100 1.564527e-02 -0.290709
2019-11-04 20:29:43,899 train 150 1.568178e-02 -0.304630
2019-11-04 20:30:04,137 train 200 1.566660e-02 -0.296666
2019-11-04 20:30:24,125 train 250 1.565438e-02 -0.290465
2019-11-04 20:30:44,064 train 300 1.570442e-02 -0.312609
2019-11-04 20:31:04,142 train 350 1.567748e-02 -0.321294
2019-11-04 20:31:24,135 train 400 1.568468e-02 -0.323676
2019-11-04 20:31:36,893 training loss; R2: 1.569188e-02 -0.323916
2019-11-04 20:31:37,924 valid 000 1.312654e-02 -1.548922
2019-11-04 20:31:56,340 validation loss; R2: 1.308995e-02 -0.523339
2019-11-04 20:31:56,375 epoch 32 lr 1.000000e-04
2019-11-04 20:31:57,485 train 000 1.407714e-02 -0.482322
2019-11-04 20:32:17,155 train 050 1.577508e-02 -0.472308
2019-11-04 20:32:36,504 train 100 1.567450e-02 -0.331853
2019-11-04 20:32:55,624 train 150 1.565942e-02 -0.309944
2019-11-04 20:33:14,984 train 200 1.559406e-02 -0.304921
2019-11-04 20:33:34,108 train 250 1.563972e-02 -0.323798
2019-11-04 20:33:53,198 train 300 1.559041e-02 -0.316840
2019-11-04 20:34:12,268 train 350 1.564484e-02 -0.300124
2019-11-04 20:34:31,374 train 400 1.563273e-02 -0.294815
2019-11-04 20:34:43,601 training loss; R2: 1.562455e-02 -0.299065
2019-11-04 20:34:44,524 valid 000 1.449241e-02 0.065529
2019-11-04 20:35:02,351 validation loss; R2: 1.340125e-02 -0.846803
2019-11-04 20:35:02,380 epoch 33 lr 1.000000e-04
2019-11-04 20:35:03,539 train 000 1.725446e-02 0.045388
2019-11-04 20:35:22,431 train 050 1.519058e-02 -0.362020
2019-11-04 20:35:41,596 train 100 1.526179e-02 -0.335973
2019-11-04 20:36:01,202 train 150 1.544021e-02 -0.351397
2019-11-04 20:36:20,541 train 200 1.551363e-02 -0.372520
2019-11-04 20:36:39,704 train 250 1.554991e-02 -0.350710
2019-11-04 20:36:59,036 train 300 1.553845e-02 -0.340172
2019-11-04 20:37:18,620 train 350 1.552371e-02 -0.332724
2019-11-04 20:37:37,875 train 400 1.548603e-02 -0.327845
2019-11-04 20:37:50,081 training loss; R2: 1.548717e-02 -0.319365
2019-11-04 20:37:51,055 valid 000 1.572223e-02 -0.329163
2019-11-04 20:38:08,879 validation loss; R2: 1.319929e-02 -0.710785
2019-11-04 20:38:08,907 epoch 34 lr 1.000000e-04
2019-11-04 20:38:10,042 train 000 1.569276e-02 -0.532346
2019-11-04 20:38:29,314 train 050 1.565535e-02 -0.283408
2019-11-04 20:38:48,637 train 100 1.560131e-02 -0.220573
2019-11-04 20:39:07,866 train 150 1.548367e-02 -0.294011
2019-11-04 20:39:27,141 train 200 1.548005e-02 -0.261206
2019-11-04 20:39:46,220 train 250 1.541412e-02 -0.261472
2019-11-04 20:40:05,334 train 300 1.544142e-02 -0.285699
2019-11-04 20:40:24,349 train 350 1.542898e-02 -0.311387
2019-11-04 20:40:43,499 train 400 1.543600e-02 -0.309326
2019-11-04 20:40:55,804 training loss; R2: 1.544218e-02 -0.308690
2019-11-04 20:40:56,838 valid 000 1.220532e-02 -1.734108
2019-11-04 20:41:15,459 validation loss; R2: 1.329197e-02 -0.831336
2019-11-04 20:41:15,488 epoch 35 lr 1.000000e-04
2019-11-04 20:41:16,587 train 000 1.589903e-02 0.096051
2019-11-04 20:41:36,100 train 050 1.523420e-02 -0.422575
2019-11-04 20:41:55,512 train 100 1.535222e-02 -0.336168
2019-11-04 20:42:14,633 train 150 1.538117e-02 -0.359190
2019-11-04 20:42:33,423 train 200 1.544313e-02 -0.349541
2019-11-04 20:42:52,214 train 250 1.539736e-02 -0.341319
2019-11-04 20:43:11,077 train 300 1.540020e-02 -0.348164
2019-11-04 20:43:30,005 train 350 1.537749e-02 -0.361193
2019-11-04 20:43:49,082 train 400 1.537058e-02 -0.343176
2019-11-04 20:44:01,167 training loss; R2: 1.536576e-02 -0.330927
2019-11-04 20:44:02,163 valid 000 1.331054e-02 -0.372517
2019-11-04 20:44:20,169 validation loss; R2: 1.290142e-02 -0.540614
2019-11-04 20:44:20,196 epoch 36 lr 1.000000e-04
2019-11-04 20:44:21,348 train 000 1.727791e-02 0.038611
2019-11-04 20:44:40,723 train 050 1.533895e-02 -0.500990
2019-11-04 20:45:00,133 train 100 1.540079e-02 -0.386014
2019-11-04 20:45:19,608 train 150 1.530813e-02 -0.382886
2019-11-04 20:45:38,830 train 200 1.531991e-02 -0.382046
2019-11-04 20:45:58,038 train 250 1.528839e-02 -0.355531
2019-11-04 20:46:17,210 train 300 1.526131e-02 -0.342183
2019-11-04 20:46:36,479 train 350 1.528207e-02 -0.341642
2019-11-04 20:46:55,885 train 400 1.530943e-02 -0.326776
2019-11-04 20:47:08,233 training loss; R2: 1.529040e-02 -0.323061
2019-11-04 20:47:09,300 valid 000 1.201656e-02 0.008775
2019-11-04 20:47:27,279 validation loss; R2: 1.290225e-02 -0.712408
2019-11-04 20:47:27,305 epoch 37 lr 1.000000e-04
2019-11-04 20:47:28,502 train 000 1.396033e-02 -0.201255
2019-11-04 20:47:47,921 train 050 1.541587e-02 -0.268179
2019-11-04 20:48:07,093 train 100 1.526148e-02 -0.293795
2019-11-04 20:48:26,466 train 150 1.526800e-02 -0.346306
2019-11-04 20:48:45,772 train 200 1.522967e-02 -0.319829
2019-11-04 20:49:04,901 train 250 1.527156e-02 -0.304614
2019-11-04 20:49:24,237 train 300 1.525901e-02 -0.311560
2019-11-04 20:49:44,167 train 350 1.523942e-02 -0.299345
2019-11-04 20:50:04,893 train 400 1.523907e-02 -0.295697
2019-11-04 20:50:18,174 training loss; R2: 1.522674e-02 -0.302061
2019-11-04 20:50:19,226 valid 000 1.212922e-02 -0.234438
2019-11-04 20:50:38,235 validation loss; R2: 1.283201e-02 -0.998110
2019-11-04 20:50:38,273 epoch 38 lr 1.000000e-04
2019-11-04 20:50:39,391 train 000 1.632193e-02 0.032595
2019-11-04 20:51:00,210 train 050 1.486212e-02 -0.239735
2019-11-04 20:51:20,890 train 100 1.495290e-02 -0.250919
2019-11-04 20:51:41,568 train 150 1.509232e-02 -0.292320
2019-11-04 20:52:02,124 train 200 1.504301e-02 -0.311969
2019-11-04 20:52:22,604 train 250 1.505639e-02 -0.306924
2019-11-04 20:52:42,702 train 300 1.507222e-02 -0.321942
2019-11-04 20:53:02,502 train 350 1.509540e-02 -0.346763
2019-11-04 20:53:22,666 train 400 1.510193e-02 -0.338307
2019-11-04 20:53:35,616 training loss; R2: 1.514568e-02 -0.331781
2019-11-04 20:53:36,640 valid 000 1.214714e-02 -1.274126
2019-11-04 20:53:55,240 validation loss; R2: 1.254146e-02 -0.672054
2019-11-04 20:53:55,266 epoch 39 lr 1.000000e-04
2019-11-04 20:53:56,430 train 000 1.373093e-02 -0.460021
2019-11-04 20:54:16,727 train 050 1.521946e-02 -0.368213
2019-11-04 20:54:36,925 train 100 1.526722e-02 -0.389710
2019-11-04 20:54:57,146 train 150 1.515603e-02 -0.384474
2019-11-04 20:55:17,408 train 200 1.503427e-02 -0.385846
2019-11-04 20:55:37,634 train 250 1.501514e-02 -0.360733
2019-11-04 20:55:57,905 train 300 1.507773e-02 -0.345639
2019-11-04 20:56:18,120 train 350 1.505649e-02 -0.343481
2019-11-04 20:56:38,403 train 400 1.506473e-02 -0.341550
2019-11-04 20:56:51,414 training loss; R2: 1.506845e-02 -0.338683
2019-11-04 20:56:52,366 valid 000 1.228590e-02 0.042270
2019-11-04 20:57:11,022 validation loss; R2: 1.257917e-02 -0.537420
2019-11-04 20:57:11,055 epoch 40 lr 1.000000e-04
2019-11-04 20:57:12,173 train 000 1.364206e-02 0.049489
2019-11-04 20:57:32,523 train 050 1.516489e-02 -0.407399
2019-11-04 20:57:52,818 train 100 1.518501e-02 -0.356977
2019-11-04 20:58:13,076 train 150 1.511523e-02 -0.345030
2019-11-04 20:58:33,283 train 200 1.516162e-02 -0.342375
2019-11-04 20:58:53,492 train 250 1.511438e-02 -0.321685
2019-11-04 20:59:13,718 train 300 1.514065e-02 -0.307199
2019-11-04 20:59:34,247 train 350 1.507123e-02 -0.313104
2019-11-04 20:59:54,850 train 400 1.502321e-02 -0.309235
2019-11-04 21:00:08,010 training loss; R2: 1.504872e-02 -0.327613
2019-11-04 21:00:09,043 valid 000 1.165293e-02 -0.459226
2019-11-04 21:00:28,090 validation loss; R2: 1.270498e-02 -0.729662
2019-11-04 21:00:28,120 epoch 41 lr 1.000000e-04
2019-11-04 21:00:29,282 train 000 1.622884e-02 -0.392731
2019-11-04 21:00:50,123 train 050 1.505023e-02 -0.325870
2019-11-04 21:01:10,842 train 100 1.533019e-02 -0.395196
2019-11-04 21:01:31,562 train 150 1.520337e-02 -0.363574
2019-11-04 21:01:52,288 train 200 1.517649e-02 -0.328549
2019-11-04 21:02:12,951 train 250 1.512120e-02 -0.338532
2019-11-04 21:02:33,491 train 300 1.506223e-02 -0.338254
2019-11-04 21:02:53,106 train 350 1.501813e-02 -0.323144
2019-11-04 21:03:13,308 train 400 1.499235e-02 -0.336004
2019-11-04 21:03:26,210 training loss; R2: 1.499394e-02 -0.321372
2019-11-04 21:03:27,152 valid 000 1.184487e-02 -0.651939
2019-11-04 21:03:45,670 validation loss; R2: 1.275633e-02 -0.699102
2019-11-04 21:03:45,698 epoch 42 lr 1.000000e-04
2019-11-04 21:03:46,791 train 000 1.364306e-02 -0.422761
2019-11-04 21:04:06,939 train 050 1.503100e-02 -0.285776
2019-11-04 21:04:27,109 train 100 1.498833e-02 -0.272281
2019-11-04 21:04:47,437 train 150 1.494445e-02 -0.296780
2019-11-04 21:05:07,842 train 200 1.495735e-02 -0.351141
2019-11-04 21:05:28,296 train 250 1.492932e-02 -0.325409
2019-11-04 21:05:48,609 train 300 1.492025e-02 -0.318259
2019-11-04 21:06:08,560 train 350 1.491285e-02 -0.315460
2019-11-04 21:06:29,196 train 400 1.492169e-02 -0.309463
2019-11-04 21:06:42,295 training loss; R2: 1.488906e-02 -0.309481
2019-11-04 21:06:43,251 valid 000 1.055391e-02 -0.194509
2019-11-04 21:07:01,931 validation loss; R2: 1.205705e-02 -0.414636
2019-11-04 21:07:01,960 epoch 43 lr 1.000000e-04
2019-11-04 21:07:03,101 train 000 1.513603e-02 0.101423
2019-11-04 21:07:23,554 train 050 1.505137e-02 -0.340322
2019-11-04 21:07:43,855 train 100 1.492710e-02 -0.423262
2019-11-04 21:08:04,237 train 150 1.490519e-02 -0.416410
2019-11-04 21:08:24,828 train 200 1.493673e-02 -0.357346
2019-11-04 21:08:45,243 train 250 1.493517e-02 -0.347804
2019-11-04 21:09:05,676 train 300 1.489175e-02 -0.352372
2019-11-04 21:09:26,355 train 350 1.486997e-02 -0.353492
2019-11-04 21:09:46,988 train 400 1.484873e-02 -0.344207
2019-11-04 21:10:00,405 training loss; R2: 1.484402e-02 -0.337137
2019-11-04 21:10:01,482 valid 000 1.233539e-02 -0.428152
2019-11-04 21:10:20,421 validation loss; R2: 1.251878e-02 -0.713291
2019-11-04 21:10:20,447 epoch 44 lr 1.000000e-04
2019-11-04 21:10:21,607 train 000 1.423515e-02 -0.393346
2019-11-04 21:10:42,292 train 050 1.477535e-02 -0.420121
2019-11-04 21:11:02,898 train 100 1.481301e-02 -0.383971
2019-11-04 21:11:23,444 train 150 1.467068e-02 -0.369754
2019-11-04 21:11:44,205 train 200 1.467610e-02 -0.355173
2019-11-04 21:12:05,066 train 250 1.472102e-02 -0.341163
2019-11-04 21:12:25,956 train 300 1.474378e-02 -0.331974
2019-11-04 21:12:45,842 train 350 1.475209e-02 -0.320966
2019-11-04 21:13:06,007 train 400 1.476056e-02 -0.334378
2019-11-04 21:13:18,961 training loss; R2: 1.476016e-02 -0.328914
2019-11-04 21:13:19,977 valid 000 1.180715e-02 -3.825890
2019-11-04 21:13:39,551 validation loss; R2: 1.270734e-02 -0.926803
2019-11-04 21:13:39,587 epoch 45 lr 1.000000e-04
2019-11-04 21:13:40,730 train 000 1.492915e-02 0.109331
2019-11-04 21:14:01,014 train 050 1.480268e-02 -0.390024
2019-11-04 21:14:21,366 train 100 1.485104e-02 -0.398919
2019-11-04 21:14:41,937 train 150 1.481156e-02 -0.365319
2019-11-04 21:15:02,468 train 200 1.475500e-02 -0.336519
2019-11-04 21:15:23,031 train 250 1.471606e-02 -0.311415
2019-11-04 21:15:44,136 train 300 1.469748e-02 -0.330341
2019-11-04 21:16:05,659 train 350 1.470110e-02 -0.313041
2019-11-04 21:16:27,429 train 400 1.469065e-02 -0.317219
2019-11-04 21:16:41,093 training loss; R2: 1.466674e-02 -0.314217
2019-11-04 21:16:42,177 valid 000 1.261622e-02 -0.622277
2019-11-04 21:17:02,595 validation loss; R2: 1.216343e-02 -0.705765
2019-11-04 21:17:02,626 epoch 46 lr 1.000000e-04
2019-11-04 21:17:03,778 train 000 1.523094e-02 -0.574070
2019-11-04 21:17:25,190 train 050 1.454673e-02 -0.265373
2019-11-04 21:17:46,415 train 100 1.459958e-02 -0.318555
2019-11-04 21:18:07,591 train 150 1.463771e-02 -0.281356
2019-11-04 21:18:28,933 train 200 1.466700e-02 -0.290541
2019-11-04 21:18:49,920 train 250 1.465164e-02 -0.333902
2019-11-04 21:19:10,906 train 300 1.467127e-02 -0.321415
2019-11-04 21:19:32,273 train 350 1.469384e-02 -0.318187
2019-11-04 21:19:53,852 train 400 1.466619e-02 -0.326696
2019-11-04 21:20:07,697 training loss; R2: 1.467176e-02 -0.325086
2019-11-04 21:20:08,744 valid 000 1.244225e-02 -3.740427
2019-11-04 21:20:28,672 validation loss; R2: 1.254440e-02 -0.732555
2019-11-04 21:20:28,701 epoch 47 lr 1.000000e-04
2019-11-04 21:20:29,848 train 000 1.186840e-02 -0.277587
2019-11-04 21:20:51,675 train 050 1.453810e-02 -0.242130
2019-11-04 21:21:13,381 train 100 1.455056e-02 -0.261693
2019-11-04 21:21:35,108 train 150 1.463351e-02 -0.308490
2019-11-04 21:21:56,820 train 200 1.465880e-02 -0.308207
2019-11-04 21:22:18,793 train 250 1.464524e-02 -0.342215
2019-11-04 21:22:40,578 train 300 1.465056e-02 -0.339862
2019-11-04 21:23:02,206 train 350 1.465784e-02 -0.330742
2019-11-04 21:23:23,917 train 400 1.461228e-02 -0.326171
2019-11-04 21:23:37,700 training loss; R2: 1.459516e-02 -0.326152
2019-11-04 21:23:38,730 valid 000 1.459618e-02 -0.089037
2019-11-04 21:23:58,497 validation loss; R2: 1.268573e-02 -0.924819
2019-11-04 21:23:58,525 epoch 48 lr 1.000000e-04
2019-11-04 21:23:59,665 train 000 1.514478e-02 -0.030596
2019-11-04 21:24:21,288 train 050 1.460585e-02 -0.337827
2019-11-04 21:24:42,861 train 100 1.470865e-02 -0.305605
2019-11-04 21:25:04,369 train 150 1.460576e-02 -0.312407
2019-11-04 21:25:25,781 train 200 1.454298e-02 -0.290593
2019-11-04 21:25:47,192 train 250 1.450360e-02 -0.298372
2019-11-04 21:26:08,673 train 300 1.454716e-02 -0.302370
2019-11-04 21:26:29,888 train 350 1.453569e-02 -0.291069
2019-11-04 21:26:51,222 train 400 1.457027e-02 -0.290427
2019-11-04 21:27:04,892 training loss; R2: 1.456917e-02 -0.294792
2019-11-04 21:27:05,937 valid 000 1.172343e-02 -0.163768
2019-11-04 21:27:26,076 validation loss; R2: 1.210116e-02 -0.604870
2019-11-04 21:27:26,104 epoch 49 lr 1.000000e-04
2019-11-04 21:27:27,274 train 000 1.508558e-02 0.091189
2019-11-04 21:27:49,121 train 050 1.456919e-02 -0.239979
2019-11-04 21:28:11,420 train 100 1.455533e-02 -0.517850
2019-11-04 21:28:33,097 train 150 1.455609e-02 -0.451714
2019-11-04 21:28:54,818 train 200 1.457162e-02 -0.425010
2019-11-04 21:29:16,141 train 250 1.453876e-02 -0.394711
2019-11-04 21:29:37,654 train 300 1.450108e-02 -0.388448
2019-11-04 21:29:59,287 train 350 1.451574e-02 -0.375911
2019-11-04 21:30:21,197 train 400 1.452200e-02 -0.366195
2019-11-04 21:30:35,159 training loss; R2: 1.453353e-02 -0.361003
2019-11-04 21:30:36,155 valid 000 1.431001e-02 0.117261
2019-11-04 21:30:56,046 validation loss; R2: 1.251391e-02 -0.649558
2019-11-04 21:30:56,083 epoch 50 lr 1.000000e-04
2019-11-04 21:30:57,247 train 000 1.316531e-02 0.050556
2019-11-04 21:31:18,939 train 050 1.448807e-02 -0.220106
2019-11-04 21:31:40,528 train 100 1.444291e-02 -0.267804
2019-11-04 21:32:02,022 train 150 1.456296e-02 -0.277668
2019-11-04 21:32:23,451 train 200 1.455094e-02 -0.285033
2019-11-04 21:32:44,875 train 250 1.451906e-02 -0.269452
2019-11-04 21:33:06,291 train 300 1.455622e-02 -4.125332
2019-11-04 21:33:27,797 train 350 1.453798e-02 -3.592588
2019-11-04 21:33:49,234 train 400 1.451947e-02 -3.189194
2019-11-04 21:34:02,880 training loss; R2: 1.451885e-02 -2.980313
2019-11-04 21:34:03,899 valid 000 1.284637e-02 -0.982240
2019-11-04 21:34:23,749 validation loss; R2: 1.342859e-02 -0.910680
2019-11-04 21:34:23,776 epoch 51 lr 1.000000e-04
2019-11-04 21:34:24,958 train 000 1.534115e-02 -0.275277
2019-11-04 21:34:46,582 train 050 1.454716e-02 -0.310649
2019-11-04 21:35:08,149 train 100 1.453851e-02 -0.331064
2019-11-04 21:35:29,765 train 150 1.451115e-02 -0.344869
2019-11-04 21:35:51,349 train 200 1.448218e-02 -0.331532
2019-11-04 21:36:13,273 train 250 1.443122e-02 -0.330816
2019-11-04 21:36:35,056 train 300 1.439814e-02 -0.302440
2019-11-04 21:36:56,348 train 350 1.439994e-02 -0.296549
2019-11-04 21:37:17,732 train 400 1.440259e-02 -0.306003
2019-11-04 21:37:31,420 training loss; R2: 1.441946e-02 -0.310766
2019-11-04 21:37:32,400 valid 000 1.197161e-02 -0.159062
2019-11-04 21:37:52,145 validation loss; R2: 1.231745e-02 -0.779696
2019-11-04 21:37:52,172 epoch 52 lr 1.000000e-04
2019-11-04 21:37:53,379 train 000 1.565587e-02 -0.912006
2019-11-04 21:38:14,848 train 050 1.428234e-02 -0.380414
2019-11-04 21:38:36,233 train 100 1.434920e-02 -0.379382
2019-11-04 21:38:57,470 train 150 1.436700e-02 -0.379851
2019-11-04 21:39:18,761 train 200 1.437394e-02 -0.368577
2019-11-04 21:39:40,197 train 250 1.438368e-02 -0.376038
2019-11-04 21:40:02,143 train 300 1.439131e-02 -0.338522
2019-11-04 21:40:23,692 train 350 1.435666e-02 -0.334570
2019-11-04 21:40:45,350 train 400 1.435916e-02 -0.321092
2019-11-04 21:40:59,252 training loss; R2: 1.437117e-02 -0.325695
2019-11-04 21:41:00,308 valid 000 1.140282e-02 -1.184537
2019-11-04 21:41:20,393 validation loss; R2: 1.237710e-02 -1.724328
2019-11-04 21:41:20,429 epoch 53 lr 1.000000e-04
2019-11-04 21:41:21,622 train 000 1.358666e-02 -0.742209
2019-11-04 21:41:43,612 train 050 1.444538e-02 -0.244622
2019-11-04 21:42:05,218 train 100 1.445130e-02 -0.336200
2019-11-04 21:42:26,837 train 150 1.440016e-02 -0.335361
2019-11-04 21:42:48,641 train 200 1.439743e-02 -0.330846
2019-11-04 21:43:10,696 train 250 1.437807e-02 -0.331974
2019-11-04 21:43:32,274 train 300 1.436984e-02 -0.321200
2019-11-04 21:43:53,971 train 350 1.437250e-02 -0.334344
2019-11-04 21:44:15,679 train 400 1.435825e-02 -0.340961
2019-11-04 21:44:29,430 training loss; R2: 1.436250e-02 -0.330240
2019-11-04 21:44:30,518 valid 000 1.341339e-02 -0.210517
2019-11-04 21:44:50,431 validation loss; R2: 1.207149e-02 -0.808952
2019-11-04 21:44:50,459 epoch 54 lr 1.000000e-04
2019-11-04 21:44:51,596 train 000 1.494139e-02 -1.537047
2019-11-04 21:45:13,279 train 050 1.443950e-02 -0.321139
2019-11-04 21:45:34,861 train 100 1.436461e-02 -0.248190
2019-11-04 21:45:56,361 train 150 1.432522e-02 -0.239181
2019-11-04 21:46:17,872 train 200 1.433733e-02 -0.266796
2019-11-04 21:46:39,293 train 250 1.434812e-02 -0.327298
2019-11-04 21:47:00,628 train 300 1.434969e-02 -0.324742
2019-11-04 21:47:21,953 train 350 1.435020e-02 -0.318117
2019-11-04 21:47:43,300 train 400 1.430741e-02 -0.313348
2019-11-04 21:47:57,039 training loss; R2: 1.429332e-02 -0.322393
2019-11-04 21:47:58,105 valid 000 1.185495e-02 -9.146638
2019-11-04 21:48:17,806 validation loss; R2: 1.244874e-02 -1.504724
2019-11-04 21:48:17,836 epoch 55 lr 1.000000e-04
2019-11-04 21:48:18,978 train 000 1.359827e-02 -0.935144
2019-11-04 21:48:40,517 train 050 1.413705e-02 -0.427190
2019-11-04 21:49:01,996 train 100 1.425788e-02 -0.441056
2019-11-04 21:49:23,701 train 150 1.425289e-02 -0.416073
2019-11-04 21:49:45,416 train 200 1.419077e-02 -0.376013
2019-11-04 21:50:07,192 train 250 1.415384e-02 -0.343611
2019-11-04 21:50:28,942 train 300 1.419212e-02 -0.324035
2019-11-04 21:50:50,210 train 350 1.418929e-02 -0.324077
2019-11-04 21:51:11,719 train 400 1.421909e-02 -0.306675
2019-11-04 21:51:25,580 training loss; R2: 1.424762e-02 -0.306876
2019-11-04 21:51:26,718 valid 000 1.169868e-02 -0.562124
2019-11-04 21:51:46,682 validation loss; R2: 1.221507e-02 -0.825780
2019-11-04 21:51:46,708 epoch 56 lr 1.000000e-04
2019-11-04 21:51:47,891 train 000 1.477425e-02 -0.531055
2019-11-04 21:52:09,893 train 050 1.418641e-02 -0.345626
2019-11-04 21:52:32,026 train 100 1.426929e-02 -0.338705
2019-11-04 21:52:54,125 train 150 1.421313e-02 -0.328615
2019-11-04 21:53:15,889 train 200 1.423967e-02 -0.356985
2019-11-04 21:53:37,589 train 250 1.416455e-02 -0.347091
2019-11-04 21:53:59,037 train 300 1.419207e-02 -0.319511
2019-11-04 21:54:20,532 train 350 1.422396e-02 -0.331833
2019-11-04 21:54:42,036 train 400 1.422769e-02 -0.327368
2019-11-04 21:54:55,834 training loss; R2: 1.423704e-02 -0.323700
2019-11-04 21:54:56,876 valid 000 1.349938e-02 -0.319802
2019-11-04 21:55:16,720 validation loss; R2: 1.226886e-02 -0.964585
2019-11-04 21:55:16,745 epoch 57 lr 1.000000e-04
2019-11-04 21:55:17,881 train 000 1.491190e-02 0.077299
2019-11-04 21:55:39,571 train 050 1.428873e-02 -0.237665
2019-11-04 21:56:01,193 train 100 1.436844e-02 -0.213361
2019-11-04 21:56:22,699 train 150 1.432165e-02 -0.232172
2019-11-04 21:56:44,321 train 200 1.426131e-02 -0.267408
2019-11-04 21:57:05,840 train 250 1.424006e-02 -0.289321
2019-11-04 21:57:27,296 train 300 1.420137e-02 -0.286842
2019-11-04 21:57:48,730 train 350 1.420606e-02 -0.286747
2019-11-04 21:58:10,292 train 400 1.420447e-02 -0.318517
2019-11-04 21:58:24,023 training loss; R2: 1.418675e-02 -0.320021
2019-11-04 21:58:25,092 valid 000 1.037053e-02 -2.772591
2019-11-04 21:58:44,966 validation loss; R2: 1.191935e-02 -0.970041
2019-11-04 21:58:44,994 epoch 58 lr 1.000000e-04
2019-11-04 21:58:46,159 train 000 1.292123e-02 -0.479613
2019-11-04 21:59:07,843 train 050 1.424005e-02 -0.215718
2019-11-04 21:59:29,392 train 100 1.418052e-02 -0.231325
2019-11-04 21:59:50,962 train 150 1.420787e-02 -0.224925
2019-11-04 22:00:12,478 train 200 1.421300e-02 -0.302171
2019-11-04 22:00:33,617 train 250 1.421121e-02 -0.310515
2019-11-04 22:00:54,870 train 300 1.414385e-02 -0.311560
2019-11-04 22:01:16,599 train 350 1.414288e-02 -0.301421
2019-11-04 22:01:38,169 train 400 1.418092e-02 -0.291691
2019-11-04 22:01:51,909 training loss; R2: 1.420884e-02 -0.293874
2019-11-04 22:01:52,996 valid 000 1.158898e-02 -0.251404
2019-11-04 22:02:13,135 validation loss; R2: 1.157010e-02 -0.828167
2019-11-04 22:02:13,162 epoch 59 lr 1.000000e-04
2019-11-04 22:02:14,302 train 000 1.513812e-02 -0.788038
2019-11-04 22:02:36,019 train 050 1.425652e-02 -0.335882
2019-11-04 22:02:57,613 train 100 1.417923e-02 -0.357546
2019-11-04 22:03:19,141 train 150 1.415815e-02 -0.314394
2019-11-04 22:03:40,590 train 200 1.417514e-02 -0.311941
2019-11-04 22:04:02,088 train 250 1.416453e-02 -0.330191
2019-11-04 22:04:23,545 train 300 1.419520e-02 -0.321817
2019-11-04 22:04:45,011 train 350 1.419414e-02 -0.314800
2019-11-04 22:05:06,124 train 400 1.417542e-02 -0.314247
2019-11-04 22:05:19,587 training loss; R2: 1.418582e-02 -0.304677
2019-11-04 22:05:20,644 valid 000 1.436580e-02 -0.450943
2019-11-04 22:05:40,237 validation loss; R2: 1.253037e-02 -0.655458
2019-11-04 22:05:40,264 epoch 60 lr 1.000000e-04
2019-11-04 22:05:41,424 train 000 1.471421e-02 -0.268888
2019-11-04 22:06:02,832 train 050 1.405336e-02 -0.364235
2019-11-04 22:06:24,110 train 100 1.413337e-02 -0.385518
2019-11-04 22:06:45,340 train 150 1.416746e-02 -0.361926
2019-11-04 22:07:06,714 train 200 1.414093e-02 -0.328764
2019-11-04 22:07:28,064 train 250 1.413396e-02 -0.321475
2019-11-04 22:07:49,579 train 300 1.419538e-02 -0.323688
2019-11-04 22:08:11,191 train 350 1.418218e-02 -0.330902
2019-11-04 22:08:33,024 train 400 1.419569e-02 -0.311783
2019-11-04 22:08:46,999 training loss; R2: 1.417267e-02 -0.318836
2019-11-04 22:08:48,082 valid 000 1.074069e-02 -0.011204
2019-11-04 22:09:08,337 validation loss; R2: 1.197541e-02 -0.576973
2019-11-04 22:09:08,362 epoch 61 lr 1.000000e-04
2019-11-04 22:09:09,505 train 000 1.331573e-02 -0.380262
2019-11-04 22:09:31,366 train 050 1.412888e-02 -0.306821
2019-11-04 22:09:53,294 train 100 1.418170e-02 -0.317997
2019-11-04 22:10:15,481 train 150 1.416499e-02 -0.312788
2019-11-04 22:10:37,286 train 200 1.415513e-02 -0.293910
2019-11-04 22:10:58,780 train 250 1.410687e-02 -0.302058
2019-11-04 22:11:20,340 train 300 1.413036e-02 -0.301930
2019-11-04 22:11:41,825 train 350 1.410118e-02 -0.304420
2019-11-04 22:12:03,018 train 400 1.408605e-02 -0.295782
2019-11-04 22:12:16,590 training loss; R2: 1.410333e-02 -0.306325
2019-11-04 22:12:17,598 valid 000 1.130061e-02 -0.107417
2019-11-04 22:12:37,299 validation loss; R2: 1.197717e-02 -0.601953
2019-11-04 22:12:37,328 epoch 62 lr 1.000000e-04
2019-11-04 22:12:38,521 train 000 1.371375e-02 -1.268398
2019-11-04 22:12:59,836 train 050 1.426400e-02 -0.260410
2019-11-04 22:13:21,076 train 100 1.421432e-02 -0.283137
2019-11-04 22:13:42,411 train 150 1.414458e-02 -0.314296
2019-11-04 22:14:03,673 train 200 1.414349e-02 -0.457862
2019-11-04 22:14:25,027 train 250 1.414323e-02 -0.426860
2019-11-04 22:14:46,400 train 300 1.416934e-02 -0.393061
2019-11-04 22:15:07,934 train 350 1.415968e-02 -0.372748
2019-11-04 22:15:29,583 train 400 1.409512e-02 -0.363392
2019-11-04 22:15:43,406 training loss; R2: 1.412067e-02 -0.357749
2019-11-04 22:15:44,508 valid 000 1.172447e-02 -0.492208
2019-11-04 22:16:05,054 validation loss; R2: 1.220359e-02 -0.763887
2019-11-04 22:16:05,079 epoch 63 lr 1.000000e-04
2019-11-04 22:16:06,402 train 000 1.420687e-02 -0.131147
2019-11-04 22:16:28,059 train 050 1.424865e-02 -0.383432
2019-11-04 22:16:49,424 train 100 1.420770e-02 -0.341599
2019-11-04 22:17:10,852 train 150 1.417752e-02 -0.310057
2019-11-04 22:17:32,475 train 200 1.412722e-02 -0.288343
2019-11-04 22:17:53,846 train 250 1.409371e-02 -0.295965
2019-11-04 22:18:15,256 train 300 1.409792e-02 -0.303044
2019-11-04 22:18:36,581 train 350 1.408700e-02 -0.321628
2019-11-04 22:18:57,943 train 400 1.407639e-02 -0.481965
2019-11-04 22:19:11,748 training loss; R2: 1.405694e-02 -0.475009
2019-11-04 22:19:12,811 valid 000 1.184101e-02 -0.164056
2019-11-04 22:19:32,687 validation loss; R2: 1.198074e-02 -0.737763
2019-11-04 22:19:32,710 epoch 64 lr 1.000000e-04
2019-11-04 22:19:33,839 train 000 1.401328e-02 0.154517
2019-11-04 22:19:55,637 train 050 1.423004e-02 -0.247184
2019-11-04 22:20:17,194 train 100 1.421559e-02 -0.359228
2019-11-04 22:20:38,804 train 150 1.412605e-02 -0.344607
2019-11-04 22:21:00,261 train 200 1.411302e-02 -0.322334
2019-11-04 22:21:21,730 train 250 1.409302e-02 -0.301655
2019-11-04 22:21:43,275 train 300 1.410186e-02 -0.294705
2019-11-04 22:22:04,867 train 350 1.410012e-02 -0.283357
2019-11-04 22:22:26,431 train 400 1.409667e-02 -0.290610
2019-11-04 22:22:40,032 training loss; R2: 1.407697e-02 -0.289108
2019-11-04 22:22:41,029 valid 000 1.111623e-02 -1.152768
2019-11-04 22:23:00,797 validation loss; R2: 1.171286e-02 -0.624189
2019-11-04 22:23:00,823 epoch 65 lr 1.000000e-04
2019-11-04 22:23:01,951 train 000 1.450210e-02 0.002001
2019-11-04 22:23:23,514 train 050 1.404960e-02 -0.429325
2019-11-04 22:23:44,947 train 100 1.426548e-02 -0.310366
2019-11-04 22:24:06,352 train 150 1.424159e-02 -0.409651
2019-11-04 22:24:27,815 train 200 1.421252e-02 -0.409043
2019-11-04 22:24:49,256 train 250 1.418698e-02 -0.386475
2019-11-04 22:25:10,713 train 300 1.413088e-02 -0.371277
2019-11-04 22:25:31,916 train 350 1.415766e-02 -0.350731
2019-11-04 22:25:53,287 train 400 1.412178e-02 -0.345715
2019-11-04 22:26:06,933 training loss; R2: 1.413713e-02 -0.349891
2019-11-04 22:26:08,000 valid 000 1.194457e-02 -2.905215
2019-11-04 22:26:27,802 validation loss; R2: 1.187846e-02 -0.611371
2019-11-04 22:26:27,830 epoch 66 lr 1.000000e-04
2019-11-04 22:26:29,024 train 000 1.492370e-02 -0.171728
2019-11-04 22:26:50,406 train 050 1.405246e-02 -0.261423
2019-11-04 22:27:11,681 train 100 1.398229e-02 -0.399719
2019-11-04 22:27:33,047 train 150 1.399112e-02 -0.379451
2019-11-04 22:27:54,476 train 200 1.402687e-02 -0.368214
2019-11-04 22:28:16,023 train 250 1.400884e-02 -0.337213
2019-11-04 22:28:37,337 train 300 1.405520e-02 -0.330705
2019-11-04 22:28:58,539 train 350 1.405991e-02 -0.324335
2019-11-04 22:29:19,841 train 400 1.405630e-02 -0.407497
2019-11-04 22:29:33,424 training loss; R2: 1.405965e-02 -0.401853
2019-11-04 22:29:34,420 valid 000 9.738890e-03 -0.325280
2019-11-04 22:29:54,082 validation loss; R2: 1.157437e-02 -0.894956
2019-11-04 22:29:54,108 epoch 67 lr 1.000000e-04
2019-11-04 22:29:55,257 train 000 1.540289e-02 -1.247188
2019-11-04 22:30:16,538 train 050 1.409579e-02 -0.388254
2019-11-04 22:30:37,833 train 100 1.413747e-02 -0.377479
2019-11-04 22:30:59,157 train 150 1.422241e-02 -0.376109
2019-11-04 22:31:20,463 train 200 1.417182e-02 -0.339994
2019-11-04 22:31:42,065 train 250 1.414168e-02 -0.330249
2019-11-04 22:32:03,280 train 300 1.408722e-02 -0.321866
2019-11-04 22:32:24,508 train 350 1.406597e-02 -0.312342
2019-11-04 22:32:45,786 train 400 1.405139e-02 -0.308364
2019-11-04 22:32:59,364 training loss; R2: 1.406293e-02 -0.312842
2019-11-04 22:33:00,361 valid 000 1.175090e-02 -0.609795
2019-11-04 22:33:20,638 validation loss; R2: 1.137430e-02 -0.648527
2019-11-04 22:33:20,668 epoch 68 lr 1.000000e-04
2019-11-04 22:33:21,854 train 000 1.392001e-02 -0.223489
2019-11-04 22:33:43,517 train 050 1.412582e-02 -0.543067
2019-11-04 22:34:05,221 train 100 1.403472e-02 -0.415461
2019-11-04 22:34:26,994 train 150 1.408242e-02 -0.356881
2019-11-04 22:34:48,605 train 200 1.401057e-02 -0.357321
2019-11-04 22:35:10,193 train 250 1.399003e-02 -0.343021
2019-11-04 22:35:31,810 train 300 1.402767e-02 -0.328984
2019-11-04 22:35:53,254 train 350 1.403226e-02 -0.319334
2019-11-04 22:36:14,711 train 400 1.400131e-02 -0.316130
2019-11-04 22:36:28,598 training loss; R2: 1.398943e-02 -0.310865
2019-11-04 22:36:29,618 valid 000 9.267739e-03 -1.205933
2019-11-04 22:36:49,806 validation loss; R2: 1.143185e-02 -0.852347
2019-11-04 22:36:49,836 epoch 69 lr 1.000000e-04
2019-11-04 22:36:51,021 train 000 1.311412e-02 -0.646444
2019-11-04 22:37:12,684 train 050 1.397093e-02 -0.351222
2019-11-04 22:37:34,339 train 100 1.394830e-02 -0.487401
2019-11-04 22:37:55,917 train 150 1.397179e-02 -0.427209
2019-11-04 22:38:17,462 train 200 1.392053e-02 -0.367284
2019-11-04 22:38:38,978 train 250 1.397037e-02 -0.359541
2019-11-04 22:39:00,496 train 300 1.397638e-02 -0.333461
2019-11-04 22:39:21,724 train 350 1.396255e-02 -0.325869
2019-11-04 22:39:42,947 train 400 1.396712e-02 -0.318281
2019-11-04 22:39:56,523 training loss; R2: 1.396437e-02 -0.316503
2019-11-04 22:39:57,533 valid 000 1.235513e-02 0.084845
2019-11-04 22:40:17,416 validation loss; R2: 1.125092e-02 -0.649985
2019-11-04 22:40:17,439 epoch 70 lr 1.000000e-04
2019-11-04 22:40:18,552 train 000 1.657857e-02 -0.381507
2019-11-04 22:40:40,111 train 050 1.423775e-02 -0.290207
2019-11-04 22:41:01,561 train 100 1.403889e-02 -0.373652
2019-11-04 22:41:23,080 train 150 1.396501e-02 -0.352448
2019-11-04 22:41:44,598 train 200 1.399841e-02 -0.339878
2019-11-04 22:42:06,247 train 250 1.399821e-02 -0.323220
2019-11-04 22:42:27,872 train 300 1.398268e-02 -0.310350
2019-11-04 22:42:49,254 train 350 1.397631e-02 -0.290519
2019-11-04 22:43:10,488 train 400 1.399975e-02 -0.298470
2019-11-04 22:43:24,211 training loss; R2: 1.401223e-02 -0.288498
2019-11-04 22:43:25,236 valid 000 1.215512e-02 -0.358286
2019-11-04 22:43:45,225 validation loss; R2: 1.227895e-02 -0.674312
2019-11-04 22:43:45,256 epoch 71 lr 1.000000e-04
2019-11-04 22:43:46,398 train 000 1.457294e-02 -0.312949
2019-11-04 22:44:08,311 train 050 1.406857e-02 -0.209836
2019-11-04 22:44:29,989 train 100 1.397306e-02 -0.224703
2019-11-04 22:44:51,636 train 150 1.390842e-02 -0.232735
2019-11-04 22:45:13,453 train 200 1.389956e-02 -0.301205
2019-11-04 22:45:35,229 train 250 1.395530e-02 -0.304886
2019-11-04 22:45:57,172 train 300 1.394345e-02 -0.332008
2019-11-04 22:46:19,021 train 350 1.395253e-02 -0.315885
2019-11-04 22:46:41,049 train 400 1.397002e-02 -0.324742
2019-11-04 22:46:54,876 training loss; R2: 1.395732e-02 -0.334495
2019-11-04 22:46:55,958 valid 000 1.079453e-02 -0.667702
2019-11-04 22:47:15,835 validation loss; R2: 1.146270e-02 -0.472445
2019-11-04 22:47:15,866 epoch 72 lr 1.000000e-04
2019-11-04 22:47:16,996 train 000 1.362160e-02 0.074526
2019-11-04 22:47:38,589 train 050 1.404646e-02 -0.265079
2019-11-04 22:47:59,920 train 100 1.410974e-02 -0.255870
2019-11-04 22:48:21,284 train 150 1.410675e-02 -0.282381
2019-11-04 22:48:42,583 train 200 1.405752e-02 -0.296637
2019-11-04 22:49:03,786 train 250 1.401862e-02 -0.308064
2019-11-04 22:49:25,088 train 300 1.398677e-02 -0.320671
2019-11-04 22:49:46,682 train 350 1.395811e-02 -0.304479
2019-11-04 22:50:08,377 train 400 1.396269e-02 -0.311667
2019-11-04 22:50:22,242 training loss; R2: 1.393533e-02 -0.309245
2019-11-04 22:50:23,309 valid 000 1.147755e-02 -0.420149
2019-11-04 22:50:43,518 validation loss; R2: 1.170861e-02 -0.778073
2019-11-04 22:50:43,550 epoch 73 lr 1.000000e-04
2019-11-04 22:50:44,737 train 000 1.188491e-02 -0.276853
2019-11-04 22:51:06,241 train 050 1.377957e-02 -0.270175
2019-11-04 22:51:27,585 train 100 1.384350e-02 -0.325341
2019-11-04 22:51:49,266 train 150 1.389091e-02 -0.330921
2019-11-04 22:52:10,707 train 200 1.393407e-02 -0.304688
2019-11-04 22:52:32,166 train 250 1.394611e-02 -0.286572
2019-11-04 22:52:53,815 train 300 1.398662e-02 -0.306909
2019-11-04 22:53:15,016 train 350 1.397287e-02 -0.319485
2019-11-04 22:53:36,622 train 400 1.396540e-02 -0.321213
2019-11-04 22:53:50,204 training loss; R2: 1.396610e-02 -0.328067
2019-11-04 22:53:51,310 valid 000 1.290336e-02 0.028065
2019-11-04 22:54:11,970 validation loss; R2: 1.204760e-02 -0.668340
2019-11-04 22:54:11,999 epoch 74 lr 1.000000e-04
2019-11-04 22:54:13,159 train 000 1.497240e-02 0.098716
2019-11-04 22:54:35,012 train 050 1.398383e-02 -0.229777
2019-11-04 22:54:56,289 train 100 1.393532e-02 -0.227778
2019-11-04 22:55:17,498 train 150 1.387920e-02 -0.271518
2019-11-04 22:55:38,697 train 200 1.396488e-02 -0.299973
2019-11-04 22:55:59,861 train 250 1.394714e-02 -0.366915
2019-11-04 22:56:20,899 train 300 1.392529e-02 -0.371166
2019-11-04 22:56:42,101 train 350 1.391069e-02 -0.389725
2019-11-04 22:57:03,478 train 400 1.392802e-02 -0.372774
2019-11-04 22:57:17,116 training loss; R2: 1.390944e-02 -0.367983
2019-11-04 22:57:18,143 valid 000 1.359109e-02 -0.040941
2019-11-04 22:57:38,742 validation loss; R2: 1.222053e-02 -0.915958
2019-11-04 22:57:38,771 epoch 75 lr 1.000000e-04
2019-11-04 22:57:39,914 train 000 1.583696e-02 -0.044408
2019-11-04 22:58:01,769 train 050 1.388551e-02 -0.378305
2019-11-04 22:58:23,398 train 100 1.390749e-02 -0.287371
2019-11-04 22:58:45,019 train 150 1.390701e-02 -0.332808
2019-11-04 22:59:06,305 train 200 1.398871e-02 -0.295459
2019-11-04 22:59:27,619 train 250 1.395747e-02 -0.318221
2019-11-04 22:59:49,008 train 300 1.391672e-02 -0.297206
2019-11-04 23:00:10,775 train 350 1.389219e-02 -0.311965
2019-11-04 23:00:32,577 train 400 1.388892e-02 -0.327393
2019-11-04 23:00:46,330 training loss; R2: 1.387754e-02 -0.327219
2019-11-04 23:00:47,410 valid 000 1.247008e-02 0.116052
2019-11-04 23:01:07,308 validation loss; R2: 1.168499e-02 -0.642761
2019-11-04 23:01:07,336 epoch 76 lr 1.000000e-04
2019-11-04 23:01:08,488 train 000 1.309334e-02 0.020799
2019-11-04 23:01:30,283 train 050 1.381657e-02 -0.292618
2019-11-04 23:01:52,116 train 100 1.392405e-02 -0.254332
2019-11-04 23:02:13,772 train 150 1.401711e-02 -0.280896
2019-11-04 23:02:35,365 train 200 1.403388e-02 -0.321839
2019-11-04 23:02:57,232 train 250 1.401984e-02 -0.319112
2019-11-04 23:03:18,959 train 300 1.399323e-02 -0.301598
2019-11-04 23:03:40,651 train 350 1.397582e-02 -0.296864
2019-11-04 23:04:02,066 train 400 1.398074e-02 -0.297139
2019-11-04 23:04:15,781 training loss; R2: 1.398368e-02 -0.284373
2019-11-04 23:04:16,888 valid 000 1.236731e-02 -2.127165
2019-11-04 23:04:36,796 validation loss; R2: 1.147836e-02 -0.624115
2019-11-04 23:04:36,830 epoch 77 lr 1.000000e-04
2019-11-04 23:04:37,986 train 000 1.278447e-02 -0.298945
2019-11-04 23:04:59,768 train 050 1.377953e-02 -0.225371
2019-11-04 23:05:21,465 train 100 1.379830e-02 -0.207027
2019-11-04 23:05:43,117 train 150 1.390922e-02 -0.209139
2019-11-04 23:06:04,865 train 200 1.388639e-02 -0.222739
2019-11-04 23:06:26,575 train 250 1.387725e-02 -0.247411
2019-11-04 23:06:48,207 train 300 1.389376e-02 -0.239372
2019-11-04 23:07:09,670 train 350 1.387271e-02 -0.257990
2019-11-04 23:07:31,790 train 400 1.388235e-02 -0.255284
2019-11-04 23:07:45,827 training loss; R2: 1.390144e-02 -0.266870
2019-11-04 23:07:46,860 valid 000 1.149410e-02 -3.567618
2019-11-04 23:08:07,098 validation loss; R2: 1.176722e-02 -0.883791
2019-11-04 23:08:07,124 epoch 78 lr 1.000000e-04
2019-11-04 23:08:08,299 train 000 1.324632e-02 -0.357659
2019-11-04 23:08:30,274 train 050 1.378219e-02 -0.413329
2019-11-04 23:08:51,963 train 100 1.384797e-02 -0.452279
2019-11-04 23:09:13,621 train 150 1.388613e-02 -0.421570
2019-11-04 23:09:35,261 train 200 1.388178e-02 -0.412064
2019-11-04 23:09:56,823 train 250 1.385276e-02 -0.362927
2019-11-04 23:10:18,651 train 300 1.381748e-02 -0.353915
2019-11-04 23:10:40,268 train 350 1.384793e-02 -0.334503
2019-11-04 23:11:01,904 train 400 1.386524e-02 -0.333023
2019-11-04 23:11:15,670 training loss; R2: 1.387287e-02 -0.333000
2019-11-04 23:11:16,682 valid 000 1.259526e-02 -0.250299
2019-11-04 23:11:36,513 validation loss; R2: 1.202658e-02 -1.187272
2019-11-04 23:11:36,540 epoch 79 lr 1.000000e-04
2019-11-04 23:11:37,729 train 000 1.499960e-02 -0.119554
2019-11-04 23:11:59,530 train 050 1.375901e-02 -0.159369
2019-11-04 23:12:21,190 train 100 1.391815e-02 -0.227803
2019-11-04 23:12:42,862 train 150 1.391933e-02 -0.241659
2019-11-04 23:13:04,190 train 200 1.384759e-02 -0.252103
2019-11-04 23:13:25,529 train 250 1.386200e-02 -0.261375
2019-11-04 23:13:46,874 train 300 1.383722e-02 -0.292853
2019-11-04 23:14:08,566 train 350 1.381228e-02 -0.298076
2019-11-04 23:14:30,435 train 400 1.381199e-02 -0.294529
2019-11-04 23:14:44,251 training loss; R2: 1.382789e-02 -0.311527
2019-11-04 23:14:45,334 valid 000 9.735123e-03 -0.133714
2019-11-04 23:15:05,257 validation loss; R2: 1.105210e-02 -0.553220
2019-11-04 23:15:05,285 epoch 80 lr 1.000000e-04
2019-11-04 23:15:06,466 train 000 1.442612e-02 0.038883
2019-11-04 23:15:28,131 train 050 1.404746e-02 -0.382282
2019-11-04 23:15:49,744 train 100 1.398366e-02 -0.324183
2019-11-04 23:16:11,409 train 150 1.397951e-02 -0.309401
2019-11-04 23:16:33,007 train 200 1.395922e-02 -0.337478
2019-11-04 23:16:54,641 train 250 1.396038e-02 -0.318872
2019-11-04 23:17:16,236 train 300 1.392833e-02 -0.311844
2019-11-04 23:17:37,666 train 350 1.388084e-02 -0.314705
2019-11-04 23:17:58,983 train 400 1.386165e-02 -0.310545
2019-11-04 23:18:12,653 training loss; R2: 1.388875e-02 -0.308315
2019-11-04 23:18:13,714 valid 000 1.164590e-02 0.135227
2019-11-04 23:18:33,478 validation loss; R2: 1.135280e-02 -0.547451
2019-11-04 23:18:33,500 epoch 81 lr 1.000000e-04
2019-11-04 23:18:34,691 train 000 1.462562e-02 -0.654305
2019-11-04 23:18:56,464 train 050 1.376737e-02 -0.656743
2019-11-04 23:19:18,050 train 100 1.375306e-02 -0.448113
2019-11-04 23:19:39,802 train 150 1.386891e-02 -0.402521
2019-11-04 23:20:01,452 train 200 1.394898e-02 -0.374634
2019-11-04 23:20:23,268 train 250 1.392818e-02 -0.361817
2019-11-04 23:20:44,910 train 300 1.389909e-02 -0.352099
2019-11-04 23:21:06,450 train 350 1.389508e-02 -0.352071
2019-11-04 23:21:27,918 train 400 1.388655e-02 -0.343608
2019-11-04 23:21:41,657 training loss; R2: 1.391691e-02 -0.334557
2019-11-04 23:21:42,670 valid 000 1.020394e-02 0.184523
2019-11-04 23:22:02,467 validation loss; R2: 1.122572e-02 -0.345453
2019-11-04 23:22:02,499 epoch 82 lr 1.000000e-04
2019-11-04 23:22:03,647 train 000 1.456617e-02 -0.579484
2019-11-04 23:22:25,289 train 050 1.380047e-02 -0.332536
2019-11-04 23:22:46,781 train 100 1.396854e-02 -0.372032
2019-11-04 23:23:08,344 train 150 1.398895e-02 -0.394809
2019-11-04 23:23:29,845 train 200 1.399399e-02 -0.401973
2019-11-04 23:23:51,378 train 250 1.397605e-02 -0.388723
2019-11-04 23:24:12,893 train 300 1.394900e-02 -0.369591
2019-11-04 23:24:34,421 train 350 1.391000e-02 -0.363718
2019-11-04 23:24:55,924 train 400 1.393053e-02 -0.346180
2019-11-04 23:25:09,616 training loss; R2: 1.393068e-02 -0.332067
2019-11-04 23:25:10,648 valid 000 1.253021e-02 -0.254327
2019-11-04 23:25:30,637 validation loss; R2: 1.180277e-02 -0.771992
2019-11-04 23:25:30,667 epoch 83 lr 1.000000e-04
2019-11-04 23:25:31,855 train 000 1.390393e-02 -0.081557
2019-11-04 23:25:53,635 train 050 1.400557e-02 -0.244114
2019-11-04 23:26:15,344 train 100 1.401008e-02 -0.209069
2019-11-04 23:26:37,043 train 150 1.397940e-02 -0.231488
2019-11-04 23:26:58,020 train 200 1.388169e-02 -0.221600
2019-11-04 23:27:19,521 train 250 1.392046e-02 -0.239363
2019-11-04 23:27:40,888 train 300 1.390665e-02 -0.247780
2019-11-04 23:28:02,205 train 350 1.389145e-02 -0.257443
2019-11-04 23:28:23,470 train 400 1.390940e-02 -0.271310
2019-11-04 23:28:37,028 training loss; R2: 1.389124e-02 -0.277284
2019-11-04 23:28:38,048 valid 000 1.117968e-02 -0.664242
2019-11-04 23:28:58,315 validation loss; R2: 1.167764e-02 -0.739624
2019-11-04 23:28:58,341 epoch 84 lr 1.000000e-04
2019-11-04 23:28:59,494 train 000 1.627291e-02 -0.029883
2019-11-04 23:29:21,353 train 050 1.383570e-02 -0.291349
2019-11-04 23:29:43,103 train 100 1.381761e-02 -0.357358
2019-11-04 23:30:04,975 train 150 1.387141e-02 -0.316967
2019-11-04 23:30:26,959 train 200 1.394236e-02 -0.303386
2019-11-04 23:30:48,845 train 250 1.396760e-02 -0.294161
2019-11-04 23:31:10,428 train 300 1.393710e-02 -0.303682
2019-11-04 23:31:31,978 train 350 1.389985e-02 -0.329499
2019-11-04 23:31:53,539 train 400 1.391673e-02 -0.344142
2019-11-04 23:32:07,316 training loss; R2: 1.391030e-02 -0.335566
2019-11-04 23:32:08,326 valid 000 1.351324e-02 -0.689148
2019-11-04 23:32:28,144 validation loss; R2: 1.258644e-02 -1.513174
2019-11-04 23:32:28,177 epoch 85 lr 1.000000e-04
2019-11-04 23:32:29,317 train 000 1.452083e-02 -1.449427
2019-11-04 23:32:51,127 train 050 1.412767e-02 -0.262572
2019-11-04 23:33:12,772 train 100 1.399935e-02 -0.263763
2019-11-04 23:33:34,302 train 150 1.394393e-02 -0.306471
2019-11-04 23:33:55,838 train 200 1.392861e-02 -0.314533
2019-11-04 23:34:17,431 train 250 1.392273e-02 -0.328520
2019-11-04 23:34:38,773 train 300 1.390905e-02 -0.313490
2019-11-04 23:35:00,174 train 350 1.394898e-02 -0.305450
2019-11-04 23:35:21,665 train 400 1.395917e-02 -0.325100
2019-11-04 23:35:35,406 training loss; R2: 1.395816e-02 -0.338387
2019-11-04 23:35:36,443 valid 000 1.207820e-02 0.256681
2019-11-04 23:35:56,170 validation loss; R2: 1.097068e-02 -0.608345
2019-11-04 23:35:56,198 epoch 86 lr 1.000000e-04
2019-11-04 23:35:57,380 train 000 1.475894e-02 -0.114166
2019-11-04 23:36:19,047 train 050 1.393247e-02 -0.190632
2019-11-04 23:36:40,575 train 100 1.369504e-02 -0.261197
2019-11-04 23:37:02,109 train 150 1.376543e-02 -0.273964
2019-11-04 23:37:23,600 train 200 1.374025e-02 -0.293715
2019-11-04 23:37:45,099 train 250 1.375539e-02 -0.275515
2019-11-04 23:38:06,355 train 300 1.379531e-02 -0.280752
2019-11-04 23:38:27,591 train 350 1.381023e-02 -0.274822
2019-11-04 23:38:48,846 train 400 1.381833e-02 -0.277046
2019-11-04 23:39:02,367 training loss; R2: 1.380018e-02 -0.276405
2019-11-04 23:39:03,412 valid 000 1.224712e-02 -2.688356
2019-11-04 23:39:23,107 validation loss; R2: 1.226558e-02 -0.882921
2019-11-04 23:39:23,142 epoch 87 lr 1.000000e-04
2019-11-04 23:39:24,331 train 000 1.630900e-02 -0.149783
2019-11-04 23:39:45,824 train 050 1.392526e-02 -0.222952
2019-11-04 23:40:07,190 train 100 1.395889e-02 -0.311085
2019-11-04 23:40:28,475 train 150 1.398589e-02 -0.306436
2019-11-04 23:40:50,102 train 200 1.393842e-02 -0.280346
2019-11-04 23:41:11,789 train 250 1.389521e-02 -0.289724
2019-11-04 23:41:33,438 train 300 1.390362e-02 -0.290863
2019-11-04 23:41:55,182 train 350 1.389352e-02 -0.293714
2019-11-04 23:42:16,787 train 400 1.386363e-02 -0.280341
2019-11-04 23:42:30,531 training loss; R2: 1.387134e-02 -0.279863
2019-11-04 23:42:31,543 valid 000 1.016148e-02 -0.434477
2019-11-04 23:42:51,532 validation loss; R2: 1.230554e-02 -0.726061
2019-11-04 23:42:51,556 epoch 88 lr 1.000000e-04
2019-11-04 23:42:52,692 train 000 1.533066e-02 -0.306619
2019-11-04 23:43:14,467 train 050 1.396194e-02 -0.191321
2019-11-04 23:43:36,034 train 100 1.394422e-02 -0.208979
2019-11-04 23:43:57,531 train 150 1.392829e-02 -0.285077
2019-11-04 23:44:19,100 train 200 1.391499e-02 -0.266952
2019-11-04 23:44:40,238 train 250 1.390574e-02 -0.269691
2019-11-04 23:45:01,467 train 300 1.389950e-02 -0.295024
2019-11-04 23:45:22,661 train 350 1.388832e-02 -0.322148
2019-11-04 23:45:43,980 train 400 1.390454e-02 -0.322374
2019-11-04 23:45:57,630 training loss; R2: 1.391107e-02 -0.323549
2019-11-04 23:45:58,646 valid 000 1.127323e-02 0.223442
2019-11-04 23:46:18,391 validation loss; R2: 1.113496e-02 -0.635006
2019-11-04 23:46:18,419 epoch 89 lr 1.000000e-04
2019-11-04 23:46:19,556 train 000 1.470392e-02 0.028661
2019-11-04 23:46:41,174 train 050 1.395681e-02 -0.483209
2019-11-04 23:47:02,642 train 100 1.389770e-02 -0.439227
2019-11-04 23:47:24,106 train 150 1.387182e-02 -0.387228
2019-11-04 23:47:45,456 train 200 1.387235e-02 -0.333953
2019-11-04 23:48:06,883 train 250 1.379259e-02 -0.315597
2019-11-04 23:48:28,417 train 300 1.383600e-02 -0.316033
2019-11-04 23:48:50,206 train 350 1.381153e-02 -0.314773
2019-11-04 23:49:12,277 train 400 1.382205e-02 -0.313301
2019-11-04 23:49:26,549 training loss; R2: 1.381223e-02 -0.324644
2019-11-04 23:49:27,695 valid 000 1.256973e-02 -0.809296
2019-11-04 23:49:47,848 validation loss; R2: 1.173580e-02 -0.654984
2019-11-04 23:49:47,875 epoch 90 lr 1.000000e-04
2019-11-04 23:49:49,073 train 000 1.150548e-02 0.063218
2019-11-04 23:50:10,844 train 050 1.396191e-02 -0.247072
2019-11-04 23:50:32,583 train 100 1.391538e-02 -0.268623
2019-11-04 23:50:54,165 train 150 1.395573e-02 -0.343666
2019-11-04 23:51:15,198 train 200 1.396396e-02 -0.301863
2019-11-04 23:51:36,397 train 250 1.395440e-02 -0.294056
2019-11-04 23:51:57,747 train 300 1.396587e-02 -0.306149
2019-11-04 23:52:19,239 train 350 1.395053e-02 -0.308144
2019-11-04 23:52:40,862 train 400 1.392312e-02 -0.311128
2019-11-04 23:52:54,629 training loss; R2: 1.390515e-02 -0.316864
2019-11-04 23:52:55,755 valid 000 1.235296e-02 0.061289
2019-11-04 23:53:15,655 validation loss; R2: 1.181715e-02 -0.843760
2019-11-04 23:53:15,688 epoch 91 lr 1.000000e-04
2019-11-04 23:53:16,944 train 000 1.400933e-02 -0.068731
2019-11-04 23:53:38,424 train 050 1.386574e-02 -0.472240
2019-11-04 23:53:59,968 train 100 1.408149e-02 -0.364255
2019-11-04 23:54:21,503 train 150 1.408717e-02 -0.335407
2019-11-04 23:54:42,929 train 200 1.399689e-02 -0.332425
2019-11-04 23:55:04,446 train 250 1.393771e-02 -0.317199
2019-11-04 23:55:25,765 train 300 1.388053e-02 -0.307616
2019-11-04 23:55:47,133 train 350 1.385701e-02 -0.309034
2019-11-04 23:56:08,765 train 400 1.383627e-02 -0.301019
2019-11-04 23:56:22,538 training loss; R2: 1.383565e-02 -0.296656
2019-11-04 23:56:23,568 valid 000 1.124930e-02 -0.574537
2019-11-04 23:56:43,972 validation loss; R2: 1.131726e-02 -0.557719
2019-11-04 23:56:44,000 epoch 92 lr 1.000000e-04
2019-11-04 23:56:45,217 train 000 1.286122e-02 0.101487
2019-11-04 23:57:07,020 train 050 1.396828e-02 -0.224146
2019-11-04 23:57:28,721 train 100 1.387744e-02 -0.262356
2019-11-04 23:57:50,353 train 150 1.390862e-02 -0.251977
2019-11-04 23:58:11,639 train 200 1.389478e-02 -0.266298
2019-11-04 23:58:32,807 train 250 1.383231e-02 -0.266118
2019-11-04 23:58:54,219 train 300 1.380171e-02 -0.280560
2019-11-04 23:59:16,134 train 350 1.381152e-02 -0.281933
2019-11-04 23:59:38,085 train 400 1.379543e-02 -0.294423
2019-11-04 23:59:52,370 training loss; R2: 1.379099e-02 -0.288344
2019-11-04 23:59:53,396 valid 000 1.347585e-02 0.009966
2019-11-05 00:00:13,710 validation loss; R2: 1.188790e-02 -0.689001
2019-11-05 00:00:13,740 epoch 93 lr 1.000000e-04
2019-11-05 00:00:14,892 train 000 1.309859e-02 -0.484492
2019-11-05 00:00:36,882 train 050 1.363870e-02 -0.310011
2019-11-05 00:00:58,576 train 100 1.376982e-02 -0.299783
2019-11-05 00:01:20,353 train 150 1.369911e-02 -0.277458
2019-11-05 00:01:42,045 train 200 1.368928e-02 -0.277477
2019-11-05 00:02:03,634 train 250 1.370206e-02 -0.279647
2019-11-05 00:02:25,349 train 300 1.370066e-02 -0.283017
2019-11-05 00:02:46,797 train 350 1.373291e-02 -0.280622
2019-11-05 00:03:08,174 train 400 1.375288e-02 -0.280428
2019-11-05 00:03:21,892 training loss; R2: 1.377065e-02 -0.268324
2019-11-05 00:03:23,021 valid 000 1.143251e-02 -0.060145
2019-11-05 00:03:42,833 validation loss; R2: 1.194949e-02 -0.930987
2019-11-05 00:03:42,864 epoch 94 lr 1.000000e-04
2019-11-05 00:03:44,090 train 000 1.428706e-02 0.082224
2019-11-05 00:04:05,470 train 050 1.345964e-02 -0.338326
2019-11-05 00:04:26,872 train 100 1.368292e-02 -0.352474
2019-11-05 00:04:48,080 train 150 1.360290e-02 -0.350705
2019-11-05 00:05:09,432 train 200 1.362761e-02 -0.346708
2019-11-05 00:05:30,871 train 250 1.367949e-02 -0.320992
2019-11-05 00:05:52,238 train 300 1.372038e-02 -0.316617
2019-11-05 00:06:13,517 train 350 1.372626e-02 -0.313816
2019-11-05 00:06:34,902 train 400 1.371475e-02 -0.290644
2019-11-05 00:06:48,500 training loss; R2: 1.372194e-02 -0.292724
2019-11-05 00:06:49,602 valid 000 1.194116e-02 -0.027477
2019-11-05 00:07:09,686 validation loss; R2: 1.226346e-02 -0.646889
2019-11-05 00:07:09,715 epoch 95 lr 1.000000e-04
2019-11-05 00:07:10,964 train 000 1.392802e-02 -0.059022
2019-11-05 00:07:32,306 train 050 1.390698e-02 -0.226200
2019-11-05 00:07:53,715 train 100 1.396001e-02 -0.235681
2019-11-05 00:08:14,921 train 150 1.387042e-02 -0.261792
2019-11-05 00:08:36,154 train 200 1.386056e-02 -0.272415
2019-11-05 00:08:57,548 train 250 1.383412e-02 -0.272954
2019-11-05 00:09:19,064 train 300 1.377969e-02 -0.291056
2019-11-05 00:09:40,518 train 350 1.378472e-02 -0.279360
2019-11-05 00:10:01,931 train 400 1.377446e-02 -0.282242
2019-11-05 00:10:15,566 training loss; R2: 1.374817e-02 -0.292483
2019-11-05 00:10:16,688 valid 000 1.225395e-02 0.161139
2019-11-05 00:10:36,526 validation loss; R2: 1.214016e-02 -1.069101
2019-11-05 00:10:36,555 epoch 96 lr 1.000000e-04
2019-11-05 00:10:37,780 train 000 1.249622e-02 0.017032
2019-11-05 00:10:59,465 train 050 1.378278e-02 -0.381258
2019-11-05 00:11:20,960 train 100 1.390420e-02 -0.327777
2019-11-05 00:11:42,441 train 150 1.383377e-02 -0.547695
2019-11-05 00:12:03,665 train 200 1.374487e-02 -0.452278
2019-11-05 00:12:25,044 train 250 1.379265e-02 -0.393320
2019-11-05 00:12:46,480 train 300 1.379630e-02 -0.383413
2019-11-05 00:13:08,011 train 350 1.380139e-02 -0.356233
2019-11-05 00:13:29,344 train 400 1.380028e-02 -0.345002
2019-11-05 00:13:42,986 training loss; R2: 1.376966e-02 -0.350672
2019-11-05 00:13:44,080 valid 000 1.292139e-02 0.127939
2019-11-05 00:14:03,946 validation loss; R2: 1.148609e-02 -0.572852
2019-11-05 00:14:03,973 epoch 97 lr 1.000000e-04
2019-11-05 00:14:05,196 train 000 1.114072e-02 -0.179993
2019-11-05 00:14:26,738 train 050 1.360669e-02 -0.168901
2019-11-05 00:14:48,375 train 100 1.361645e-02 -0.205127
2019-11-05 00:15:09,858 train 150 1.356337e-02 -0.248708
2019-11-05 00:15:31,044 train 200 1.361738e-02 -0.262968
2019-11-05 00:15:52,602 train 250 1.365335e-02 -0.276331
2019-11-05 00:16:13,864 train 300 1.369671e-02 -0.267518
2019-11-05 00:16:35,183 train 350 1.372885e-02 -0.261060
2019-11-05 00:16:56,438 train 400 1.372221e-02 -0.264790
2019-11-05 00:17:09,938 training loss; R2: 1.372936e-02 -0.275099
2019-11-05 00:17:11,044 valid 000 1.142076e-02 0.005049
2019-11-05 00:17:30,621 validation loss; R2: 1.114778e-02 -0.416825
2019-11-05 00:17:30,650 epoch 98 lr 1.000000e-04
2019-11-05 00:17:31,785 train 000 1.548149e-02 -0.340317
2019-11-05 00:17:53,155 train 050 1.362660e-02 -0.307528
2019-11-05 00:18:14,507 train 100 1.359097e-02 -0.235515
2019-11-05 00:18:35,748 train 150 1.362297e-02 -0.251406
2019-11-05 00:18:57,324 train 200 1.363715e-02 -0.256483
2019-11-05 00:19:18,956 train 250 1.366543e-02 -0.262292
2019-11-05 00:19:40,495 train 300 1.367303e-02 -0.275803
2019-11-05 00:20:02,144 train 350 1.365915e-02 -0.275894
2019-11-05 00:20:23,700 train 400 1.364014e-02 -0.396540
2019-11-05 00:20:37,409 training loss; R2: 1.362358e-02 -0.394077
2019-11-05 00:20:38,476 valid 000 1.094059e-02 -2.635372
2019-11-05 00:20:58,360 validation loss; R2: 1.148821e-02 -0.713546
2019-11-05 00:20:58,386 epoch 99 lr 1.000000e-04
2019-11-05 00:20:59,569 train 000 1.357770e-02 -0.130012
2019-11-05 00:21:21,278 train 050 1.376945e-02 -0.114812
2019-11-05 00:21:42,893 train 100 1.373922e-02 -0.204694
2019-11-05 00:22:04,481 train 150 1.366397e-02 -0.259623
2019-11-05 00:22:25,823 train 200 1.371668e-02 -0.262458
2019-11-05 00:22:47,148 train 250 1.368542e-02 -0.328111
2019-11-05 00:23:08,527 train 300 1.363455e-02 -0.316531
2019-11-05 00:23:30,087 train 350 1.360176e-02 -0.307978
2019-11-05 00:23:51,756 train 400 1.361161e-02 -0.302492
2019-11-05 00:24:05,544 training loss; R2: 1.364458e-02 -0.299983
2019-11-05 00:24:06,576 valid 000 9.784797e-03 -2.026184
2019-11-05 00:24:26,560 validation loss; R2: 1.144358e-02 -0.819842
2019-11-05 00:24:26,587 epoch 100 lr 1.000000e-04
2019-11-05 00:24:27,736 train 000 1.616158e-02 -1.869302
2019-11-05 00:24:49,531 train 050 1.362079e-02 -0.298379
2019-11-05 00:25:11,183 train 100 1.365475e-02 -0.337448
2019-11-05 00:25:32,749 train 150 1.365002e-02 -0.320962
2019-11-05 00:25:54,307 train 200 1.359894e-02 -0.368815
2019-11-05 00:26:15,945 train 250 1.362417e-02 -0.362205
2019-11-05 00:26:37,286 train 300 1.361428e-02 -0.356393
2019-11-05 00:26:58,417 train 350 1.363118e-02 -0.370076
2019-11-05 00:27:19,622 train 400 1.363510e-02 -0.353551
2019-11-05 00:27:33,179 training loss; R2: 1.363230e-02 -0.353147
2019-11-05 00:27:34,174 valid 000 1.240658e-02 -0.305785
2019-11-05 00:27:53,785 validation loss; R2: 1.117354e-02 -0.739009
2019-11-05 00:27:53,811 epoch 101 lr 1.000000e-04
2019-11-05 00:27:54,997 train 000 1.360332e-02 -1.001499
2019-11-05 00:28:16,437 train 050 1.369695e-02 -0.299392
2019-11-05 00:28:37,790 train 100 1.368829e-02 -0.359950
2019-11-05 00:28:59,110 train 150 1.370845e-02 -0.337514
2019-11-05 00:29:20,660 train 200 1.371237e-02 -0.314369
2019-11-05 00:29:42,704 train 250 1.366402e-02 -0.361126
2019-11-05 00:30:04,388 train 300 1.368176e-02 -0.377041
2019-11-05 00:30:26,052 train 350 1.367121e-02 -0.378324
2019-11-05 00:30:47,673 train 400 1.366790e-02 -0.374777
2019-11-05 00:31:01,448 training loss; R2: 1.368546e-02 -0.379488
2019-11-05 00:31:02,467 valid 000 1.278184e-02 0.049087
2019-11-05 00:31:22,463 validation loss; R2: 1.301879e-02 -5.688624
2019-11-05 00:31:22,490 epoch 102 lr 1.000000e-04
2019-11-05 00:31:23,673 train 000 1.520111e-02 -0.062577
2019-11-05 00:31:45,368 train 050 1.356040e-02 -0.223602
2019-11-05 00:32:06,978 train 100 1.362217e-02 -0.274457
2019-11-05 00:32:28,520 train 150 1.359040e-02 -0.302735
2019-11-05 00:32:49,993 train 200 1.358040e-02 -0.288056
2019-11-05 00:33:11,505 train 250 1.358414e-02 -0.337631
2019-11-05 00:33:33,025 train 300 1.362749e-02 -0.336638
2019-11-05 00:33:54,883 train 350 1.361824e-02 -0.343299
2019-11-05 00:34:16,587 train 400 1.367575e-02 -0.339152
2019-11-05 00:34:30,426 training loss; R2: 1.367105e-02 -0.320509
2019-11-05 00:34:31,470 valid 000 1.105027e-02 -0.473526
2019-11-05 00:34:51,854 validation loss; R2: 1.109438e-02 -0.435490
2019-11-05 00:34:51,885 epoch 103 lr 1.000000e-04
2019-11-05 00:34:53,118 train 000 1.512184e-02 -0.844794
2019-11-05 00:35:14,887 train 050 1.364743e-02 -0.364207
2019-11-05 00:35:36,306 train 100 1.362322e-02 -0.343950
2019-11-05 00:35:57,798 train 150 1.355809e-02 -0.323862
2019-11-05 00:36:18,896 train 200 1.358309e-02 -0.360578
2019-11-05 00:36:40,409 train 250 1.354990e-02 -0.401928
2019-11-05 00:37:02,065 train 300 1.358322e-02 -0.388670
2019-11-05 00:37:23,561 train 350 1.359623e-02 -0.356046
2019-11-05 00:37:45,022 train 400 1.360099e-02 -0.360078
2019-11-05 00:37:58,718 training loss; R2: 1.362335e-02 -0.370073
2019-11-05 00:37:59,811 valid 000 1.293184e-02 -1.184686
2019-11-05 00:38:19,778 validation loss; R2: 1.265857e-02 -0.707276
2019-11-05 00:38:19,804 epoch 104 lr 1.000000e-04
2019-11-05 00:38:21,053 train 000 1.216945e-02 -0.536608
2019-11-05 00:38:42,731 train 050 1.374614e-02 -0.212792
2019-11-05 00:39:04,340 train 100 1.371104e-02 -0.266683
2019-11-05 00:39:25,931 train 150 1.375747e-02 -0.272169
2019-11-05 00:39:47,313 train 200 1.371163e-02 -0.284461
2019-11-05 00:40:08,858 train 250 1.369022e-02 -0.287857
2019-11-05 00:40:30,391 train 300 1.365969e-02 -0.285165
2019-11-05 00:40:51,752 train 350 1.365784e-02 -0.304737
2019-11-05 00:41:13,269 train 400 1.367511e-02 -0.388264
2019-11-05 00:41:27,040 training loss; R2: 1.367383e-02 -0.373008
2019-11-05 00:41:28,132 valid 000 1.023694e-02 0.152603
2019-11-05 00:41:47,992 validation loss; R2: 1.101769e-02 -0.497263
2019-11-05 00:41:48,018 epoch 105 lr 1.000000e-04
2019-11-05 00:41:49,223 train 000 1.471665e-02 -0.128337
2019-11-05 00:42:10,774 train 050 1.377062e-02 -0.147686
2019-11-05 00:42:32,193 train 100 1.376289e-02 -0.263955
2019-11-05 00:42:53,655 train 150 1.365228e-02 -0.285952
2019-11-05 00:43:15,063 train 200 1.365831e-02 -0.277343
2019-11-05 00:43:36,180 train 250 1.366939e-02 -0.267909
2019-11-05 00:43:57,456 train 300 1.367679e-02 -0.318367
2019-11-05 00:44:18,623 train 350 1.365854e-02 -0.328429
2019-11-05 00:44:40,119 train 400 1.362579e-02 -0.318481
2019-11-05 00:44:53,897 training loss; R2: 1.364174e-02 -0.316200
2019-11-05 00:44:54,908 valid 000 1.103292e-02 -0.367713
2019-11-05 00:45:15,408 validation loss; R2: 1.220888e-02 -0.773366
2019-11-05 00:45:15,439 epoch 106 lr 1.000000e-04
2019-11-05 00:45:16,590 train 000 1.265481e-02 -0.024486
2019-11-05 00:45:38,271 train 050 1.380248e-02 -0.184564
2019-11-05 00:45:59,928 train 100 1.364485e-02 -0.211655
2019-11-05 00:46:21,572 train 150 1.364520e-02 -0.224341
2019-11-05 00:46:42,791 train 200 1.363625e-02 -0.302872
2019-11-05 00:47:04,602 train 250 1.370124e-02 -0.346475
2019-11-05 00:47:26,247 train 300 1.369753e-02 -0.332125
2019-11-05 00:47:47,932 train 350 1.371192e-02 -0.308918
2019-11-05 00:48:09,822 train 400 1.369873e-02 -0.299946
2019-11-05 00:48:23,539 training loss; R2: 1.368439e-02 -0.298348
2019-11-05 00:48:24,543 valid 000 1.103050e-02 -1.143116
2019-11-05 00:48:44,765 validation loss; R2: 1.163823e-02 -0.424356
2019-11-05 00:48:44,791 epoch 107 lr 1.000000e-04
2019-11-05 00:48:46,000 train 000 1.386282e-02 0.053990
2019-11-05 00:49:07,925 train 050 1.348845e-02 -0.303104
2019-11-05 00:49:29,746 train 100 1.353749e-02 -0.318147
2019-11-05 00:49:51,539 train 150 1.353377e-02 -0.289870
2019-11-05 00:50:13,324 train 200 1.357245e-02 -0.290667
2019-11-05 00:50:34,766 train 250 1.362779e-02 -0.287357
2019-11-05 00:50:56,377 train 300 1.362141e-02 -0.281081
2019-11-05 00:51:18,110 train 350 1.357657e-02 -0.292953
2019-11-05 00:51:39,546 train 400 1.359987e-02 -0.296075
2019-11-05 00:51:53,273 training loss; R2: 1.360563e-02 -0.293841
2019-11-05 00:51:54,372 valid 000 1.086508e-02 0.287372
2019-11-05 00:52:15,022 validation loss; R2: 1.069984e-02 -0.443182
2019-11-05 00:52:15,064 epoch 108 lr 1.000000e-04
2019-11-05 00:52:16,288 train 000 1.264170e-02 0.081030
2019-11-05 00:52:38,112 train 050 1.346168e-02 -0.315197
2019-11-05 00:52:59,631 train 100 1.358395e-02 -0.288745
2019-11-05 00:53:21,015 train 150 1.363331e-02 -0.306954
2019-11-05 00:53:42,414 train 200 1.363538e-02 -0.302561
2019-11-05 00:54:03,900 train 250 1.366743e-02 -0.307059
2019-11-05 00:54:25,073 train 300 1.368764e-02 -0.295125
2019-11-05 00:54:46,175 train 350 1.365302e-02 -0.302454
2019-11-05 00:55:07,237 train 400 1.365065e-02 -0.342649
2019-11-05 00:55:20,661 training loss; R2: 1.362582e-02 -0.341505
2019-11-05 00:55:21,694 valid 000 1.088514e-02 -1.336868
2019-11-05 00:55:41,244 validation loss; R2: 1.105260e-02 -0.587749
2019-11-05 00:55:41,281 epoch 109 lr 1.000000e-04
2019-11-05 00:55:42,422 train 000 1.491515e-02 -3.775887
2019-11-05 00:56:03,838 train 050 1.360700e-02 -0.425564
2019-11-05 00:56:25,058 train 100 1.358789e-02 -0.402635
2019-11-05 00:56:46,303 train 150 1.364163e-02 -0.355423
2019-11-05 00:57:07,436 train 200 1.363042e-02 -0.364355
2019-11-05 00:57:28,816 train 250 1.360163e-02 -0.340369
2019-11-05 00:57:50,483 train 300 1.362573e-02 -0.322228
2019-11-05 00:58:11,991 train 350 1.362463e-02 -0.321035
2019-11-05 00:58:33,072 train 400 1.358311e-02 -0.317451
2019-11-05 00:58:46,648 training loss; R2: 1.359321e-02 -0.312140
2019-11-05 00:58:47,716 valid 000 1.186157e-02 0.046881
2019-11-05 00:59:07,210 validation loss; R2: 1.109890e-02 -0.636168
2019-11-05 00:59:07,236 epoch 110 lr 1.000000e-04
2019-11-05 00:59:08,440 train 000 3.102956e-02 -0.292320
2019-11-05 00:59:29,531 train 050 1.495848e-02 -0.225235
2019-11-05 00:59:50,737 train 100 1.428442e-02 -0.269563
2019-11-05 01:00:11,954 train 150 1.408202e-02 -0.303194
2019-11-05 01:00:33,046 train 200 1.397993e-02 -0.286712
2019-11-05 01:00:54,245 train 250 1.389389e-02 -0.290485
2019-11-05 01:01:15,593 train 300 1.383537e-02 -0.311736
2019-11-05 01:01:37,159 train 350 1.380051e-02 -0.317792
2019-11-05 01:01:58,831 train 400 1.377793e-02 -0.297563
2019-11-05 01:02:12,616 training loss; R2: 1.376433e-02 -0.298240
2019-11-05 01:02:13,702 valid 000 1.014286e-02 0.163831
2019-11-05 01:02:34,053 validation loss; R2: 1.104536e-02 -0.288684
2019-11-05 01:02:34,084 epoch 111 lr 1.000000e-04
2019-11-05 01:02:35,255 train 000 1.273611e-02 -0.045604
2019-11-05 01:02:57,011 train 050 1.389064e-02 -0.265450
2019-11-05 01:03:18,746 train 100 1.371882e-02 -0.255330
2019-11-05 01:03:40,540 train 150 1.373696e-02 -0.250972
2019-11-05 01:04:02,170 train 200 1.374546e-02 -0.271596
2019-11-05 01:04:23,443 train 250 1.366490e-02 -0.298140
2019-11-05 01:04:44,456 train 300 1.364257e-02 -0.300563
2019-11-05 01:05:05,661 train 350 1.361897e-02 -0.287639
2019-11-05 01:05:27,338 train 400 1.360423e-02 -0.298805
2019-11-05 01:05:41,151 training loss; R2: 1.362103e-02 -0.299640
2019-11-05 01:05:42,256 valid 000 1.329845e-02 0.012908
2019-11-05 01:06:02,274 validation loss; R2: 1.215495e-02 -0.658030
2019-11-05 01:06:02,301 epoch 112 lr 1.000000e-04
2019-11-05 01:06:03,522 train 000 1.375467e-02 -0.035654
2019-11-05 01:06:25,403 train 050 1.347815e-02 -0.419559
2019-11-05 01:06:46,891 train 100 1.351300e-02 -0.393898
2019-11-05 01:07:08,387 train 150 1.354844e-02 -0.349370
2019-11-05 01:07:29,776 train 200 1.355011e-02 -0.322638
2019-11-05 01:07:51,344 train 250 1.357698e-02 -0.306908
2019-11-05 01:08:12,729 train 300 1.357454e-02 -0.290361
2019-11-05 01:08:34,449 train 350 1.355733e-02 -0.291760
2019-11-05 01:08:56,116 train 400 1.355476e-02 -0.301978
2019-11-05 01:09:09,771 training loss; R2: 1.354635e-02 -0.294020
2019-11-05 01:09:10,864 valid 000 1.052163e-02 -1.183514
2019-11-05 01:09:31,187 validation loss; R2: 1.119876e-02 -0.673231
2019-11-05 01:09:31,212 epoch 113 lr 1.000000e-04
2019-11-05 01:09:32,404 train 000 1.570878e-02 -0.259861
2019-11-05 01:09:54,455 train 050 1.352170e-02 -0.337867
2019-11-05 01:10:16,381 train 100 1.363481e-02 -0.322416
2019-11-05 01:10:38,250 train 150 1.363538e-02 -0.290797
2019-11-05 01:11:00,128 train 200 1.361041e-02 -0.363972
2019-11-05 01:11:21,633 train 250 1.358630e-02 -0.360242
2019-11-05 01:11:42,986 train 300 1.362544e-02 -0.357774
2019-11-05 01:12:04,379 train 350 1.360475e-02 -0.346746
2019-11-05 01:12:25,817 train 400 1.359607e-02 -0.331878
2019-11-05 01:12:39,536 training loss; R2: 1.359222e-02 -0.335995
2019-11-05 01:12:40,627 valid 000 1.240601e-02 -1.402204
2019-11-05 01:13:00,979 validation loss; R2: 1.117537e-02 -0.638156
2019-11-05 01:13:01,010 epoch 114 lr 1.000000e-04
2019-11-05 01:13:02,247 train 000 1.297856e-02 0.055477
2019-11-05 01:13:23,835 train 050 1.346254e-02 -0.241641
2019-11-05 01:13:45,172 train 100 1.340691e-02 -0.224069
2019-11-05 01:14:06,456 train 150 1.352327e-02 -0.231870
2019-11-05 01:14:28,079 train 200 1.357296e-02 -0.269221
2019-11-05 01:14:49,827 train 250 1.360484e-02 -0.266162
2019-11-05 01:15:11,318 train 300 1.360691e-02 -0.277331
2019-11-05 01:15:33,111 train 350 1.359123e-02 -0.288542
2019-11-05 01:15:54,918 train 400 1.357120e-02 -0.302319
2019-11-05 01:16:08,920 training loss; R2: 1.359161e-02 -0.300235
2019-11-05 01:16:09,990 valid 000 1.256565e-02 -1.308993
2019-11-05 01:16:30,215 validation loss; R2: 1.232174e-02 -0.706232
2019-11-05 01:16:30,248 epoch 115 lr 1.000000e-04
2019-11-05 01:16:31,479 train 000 1.395043e-02 -0.028292
2019-11-05 01:16:53,108 train 050 1.351259e-02 -1.108549
2019-11-05 01:17:14,600 train 100 1.360292e-02 -0.753759
2019-11-05 01:17:36,008 train 150 1.363594e-02 -0.605132
2019-11-05 01:17:57,423 train 200 1.364588e-02 -0.533992
2019-11-05 01:18:19,224 train 250 1.367996e-02 -0.500878
2019-11-05 01:18:40,522 train 300 1.368760e-02 -0.471721
2019-11-05 01:19:01,762 train 350 1.366200e-02 -0.455271
2019-11-05 01:19:23,405 train 400 1.366615e-02 -0.428362
2019-11-05 01:19:37,361 training loss; R2: 1.367719e-02 -0.415671
2019-11-05 01:19:38,466 valid 000 9.991913e-03 -0.210567
2019-11-05 01:19:58,621 validation loss; R2: 1.074713e-02 -0.453309
2019-11-05 01:19:58,649 epoch 116 lr 1.000000e-04
2019-11-05 01:19:59,800 train 000 1.354094e-02 -0.075767
2019-11-05 01:20:21,408 train 050 1.372204e-02 -0.181284
2019-11-05 01:20:42,729 train 100 1.374408e-02 -0.295861
2019-11-05 01:21:03,963 train 150 1.368077e-02 -0.328532
2019-11-05 01:21:25,086 train 200 1.358677e-02 -0.297031
2019-11-05 01:21:46,087 train 250 1.355375e-02 -0.293661
2019-11-05 01:22:07,159 train 300 1.355431e-02 -0.308495
2019-11-05 01:22:28,509 train 350 1.357023e-02 -0.317602
2019-11-05 01:22:49,971 train 400 1.355337e-02 -0.330265
2019-11-05 01:23:03,720 training loss; R2: 1.356299e-02 -0.333703
2019-11-05 01:23:04,780 valid 000 1.097141e-02 -0.181883
2019-11-05 01:23:25,275 validation loss; R2: 1.117926e-02 -0.644208
2019-11-05 01:23:25,307 epoch 117 lr 1.000000e-04
2019-11-05 01:23:26,513 train 000 1.370600e-02 -0.036509
2019-11-05 01:23:48,210 train 050 1.334136e-02 -0.273084
2019-11-05 01:24:09,759 train 100 1.339866e-02 -0.270754
2019-11-05 01:24:31,141 train 150 1.342218e-02 -0.276065
2019-11-05 01:24:52,229 train 200 1.347642e-02 -0.311130
2019-11-05 01:25:13,262 train 250 1.350398e-02 -0.312296
2019-11-05 01:25:34,639 train 300 1.349048e-02 -0.317691
2019-11-05 01:25:56,613 train 350 1.351326e-02 -0.603813
2019-11-05 01:26:18,441 train 400 1.350491e-02 -0.602024
2019-11-05 01:26:32,032 training loss; R2: 1.352092e-02 -0.574839
2019-11-05 01:26:33,077 valid 000 1.097565e-02 0.275434
2019-11-05 01:26:52,686 validation loss; R2: 1.077171e-02 -0.330128
2019-11-05 01:26:52,715 epoch 118 lr 1.000000e-04
2019-11-05 01:26:53,859 train 000 1.487397e-02 0.086334
2019-11-05 01:27:15,413 train 050 1.367373e-02 -0.286992
2019-11-05 01:27:36,883 train 100 1.362096e-02 -0.342369
2019-11-05 01:27:58,302 train 150 1.362118e-02 -0.336699
2019-11-05 01:28:19,516 train 200 1.361752e-02 -0.306105
2019-11-05 01:28:40,927 train 250 1.363783e-02 -0.309051
2019-11-05 01:29:02,017 train 300 1.359761e-02 -0.292807
2019-11-05 01:29:23,062 train 350 1.360560e-02 -0.272821
2019-11-05 01:29:44,168 train 400 1.358324e-02 -0.279787
2019-11-05 01:29:57,603 training loss; R2: 1.358764e-02 -0.277305
2019-11-05 01:29:58,636 valid 000 1.177577e-02 -0.254178
2019-11-05 01:30:18,883 validation loss; R2: 1.098642e-02 -0.704006
2019-11-05 01:30:18,908 epoch 119 lr 1.000000e-04
2019-11-05 01:30:20,090 train 000 1.274564e-02 0.164200
2019-11-05 01:30:41,572 train 050 1.328289e-02 -0.199765
2019-11-05 01:31:03,063 train 100 1.335903e-02 -0.243458
2019-11-05 01:31:24,787 train 150 1.349519e-02 -0.283296
2019-11-05 01:31:46,252 train 200 1.352605e-02 -0.257531
2019-11-05 01:32:07,503 train 250 1.351612e-02 -0.274513
2019-11-05 01:32:28,509 train 300 1.352116e-02 -0.311926
2019-11-05 01:32:49,975 train 350 1.348427e-02 -0.309091
2019-11-05 01:33:11,751 train 400 1.349180e-02 -0.317488
2019-11-05 01:33:25,510 training loss; R2: 1.351128e-02 -0.321287
2019-11-05 01:33:26,604 valid 000 1.140520e-02 -0.450330
2019-11-05 01:33:46,738 validation loss; R2: 1.134330e-02 -0.374230
2019-11-05 01:33:46,770 epoch 120 lr 1.000000e-04
2019-11-05 01:33:47,980 train 000 1.223457e-02 0.006774
2019-11-05 01:34:09,258 train 050 1.333377e-02 -0.128404
2019-11-05 01:34:30,974 train 100 1.346425e-02 -0.203011
2019-11-05 01:34:52,442 train 150 1.355521e-02 -0.230101
2019-11-05 01:35:14,121 train 200 1.361258e-02 -0.209075
2019-11-05 01:35:35,684 train 250 1.359848e-02 -0.226190
2019-11-05 01:35:57,047 train 300 1.361313e-02 -0.238876
2019-11-05 01:36:18,645 train 350 1.360361e-02 -0.249446
2019-11-05 01:36:40,316 train 400 1.364129e-02 -0.247756
2019-11-05 01:36:54,062 training loss; R2: 1.363125e-02 -0.270852
2019-11-05 01:36:55,165 valid 000 1.100123e-02 -1.585709
2019-11-05 01:37:15,201 validation loss; R2: 1.075218e-02 -0.852799
2019-11-05 01:37:15,225 epoch 121 lr 1.000000e-04
2019-11-05 01:37:16,425 train 000 1.400963e-02 0.082283
2019-11-05 01:37:37,910 train 050 1.365522e-02 -0.311275
2019-11-05 01:37:59,294 train 100 1.367273e-02 -0.291348
2019-11-05 01:38:20,829 train 150 1.361944e-02 -0.272583
2019-11-05 01:38:42,400 train 200 1.359791e-02 -0.250103
2019-11-05 01:39:03,813 train 250 1.356409e-02 -0.240238
2019-11-05 01:39:25,173 train 300 1.358363e-02 -0.266775
2019-11-05 01:39:46,760 train 350 1.359592e-02 -0.272744
2019-11-05 01:40:08,222 train 400 1.361271e-02 -0.253379
2019-11-05 01:40:21,927 training loss; R2: 1.362836e-02 -0.244245
2019-11-05 01:40:23,036 valid 000 1.146294e-02 -1.889237
2019-11-05 01:40:43,107 validation loss; R2: 1.149217e-02 -0.483310
2019-11-05 01:40:43,129 epoch 122 lr 1.000000e-04
2019-11-05 01:40:44,375 train 000 1.559851e-02 -0.199727
2019-11-05 01:41:06,276 train 050 1.377223e-02 -0.251384
2019-11-05 01:41:28,175 train 100 1.361637e-02 -0.225755
2019-11-05 01:41:50,149 train 150 1.360059e-02 -0.265712
2019-11-05 01:42:11,960 train 200 1.369286e-02 -0.268040
2019-11-05 01:42:33,323 train 250 1.367193e-02 -0.281980
2019-11-05 01:42:54,822 train 300 1.365463e-02 -0.279932
2019-11-05 01:43:16,666 train 350 1.364680e-02 -0.281700
2019-11-05 01:43:38,488 train 400 1.363446e-02 -0.279094
2019-11-05 01:43:52,359 training loss; R2: 1.362394e-02 -0.284654
2019-11-05 01:43:53,362 valid 000 1.135674e-02 -0.561591
2019-11-05 01:44:13,191 validation loss; R2: 1.182583e-02 -0.986114
2019-11-05 01:44:13,223 epoch 123 lr 1.000000e-04
2019-11-05 01:44:14,425 train 000 1.315590e-02 -0.322800
2019-11-05 01:44:36,138 train 050 1.352691e-02 -0.222121
2019-11-05 01:44:57,683 train 100 1.358370e-02 -0.231506
2019-11-05 01:45:19,212 train 150 1.357730e-02 -0.249911
2019-11-05 01:45:40,641 train 200 1.352792e-02 -0.279794
2019-11-05 01:46:02,156 train 250 1.358214e-02 -0.295767
2019-11-05 01:46:23,488 train 300 1.361400e-02 -0.281533
2019-11-05 01:46:44,954 train 350 1.365812e-02 -0.292457
2019-11-05 01:47:06,243 train 400 1.367765e-02 -0.299397
2019-11-05 01:47:19,856 training loss; R2: 1.367971e-02 -0.298806
2019-11-05 01:47:20,885 valid 000 1.283086e-02 -0.527026
2019-11-05 01:47:40,727 validation loss; R2: 1.182624e-02 -0.439980
2019-11-05 01:47:40,770 epoch 124 lr 1.000000e-04
2019-11-05 01:47:41,961 train 000 1.448379e-02 -0.114156
2019-11-05 01:48:03,816 train 050 1.373250e-02 -0.275709
2019-11-05 01:48:25,134 train 100 1.372767e-02 -0.242968
2019-11-05 01:48:46,372 train 150 1.368524e-02 -0.225966
2019-11-05 01:49:07,749 train 200 1.362467e-02 -0.271207
2019-11-05 01:49:29,333 train 250 1.361591e-02 -0.305116
2019-11-05 01:49:50,635 train 300 1.358221e-02 -0.308463
2019-11-05 01:50:11,881 train 350 1.363795e-02 -0.292905
2019-11-05 01:50:33,162 train 400 1.362302e-02 -0.284420
2019-11-05 01:50:46,747 training loss; R2: 1.363889e-02 -0.279500
2019-11-05 01:50:47,733 valid 000 1.180284e-02 -0.701960
2019-11-05 01:51:07,266 validation loss; R2: 1.212941e-02 -1.074601
2019-11-05 01:51:07,303 epoch 125 lr 1.000000e-04
2019-11-05 01:51:08,465 train 000 1.335399e-02 -0.920563
2019-11-05 01:51:29,847 train 050 1.349936e-02 -0.267708
2019-11-05 01:51:51,096 train 100 1.358390e-02 -0.248831
2019-11-05 01:52:12,248 train 150 1.363702e-02 -0.345325
2019-11-05 01:52:33,665 train 200 1.364251e-02 -0.349263
2019-11-05 01:52:55,326 train 250 1.362774e-02 -0.336555
2019-11-05 01:53:17,284 train 300 1.360124e-02 -0.327131
2019-11-05 01:53:38,996 train 350 1.358450e-02 -0.339795
2019-11-05 01:54:00,742 train 400 1.359317e-02 -0.344819
2019-11-05 01:54:14,548 training loss; R2: 1.360391e-02 -0.341623
2019-11-05 01:54:15,587 valid 000 1.013679e-02 0.158339
2019-11-05 01:54:35,585 validation loss; R2: 1.095540e-02 -0.520634
2019-11-05 01:54:35,611 epoch 126 lr 1.000000e-04
2019-11-05 01:54:36,809 train 000 1.341483e-02 0.093324
2019-11-05 01:54:58,543 train 050 1.369066e-02 -0.294336
2019-11-05 01:55:20,153 train 100 1.370567e-02 -0.283807
2019-11-05 01:55:41,745 train 150 1.370590e-02 -0.274700
2019-11-05 01:56:03,252 train 200 1.368272e-02 -0.261706
2019-11-05 01:56:24,675 train 250 1.368466e-02 -0.278911
2019-11-05 01:56:45,907 train 300 1.364805e-02 -0.302953
2019-11-05 01:57:07,037 train 350 1.364842e-02 -0.272226
2019-11-05 01:57:28,263 train 400 1.367675e-02 -0.262598
2019-11-05 01:57:41,796 training loss; R2: 1.369907e-02 -0.263626
2019-11-05 01:57:42,851 valid 000 1.111295e-02 -1.374400
2019-11-05 01:58:02,481 validation loss; R2: 1.132541e-02 -0.454818
2019-11-05 01:58:02,507 epoch 127 lr 1.000000e-04
2019-11-05 01:58:03,669 train 000 3.686799e-02 -1.027635
2019-11-05 01:58:25,259 train 050 1.551918e-02 -0.287577
2019-11-05 01:58:46,642 train 100 1.463074e-02 -0.375320
2019-11-05 01:59:07,859 train 150 1.431536e-02 -0.393233
2019-11-05 01:59:29,209 train 200 1.414048e-02 -0.369211
2019-11-05 01:59:50,841 train 250 1.402969e-02 -0.365027
2019-11-05 02:00:12,564 train 300 1.396640e-02 -0.336515
2019-11-05 02:00:34,185 train 350 1.391800e-02 -0.332929
2019-11-05 02:00:55,752 train 400 1.389102e-02 -0.321501
2019-11-05 02:01:09,512 training loss; R2: 1.386308e-02 -0.318376
2019-11-05 02:01:10,584 valid 000 9.713412e-03 0.034244
2019-11-05 02:01:30,483 validation loss; R2: 1.092029e-02 -0.408159
2019-11-05 02:01:30,512 epoch 128 lr 1.000000e-04
2019-11-05 02:01:31,670 train 000 1.359651e-02 -0.469597
2019-11-05 02:01:53,259 train 050 1.377432e-02 -0.224643
2019-11-05 02:02:14,698 train 100 1.375013e-02 -0.238113
2019-11-05 02:02:36,092 train 150 1.368289e-02 -0.249423
2019-11-05 02:02:57,574 train 200 1.367973e-02 -0.286268
2019-11-05 02:03:19,138 train 250 1.366265e-02 -0.291912
2019-11-05 02:03:40,378 train 300 1.366225e-02 -0.310111
2019-11-05 02:04:01,662 train 350 1.367568e-02 -0.316341
2019-11-05 02:04:22,877 train 400 1.364708e-02 -0.305470
2019-11-05 02:04:36,329 training loss; R2: 1.364317e-02 -0.300490
2019-11-05 02:04:37,329 valid 000 1.249562e-02 0.128227
2019-11-05 02:04:56,812 validation loss; R2: 1.298868e-02 -0.976965
2019-11-05 02:04:56,840 epoch 129 lr 1.000000e-04
2019-11-05 02:04:57,959 train 000 1.255792e-02 0.143334
2019-11-05 02:05:19,455 train 050 1.342120e-02 -0.368651
2019-11-05 02:05:40,861 train 100 1.360392e-02 -0.313003
2019-11-05 02:06:02,271 train 150 1.360078e-02 -0.370516
2019-11-05 02:06:23,736 train 200 1.358341e-02 -0.360745
2019-11-05 02:06:45,393 train 250 1.358547e-02 -0.362721
2019-11-05 02:07:06,793 train 300 1.357872e-02 -0.361696
2019-11-05 02:07:28,208 train 350 1.358807e-02 -0.339821
2019-11-05 02:07:49,552 train 400 1.358825e-02 -0.325559
2019-11-05 02:08:03,137 training loss; R2: 1.362950e-02 -0.322891
2019-11-05 02:08:04,191 valid 000 1.084942e-02 -0.347666
2019-11-05 02:08:23,777 validation loss; R2: 1.094362e-02 -0.402998
2019-11-05 02:08:23,815 epoch 130 lr 1.000000e-04
2019-11-05 02:08:25,003 train 000 1.434565e-02 -0.066684
2019-11-05 02:08:46,430 train 050 1.357158e-02 -0.330804
2019-11-05 02:09:07,839 train 100 1.361222e-02 -0.304507
2019-11-05 02:09:29,249 train 150 1.360815e-02 -0.310502
2019-11-05 02:09:50,703 train 200 1.369553e-02 -0.297102
2019-11-05 02:10:12,327 train 250 1.373322e-02 -0.279518
2019-11-05 02:10:34,320 train 300 1.368228e-02 -0.269060
2019-11-05 02:10:56,159 train 350 1.372238e-02 -0.278819
2019-11-05 02:11:17,866 train 400 1.373200e-02 -0.282217
2019-11-05 02:11:31,797 training loss; R2: 1.372940e-02 -0.276373
2019-11-05 02:11:32,808 valid 000 1.517713e-02 -0.908765
2019-11-05 02:11:52,784 validation loss; R2: 1.386078e-02 -0.939047
2019-11-05 02:11:52,815 epoch 131 lr 1.000000e-04
2019-11-05 02:11:53,965 train 000 1.190528e-02 -0.115234
2019-11-05 02:12:15,683 train 050 1.346569e-02 -0.277410
2019-11-05 02:12:37,352 train 100 1.360776e-02 -0.262408
2019-11-05 02:12:58,903 train 150 1.360343e-02 -0.310071
2019-11-05 02:13:20,527 train 200 1.358415e-02 -0.309817
2019-11-05 02:13:42,042 train 250 1.362726e-02 -0.282018
2019-11-05 02:14:03,351 train 300 1.362200e-02 -0.278003
2019-11-05 02:14:24,705 train 350 1.364012e-02 -0.273727
2019-11-05 02:14:45,983 train 400 1.367122e-02 -0.275523
2019-11-05 02:14:59,564 training loss; R2: 1.366727e-02 -0.291607
2019-11-05 02:15:00,570 valid 000 1.290697e-02 0.086969
2019-11-05 02:15:20,163 validation loss; R2: 1.289197e-02 -0.810620
2019-11-05 02:15:20,189 epoch 132 lr 1.000000e-04
2019-11-05 02:15:21,406 train 000 1.330821e-02 0.183350
2019-11-05 02:15:42,780 train 050 1.345213e-02 -0.179985
2019-11-05 02:16:04,044 train 100 1.354167e-02 -0.181868
2019-11-05 02:16:25,368 train 150 1.355897e-02 -0.217765
2019-11-05 02:16:46,563 train 200 1.346053e-02 -0.219348
2019-11-05 02:17:08,177 train 250 1.352043e-02 -0.238808
2019-11-05 02:17:29,410 train 300 1.354961e-02 -0.257505
2019-11-05 02:17:50,640 train 350 1.356175e-02 -0.252654
2019-11-05 02:18:11,814 train 400 1.358843e-02 -0.262100
2019-11-05 02:18:25,501 training loss; R2: 1.359918e-02 -0.262413
2019-11-05 02:18:26,514 valid 000 1.073136e-02 -1.447170
2019-11-05 02:18:46,544 validation loss; R2: 1.109298e-02 -0.410231
2019-11-05 02:18:46,579 epoch 133 lr 1.000000e-04
2019-11-05 02:18:47,728 train 000 1.311922e-02 -0.037003
2019-11-05 02:19:09,227 train 050 1.365533e-02 -0.224410
2019-11-05 02:19:30,710 train 100 1.354195e-02 -0.280864
2019-11-05 02:19:52,103 train 150 1.363669e-02 -0.256296
2019-11-05 02:20:13,482 train 200 1.362627e-02 -0.291972
2019-11-05 02:20:35,237 train 250 1.363301e-02 -0.287839
2019-11-05 02:20:56,982 train 300 1.363908e-02 -0.296786
2019-11-05 02:21:18,745 train 350 1.363577e-02 -0.293678
2019-11-05 02:21:40,523 train 400 1.362193e-02 -0.336845
2019-11-05 02:21:54,330 training loss; R2: 1.361320e-02 -0.335566
2019-11-05 02:21:55,390 valid 000 1.170254e-02 -0.191332
2019-11-05 02:22:15,375 validation loss; R2: 1.081589e-02 -0.601270
2019-11-05 02:22:15,401 epoch 134 lr 1.000000e-04
2019-11-05 02:22:16,552 train 000 1.332294e-02 -0.952744
2019-11-05 02:22:38,343 train 050 1.355775e-02 -0.356791
2019-11-05 02:23:00,016 train 100 1.347081e-02 -0.341632
2019-11-05 02:23:21,667 train 150 1.346086e-02 -0.320532
2019-11-05 02:23:43,258 train 200 1.354216e-02 -0.303320
2019-11-05 02:24:04,839 train 250 1.356040e-02 -0.280187
2019-11-05 02:24:26,356 train 300 1.355486e-02 -0.263338
2019-11-05 02:24:47,821 train 350 1.354444e-02 -0.264042
2019-11-05 02:25:09,130 train 400 1.356353e-02 -0.263989
2019-11-05 02:25:22,753 training loss; R2: 1.356233e-02 -0.265274
2019-11-05 02:25:23,786 valid 000 1.074382e-02 -1.141199
2019-11-05 02:25:44,081 validation loss; R2: 1.148401e-02 -0.513432
2019-11-05 02:25:44,105 epoch 135 lr 1.000000e-04
2019-11-05 02:25:45,278 train 000 1.410783e-02 -0.192889
2019-11-05 02:26:06,796 train 050 1.343030e-02 -0.242397
2019-11-05 02:26:28,250 train 100 1.359352e-02 -0.210970
2019-11-05 02:26:49,566 train 150 1.356606e-02 -0.249946
2019-11-05 02:27:10,926 train 200 1.360063e-02 -0.255488
2019-11-05 02:27:32,473 train 250 1.357601e-02 -0.261771
2019-11-05 02:27:54,237 train 300 1.355260e-02 -0.279656
2019-11-05 02:28:15,957 train 350 1.353599e-02 -0.263222
2019-11-05 02:28:38,078 train 400 1.353670e-02 -0.266001
2019-11-05 02:28:51,952 training loss; R2: 1.353452e-02 -0.275558
2019-11-05 02:28:53,000 valid 000 1.120923e-02 0.023281
2019-11-05 02:29:12,899 validation loss; R2: 1.109542e-02 -0.324157
2019-11-05 02:29:12,923 epoch 136 lr 1.000000e-04
2019-11-05 02:29:14,095 train 000 1.379846e-02 -0.636906
2019-11-05 02:29:35,815 train 050 1.368773e-02 -0.228695
2019-11-05 02:29:57,425 train 100 1.361319e-02 -0.236748
2019-11-05 02:30:19,041 train 150 1.356345e-02 -0.220799
2019-11-05 02:30:40,797 train 200 1.355843e-02 -0.265699
2019-11-05 02:31:02,518 train 250 1.357859e-02 -0.270890
2019-11-05 02:31:24,312 train 300 1.359710e-02 -0.273973
2019-11-05 02:31:46,417 train 350 1.358129e-02 -0.286749
2019-11-05 02:32:08,350 train 400 1.357360e-02 -0.283436
2019-11-05 02:32:22,337 training loss; R2: 1.357443e-02 -0.274832
2019-11-05 02:32:23,362 valid 000 1.195250e-02 -0.495732
2019-11-05 02:32:43,588 validation loss; R2: 1.253894e-02 -0.712198
2019-11-05 02:32:43,624 epoch 137 lr 1.000000e-04
2019-11-05 02:32:44,849 train 000 1.338007e-02 -0.081796
2019-11-05 02:33:06,820 train 050 1.377902e-02 -0.214855
2019-11-05 02:33:28,613 train 100 1.361054e-02 -0.208918
2019-11-05 02:33:50,412 train 150 1.361707e-02 -0.236412
2019-11-05 02:34:11,903 train 200 1.356239e-02 -0.267068
2019-11-05 02:34:33,466 train 250 1.352008e-02 -0.292120
2019-11-05 02:34:54,915 train 300 1.351890e-02 -0.301436
2019-11-05 02:35:16,229 train 350 1.353515e-02 -0.297686
2019-11-05 02:35:37,455 train 400 1.356130e-02 -0.282145
2019-11-05 02:35:50,984 training loss; R2: 1.355392e-02 -0.289330
2019-11-05 02:35:52,013 valid 000 1.097487e-02 -2.082851
2019-11-05 02:36:11,818 validation loss; R2: 1.119275e-02 -0.819203
2019-11-05 02:36:11,854 epoch 138 lr 1.000000e-04
2019-11-05 02:36:12,961 train 000 1.205543e-02 -0.031218
2019-11-05 02:36:34,316 train 050 1.334588e-02 -0.467490
2019-11-05 02:36:55,666 train 100 1.340183e-02 -0.421029
2019-11-05 02:37:16,995 train 150 1.340516e-02 -0.385049
2019-11-05 02:37:38,353 train 200 1.340206e-02 -0.357063
2019-11-05 02:37:59,905 train 250 1.340463e-02 -0.323424
2019-11-05 02:38:21,568 train 300 1.338673e-02 -0.310917
2019-11-05 02:38:43,045 train 350 1.340289e-02 -0.307722
2019-11-05 02:39:04,611 train 400 1.341200e-02 -0.315756
2019-11-05 02:39:18,323 training loss; R2: 1.341921e-02 -0.317895
2019-11-05 02:39:19,406 valid 000 1.084240e-02 -0.124496
2019-11-05 02:39:39,354 validation loss; R2: 1.201810e-02 -0.578970
2019-11-05 02:39:39,381 epoch 139 lr 1.000000e-04
2019-11-05 02:39:40,516 train 000 1.448024e-02 0.030054
2019-11-05 02:40:02,237 train 050 1.344630e-02 -0.360441
2019-11-05 02:40:23,775 train 100 1.341466e-02 -0.283091
2019-11-05 02:40:45,361 train 150 1.344657e-02 -0.367542
2019-11-05 02:41:06,977 train 200 1.341720e-02 -0.346351
2019-11-05 02:41:28,505 train 250 1.350500e-02 -0.308590
2019-11-05 02:41:49,885 train 300 1.348936e-02 -0.325227
2019-11-05 02:42:11,198 train 350 1.352119e-02 -0.323942
2019-11-05 02:42:32,478 train 400 1.348883e-02 -0.339761
2019-11-05 02:42:46,127 training loss; R2: 1.348511e-02 -0.326849
2019-11-05 02:42:47,142 valid 000 1.010931e-02 -0.334018
2019-11-05 02:43:06,833 validation loss; R2: 1.119892e-02 -0.367268
2019-11-05 02:43:06,857 epoch 140 lr 1.000000e-04
2019-11-05 02:43:07,977 train 000 1.450569e-02 -0.756862
2019-11-05 02:43:29,424 train 050 1.368150e-02 -0.407407
2019-11-05 02:43:50,688 train 100 1.355171e-02 -0.307375
2019-11-05 02:44:12,020 train 150 1.361738e-02 -0.358974
2019-11-05 02:44:33,263 train 200 1.363845e-02 -0.350919
2019-11-05 02:44:54,636 train 250 1.356805e-02 -0.338078
2019-11-05 02:45:16,383 train 300 1.356184e-02 -0.349341
2019-11-05 02:45:37,847 train 350 1.354128e-02 -0.337286
2019-11-05 02:45:59,325 train 400 1.351664e-02 -0.322394
2019-11-05 02:46:12,969 training loss; R2: 1.350172e-02 -0.335181
2019-11-05 02:46:13,968 valid 000 1.175709e-02 0.035518
2019-11-05 02:46:33,775 validation loss; R2: 1.151019e-02 -0.515816
2019-11-05 02:46:33,805 epoch 141 lr 1.000000e-04
2019-11-05 02:46:34,963 train 000 1.230143e-02 -0.401460
2019-11-05 02:46:56,536 train 050 1.364392e-02 -0.179647
2019-11-05 02:47:17,965 train 100 1.359166e-02 -0.277577
2019-11-05 02:47:39,358 train 150 1.356422e-02 -0.264996
2019-11-05 02:48:01,023 train 200 1.350822e-02 -0.289897
2019-11-05 02:48:22,637 train 250 1.347298e-02 -0.305389
2019-11-05 02:48:44,138 train 300 1.350351e-02 -0.316231
2019-11-05 02:49:05,315 train 350 1.352080e-02 -0.324210
2019-11-05 02:49:26,634 train 400 1.350530e-02 -0.335394
2019-11-05 02:49:40,285 training loss; R2: 1.351563e-02 -0.329158
2019-11-05 02:49:41,295 valid 000 1.167080e-02 0.123626
2019-11-05 02:50:00,911 validation loss; R2: 1.245479e-02 -0.099272
2019-11-05 02:50:00,938 epoch 142 lr 1.000000e-04
2019-11-05 02:50:02,099 train 000 1.226463e-02 0.245705
2019-11-05 02:50:23,348 train 050 1.340704e-02 -0.358204
2019-11-05 02:50:44,616 train 100 1.346496e-02 -0.246781
2019-11-05 02:51:05,855 train 150 1.356777e-02 -0.304380
2019-11-05 02:51:27,131 train 200 1.349939e-02 -0.279538
2019-11-05 02:51:48,403 train 250 1.345747e-02 -0.277589
2019-11-05 02:52:09,922 train 300 1.342363e-02 -0.268367
2019-11-05 02:52:31,726 train 350 1.341988e-02 -0.275494
2019-11-05 02:52:53,424 train 400 1.343629e-02 -0.279511
2019-11-05 02:53:07,187 training loss; R2: 1.342353e-02 -0.294723
2019-11-05 02:53:08,207 valid 000 1.439344e-02 -0.195651
2019-11-05 02:53:28,176 validation loss; R2: 1.301116e-02 -0.310171
2019-11-05 02:53:28,200 epoch 143 lr 1.000000e-04
2019-11-05 02:53:29,327 train 000 1.329061e-02 -0.392904
2019-11-05 02:53:51,021 train 050 1.342493e-02 -0.245503
2019-11-05 02:54:12,553 train 100 1.348536e-02 -0.258906
2019-11-05 02:54:34,129 train 150 1.340209e-02 -0.282092
2019-11-05 02:54:55,726 train 200 1.343991e-02 -0.259616
2019-11-05 02:55:17,127 train 250 1.340947e-02 -0.264258
2019-11-05 02:55:38,342 train 300 1.337696e-02 -0.266276
2019-11-05 02:55:59,844 train 350 1.339627e-02 -0.293246
2019-11-05 02:56:21,176 train 400 1.341888e-02 -0.269644
2019-11-05 02:56:34,790 training loss; R2: 1.342375e-02 -0.276695
2019-11-05 02:56:35,770 valid 000 1.342487e-02 0.175285
2019-11-05 02:56:55,464 validation loss; R2: 1.198287e-02 -0.482963
2019-11-05 02:56:55,488 epoch 144 lr 1.000000e-04
2019-11-05 02:56:56,649 train 000 1.295166e-02 -0.650155
2019-11-05 02:57:18,077 train 050 1.328574e-02 -0.222078
2019-11-05 02:57:39,594 train 100 1.326923e-02 -0.277917
2019-11-05 02:58:01,249 train 150 1.338012e-02 -0.268318
2019-11-05 02:58:22,932 train 200 1.337570e-02 -0.294007
2019-11-05 02:58:44,278 train 250 1.339210e-02 -0.292009
2019-11-05 02:59:05,906 train 300 1.342307e-02 -0.273364
2019-11-05 02:59:27,506 train 350 1.341793e-02 -0.286456
2019-11-05 02:59:49,119 train 400 1.341132e-02 -0.300310
2019-11-05 03:00:02,936 training loss; R2: 1.339802e-02 -0.314409
2019-11-05 03:00:03,973 valid 000 1.115777e-02 0.028843
2019-11-05 03:00:24,016 validation loss; R2: 1.216895e-02 -0.568705
2019-11-05 03:00:24,043 epoch 145 lr 1.000000e-04
2019-11-05 03:00:25,216 train 000 1.321885e-02 -0.064447
2019-11-05 03:00:46,910 train 050 1.334664e-02 -0.293107
2019-11-05 03:01:08,476 train 100 1.336722e-02 -0.321955
2019-11-05 03:01:30,137 train 150 1.331508e-02 -0.250292
2019-11-05 03:01:51,778 train 200 1.331647e-02 -0.249098
2019-11-05 03:02:13,263 train 250 1.331558e-02 -0.261048
2019-11-05 03:02:34,465 train 300 1.337608e-02 -0.245764
2019-11-05 03:02:56,139 train 350 1.340335e-02 -0.280380
2019-11-05 03:03:17,620 train 400 1.340386e-02 -0.303709
2019-11-05 03:03:31,348 training loss; R2: 1.338671e-02 -0.300825
2019-11-05 03:03:32,349 valid 000 1.100707e-02 -0.210897
2019-11-05 03:03:52,246 validation loss; R2: 1.150425e-02 -0.353138
2019-11-05 03:03:52,279 epoch 146 lr 1.000000e-04
2019-11-05 03:03:53,495 train 000 1.320363e-02 -0.057423
2019-11-05 03:04:15,195 train 050 1.350180e-02 -0.262399
2019-11-05 03:04:36,728 train 100 1.338943e-02 -0.321913
2019-11-05 03:04:58,152 train 150 1.338287e-02 -0.273812
2019-11-05 03:05:19,718 train 200 1.345037e-02 -0.304395
2019-11-05 03:05:41,188 train 250 1.343030e-02 -0.310833
2019-11-05 03:06:02,383 train 300 1.342915e-02 -0.277450
2019-11-05 03:06:23,590 train 350 1.340557e-02 -0.318108
2019-11-05 03:06:44,747 train 400 1.340476e-02 -0.313768
2019-11-05 03:06:58,293 training loss; R2: 1.339740e-02 -0.320660
2019-11-05 03:06:59,282 valid 000 1.369201e-02 -0.009104
2019-11-05 03:07:19,015 validation loss; R2: 1.409546e-02 0.028886
2019-11-05 03:07:19,047 epoch 147 lr 1.000000e-04
2019-11-05 03:07:20,175 train 000 1.363478e-02 -0.009812
2019-11-05 03:07:41,698 train 050 1.348517e-02 -0.257521
2019-11-05 03:08:03,032 train 100 1.343369e-02 -0.307536
2019-11-05 03:08:24,437 train 150 1.344031e-02 -0.324168
2019-11-05 03:08:46,101 train 200 1.339794e-02 -0.311190
2019-11-05 03:09:07,667 train 250 1.338700e-02 -0.286309
2019-11-05 03:09:29,025 train 300 1.339639e-02 -0.279561
2019-11-05 03:09:50,310 train 350 1.340714e-02 -0.283218
2019-11-05 03:10:11,645 train 400 1.340550e-02 -0.278355
2019-11-05 03:10:25,269 training loss; R2: 1.340040e-02 -0.273625
2019-11-05 03:10:26,274 valid 000 1.440900e-02 -0.573141
2019-11-05 03:10:46,067 validation loss; R2: 1.268793e-02 -0.382591
2019-11-05 03:10:46,104 epoch 148 lr 1.000000e-04
2019-11-05 03:10:47,293 train 000 1.304283e-02 0.056540
2019-11-05 03:11:08,745 train 050 1.327848e-02 -0.140674
2019-11-05 03:11:30,109 train 100 1.323077e-02 -0.249791
2019-11-05 03:11:51,489 train 150 1.331368e-02 -0.269159
2019-11-05 03:12:13,173 train 200 1.330760e-02 -0.315085
2019-11-05 03:12:34,741 train 250 1.333042e-02 -0.299523
2019-11-05 03:12:55,827 train 300 1.335127e-02 -0.286840
2019-11-05 03:13:17,380 train 350 1.334640e-02 -0.293858
2019-11-05 03:13:38,964 train 400 1.340772e-02 -0.292903
2019-11-05 03:13:52,959 training loss; R2: 1.338851e-02 -0.287560
2019-11-05 03:13:54,067 valid 000 1.181171e-02 -0.852454
2019-11-05 03:14:14,192 validation loss; R2: 1.223808e-02 -0.343953
2019-11-05 03:14:14,219 epoch 149 lr 1.000000e-04
2019-11-05 03:14:15,421 train 000 1.365974e-02 -0.083731
2019-11-05 03:14:36,945 train 050 1.354986e-02 -0.319064
2019-11-05 03:14:58,367 train 100 1.349944e-02 -0.333217
2019-11-05 03:15:20,191 train 150 1.341746e-02 -0.293930
2019-11-05 03:15:41,551 train 200 1.342954e-02 -0.307694
2019-11-05 03:16:03,128 train 250 1.340803e-02 -0.306217
2019-11-05 03:16:24,241 train 300 1.336691e-02 -0.310400
2019-11-05 03:16:45,879 train 350 1.337904e-02 -0.310756
2019-11-05 03:17:07,268 train 400 1.337427e-02 -0.298669
2019-11-05 03:17:20,920 training loss; R2: 1.336198e-02 -0.291723
2019-11-05 03:17:21,971 valid 000 1.201653e-02 -0.594447
2019-11-05 03:17:42,080 validation loss; R2: 1.241045e-02 -0.494059
2019-11-05 03:17:42,104 epoch 150 lr 1.000000e-04
2019-11-05 03:17:43,360 train 000 1.439161e-02 0.013589
2019-11-05 03:18:05,164 train 050 1.307841e-02 -0.317065
2019-11-05 03:18:27,105 train 100 1.315370e-02 -0.320075
2019-11-05 03:18:48,916 train 150 1.327135e-02 -0.266436
2019-11-05 03:19:10,841 train 200 1.332096e-02 -0.278972
2019-11-05 03:19:32,496 train 250 1.333280e-02 -0.291430
2019-11-05 03:19:53,773 train 300 1.333371e-02 -0.292563
2019-11-05 03:20:15,188 train 350 1.333333e-02 -0.283451
2019-11-05 03:20:36,745 train 400 1.336943e-02 -0.287559
2019-11-05 03:20:50,397 training loss; R2: 1.335794e-02 -0.287047
2019-11-05 03:20:51,422 valid 000 1.073536e-02 0.026608
2019-11-05 03:21:11,225 validation loss; R2: 1.209878e-02 -0.245824
2019-11-05 03:21:11,262 epoch 151 lr 1.000000e-04
2019-11-05 03:21:12,396 train 000 1.226936e-02 -0.584061
2019-11-05 03:21:34,172 train 050 1.357442e-02 -0.306698
2019-11-05 03:21:55,858 train 100 1.351110e-02 -0.334764
2019-11-05 03:22:17,469 train 150 1.354746e-02 -0.324039
2019-11-05 03:22:39,016 train 200 1.349633e-02 -0.314938
2019-11-05 03:23:00,451 train 250 1.348035e-02 -0.308847
2019-11-05 03:23:21,787 train 300 1.343970e-02 -0.319493
2019-11-05 03:23:43,375 train 350 1.341045e-02 -0.305568
2019-11-05 03:24:04,830 train 400 1.338534e-02 -0.329364
2019-11-05 03:24:18,553 training loss; R2: 1.339577e-02 -0.315923
2019-11-05 03:24:19,582 valid 000 1.453375e-02 0.028425
2019-11-05 03:24:39,519 validation loss; R2: 1.462080e-02 -0.601542
2019-11-05 03:24:39,544 epoch 152 lr 1.000000e-04
2019-11-05 03:24:40,681 train 000 1.247790e-02 -1.372616
2019-11-05 03:25:02,399 train 050 1.343621e-02 -0.259575
2019-11-05 03:25:23,996 train 100 1.341313e-02 -0.282681
2019-11-05 03:25:45,516 train 150 1.338925e-02 -0.272494
2019-11-05 03:26:07,065 train 200 1.337196e-02 -0.288521
2019-11-05 03:26:28,549 train 250 1.337788e-02 -0.305145
2019-11-05 03:26:50,299 train 300 1.335213e-02 -0.285259
2019-11-05 03:27:11,904 train 350 1.332652e-02 -0.302782
2019-11-05 03:27:33,497 train 400 1.331818e-02 -0.289759
2019-11-05 03:27:47,263 training loss; R2: 1.331322e-02 -0.288857
2019-11-05 03:27:48,306 valid 000 1.416806e-02 -0.205227
2019-11-05 03:28:08,210 validation loss; R2: 1.287329e-02 -0.377761
2019-11-05 03:28:08,234 epoch 153 lr 1.000000e-04
2019-11-05 03:28:09,434 train 000 1.321374e-02 -0.844726
2019-11-05 03:28:31,214 train 050 1.332775e-02 -0.273997
2019-11-05 03:28:52,951 train 100 1.332512e-02 -0.286247
2019-11-05 03:29:14,597 train 150 1.329218e-02 -0.315475
2019-11-05 03:29:36,263 train 200 1.328273e-02 -0.307720
2019-11-05 03:29:57,681 train 250 1.331271e-02 -0.281303
2019-11-05 03:30:18,889 train 300 1.331272e-02 -0.310967
2019-11-05 03:30:40,798 train 350 1.331988e-02 -0.312942
2019-11-05 03:31:02,655 train 400 1.332988e-02 -0.305539
2019-11-05 03:31:16,566 training loss; R2: 1.332503e-02 -0.300031
2019-11-05 03:31:17,659 valid 000 1.394694e-02 -0.703092
2019-11-05 03:31:37,859 validation loss; R2: 1.407392e-02 -0.455224
2019-11-05 03:31:37,893 epoch 154 lr 1.000000e-04
2019-11-05 03:31:39,115 train 000 1.343404e-02 -0.475684
2019-11-05 03:32:00,714 train 050 1.336852e-02 -0.403243
2019-11-05 03:32:22,473 train 100 1.331613e-02 -0.373345
2019-11-05 03:32:43,914 train 150 1.330245e-02 -0.277524
2019-11-05 03:33:05,387 train 200 1.330538e-02 -0.269359
2019-11-05 03:33:26,740 train 250 1.326868e-02 -0.255280
2019-11-05 03:33:48,384 train 300 1.328839e-02 -0.258581
2019-11-05 03:34:09,610 train 350 1.329906e-02 -0.252374
2019-11-05 03:34:31,070 train 400 1.329099e-02 -0.279298
2019-11-05 03:34:45,074 training loss; R2: 1.328573e-02 -0.286733
2019-11-05 03:34:46,162 valid 000 1.263869e-02 0.209674
2019-11-05 03:35:06,227 validation loss; R2: 1.164091e-02 -0.227069
2019-11-05 03:35:06,255 epoch 155 lr 1.000000e-04
2019-11-05 03:35:07,420 train 000 1.248630e-02 0.124379
2019-11-05 03:35:29,311 train 050 1.323879e-02 -0.308586
2019-11-05 03:35:51,142 train 100 1.325270e-02 -0.329386
2019-11-05 03:36:13,009 train 150 1.325385e-02 -0.287737
2019-11-05 03:36:34,580 train 200 1.326556e-02 -0.323149
2019-11-05 03:36:56,361 train 250 1.321251e-02 -0.312434
2019-11-05 03:37:17,793 train 300 1.323190e-02 -0.296497
2019-11-05 03:37:39,541 train 350 1.325150e-02 -0.291765
2019-11-05 03:38:01,330 train 400 1.327349e-02 -0.293124
2019-11-05 03:38:15,119 training loss; R2: 1.327746e-02 -0.293507
2019-11-05 03:38:16,177 valid 000 1.297660e-02 -0.234337
2019-11-05 03:38:36,466 validation loss; R2: 1.275255e-02 -0.024311
2019-11-05 03:38:36,498 epoch 156 lr 1.000000e-04
2019-11-05 03:38:37,737 train 000 1.345048e-02 -0.374387
2019-11-05 03:38:59,564 train 050 1.329427e-02 -0.254647
2019-11-05 03:39:21,428 train 100 1.328635e-02 -0.301377
2019-11-05 03:39:43,698 train 150 1.335447e-02 -0.288945
2019-11-05 03:40:05,278 train 200 1.335911e-02 -0.269766
2019-11-05 03:40:26,229 train 250 1.335425e-02 -0.260221
2019-11-05 03:40:47,763 train 300 1.333762e-02 -0.304342
2019-11-05 03:41:09,218 train 350 1.329188e-02 -0.298063
2019-11-05 03:41:30,405 train 400 1.331492e-02 -0.300580
2019-11-05 03:41:44,222 training loss; R2: 1.331292e-02 -0.292188
2019-11-05 03:41:45,280 valid 000 1.232916e-02 -0.388387
2019-11-05 03:42:05,737 validation loss; R2: 1.233767e-02 -0.486379
2019-11-05 03:42:05,779 epoch 157 lr 1.000000e-04
2019-11-05 03:42:06,926 train 000 1.405627e-02 0.054072
2019-11-05 03:42:28,939 train 050 1.345325e-02 -0.274612
2019-11-05 03:42:50,750 train 100 1.336623e-02 -0.300130
2019-11-05 03:43:12,509 train 150 1.336839e-02 -0.273799
2019-11-05 03:43:33,964 train 200 1.328272e-02 -0.251714
2019-11-05 03:43:55,087 train 250 1.330353e-02 -0.228085
2019-11-05 03:44:16,648 train 300 1.329951e-02 -0.237136
2019-11-05 03:44:38,520 train 350 1.327066e-02 -0.233609
2019-11-05 03:45:00,195 train 400 1.325738e-02 -0.238111
2019-11-05 03:45:13,988 training loss; R2: 1.325974e-02 -0.248301
2019-11-05 03:45:15,073 valid 000 1.540833e-02 -0.111017
2019-11-05 03:45:35,300 validation loss; R2: 1.444405e-02 -0.426286
2019-11-05 03:45:35,332 epoch 158 lr 1.000000e-04
2019-11-05 03:45:36,524 train 000 1.153441e-02 0.149049
2019-11-05 03:45:57,883 train 050 1.335652e-02 -0.364760
2019-11-05 03:46:19,741 train 100 1.337469e-02 -0.305369
2019-11-05 03:46:41,716 train 150 1.331051e-02 -0.333120
2019-11-05 03:47:03,373 train 200 1.332365e-02 -0.361445
2019-11-05 03:47:24,437 train 250 1.328154e-02 -0.354608
2019-11-05 03:47:45,960 train 300 1.326398e-02 -0.357414
2019-11-05 03:48:07,552 train 350 1.323549e-02 -0.348594
2019-11-05 03:48:29,183 train 400 1.325970e-02 -0.348156
2019-11-05 03:48:43,064 training loss; R2: 1.327720e-02 -0.353741
2019-11-05 03:48:44,096 valid 000 1.543407e-02 -0.149915
2019-11-05 03:49:04,353 validation loss; R2: 1.664778e-02 -0.068855
2019-11-05 03:49:04,382 epoch 159 lr 1.000000e-04
2019-11-05 03:49:05,617 train 000 1.357937e-02 -0.472311
2019-11-05 03:49:27,147 train 050 1.313219e-02 -0.385177
2019-11-05 03:49:48,529 train 100 1.323696e-02 -0.413572
2019-11-05 03:50:10,111 train 150 1.326359e-02 -0.347552
2019-11-05 03:50:31,556 train 200 1.328496e-02 -0.323119
2019-11-05 03:50:52,815 train 250 1.328432e-02 -0.285184
2019-11-05 03:51:14,241 train 300 1.328756e-02 -0.334543
2019-11-05 03:51:35,755 train 350 1.329852e-02 -0.317046
2019-11-05 03:51:57,346 train 400 1.332105e-02 -0.306930
2019-11-05 03:52:11,152 training loss; R2: 1.334062e-02 -0.304944
2019-11-05 03:52:12,277 valid 000 1.318759e-02 -0.476083
2019-11-05 03:52:32,306 validation loss; R2: 1.504855e-02 -0.266437
2019-11-05 03:52:32,337 epoch 160 lr 1.000000e-04
2019-11-05 03:52:33,571 train 000 1.442326e-02 -1.579351
2019-11-05 03:52:55,032 train 050 1.349280e-02 -0.153359
2019-11-05 03:53:16,531 train 100 1.332605e-02 -0.306327
2019-11-05 03:53:38,119 train 150 1.337612e-02 -0.266527
2019-11-05 03:53:59,675 train 200 1.329960e-02 -0.245535
2019-11-05 03:54:20,953 train 250 1.327253e-02 -0.310127
2019-11-05 03:54:42,592 train 300 1.330325e-02 -0.300746
2019-11-05 03:55:04,261 train 350 1.331924e-02 -0.299524
2019-11-05 03:55:25,826 train 400 1.331584e-02 -0.309671
2019-11-05 03:55:39,618 training loss; R2: 1.328024e-02 -0.324814
2019-11-05 03:55:40,667 valid 000 1.441195e-02 -0.330695
2019-11-05 03:56:00,605 validation loss; R2: 1.512651e-02 -0.854393
2019-11-05 03:56:00,631 epoch 161 lr 1.000000e-04
2019-11-05 03:56:01,760 train 000 1.136635e-02 -0.507918
2019-11-05 03:56:23,431 train 050 1.322566e-02 -0.280058
2019-11-05 03:56:44,887 train 100 1.318688e-02 -0.320507
2019-11-05 03:57:06,405 train 150 1.329995e-02 -0.275998
2019-11-05 03:57:27,944 train 200 1.321337e-02 -0.276922
2019-11-05 03:57:48,825 train 250 1.321055e-02 -0.331140
2019-11-05 03:58:10,392 train 300 1.320572e-02 -0.302543
2019-11-05 03:58:31,901 train 350 1.323502e-02 -0.303410
2019-11-05 03:58:53,373 train 400 1.323711e-02 -0.301881
2019-11-05 03:59:07,118 training loss; R2: 1.325197e-02 -0.300981
2019-11-05 03:59:08,151 valid 000 1.266368e-02 -0.460783
2019-11-05 03:59:28,177 validation loss; R2: 1.250236e-02 -0.244476
2019-11-05 03:59:28,211 epoch 162 lr 1.000000e-04
2019-11-05 03:59:29,356 train 000 1.313606e-02 0.030710
2019-11-05 03:59:51,159 train 050 1.341827e-02 -0.264459
2019-11-05 04:00:12,753 train 100 1.326812e-02 -0.244218
2019-11-05 04:00:34,373 train 150 1.326888e-02 -0.228291
2019-11-05 04:00:55,915 train 200 1.328724e-02 -0.282251
2019-11-05 04:01:17,303 train 250 1.327864e-02 -0.313090
2019-11-05 04:01:38,617 train 300 1.327067e-02 -0.325452
2019-11-05 04:02:00,258 train 350 1.326034e-02 -0.302309
2019-11-05 04:02:22,043 train 400 1.327377e-02 -0.294042
2019-11-05 04:02:35,813 training loss; R2: 1.325719e-02 -0.298362
2019-11-05 04:02:36,834 valid 000 1.450574e-02 0.170559
2019-11-05 04:02:56,670 validation loss; R2: 1.388212e-02 -0.138779
2019-11-05 04:02:56,692 epoch 163 lr 1.000000e-04
2019-11-05 04:02:57,817 train 000 1.308899e-02 -0.223387
2019-11-05 04:03:19,446 train 050 1.320836e-02 -0.297211
2019-11-05 04:03:40,952 train 100 1.322259e-02 -0.255984
2019-11-05 04:04:02,351 train 150 1.324706e-02 -0.273396
2019-11-05 04:04:23,823 train 200 1.322659e-02 -0.264385
2019-11-05 04:04:45,449 train 250 1.320254e-02 -0.301007
2019-11-05 04:05:07,490 train 300 1.324823e-02 -0.313249
2019-11-05 04:05:29,328 train 350 1.325760e-02 -0.317531
2019-11-05 04:05:50,971 train 400 1.327363e-02 -0.310724
2019-11-05 04:06:04,865 training loss; R2: 1.328308e-02 -0.301443
2019-11-05 04:06:05,928 valid 000 1.455705e-02 0.032864
2019-11-05 04:06:25,955 validation loss; R2: 1.449355e-02 -0.449534
2019-11-05 04:06:25,984 epoch 164 lr 1.000000e-04
2019-11-05 04:06:27,174 train 000 1.336273e-02 -1.232519
2019-11-05 04:06:49,067 train 050 1.326480e-02 -0.250847
2019-11-05 04:07:10,504 train 100 1.328550e-02 -0.286095
2019-11-05 04:07:32,148 train 150 1.324189e-02 -0.254672
2019-11-05 04:07:53,905 train 200 1.323625e-02 -0.260014
2019-11-05 04:08:15,110 train 250 1.326857e-02 -0.275408
2019-11-05 04:08:36,642 train 300 1.325072e-02 -0.282342
2019-11-05 04:08:58,151 train 350 1.325574e-02 -0.272690
2019-11-05 04:09:19,608 train 400 1.325903e-02 -0.272533
2019-11-05 04:09:33,256 training loss; R2: 1.324852e-02 -0.292270
2019-11-05 04:09:34,261 valid 000 1.412043e-02 -0.178671
2019-11-05 04:09:54,079 validation loss; R2: 1.463077e-02 -0.153304
2019-11-05 04:09:54,103 epoch 165 lr 1.000000e-04
2019-11-05 04:09:55,267 train 000 1.381628e-02 -0.046351
2019-11-05 04:10:16,601 train 050 1.326043e-02 -0.284892
2019-11-05 04:10:37,811 train 100 1.330020e-02 -0.323437
2019-11-05 04:10:59,024 train 150 1.323962e-02 -0.328549
2019-11-05 04:11:20,356 train 200 1.328077e-02 -0.304198
2019-11-05 04:11:41,138 train 250 1.327104e-02 -0.324854
2019-11-05 04:12:02,494 train 300 1.327056e-02 -0.312883
2019-11-05 04:12:24,080 train 350 1.325135e-02 -0.290522
2019-11-05 04:12:45,702 train 400 1.327323e-02 -0.286576
2019-11-05 04:12:59,520 training loss; R2: 1.326337e-02 -0.297095
2019-11-05 04:13:00,524 valid 000 1.676041e-02 0.038347
2019-11-05 04:13:20,467 validation loss; R2: 1.564212e-02 -0.224449
2019-11-05 04:13:20,504 epoch 166 lr 1.000000e-04
2019-11-05 04:13:21,720 train 000 1.334912e-02 0.165131
2019-11-05 04:13:43,345 train 050 1.335260e-02 -0.351868
2019-11-05 04:14:04,945 train 100 1.321720e-02 -0.396480
2019-11-05 04:14:26,585 train 150 1.319690e-02 -0.394696
2019-11-05 04:14:48,309 train 200 1.312476e-02 -0.368973
2019-11-05 04:15:09,260 train 250 1.318914e-02 -0.376036
2019-11-05 04:15:31,187 train 300 1.318366e-02 -0.361949
2019-11-05 04:15:53,139 train 350 1.317954e-02 -0.348724
2019-11-05 04:16:15,098 train 400 1.322391e-02 -0.355364
2019-11-05 04:16:29,095 training loss; R2: 1.325120e-02 -0.348017
2019-11-05 04:16:30,110 valid 000 1.512665e-02 0.133599
2019-11-05 04:16:50,913 validation loss; R2: 1.508654e-02 -0.224010
2019-11-05 04:16:50,947 epoch 167 lr 1.000000e-04
2019-11-05 04:16:52,141 train 000 1.280506e-02 0.162311
2019-11-05 04:17:13,696 train 050 1.325707e-02 -0.303257
2019-11-05 04:17:35,097 train 100 1.325059e-02 -0.353945
2019-11-05 04:17:56,432 train 150 1.332971e-02 -0.277692
2019-11-05 04:18:17,792 train 200 1.335088e-02 -0.327216
2019-11-05 04:18:39,289 train 250 1.333165e-02 -0.355205
2019-11-05 04:19:00,692 train 300 1.328696e-02 -0.406297
2019-11-05 04:19:22,147 train 350 1.324926e-02 -0.388335
2019-11-05 04:19:43,579 train 400 1.324215e-02 -0.385569
2019-11-05 04:19:57,203 training loss; R2: 1.324651e-02 -0.382173
2019-11-05 04:19:58,249 valid 000 1.641079e-02 -0.515651
2019-11-05 04:20:17,898 validation loss; R2: 1.493271e-02 -0.133428
2019-11-05 04:20:17,926 epoch 168 lr 1.000000e-04
2019-11-05 04:20:19,062 train 000 1.297278e-02 0.148312
2019-11-05 04:20:40,627 train 050 1.296169e-02 -0.221017
2019-11-05 04:21:02,088 train 100 1.314120e-02 -0.228764
2019-11-05 04:21:23,483 train 150 1.315138e-02 -0.225580
2019-11-05 04:21:44,799 train 200 1.314503e-02 -0.218516
2019-11-05 04:22:05,365 train 250 1.312270e-02 -0.220187
2019-11-05 04:22:26,262 train 300 1.315070e-02 -0.226997
2019-11-05 04:22:47,330 train 350 1.319106e-02 -0.280240
2019-11-05 04:23:08,468 train 400 1.321505e-02 -0.291621
2019-11-05 04:23:21,914 training loss; R2: 1.323454e-02 -0.314470
2019-11-05 04:23:22,922 valid 000 1.531712e-02 -0.136571
2019-11-05 04:23:42,445 validation loss; R2: 1.572030e-02 -0.317205
2019-11-05 04:23:42,471 epoch 169 lr 1.000000e-04
2019-11-05 04:23:43,657 train 000 1.364196e-02 0.056833
2019-11-05 04:24:04,746 train 050 1.323241e-02 -0.200801
2019-11-05 04:24:26,074 train 100 1.317771e-02 -0.266810
2019-11-05 04:24:47,275 train 150 1.315105e-02 -0.302757
2019-11-05 04:25:08,502 train 200 1.316092e-02 -0.291315
2019-11-05 04:25:29,764 train 250 1.322747e-02 -0.304050
2019-11-05 04:25:51,255 train 300 1.326574e-02 -0.295607
2019-11-05 04:26:12,479 train 350 1.324087e-02 -0.290745
2019-11-05 04:26:33,709 train 400 1.322923e-02 -0.278362
2019-11-05 04:26:47,268 training loss; R2: 1.322655e-02 -0.276058
2019-11-05 04:26:48,289 valid 000 1.487727e-02 -0.788387
2019-11-05 04:27:07,861 validation loss; R2: 1.616273e-02 -0.488550
2019-11-05 04:27:07,887 epoch 170 lr 1.000000e-04
2019-11-05 04:27:09,064 train 000 1.410100e-02 -0.166352
2019-11-05 04:27:30,317 train 050 1.298814e-02 -0.309883
2019-11-05 04:27:51,680 train 100 1.321619e-02 -0.328505
2019-11-05 04:28:12,854 train 150 1.315892e-02 -0.370231
2019-11-05 04:28:34,051 train 200 1.319485e-02 -0.376132
2019-11-05 04:28:55,063 train 250 1.320634e-02 -0.325394
2019-11-05 04:29:16,369 train 300 1.321242e-02 -0.315970
2019-11-05 04:29:38,074 train 350 1.323518e-02 -0.295769
2019-11-05 04:29:59,786 train 400 1.326547e-02 -0.302939
2019-11-05 04:30:13,658 training loss; R2: 1.326789e-02 -0.303274
2019-11-05 04:30:14,736 valid 000 1.555808e-02 -0.713788
2019-11-05 04:30:34,986 validation loss; R2: 1.471032e-02 -0.555950
2019-11-05 04:30:35,015 epoch 171 lr 1.000000e-04
2019-11-05 04:30:36,231 train 000 1.545627e-02 -0.042840
2019-11-05 04:30:57,703 train 050 1.313536e-02 -0.295947
2019-11-05 04:31:19,466 train 100 1.323492e-02 -0.309148
2019-11-05 04:31:41,112 train 150 1.328035e-02 -0.324293
2019-11-05 04:32:02,454 train 200 1.317727e-02 -0.281801
2019-11-05 04:32:23,688 train 250 1.316933e-02 -0.256923
2019-11-05 04:32:45,132 train 300 1.312211e-02 -0.258442
2019-11-05 04:33:06,895 train 350 1.314783e-02 -0.266706
2019-11-05 04:33:28,158 train 400 1.319551e-02 -0.251148
2019-11-05 04:33:41,929 training loss; R2: 1.319043e-02 -0.268205
2019-11-05 04:33:43,017 valid 000 1.710963e-02 -0.625511
2019-11-05 04:34:03,519 validation loss; R2: 1.754991e-02 -0.383461
2019-11-05 04:34:03,549 epoch 172 lr 1.000000e-04
2019-11-05 04:34:04,745 train 000 1.401348e-02 -0.881389
2019-11-05 04:34:26,135 train 050 1.310872e-02 -0.382951
2019-11-05 04:34:47,400 train 100 1.318410e-02 -0.379387
2019-11-05 04:35:08,611 train 150 1.319281e-02 -0.356863
2019-11-05 04:35:29,847 train 200 1.318692e-02 -0.381675
2019-11-05 04:35:51,199 train 250 1.321160e-02 -0.353192
2019-11-05 04:36:12,609 train 300 1.325807e-02 -0.328414
2019-11-05 04:36:34,046 train 350 1.323409e-02 -0.317196
2019-11-05 04:36:55,457 train 400 1.321431e-02 -0.306961
2019-11-05 04:37:09,072 training loss; R2: 1.322082e-02 -0.306629
2019-11-05 04:37:10,073 valid 000 2.075102e-02 -0.218782
2019-11-05 04:37:29,839 validation loss; R2: 1.899228e-02 -0.374858
2019-11-05 04:37:29,867 epoch 173 lr 1.000000e-04
2019-11-05 04:37:31,049 train 000 1.171876e-02 0.018614
2019-11-05 04:37:52,591 train 050 1.304670e-02 -0.291462
2019-11-05 04:38:14,057 train 100 1.307615e-02 -0.295205
2019-11-05 04:38:35,458 train 150 1.307910e-02 -0.285436
2019-11-05 04:38:56,834 train 200 1.310856e-02 -0.299802
2019-11-05 04:39:17,979 train 250 1.313003e-02 -0.269705
2019-11-05 04:39:38,970 train 300 1.313809e-02 -0.328637
2019-11-05 04:40:00,101 train 350 1.318073e-02 -0.313172
2019-11-05 04:40:21,150 train 400 1.317307e-02 -0.331923
2019-11-05 04:40:34,610 training loss; R2: 1.318258e-02 -0.331060
2019-11-05 04:40:35,650 valid 000 2.764230e-02 -0.132156
2019-11-05 04:40:55,084 validation loss; R2: 2.407057e-02 -0.465001
2019-11-05 04:40:55,111 epoch 174 lr 1.000000e-04
2019-11-05 04:40:56,233 train 000 1.254523e-02 0.134285
2019-11-05 04:41:17,472 train 050 1.345704e-02 -0.294395
2019-11-05 04:41:38,609 train 100 1.329576e-02 -0.322912
2019-11-05 04:41:59,789 train 150 1.340514e-02 -0.351648
2019-11-05 04:42:21,016 train 200 1.331848e-02 -0.327626
2019-11-05 04:42:42,153 train 250 1.327808e-02 -0.306746
2019-11-05 04:43:03,216 train 300 1.327406e-02 -0.293065
2019-11-05 04:43:24,649 train 350 1.325180e-02 -0.287294
2019-11-05 04:43:45,913 train 400 1.322226e-02 -0.289664
2019-11-05 04:43:59,615 training loss; R2: 1.320613e-02 -0.299565
2019-11-05 04:44:00,615 valid 000 1.456245e-02 0.124306
2019-11-05 04:44:20,372 validation loss; R2: 1.487089e-02 -0.061334
2019-11-05 04:44:20,409 epoch 175 lr 1.000000e-04
2019-11-05 04:44:21,587 train 000 1.281481e-02 -0.120125
2019-11-05 04:44:43,144 train 050 1.318052e-02 -0.377678
2019-11-05 04:45:04,614 train 100 1.310693e-02 -0.327681
2019-11-05 04:45:26,016 train 150 1.309306e-02 -0.286468
2019-11-05 04:45:47,421 train 200 1.313505e-02 -0.272815
2019-11-05 04:46:08,620 train 250 1.313192e-02 -0.260160
2019-11-05 04:46:29,703 train 300 1.316095e-02 -0.276659
2019-11-05 04:46:51,087 train 350 1.319499e-02 -0.268022
2019-11-05 04:47:12,501 train 400 1.321907e-02 -0.269474
2019-11-05 04:47:26,112 training loss; R2: 1.321964e-02 -0.264494
2019-11-05 04:47:27,155 valid 000 1.499434e-02 0.135299
2019-11-05 04:47:46,832 validation loss; R2: 1.495437e-02 -1.002634
2019-11-05 04:47:46,872 epoch 176 lr 1.000000e-04
2019-11-05 04:47:48,002 train 000 1.157314e-02 -0.589148
2019-11-05 04:48:09,540 train 050 1.338566e-02 -0.277327
2019-11-05 04:48:30,992 train 100 1.335249e-02 -0.330951
2019-11-05 04:48:52,413 train 150 1.326535e-02 -0.325255
2019-11-05 04:49:13,874 train 200 1.321703e-02 -0.311805
2019-11-05 04:49:35,215 train 250 1.320487e-02 -0.309536
2019-11-05 04:49:55,980 train 300 1.316583e-02 -0.314258
2019-11-05 04:50:17,423 train 350 1.314459e-02 -0.308290
2019-11-05 04:50:38,840 train 400 1.318118e-02 -0.315020
2019-11-05 04:50:52,431 training loss; R2: 1.320618e-02 -0.306441
2019-11-05 04:50:53,473 valid 000 1.653594e-02 -1.531004
2019-11-05 04:51:13,193 validation loss; R2: 1.668037e-02 -0.543030
2019-11-05 04:51:13,219 epoch 177 lr 1.000000e-04
2019-11-05 04:51:14,372 train 000 1.187784e-02 -0.384128
2019-11-05 04:51:35,668 train 050 1.301922e-02 -0.164290
2019-11-05 04:51:57,114 train 100 1.313186e-02 -0.255504
2019-11-05 04:52:18,515 train 150 1.318890e-02 -0.336951
2019-11-05 04:52:39,918 train 200 1.313618e-02 -0.298623
2019-11-05 04:53:01,272 train 250 1.314848e-02 -0.299448
2019-11-05 04:53:21,886 train 300 1.319105e-02 -0.272375
2019-11-05 04:53:43,012 train 350 1.316050e-02 -0.281982
2019-11-05 04:54:04,161 train 400 1.316596e-02 -0.298165
2019-11-05 04:54:17,693 training loss; R2: 1.316109e-02 -0.297423
2019-11-05 04:54:18,681 valid 000 1.972377e-02 -0.348750
2019-11-05 04:54:38,178 validation loss; R2: 1.824609e-02 -0.628947
2019-11-05 04:54:38,216 epoch 178 lr 1.000000e-04
2019-11-05 04:54:39,392 train 000 1.479646e-02 -0.641680
2019-11-05 04:55:00,595 train 050 1.322801e-02 -0.340245
2019-11-05 04:55:21,696 train 100 1.332820e-02 -0.349255
2019-11-05 04:55:42,855 train 150 1.322031e-02 -0.314661
2019-11-05 04:56:04,035 train 200 1.323604e-02 -0.301027
2019-11-05 04:56:25,224 train 250 1.322277e-02 -0.288141
2019-11-05 04:56:46,050 train 300 1.322868e-02 -0.294057
2019-11-05 04:57:07,434 train 350 1.322540e-02 -0.285265
2019-11-05 04:57:28,836 train 400 1.320221e-02 -0.286768
2019-11-05 04:57:42,466 training loss; R2: 1.320397e-02 -0.283014
2019-11-05 04:57:43,480 valid 000 2.100707e-02 -0.220843
2019-11-05 04:58:03,176 validation loss; R2: 1.926962e-02 -0.244824
2019-11-05 04:58:03,203 epoch 179 lr 1.000000e-04
2019-11-05 04:58:04,372 train 000 1.292417e-02 0.163405
2019-11-05 04:58:25,761 train 050 1.301862e-02 -0.244025
2019-11-05 04:58:47,128 train 100 1.296583e-02 -0.193875
2019-11-05 04:59:08,507 train 150 1.310208e-02 -0.258666
2019-11-05 04:59:29,966 train 200 1.313972e-02 -0.260503
2019-11-05 04:59:51,346 train 250 1.312437e-02 -0.284704
2019-11-05 05:00:12,109 train 300 1.315109e-02 -0.309971
2019-11-05 05:00:33,116 train 350 1.313411e-02 -0.301586
2019-11-05 05:00:54,211 train 400 1.313927e-02 -0.312182
2019-11-05 05:01:07,671 training loss; R2: 1.314091e-02 -0.312432
2019-11-05 05:01:08,688 valid 000 1.472999e-02 0.157675
2019-11-05 05:01:28,388 validation loss; R2: 1.492244e-02 -0.087823
2019-11-05 05:01:28,421 epoch 180 lr 1.000000e-04
2019-11-05 05:01:29,628 train 000 1.393212e-02 0.062031
2019-11-05 05:01:50,997 train 050 1.314994e-02 -0.301864
2019-11-05 05:02:12,178 train 100 1.333177e-02 -0.235288
2019-11-05 05:02:33,402 train 150 1.320071e-02 -0.239763
2019-11-05 05:02:54,691 train 200 1.322707e-02 -0.271122
2019-11-05 05:03:15,826 train 250 1.324214e-02 -0.289496
2019-11-05 05:03:36,790 train 300 1.324357e-02 -0.286915
2019-11-05 05:03:57,989 train 350 1.322813e-02 -0.299952
2019-11-05 05:04:19,467 train 400 1.320527e-02 -0.311352
2019-11-05 05:04:33,039 training loss; R2: 1.318311e-02 -0.308863
2019-11-05 05:04:34,085 valid 000 2.027827e-02 -0.446933
2019-11-05 05:04:53,671 validation loss; R2: 2.039969e-02 -0.514374
2019-11-05 05:04:53,706 epoch 181 lr 1.000000e-04
2019-11-05 05:04:54,878 train 000 1.337347e-02 0.151502
2019-11-05 05:05:16,172 train 050 1.352313e-02 -0.430008
2019-11-05 05:05:37,441 train 100 1.335812e-02 -0.318584
2019-11-05 05:05:58,677 train 150 1.322138e-02 -0.315057
2019-11-05 05:06:19,893 train 200 1.318305e-02 -0.314797
2019-11-05 05:06:41,075 train 250 1.320149e-02 -0.339222
2019-11-05 05:07:02,023 train 300 1.316495e-02 -0.338192
2019-11-05 05:07:22,654 train 350 1.316518e-02 -0.342073
2019-11-05 05:07:43,737 train 400 1.313263e-02 -0.335463
2019-11-05 05:07:57,245 training loss; R2: 1.315230e-02 -0.319640
2019-11-05 05:07:58,311 valid 000 2.672646e-02 -0.088822
2019-11-05 05:08:18,184 validation loss; R2: 2.530198e-02 -0.405601
2019-11-05 05:08:18,212 epoch 182 lr 1.000000e-04
2019-11-05 05:08:19,418 train 000 1.336557e-02 0.063112
2019-11-05 05:08:40,987 train 050 1.340362e-02 -0.297784
2019-11-05 05:09:02,435 train 100 1.339366e-02 -0.249686
2019-11-05 05:09:23,912 train 150 1.334094e-02 -0.255357
2019-11-05 05:09:45,558 train 200 1.331529e-02 -0.254500
2019-11-05 05:10:07,199 train 250 1.327339e-02 -0.276105
2019-11-05 05:10:28,784 train 300 1.322571e-02 -0.288126
2019-11-05 05:10:49,802 train 350 1.322269e-02 -0.298311
2019-11-05 05:11:11,365 train 400 1.324247e-02 -0.298166
2019-11-05 05:11:25,153 training loss; R2: 1.321824e-02 -0.292074
2019-11-05 05:11:26,243 valid 000 2.362883e-02 -0.172254
2019-11-05 05:11:46,375 validation loss; R2: 2.437084e-02 -0.372585
2019-11-05 05:11:46,404 epoch 183 lr 1.000000e-04
2019-11-05 05:11:47,580 train 000 1.175349e-02 0.052656
2019-11-05 05:12:09,368 train 050 1.311670e-02 -0.329030
2019-11-05 05:12:31,065 train 100 1.317395e-02 -0.245571
2019-11-05 05:12:52,700 train 150 1.317251e-02 -0.294357
2019-11-05 05:13:14,357 train 200 1.322949e-02 -0.322952
2019-11-05 05:13:36,043 train 250 1.327170e-02 -0.329322
2019-11-05 05:13:57,648 train 300 1.328115e-02 -0.324569
2019-11-05 05:14:19,208 train 350 1.323437e-02 -0.307364
2019-11-05 05:14:40,881 train 400 1.322755e-02 -0.333331
2019-11-05 05:14:54,709 training loss; R2: 1.321278e-02 -0.327065
2019-11-05 05:14:55,794 valid 000 2.429146e-02 -0.232977
2019-11-05 05:15:16,006 validation loss; R2: 2.675107e-02 -0.516939
2019-11-05 05:15:16,039 epoch 184 lr 1.000000e-04
2019-11-05 05:15:17,263 train 000 1.300252e-02 0.136760
2019-11-05 05:15:38,702 train 050 1.276452e-02 -0.275224
2019-11-05 05:16:00,522 train 100 1.293809e-02 -0.276599
2019-11-05 05:16:22,057 train 150 1.295161e-02 -0.299287
2019-11-05 05:16:43,410 train 200 1.299639e-02 -0.290003
2019-11-05 05:17:05,231 train 250 1.309975e-02 -0.308767
2019-11-05 05:17:26,431 train 300 1.349920e-02 -0.336413
2019-11-05 05:17:48,218 train 350 1.347889e-02 -0.329315
2019-11-05 05:18:09,600 train 400 1.348474e-02 -0.316695
2019-11-05 05:18:23,473 training loss; R2: 1.346421e-02 -0.318784
2019-11-05 05:18:24,534 valid 000 2.219047e-02 -0.647840
2019-11-05 05:18:44,795 validation loss; R2: 2.197961e-02 -0.340670
2019-11-05 05:18:44,824 epoch 185 lr 1.000000e-04
2019-11-05 05:18:46,028 train 000 1.232477e-02 0.202794
2019-11-05 05:19:07,628 train 050 1.316367e-02 -0.947013
2019-11-05 05:19:29,095 train 100 1.315760e-02 -0.634307
2019-11-05 05:19:50,433 train 150 1.316393e-02 -0.503448
2019-11-05 05:20:11,673 train 200 1.320363e-02 -0.452296
2019-11-05 05:20:33,063 train 250 1.318412e-02 -0.411525
2019-11-05 05:20:54,234 train 300 1.319726e-02 -0.388427
2019-11-05 05:21:15,571 train 350 1.314959e-02 -0.374288
2019-11-05 05:21:36,746 train 400 1.314293e-02 -0.361897
2019-11-05 05:21:50,248 training loss; R2: 1.314246e-02 -0.362449
2019-11-05 05:21:51,251 valid 000 1.603405e-02 -0.245156
2019-11-05 05:22:10,930 validation loss; R2: 1.644533e-02 -0.342166
2019-11-05 05:22:10,967 epoch 186 lr 1.000000e-04
2019-11-05 05:22:12,173 train 000 1.362650e-02 -0.838885
2019-11-05 05:22:33,569 train 050 1.306117e-02 -0.316335
2019-11-05 05:22:54,876 train 100 1.307187e-02 -0.304911
2019-11-05 05:23:16,163 train 150 1.311821e-02 -0.280799
2019-11-05 05:23:37,626 train 200 1.310463e-02 -0.253661
2019-11-05 05:23:59,017 train 250 1.312190e-02 -0.260185
2019-11-05 05:24:20,236 train 300 1.315031e-02 -0.271063
2019-11-05 05:24:41,222 train 350 1.316173e-02 -0.288133
2019-11-05 05:25:02,511 train 400 1.315091e-02 -0.274429
2019-11-05 05:25:16,172 training loss; R2: 1.318333e-02 -0.278485
2019-11-05 05:25:17,196 valid 000 2.489916e-02 -0.108882
2019-11-05 05:25:36,638 validation loss; R2: 2.541327e-02 -0.977971
2019-11-05 05:25:36,675 epoch 187 lr 1.000000e-04
2019-11-05 05:25:37,806 train 000 1.253999e-02 -0.108752
2019-11-05 05:25:59,130 train 050 1.308478e-02 -0.255983
2019-11-05 05:26:20,372 train 100 1.306949e-02 -0.219067
2019-11-05 05:26:41,549 train 150 1.317650e-02 -0.246386
2019-11-05 05:27:02,810 train 200 1.314805e-02 -0.270856
2019-11-05 05:27:23,915 train 250 1.319252e-02 -0.263585
2019-11-05 05:27:45,193 train 300 1.318674e-02 -0.256694
2019-11-05 05:28:06,168 train 350 1.322803e-02 -0.259315
2019-11-05 05:28:27,528 train 400 1.322060e-02 -0.270153
2019-11-05 05:28:41,117 training loss; R2: 1.322124e-02 -0.275861
2019-11-05 05:28:42,144 valid 000 2.277807e-02 -0.886000
2019-11-05 05:29:01,756 validation loss; R2: 2.280223e-02 -0.670509
2019-11-05 05:29:01,789 epoch 188 lr 1.000000e-04
2019-11-05 05:29:02,978 train 000 1.318345e-02 0.105301
2019-11-05 05:29:24,428 train 050 1.308934e-02 -0.387929
2019-11-05 05:29:45,837 train 100 1.307460e-02 -0.314775
2019-11-05 05:30:07,246 train 150 1.320762e-02 -0.330928
2019-11-05 05:30:28,627 train 200 1.322026e-02 -0.323108
2019-11-05 05:30:49,986 train 250 1.316187e-02 -0.296591
2019-11-05 05:31:11,230 train 300 1.314352e-02 -0.316348
2019-11-05 05:31:31,890 train 350 1.311935e-02 -0.287543
2019-11-05 05:31:53,439 train 400 1.312057e-02 -0.290100
2019-11-05 05:32:07,128 training loss; R2: 1.311522e-02 -0.297315
2019-11-05 05:32:08,202 valid 000 1.827928e-02 -0.358987
2019-11-05 05:32:27,993 validation loss; R2: 1.943995e-02 -0.766153
2019-11-05 05:32:28,018 epoch 189 lr 1.000000e-04
2019-11-05 05:32:29,142 train 000 1.245233e-02 -0.200846
2019-11-05 05:32:50,739 train 050 1.302094e-02 -0.660895
2019-11-05 05:33:12,261 train 100 1.307037e-02 -0.504551
2019-11-05 05:33:33,828 train 150 1.305827e-02 -0.463262
2019-11-05 05:33:55,503 train 200 1.309856e-02 -0.422258
2019-11-05 05:34:17,134 train 250 1.309335e-02 -0.409592
2019-11-05 05:34:38,590 train 300 1.310179e-02 -0.382635
2019-11-05 05:34:59,532 train 350 1.309880e-02 -0.384930
2019-11-05 05:35:21,022 train 400 1.308128e-02 -0.351774
2019-11-05 05:35:34,648 training loss; R2: 1.309082e-02 -0.337119
2019-11-05 05:35:35,629 valid 000 3.181008e-02 -1.745480
2019-11-05 05:35:55,253 validation loss; R2: 2.977422e-02 -1.460167
2019-11-05 05:35:55,280 epoch 190 lr 1.000000e-04
2019-11-05 05:35:56,464 train 000 1.249861e-02 -0.109988
2019-11-05 05:36:17,976 train 050 1.292150e-02 -0.345014
2019-11-05 05:36:39,359 train 100 1.302318e-02 -0.369707
2019-11-05 05:37:00,777 train 150 1.307661e-02 -0.319296
2019-11-05 05:37:22,205 train 200 1.313246e-02 -0.302892
2019-11-05 05:37:43,654 train 250 1.311646e-02 -0.311288
2019-11-05 05:38:05,045 train 300 1.308986e-02 -0.331726
2019-11-05 05:38:26,066 train 350 1.311969e-02 -0.324885
2019-11-05 05:38:47,762 train 400 1.312606e-02 -0.314679
2019-11-05 05:39:01,547 training loss; R2: 1.313722e-02 -0.305281
2019-11-05 05:39:02,550 valid 000 2.667647e-02 -0.221005
2019-11-05 05:39:22,453 validation loss; R2: 2.959353e-02 -0.436317
2019-11-05 05:39:22,480 epoch 191 lr 1.000000e-04
2019-11-05 05:39:23,689 train 000 1.336363e-02 -0.096727
2019-11-05 05:39:45,493 train 050 1.303101e-02 -0.199291
2019-11-05 05:40:07,059 train 100 1.314977e-02 -0.283211
2019-11-05 05:40:28,695 train 150 1.314100e-02 -0.262345
2019-11-05 05:40:50,310 train 200 1.313073e-02 -0.298757
2019-11-05 05:41:11,791 train 250 1.316012e-02 -0.318468
2019-11-05 05:41:33,252 train 300 1.317671e-02 -0.454360
2019-11-05 05:41:54,720 train 350 1.315124e-02 -0.427997
2019-11-05 05:42:16,072 train 400 1.315431e-02 -0.414958
2019-11-05 05:42:29,823 training loss; R2: 1.316610e-02 -0.405885
2019-11-05 05:42:30,825 valid 000 2.727184e-02 -0.403900
2019-11-05 05:42:51,227 validation loss; R2: 2.587205e-02 -0.566689
2019-11-05 05:42:51,266 epoch 192 lr 1.000000e-04
2019-11-05 05:42:52,463 train 000 1.158538e-02 -0.099077
2019-11-05 05:43:14,542 train 050 1.307424e-02 -0.341997
2019-11-05 05:43:36,226 train 100 1.310149e-02 -0.308885
2019-11-05 05:43:57,907 train 150 1.309397e-02 -0.284593
2019-11-05 05:44:19,484 train 200 1.303807e-02 -0.282194
2019-11-05 05:44:41,033 train 250 1.307842e-02 -0.276880
2019-11-05 05:45:02,524 train 300 1.309632e-02 -0.298458
2019-11-05 05:45:23,536 train 350 1.310925e-02 -0.313043
2019-11-05 05:45:44,777 train 400 1.309685e-02 -0.296930
2019-11-05 05:45:58,314 training loss; R2: 1.312013e-02 -0.299366
2019-11-05 05:45:59,361 valid 000 1.882566e-02 -1.468606
2019-11-05 05:46:18,970 validation loss; R2: 1.903579e-02 -0.387697
2019-11-05 05:46:19,002 epoch 193 lr 1.000000e-04
2019-11-05 05:46:20,161 train 000 1.240563e-02 0.088042
2019-11-05 05:46:41,566 train 050 1.312650e-02 -0.227951
2019-11-05 05:47:02,848 train 100 1.305355e-02 -0.358021
2019-11-05 05:47:24,103 train 150 1.312881e-02 -0.457476
2019-11-05 05:47:45,419 train 200 1.312908e-02 -0.459388
2019-11-05 05:48:06,785 train 250 1.309935e-02 -0.396929
2019-11-05 05:48:28,145 train 300 1.310717e-02 -0.414558
2019-11-05 05:48:49,810 train 350 1.311541e-02 -0.393174
2019-11-05 05:49:11,081 train 400 1.310199e-02 -0.395881
2019-11-05 05:49:24,645 training loss; R2: 1.310163e-02 -0.402810
2019-11-05 05:49:25,628 valid 000 2.156885e-02 -0.134798
2019-11-05 05:49:45,277 validation loss; R2: 2.183310e-02 -0.471598
2019-11-05 05:49:45,300 epoch 194 lr 1.000000e-04
2019-11-05 05:49:46,419 train 000 1.328646e-02 -0.148709
2019-11-05 05:50:07,910 train 050 1.293793e-02 -0.206069
2019-11-05 05:50:29,318 train 100 1.308852e-02 -0.201532
2019-11-05 05:50:50,739 train 150 1.310312e-02 -0.271360
2019-11-05 05:51:12,388 train 200 1.306203e-02 -0.296818
2019-11-05 05:51:33,681 train 250 1.308449e-02 -0.288972
2019-11-05 05:51:55,119 train 300 1.304451e-02 -0.268255
2019-11-05 05:52:16,901 train 350 1.312444e-02 -0.274589
2019-11-05 05:52:38,398 train 400 1.315318e-02 -0.275518
2019-11-05 05:52:52,041 training loss; R2: 1.332011e-02 -0.288810
2019-11-05 05:52:53,066 valid 000 2.415162e-02 -0.070173
2019-11-05 05:53:12,770 validation loss; R2: 2.482026e-02 -0.586685
2019-11-05 05:53:12,807 epoch 195 lr 1.000000e-04
2019-11-05 05:53:13,967 train 000 1.259744e-02 -0.623248
2019-11-05 05:53:35,426 train 050 1.314404e-02 -0.237636
2019-11-05 05:53:56,860 train 100 1.322874e-02 -0.237877
2019-11-05 05:54:18,187 train 150 1.316439e-02 -0.232226
2019-11-05 05:54:39,598 train 200 1.314857e-02 -0.248908
2019-11-05 05:55:01,254 train 250 1.312567e-02 -0.249855
2019-11-05 05:55:22,867 train 300 1.312459e-02 -0.287325
2019-11-05 05:55:44,910 train 350 1.316265e-02 -0.293839
2019-11-05 05:56:06,471 train 400 1.317247e-02 -0.276611
2019-11-05 05:56:20,150 training loss; R2: 1.318006e-02 -0.269194
2019-11-05 05:56:21,176 valid 000 2.502014e-02 -0.092122
2019-11-05 05:56:40,910 validation loss; R2: 2.493835e-02 -0.717297
2019-11-05 05:56:40,936 epoch 196 lr 1.000000e-04
2019-11-05 05:56:42,106 train 000 1.322790e-02 0.036151
2019-11-05 05:57:03,650 train 050 1.296982e-02 -0.269471
2019-11-05 05:57:25,121 train 100 1.317986e-02 -0.254144
2019-11-05 05:57:46,581 train 150 1.310002e-02 -0.229829
2019-11-05 05:58:08,065 train 200 1.311546e-02 -0.226920
2019-11-05 05:58:29,601 train 250 1.304440e-02 -0.237403
2019-11-05 05:58:51,303 train 300 1.307025e-02 -0.271688
2019-11-05 05:59:12,994 train 350 1.309499e-02 -0.281483
2019-11-05 05:59:34,461 train 400 1.309338e-02 -0.289337
2019-11-05 05:59:48,322 training loss; R2: 1.311384e-02 -0.282387
2019-11-05 05:59:49,422 valid 000 1.959302e-02 -0.631572
2019-11-05 06:00:09,220 validation loss; R2: 2.181349e-02 -0.265181
2019-11-05 06:00:09,246 epoch 197 lr 1.000000e-04
2019-11-05 06:00:10,467 train 000 1.275677e-02 -0.051449
2019-11-05 06:00:32,101 train 050 1.325268e-02 -0.304794
2019-11-05 06:00:53,640 train 100 1.311537e-02 -0.398050
2019-11-05 06:01:15,112 train 150 1.339479e-02 -0.355466
2019-11-05 06:01:36,530 train 200 1.341316e-02 -0.373622
2019-11-05 06:01:58,142 train 250 1.332094e-02 -0.347531
2019-11-05 06:02:19,665 train 300 1.333687e-02 -0.341229
2019-11-05 06:02:41,064 train 350 1.328609e-02 -0.341249
2019-11-05 06:03:02,296 train 400 1.329627e-02 -0.315163
2019-11-05 06:03:15,960 training loss; R2: 1.325427e-02 -0.309580
2019-11-05 06:03:17,085 valid 000 2.589826e-02 -0.103330
2019-11-05 06:03:37,365 validation loss; R2: 2.462666e-02 -1.032625
2019-11-05 06:03:37,392 epoch 198 lr 1.000000e-04
2019-11-05 06:03:38,595 train 000 1.349836e-02 0.103856
2019-11-05 06:04:00,359 train 050 1.320506e-02 -0.258024
2019-11-05 06:04:21,729 train 100 1.308844e-02 -0.262118
2019-11-05 06:04:43,227 train 150 1.306323e-02 -0.264055
2019-11-05 06:05:04,726 train 200 1.311843e-02 -0.298404
2019-11-05 06:05:26,255 train 250 1.310104e-02 -0.283380
2019-11-05 06:05:47,650 train 300 1.306604e-02 -0.269889
2019-11-05 06:06:08,656 train 350 1.305762e-02 -0.281581
2019-11-05 06:06:29,825 train 400 1.305916e-02 -0.291948
2019-11-05 06:06:43,313 training loss; R2: 1.307065e-02 -0.304175
2019-11-05 06:06:44,304 valid 000 2.014559e-02 0.051452
2019-11-05 06:07:03,826 validation loss; R2: 2.118349e-02 -0.490836
2019-11-05 06:07:03,853 epoch 199 lr 1.000000e-04
2019-11-05 06:07:05,014 train 000 1.421445e-02 -0.253129
2019-11-05 06:07:26,331 train 050 1.290446e-02 -0.191438
2019-11-05 06:07:47,583 train 100 1.309930e-02 -0.199804
2019-11-05 06:08:08,876 train 150 1.313571e-02 -0.224959
2019-11-05 06:08:30,356 train 200 1.313850e-02 -0.251532
2019-11-05 06:08:51,761 train 250 1.316325e-02 -0.306363
2019-11-05 06:09:13,090 train 300 1.316070e-02 -0.280504
2019-11-05 06:09:34,590 train 350 1.320509e-02 -0.282669
2019-11-05 06:09:56,361 train 400 1.315255e-02 -0.279284
2019-11-05 06:10:10,123 training loss; R2: 1.311928e-02 -0.300122
2019-11-05 06:10:11,103 valid 000 2.419340e-02 -0.503191
2019-11-05 06:10:30,811 validation loss; R2: 2.512403e-02 -0.553832
