2019-11-01 17:08:04,859 gpu device = 3
2019-11-01 17:08:04,860 args = Namespace(arch_learning_rate=0.0003, arch_weight_decay=0.001, batch_size=20, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.3, epochs=50, gpu=3, grad_clip=5, gz_regression=False, init_channels=16, layers=8, learning_rate=0.01, learning_rate_min=0.001, model_path='saved_models', momentum=0.9, report_freq=50, save='search-GALAXY_ZOO-20191101-170804', seed=2, train_portion=0.5, unrolled=True, weight_decay=1e-06)
2019-11-01 17:08:08,472 param size = 1.937557MB
2019-11-01 17:08:08,485 epoch 0 lr 1.000000e-02
2019-11-01 17:08:08,486 genotype = Genotype(normal=[('dil_conv_5x5', 1), ('sep_conv_5x5', 0), ('sep_conv_5x5', 1), ('max_pool_3x3', 0), ('dil_conv_3x3', 0), ('avg_pool_3x3', 2), ('sep_conv_5x5', 3), ('dil_conv_3x3', 2)], normal_concat=range(2, 6), reduce=[('avg_pool_3x3', 1), ('max_pool_3x3', 0), ('dil_conv_3x3', 1), ('sep_conv_5x5', 2), ('max_pool_3x3', 1), ('dil_conv_3x3', 0), ('dil_conv_3x3', 3), ('dil_conv_5x5', 4)], reduce_concat=range(2, 6))
2019-11-01 17:08:08,488 
alphas_normal = Variable containing:
 0.1251  0.1250  0.1249  0.1249  0.1251  0.1251  0.1250  0.1248
 0.1250  0.1248  0.1250  0.1249  0.1250  0.1251  0.1250  0.1252
 0.1248  0.1252  0.1248  0.1251  0.1251  0.1251  0.1250  0.1249
 0.1248  0.1250  0.1248  0.1249  0.1252  0.1253  0.1250  0.1250
 0.1252  0.1250  0.1250  0.1249  0.1251  0.1249  0.1249  0.1250
 0.1250  0.1250  0.1248  0.1250  0.1251  0.1250  0.1253  0.1249
 0.1251  0.1248  0.1250  0.1251  0.1250  0.1250  0.1252  0.1248
 0.1249  0.1250  0.1252  0.1252  0.1251  0.1248  0.1249  0.1251
 0.1250  0.1252  0.1250  0.1251  0.1249  0.1249  0.1249  0.1250
 0.1250  0.1250  0.1251  0.1250  0.1250  0.1249  0.1250  0.1249
 0.1251  0.1251  0.1251  0.1250  0.1248  0.1250  0.1249  0.1251
 0.1251  0.1252  0.1249  0.1249  0.1250  0.1249  0.1252  0.1249
 0.1249  0.1250  0.1250  0.1251  0.1250  0.1253  0.1248  0.1249
 0.1251  0.1250  0.1248  0.1250  0.1251  0.1249  0.1250  0.1251
[torch.cuda.FloatTensor of size 14x8 (GPU 3)]

2019-11-01 17:08:08,490 
alphas_reduce = Variable containing:
 0.1250  0.1252  0.1249  0.1249  0.1250  0.1251  0.1249  0.1250
 0.1250  0.1249  0.1252  0.1251  0.1249  0.1249  0.1251  0.1250
 0.1252  0.1251  0.1249  0.1250  0.1251  0.1249  0.1249  0.1249
 0.1250  0.1249  0.1250  0.1249  0.1250  0.1249  0.1252  0.1250
 0.1249  0.1250  0.1250  0.1250  0.1248  0.1252  0.1252  0.1250
 0.1251  0.1247  0.1249  0.1250  0.1251  0.1250  0.1252  0.1250
 0.1249  0.1253  0.1248  0.1249  0.1250  0.1250  0.1251  0.1250
 0.1249  0.1250  0.1249  0.1249  0.1250  0.1252  0.1252  0.1250
 0.1251  0.1250  0.1249  0.1250  0.1252  0.1249  0.1251  0.1248
 0.1252  0.1250  0.1249  0.1249  0.1250  0.1250  0.1249  0.1250
 0.1250  0.1250  0.1250  0.1249  0.1251  0.1249  0.1250  0.1251
 0.1251  0.1249  0.1250  0.1250  0.1249  0.1250  0.1252  0.1250
 0.1250  0.1249  0.1249  0.1249  0.1251  0.1249  0.1252  0.1251
 0.1252  0.1250  0.1249  0.1250  0.1247  0.1249  0.1250  0.1252
[torch.cuda.FloatTensor of size 14x8 (GPU 3)]

2019-11-01 17:08:16,312 train 000 3.625649e-02 -5.290985
2019-11-01 17:10:57,850 train 050 3.808340e-02 -5.708238
2019-11-01 17:13:43,268 train 100 3.588491e-02 -5.533822
2019-11-01 17:16:29,980 train 150 3.347678e-02 -5.611424
2019-11-01 17:19:13,327 train 200 3.214684e-02 -5.382835
2019-11-01 17:21:55,914 train 250 3.129394e-02 -5.053134
2019-11-01 17:24:40,004 train 300 3.046303e-02 -4.770790
2019-11-01 17:27:23,908 train 350 2.982147e-02 -4.992518
2019-11-01 17:30:06,517 train 400 2.923215e-02 -4.825286
2019-11-01 17:32:48,652 train 450 2.858131e-02 -4.697080
2019-11-01 17:35:31,743 train 500 2.809014e-02 -4.525122
2019-11-01 17:38:15,595 train 550 2.764505e-02 -4.404111
2019-11-01 17:41:01,538 train 600 2.724248e-02 -4.269150
2019-11-01 17:43:44,425 train 650 2.686758e-02 -4.158202
2019-11-01 17:46:28,187 train 700 2.655603e-02 -4.116374
2019-11-01 17:49:10,876 train 750 2.618504e-02 -4.113912
2019-11-01 17:51:53,122 train 800 2.586401e-02 -3.992522
2019-11-01 17:54:34,842 train 850 2.555309e-02 -4.038340
2019-11-01 17:57:17,573 train 900 2.529287e-02 -4.020597
2019-11-01 18:00:00,729 train 950 2.506174e-02 -3.954398
2019-11-01 18:02:43,619 train 1000 2.486172e-02 -3.893990
2019-11-01 18:05:26,355 train 1050 2.463074e-02 -3.844579
2019-11-01 18:08:07,788 train 1100 2.442502e-02 -3.768983
2019-11-01 18:10:49,125 train 1150 2.421056e-02 -3.733688
2019-11-01 18:13:31,715 train 1200 2.401865e-02 -3.713942
2019-11-01 18:16:13,978 train 1250 2.380357e-02 -3.758802
2019-11-01 18:18:56,924 train 1300 2.361096e-02 -3.696839
2019-11-01 18:21:39,118 train 1350 2.347123e-02 -3.654044
2019-11-01 18:24:21,270 train 1400 2.329861e-02 -3.619750
2019-11-01 18:27:02,069 train 1450 2.312514e-02 -3.572045
2019-11-01 18:29:44,814 train 1500 2.295387e-02 -3.551898
2019-11-01 18:31:54,680 training loss; accuracy or R2: 2.283724e-02 -3.612808
2019-11-01 18:31:55,194 valid 000 1.746131e-02 -3.421144
2019-11-01 18:32:03,533 valid 050 1.863339e-02 -2.213504
2019-11-01 18:32:11,842 valid 100 1.831099e-02 -2.310997
2019-11-01 18:32:20,184 valid 150 1.844916e-02 -2.414146
2019-11-01 18:32:28,560 valid 200 1.844902e-02 -2.380701
2019-11-01 18:32:36,933 valid 250 1.862437e-02 -2.310671
2019-11-01 18:32:45,445 valid 300 1.868513e-02 -30.957419
2019-11-01 18:32:53,811 valid 350 1.868687e-02 -26.966010
2019-11-01 18:33:02,230 valid 400 1.860153e-02 -23.965601
2019-11-01 18:33:10,717 valid 450 1.857739e-02 -21.680944
2019-11-01 18:33:19,079 valid 500 1.857747e-02 -19.823483
2019-11-01 18:33:27,377 valid 550 1.858554e-02 -18.197292
2019-11-01 18:33:35,624 valid 600 1.856597e-02 -16.861779
2019-11-01 18:33:43,884 valid 650 1.862001e-02 -15.833937
2019-11-01 18:33:52,160 valid 700 1.862421e-02 -14.862726
2019-11-01 18:34:00,376 valid 750 1.863195e-02 -14.106899
2019-11-01 18:34:08,649 valid 800 1.871661e-02 -13.357313
2019-11-01 18:34:17,111 valid 850 1.873105e-02 -12.706062
2019-11-01 18:34:25,477 valid 900 1.871591e-02 -12.114686
2019-11-01 18:34:33,739 valid 950 1.872814e-02 -11.613915
2019-11-01 18:34:41,952 valid 1000 1.872938e-02 -11.727516
2019-11-01 18:34:50,187 valid 1050 1.873612e-02 -11.349118
2019-11-01 18:34:58,566 valid 1100 1.870839e-02 -10.991945
2019-11-01 18:35:06,869 valid 1150 1.871457e-02 -10.670638
2019-11-01 18:35:15,154 valid 1200 1.868698e-02 -10.331977
2019-11-01 18:35:23,361 valid 1250 1.869036e-02 -10.108625
2019-11-01 18:35:31,591 valid 1300 1.869385e-02 -9.858504
2019-11-01 18:35:39,826 valid 1350 1.869643e-02 -9.589229
2019-11-01 18:35:48,088 valid 1400 1.869325e-02 -9.551361
2019-11-01 18:35:56,351 valid 1450 1.872214e-02 -9.310642
2019-11-01 18:36:04,614 valid 1500 1.872203e-02 -9.073406
2019-11-01 18:36:10,990 validation loss; accuracy or R2: 1.874864e-02 -8.911551
2019-11-01 18:36:11,115 epoch 1 lr 9.991120e-03
2019-11-01 18:36:11,115 genotype = Genotype(normal=[('dil_conv_5x5', 0), ('sep_conv_5x5', 1), ('sep_conv_5x5', 2), ('sep_conv_5x5', 0), ('dil_conv_3x3', 2), ('dil_conv_3x3', 0), ('sep_conv_5x5', 3), ('dil_conv_5x5', 4)], normal_concat=range(2, 6), reduce=[('sep_conv_5x5', 1), ('sep_conv_5x5', 0), ('sep_conv_5x5', 1), ('skip_connect', 0), ('sep_conv_5x5', 0), ('sep_conv_3x3', 3), ('sep_conv_5x5', 4), ('sep_conv_5x5', 2)], reduce_concat=range(2, 6))
2019-11-01 18:36:11,117 
alphas_normal = Variable containing:
 0.1253  0.1249  0.1218  0.1218  0.1265  0.1245  0.1261  0.1292
 0.1240  0.1244  0.1216  0.1214  0.1244  0.1289  0.1286  0.1266
 0.1258  0.1239  0.1222  0.1218  0.1253  0.1273  0.1271  0.1266
 0.1242  0.1270  0.1237  0.1238  0.1233  0.1256  0.1268  0.1255
 0.1230  0.1262  0.1236  0.1231  0.1230  0.1305  0.1272  0.1236
 0.1245  0.1243  0.1235  0.1231  0.1259  0.1248  0.1276  0.1262
 0.1243  0.1257  0.1249  0.1246  0.1243  0.1274  0.1230  0.1259
 0.1242  0.1241  0.1240  0.1241  0.1234  0.1266  0.1277  0.1260
 0.1243  0.1253  0.1242  0.1241  0.1273  0.1254  0.1235  0.1259
 0.1259  0.1244  0.1231  0.1234  0.1270  0.1272  0.1242  0.1248
 0.1240  0.1271  0.1254  0.1251  0.1258  0.1252  0.1232  0.1242
 0.1230  0.1273  0.1254  0.1247  0.1252  0.1256  0.1259  0.1229
 0.1225  0.1258  0.1239  0.1239  0.1247  0.1297  0.1242  0.1253
 0.1228  0.1266  0.1249  0.1250  0.1245  0.1247  0.1239  0.1276
[torch.cuda.FloatTensor of size 14x8 (GPU 3)]

2019-11-01 18:36:11,118 
alphas_reduce = Variable containing:
 0.1243  0.1249  0.1237  0.1222  0.1265  0.1272  0.1250  0.1261
 0.1259  0.1234  0.1220  0.1213  0.1256  0.1289  0.1269  0.1260
 0.1260  0.1240  0.1240  0.1264  0.1240  0.1260  0.1238  0.1258
 0.1255  0.1238  0.1236  0.1263  0.1231  0.1270  0.1251  0.1256
 0.1243  0.1258  0.1248  0.1246  0.1258  0.1250  0.1262  0.1235
 0.1251  0.1242  0.1240  0.1235  0.1261  0.1271  0.1257  0.1242
 0.1259  0.1252  0.1244  0.1237  0.1239  0.1265  0.1253  0.1250
 0.1247  0.1258  0.1238  0.1245  0.1255  0.1264  0.1247  0.1244
 0.1243  0.1255  0.1242  0.1242  0.1271  0.1241  0.1251  0.1255
 0.1255  0.1244  0.1242  0.1258  0.1260  0.1254  0.1247  0.1240
 0.1253  0.1247  0.1242  0.1254  0.1264  0.1249  0.1249  0.1242
 0.1243  0.1256  0.1237  0.1243  0.1256  0.1265  0.1247  0.1254
 0.1252  0.1264  0.1251  0.1248  0.1233  0.1259  0.1248  0.1244
 0.1238  0.1257  0.1242  0.1244  0.1253  0.1284  0.1239  0.1242
[torch.cuda.FloatTensor of size 14x8 (GPU 3)]

2019-11-01 18:36:14,586 train 000 2.398415e-02 -3.696680
2019-11-01 18:38:55,662 train 050 1.903519e-02 -54.477503
2019-11-01 18:41:40,773 train 100 1.868075e-02 -28.940662
2019-11-01 18:44:28,094 train 150 1.818002e-02 -20.033385
2019-11-01 18:47:06,885 train 200 1.822037e-02 -20.011943
2019-11-01 18:49:45,073 train 250 1.818378e-02 -16.609815
2019-11-01 18:52:23,830 train 300 1.812340e-02 -14.390532
2019-11-01 18:55:02,274 train 350 1.806771e-02 -12.649502
2019-11-01 18:57:41,170 train 400 1.804569e-02 -11.328254
2019-11-01 19:00:25,022 train 450 1.784831e-02 -10.318044
2019-11-01 19:03:04,506 train 500 1.771129e-02 -9.493977
2019-11-01 19:05:43,248 train 550 1.761242e-02 -8.799829
2019-11-01 19:08:22,382 train 600 1.757017e-02 -8.270615
2019-11-01 19:11:07,556 train 650 1.751820e-02 -7.770976
2019-11-01 19:13:49,284 train 700 1.746156e-02 -7.388071
2019-11-01 19:16:45,045 train 750 1.742942e-02 -7.040481
2019-11-01 19:19:22,784 train 800 1.737254e-02 -6.710152
2019-11-01 19:22:08,465 train 850 1.729381e-02 -6.432937
2019-11-01 19:24:50,513 train 900 1.730397e-02 -6.226325
2019-11-01 19:27:32,907 train 950 1.726683e-02 -6.002904
2019-11-01 19:30:13,214 train 1000 1.721979e-02 -5.980589
2019-11-01 19:32:55,181 train 1050 1.720317e-02 -5.794417
2019-11-01 19:35:37,808 train 1100 1.715753e-02 -5.631821
2019-11-01 19:38:16,444 train 1150 1.710891e-02 -5.560429
2019-11-01 19:40:55,555 train 1200 1.706717e-02 -5.452988
2019-11-01 19:43:38,969 train 1250 1.703514e-02 -5.319301
2019-11-01 19:46:19,756 train 1300 1.703222e-02 -5.184044
2019-11-01 19:49:00,301 train 1350 1.700295e-02 -5.072267
2019-11-01 19:51:40,610 train 1400 1.699191e-02 -4.960072
2019-11-01 19:54:23,309 train 1450 1.696393e-02 -5.528803
2019-11-01 19:57:03,330 train 1500 1.693408e-02 -5.434646
2019-11-01 19:59:09,058 training loss; accuracy or R2: 1.689362e-02 -5.334690
2019-11-01 19:59:09,527 valid 000 1.505093e-02 -3.929475
2019-11-01 19:59:17,848 valid 050 1.563979e-02 -2.025981
2019-11-01 19:59:26,128 valid 100 1.549660e-02 -1.782398
2019-11-01 19:59:34,511 valid 150 1.541686e-02 -2.028249
2019-11-01 19:59:43,728 valid 200 1.524036e-02 -1.982108
2019-11-01 19:59:53,814 valid 250 1.518853e-02 -2.058274
2019-11-01 20:00:02,472 valid 300 1.518932e-02 -2.022557
2019-11-01 20:00:10,764 valid 350 1.517988e-02 -1.988619
2019-11-01 20:00:19,632 valid 400 1.516268e-02 -2.037008
2019-11-01 20:00:28,536 valid 450 1.521323e-02 -2.165141
2019-11-01 20:00:36,799 valid 500 1.525576e-02 -2.086380
2019-11-01 20:00:44,867 valid 550 1.527527e-02 -2.029140
2019-11-01 20:00:52,892 valid 600 1.525386e-02 -1.992261
2019-11-01 20:01:00,848 valid 650 1.523235e-02 -1.958057
2019-11-01 20:01:08,811 valid 700 1.523775e-02 -1.979815
2019-11-01 20:01:16,729 valid 750 1.523251e-02 -1.972666
2019-11-01 20:01:24,677 valid 800 1.525101e-02 -1.997343
2019-11-01 20:01:32,588 valid 850 1.525917e-02 -1.988682
2019-11-01 20:01:40,538 valid 900 1.523330e-02 -1.954632
2019-11-01 20:01:48,507 valid 950 1.525582e-02 -1.963159
2019-11-01 20:01:56,443 valid 1000 1.523703e-02 -1.938196
2019-11-01 20:02:04,363 valid 1050 1.526782e-02 -1.955982
2019-11-01 20:02:12,274 valid 1100 1.526764e-02 -1.961642
2019-11-01 20:02:20,184 valid 1150 1.526972e-02 -1.981840
2019-11-01 20:02:28,060 valid 1200 1.525410e-02 -2.002659
2019-11-01 20:02:35,969 valid 1250 1.526019e-02 -2.015837
2019-11-01 20:02:43,873 valid 1300 1.525635e-02 -1.999335
2019-11-01 20:02:51,784 valid 1350 1.526081e-02 -1.989610
2019-11-01 20:02:59,641 valid 1400 1.525673e-02 -1.980979
2019-11-01 20:03:07,499 valid 1450 1.524113e-02 -1.971900
2019-11-01 20:03:15,365 valid 1500 1.526098e-02 -1.970254
2019-11-01 20:03:21,496 validation loss; accuracy or R2: 1.526929e-02 -4.090856
2019-11-01 20:03:21,623 epoch 2 lr 9.964516e-03
2019-11-01 20:03:21,624 genotype = Genotype(normal=[('sep_conv_5x5', 1), ('dil_conv_5x5', 0), ('sep_conv_5x5', 2), ('sep_conv_5x5', 0), ('dil_conv_5x5', 0), ('max_pool_3x3', 3), ('sep_conv_5x5', 3), ('sep_conv_3x3', 2)], normal_concat=range(2, 6), reduce=[('sep_conv_5x5', 0), ('dil_conv_3x3', 1), ('sep_conv_3x3', 1), ('max_pool_3x3', 2), ('max_pool_3x3', 2), ('max_pool_3x3', 3), ('dil_conv_5x5', 3), ('dil_conv_3x3', 0)], reduce_concat=range(2, 6))
2019-11-01 20:03:21,625 
alphas_normal = Variable containing:
 0.1259  0.1250  0.1221  0.1225  0.1260  0.1241  0.1264  0.1280
 0.1239  0.1248  0.1219  0.1223  0.1225  0.1295  0.1279  0.1271
 0.1249  0.1225  0.1214  0.1214  0.1270  0.1290  0.1260  0.1279
 0.1246  0.1267  0.1245  0.1252  0.1250  0.1253  0.1237  0.1249
 0.1239  0.1265  0.1234  0.1234  0.1238  0.1296  0.1248  0.1246
 0.1247  0.1247  0.1240  0.1240  0.1242  0.1258  0.1247  0.1280
 0.1238  0.1262  0.1255  0.1258  0.1252  0.1250  0.1243  0.1242
 0.1236  0.1254  0.1243  0.1245  0.1253  0.1264  0.1252  0.1253
 0.1236  0.1265  0.1246  0.1249  0.1261  0.1251  0.1249  0.1243
 0.1256  0.1260  0.1245  0.1248  0.1248  0.1244  0.1270  0.1230
 0.1243  0.1270  0.1252  0.1253  0.1233  0.1248  0.1246  0.1256
 0.1240  0.1264  0.1244  0.1242  0.1279  0.1253  0.1242  0.1236
 0.1231  0.1263  0.1237  0.1245  0.1223  0.1285  0.1255  0.1262
 0.1244  0.1270  0.1249  0.1256  0.1230  0.1272  0.1223  0.1257
[torch.cuda.FloatTensor of size 14x8 (GPU 3)]

2019-11-01 20:03:21,627 
alphas_reduce = Variable containing:
 0.1243  0.1254  0.1234  0.1237  0.1243  0.1277  0.1247  0.1266
 0.1261  0.1244  0.1226  0.1245  0.1248  0.1258  0.1259  0.1258
 0.1247  0.1253  0.1245  0.1246  0.1255  0.1242  0.1251  0.1261
 0.1244  0.1239  0.1239  0.1250  0.1271  0.1268  0.1226  0.1263
 0.1252  0.1261  0.1240  0.1245  0.1251  0.1260  0.1247  0.1244
 0.1244  0.1245  0.1238  0.1257  0.1246  0.1262  0.1255  0.1253
 0.1252  0.1242  0.1234  0.1251  0.1263  0.1258  0.1249  0.1251
 0.1249  0.1272  0.1237  0.1244  0.1266  0.1251  0.1240  0.1240
 0.1246  0.1266  0.1242  0.1245  0.1260  0.1242  0.1247  0.1250
 0.1249  0.1243  0.1235  0.1237  0.1259  0.1265  0.1265  0.1248
 0.1251  0.1249  0.1239  0.1245  0.1262  0.1257  0.1262  0.1235
 0.1252  0.1257  0.1235  0.1245  0.1255  0.1249  0.1255  0.1251
 0.1253  0.1249  0.1237  0.1242  0.1254  0.1252  0.1244  0.1268
 0.1260  0.1255  0.1245  0.1240  0.1229  0.1254  0.1264  0.1253
[torch.cuda.FloatTensor of size 14x8 (GPU 3)]

2019-11-01 20:03:25,045 train 000 1.848136e-02 -0.811032
2019-11-01 20:06:03,446 train 050 1.594107e-02 -2.954506
2019-11-01 20:08:47,555 train 100 1.577290e-02 -10.137300
2019-11-01 20:11:32,138 train 150 1.606260e-02 -13.993704
2019-11-01 20:14:16,187 train 200 1.624409e-02 -11.554810
2019-11-01 20:17:01,964 train 250 1.616652e-02 -9.657193
2019-11-01 20:19:45,146 train 300 1.597293e-02 -8.365865
2019-11-01 20:22:28,969 train 350 1.592831e-02 -9.724233
2019-11-01 20:25:12,178 train 400 1.596631e-02 -9.067119
2019-11-01 20:27:54,865 train 450 1.581834e-02 -8.311643
2019-11-01 20:30:37,348 train 500 1.575081e-02 -7.852059
2019-11-01 20:33:20,317 train 550 1.566800e-02 -7.402739
2019-11-01 20:36:02,372 train 600 1.557794e-02 -6.915752
2019-11-01 20:38:46,094 train 650 1.555751e-02 -6.524037
2019-11-01 20:41:28,475 train 700 1.548368e-02 -6.238130
2019-11-01 20:44:10,501 train 750 1.544744e-02 -5.941937
2019-11-01 20:46:53,366 train 800 1.542883e-02 -5.697480
2019-11-01 20:49:35,889 train 850 1.542791e-02 -5.458426
2019-11-01 20:52:18,526 train 900 1.547028e-02 -5.234969
2019-11-01 20:55:01,091 train 950 1.541688e-02 -5.492372
2019-11-01 20:57:44,347 train 1000 1.538054e-02 -5.439944
2019-11-01 21:00:28,083 train 1050 1.535665e-02 -5.293056
2019-11-01 21:03:12,382 train 1100 1.534394e-02 -5.127998
2019-11-01 21:05:55,797 train 1150 1.532706e-02 -4.998300
2019-11-01 21:08:39,480 train 1200 1.533707e-02 -5.286441
2019-11-01 21:11:22,748 train 1250 1.534013e-02 -5.136056
2019-11-01 21:14:05,839 train 1300 1.533907e-02 -5.014250
2019-11-01 21:16:48,693 train 1350 1.533478e-02 -4.888981
2019-11-01 21:19:29,076 train 1400 1.531354e-02 -5.901886
2019-11-01 21:22:10,643 train 1450 1.529156e-02 -5.773536
2019-11-01 21:24:53,495 train 1500 1.528231e-02 -5.635560
2019-11-01 21:27:00,539 training loss; accuracy or R2: 1.524965e-02 -5.545290
2019-11-01 21:27:01,051 valid 000 1.651835e-02 -0.454278
2019-11-01 21:27:09,621 valid 050 1.456757e-02 -1.469926
2019-11-01 21:27:18,233 valid 100 1.436175e-02 -1.564056
2019-11-01 21:27:27,257 valid 150 1.422606e-02 -3.562524
2019-11-01 21:27:35,932 valid 200 1.436789e-02 -3.054742
2019-11-01 21:27:44,563 valid 250 1.444817e-02 -3.016198
2019-11-01 21:27:53,166 valid 300 1.453452e-02 -63.674858
2019-11-01 21:28:01,871 valid 350 1.451253e-02 -55.523347
2019-11-01 21:28:10,601 valid 400 1.447554e-02 -51.256890
2019-11-01 21:28:19,282 valid 450 1.447716e-02 -55.120226
2019-11-01 21:28:27,928 valid 500 1.442703e-02 -49.764850
2019-11-01 21:28:36,715 valid 550 1.441645e-02 -45.573263
2019-11-01 21:28:46,360 valid 600 1.438472e-02 -41.963937
2019-11-01 21:28:55,813 valid 650 1.436620e-02 -38.881072
2019-11-01 21:29:04,666 valid 700 1.439464e-02 -36.330845
2019-11-01 21:29:13,695 valid 750 1.443111e-02 -34.020738
2019-11-01 21:29:22,324 valid 800 1.438932e-02 -32.019144
2019-11-01 21:29:31,363 valid 850 1.441309e-02 -30.232447
2019-11-01 21:29:40,169 valid 900 1.441815e-02 -28.628538
2019-11-01 21:29:49,010 valid 950 1.443978e-02 -27.255726
2019-11-01 21:29:58,079 valid 1000 1.442176e-02 -25.970303
2019-11-01 21:30:06,618 valid 1050 1.442651e-02 -24.819813
2019-11-01 21:30:15,008 valid 1100 1.443392e-02 -23.769366
2019-11-01 21:30:23,140 valid 1150 1.443537e-02 -22.829610
2019-11-01 21:30:31,232 valid 1200 1.443564e-02 -21.958234
2019-11-01 21:30:39,333 valid 1250 1.443578e-02 -21.164256
2019-11-01 21:30:47,436 valid 1300 1.445464e-02 -20.447354
2019-11-01 21:30:55,576 valid 1350 1.445340e-02 -19.767322
2019-11-01 21:31:03,662 valid 1400 1.443626e-02 -19.120971
2019-11-01 21:31:11,738 valid 1450 1.443109e-02 -18.520633
2019-11-01 21:31:19,821 valid 1500 1.441452e-02 -17.965391
2019-11-01 21:31:26,136 validation loss; accuracy or R2: 1.442288e-02 -17.557700
2019-11-01 21:31:26,271 epoch 3 lr 9.920293e-03
2019-11-01 21:31:26,272 genotype = Genotype(normal=[('sep_conv_5x5', 1), ('max_pool_3x3', 0), ('sep_conv_5x5', 2), ('sep_conv_3x3', 1), ('sep_conv_5x5', 2), ('dil_conv_5x5', 0), ('dil_conv_5x5', 1), ('sep_conv_5x5', 3)], normal_concat=range(2, 6), reduce=[('sep_conv_5x5', 1), ('sep_conv_5x5', 0), ('sep_conv_3x3', 1), ('sep_conv_5x5', 2), ('sep_conv_3x3', 3), ('sep_conv_5x5', 2), ('sep_conv_3x3', 1), ('sep_conv_5x5', 3)], reduce_concat=range(2, 6))
2019-11-01 21:31:26,274 
alphas_normal = Variable containing:
 0.1243  0.1272  0.1246  0.1248  0.1233  0.1244  0.1245  0.1270
 0.1248  0.1246  0.1225  0.1224  0.1230  0.1299  0.1259  0.1268
 0.1245  0.1234  0.1216  0.1222  0.1275  0.1266  0.1262  0.1279
 0.1254  0.1245  0.1231  0.1232  0.1282  0.1252  0.1245  0.1258
 0.1249  0.1237  0.1221  0.1230  0.1248  0.1303  0.1254  0.1257
 0.1243  0.1265  0.1247  0.1248  0.1234  0.1266  0.1230  0.1267
 0.1251  0.1248  0.1240  0.1243  0.1253  0.1264  0.1258  0.1243
 0.1239  0.1237  0.1230  0.1231  0.1267  0.1295  0.1257  0.1244
 0.1237  0.1258  0.1247  0.1248  0.1256  0.1266  0.1248  0.1239
 0.1248  0.1262  0.1248  0.1252  0.1248  0.1266  0.1256  0.1221
 0.1238  0.1251  0.1241  0.1241  0.1254  0.1266  0.1234  0.1275
 0.1244  0.1258  0.1243  0.1246  0.1269  0.1250  0.1263  0.1226
 0.1230  0.1268  0.1248  0.1254  0.1235  0.1273  0.1233  0.1260
 0.1238  0.1271  0.1256  0.1266  0.1240  0.1230  0.1235  0.1265
[torch.cuda.FloatTensor of size 14x8 (GPU 3)]

2019-11-01 21:31:26,275 
alphas_reduce = Variable containing:
 0.1244  0.1260  0.1245  0.1238  0.1254  0.1262  0.1248  0.1247
 0.1255  0.1250  0.1231  0.1235  0.1254  0.1266  0.1249  0.1260
 0.1244  0.1256  0.1252  0.1247  0.1253  0.1252  0.1246  0.1250
 0.1252  0.1249  0.1246  0.1258  0.1271  0.1260  0.1228  0.1236
 0.1248  0.1258  0.1246  0.1247  0.1246  0.1271  0.1249  0.1235
 0.1250  0.1251  0.1240  0.1262  0.1255  0.1258  0.1245  0.1240
 0.1253  0.1244  0.1233  0.1261  0.1259  0.1245  0.1256  0.1249
 0.1252  0.1259  0.1233  0.1244  0.1244  0.1267  0.1247  0.1254
 0.1241  0.1253  0.1234  0.1235  0.1272  0.1255  0.1256  0.1253
 0.1255  0.1246  0.1239  0.1237  0.1255  0.1266  0.1252  0.1251
 0.1258  0.1239  0.1233  0.1250  0.1268  0.1262  0.1242  0.1248
 0.1254  0.1244  0.1232  0.1243  0.1249  0.1265  0.1261  0.1252
 0.1250  0.1247  0.1239  0.1240  0.1249  0.1266  0.1247  0.1261
 0.1258  0.1251  0.1235  0.1232  0.1258  0.1264  0.1251  0.1251
[torch.cuda.FloatTensor of size 14x8 (GPU 3)]

2019-11-01 21:31:29,787 train 000 1.446684e-02 -1.388455
2019-11-01 21:34:15,408 train 050 1.452179e-02 -1.758049
2019-11-01 21:36:59,238 train 100 1.498582e-02 -2.028193
2019-11-01 21:39:40,964 train 150 1.491710e-02 -5.581841
2019-11-01 21:42:23,401 train 200 1.477575e-02 -4.968225
2019-11-01 21:45:07,517 train 250 1.479054e-02 -4.489624
2019-11-01 21:47:52,050 train 300 1.469527e-02 -4.235616
2019-11-01 21:50:36,256 train 350 1.476081e-02 -3.908678
2019-11-01 21:53:24,123 train 400 1.469015e-02 -10.196331
2019-11-01 21:56:06,516 train 450 1.462818e-02 -9.266066
2019-11-01 21:58:47,557 train 500 1.466955e-02 -9.015165
2019-11-01 22:01:33,441 train 550 1.460706e-02 -8.397342
2019-11-01 22:04:15,629 train 600 1.455404e-02 -9.186041
2019-11-01 22:06:57,535 train 650 1.454758e-02 -8.654726
2019-11-01 22:09:38,442 train 700 1.455114e-02 -8.201007
2019-11-01 22:12:18,846 train 750 1.453426e-02 -7.798059
2019-11-01 22:15:00,448 train 800 1.451721e-02 -8.694360
2019-11-01 22:17:41,585 train 850 1.448228e-02 -8.265960
2019-11-01 22:20:23,065 train 900 1.446505e-02 -7.922066
2019-11-01 22:23:08,126 train 950 1.443176e-02 -7.587848
2019-11-01 22:25:53,125 train 1000 1.442079e-02 -7.284368
2019-11-01 22:28:37,493 train 1050 1.439249e-02 -7.058561
2019-11-01 22:31:20,490 train 1100 1.437370e-02 -6.842917
2019-11-01 22:34:01,184 train 1150 1.436912e-02 -6.633285
2019-11-01 22:36:44,888 train 1200 1.436256e-02 -6.438597
2019-11-01 22:39:27,275 train 1250 1.436821e-02 -6.265388
2019-11-01 22:42:08,556 train 1300 1.435714e-02 -6.095638
2019-11-01 22:44:48,907 train 1350 1.434999e-02 -5.928006
2019-11-01 22:47:28,872 train 1400 1.434742e-02 -5.801128
2019-11-01 22:50:12,249 train 1450 1.434220e-02 -5.692185
2019-11-01 22:52:51,608 train 1500 1.433787e-02 -5.666817
2019-11-01 22:54:56,694 training loss; accuracy or R2: 1.432084e-02 -5.558289
2019-11-01 22:54:57,169 valid 000 1.559459e-02 -1.868210
2019-11-01 22:55:05,545 valid 050 1.327209e-02 -1.811431
2019-11-01 22:55:13,868 valid 100 1.360629e-02 -2.670963
2019-11-01 22:55:22,243 valid 150 1.371687e-02 -2.456281
2019-11-01 22:55:30,602 valid 200 1.379190e-02 -7.770394
2019-11-01 22:55:38,949 valid 250 1.393055e-02 -6.607770
2019-11-01 22:55:47,303 valid 300 1.392296e-02 -5.775762
2019-11-01 22:55:55,637 valid 350 1.396716e-02 -5.296950
2019-11-01 22:56:04,015 valid 400 1.403649e-02 -4.885905
2019-11-01 22:56:12,386 valid 450 1.404682e-02 -4.633490
2019-11-01 22:56:20,798 valid 500 1.401567e-02 -4.319551
2019-11-01 22:56:29,212 valid 550 1.399958e-02 -4.087564
2019-11-01 22:56:37,610 valid 600 1.402492e-02 -3.949529
2019-11-01 22:56:46,027 valid 650 1.401746e-02 -3.759268
2019-11-01 22:56:54,436 valid 700 1.403083e-02 -3.590342
2019-11-01 22:57:02,864 valid 750 1.400111e-02 -3.465747
2019-11-01 22:57:11,244 valid 800 1.402205e-02 -3.454557
2019-11-01 22:57:19,636 valid 850 1.401579e-02 -3.489811
2019-11-01 22:57:28,016 valid 900 1.406154e-02 -3.406561
2019-11-01 22:57:36,432 valid 950 1.404746e-02 -3.321133
2019-11-01 22:57:44,792 valid 1000 1.404200e-02 -3.236985
2019-11-01 22:57:53,230 valid 1050 1.405147e-02 -3.160981
2019-11-01 22:58:01,637 valid 1100 1.403356e-02 -3.122771
2019-11-01 22:58:10,056 valid 1150 1.400500e-02 -3.193009
2019-11-01 22:58:18,367 valid 1200 1.401516e-02 -3.165618
2019-11-01 22:58:26,703 valid 1250 1.403080e-02 -3.125935
2019-11-01 22:58:35,071 valid 1300 1.400260e-02 -3.109365
2019-11-01 22:58:43,412 valid 1350 1.399959e-02 -3.111829
2019-11-01 22:58:51,743 valid 1400 1.400553e-02 -3.078273
2019-11-01 22:59:00,106 valid 1450 1.398915e-02 -3.034402
2019-11-01 22:59:08,427 valid 1500 1.398427e-02 -3.001045
2019-11-01 22:59:14,885 validation loss; accuracy or R2: 1.397745e-02 -2.975830
2019-11-01 22:59:15,011 epoch 4 lr 9.858624e-03
2019-11-01 22:59:15,012 genotype = Genotype(normal=[('dil_conv_5x5', 0), ('sep_conv_5x5', 1), ('dil_conv_5x5', 0), ('sep_conv_5x5', 2), ('sep_conv_5x5', 1), ('sep_conv_5x5', 2), ('sep_conv_5x5', 3), ('sep_conv_3x3', 2)], normal_concat=range(2, 6), reduce=[('skip_connect', 1), ('max_pool_3x3', 0), ('sep_conv_3x3', 1), ('max_pool_3x3', 2), ('dil_conv_5x5', 1), ('sep_conv_5x5', 0), ('sep_conv_5x5', 1), ('max_pool_3x3', 4)], reduce_concat=range(2, 6))
2019-11-01 22:59:15,014 
alphas_normal = Variable containing:
 0.1249  0.1246  0.1224  0.1232  0.1244  0.1261  0.1250  0.1294
 0.1247  0.1246  0.1219  0.1225  0.1246  0.1289  0.1262  0.1265
 0.1250  0.1243  0.1227  0.1231  0.1254  0.1256  0.1252  0.1287
 0.1240  0.1263  0.1247  0.1247  0.1239  0.1244  0.1267  0.1254
 0.1234  0.1262  0.1244  0.1242  0.1259  0.1276  0.1240  0.1242
 0.1254  0.1249  0.1234  0.1235  0.1247  0.1266  0.1249  0.1265
 0.1246  0.1248  0.1233  0.1233  0.1255  0.1274  0.1263  0.1248
 0.1249  0.1241  0.1233  0.1242  0.1269  0.1274  0.1258  0.1233
 0.1245  0.1255  0.1241  0.1248  0.1248  0.1252  0.1238  0.1272
 0.1247  0.1253  0.1239  0.1244  0.1247  0.1261  0.1263  0.1245
 0.1247  0.1252  0.1244  0.1242  0.1259  0.1246  0.1257  0.1253
 0.1246  0.1255  0.1246  0.1249  0.1275  0.1251  0.1248  0.1232
 0.1250  0.1249  0.1233  0.1242  0.1238  0.1289  0.1240  0.1259
 0.1238  0.1259  0.1251  0.1257  0.1242  0.1234  0.1249  0.1270
[torch.cuda.FloatTensor of size 14x8 (GPU 3)]

2019-11-01 22:59:15,015 
alphas_reduce = Variable containing:
 0.1240  0.1264  0.1257  0.1246  0.1244  0.1261  0.1240  0.1247
 0.1266  0.1237  0.1227  0.1271  0.1232  0.1261  0.1245  0.1261
 0.1239  0.1266  0.1263  0.1246  0.1237  0.1256  0.1241  0.1252
 0.1253  0.1238  0.1233  0.1261  0.1267  0.1251  0.1246  0.1251
 0.1250  0.1266  0.1250  0.1251  0.1244  0.1241  0.1249  0.1248
 0.1237  0.1257  0.1253  0.1239  0.1258  0.1260  0.1254  0.1242
 0.1255  0.1230  0.1229  0.1256  0.1266  0.1251  0.1245  0.1268
 0.1257  0.1253  0.1242  0.1247  0.1259  0.1247  0.1248  0.1246
 0.1259  0.1248  0.1237  0.1248  0.1248  0.1260  0.1248  0.1250
 0.1244  0.1259  0.1257  0.1248  0.1249  0.1243  0.1256  0.1244
 0.1247  0.1246  0.1243  0.1251  0.1246  0.1268  0.1250  0.1249
 0.1250  0.1255  0.1245  0.1246  0.1248  0.1256  0.1254  0.1246
 0.1251  0.1255  0.1244  0.1248  0.1256  0.1256  0.1227  0.1263
 0.1252  0.1264  0.1255  0.1253  0.1247  0.1255  0.1229  0.1245
[torch.cuda.FloatTensor of size 14x8 (GPU 3)]

2019-11-01 22:59:18,461 train 000 1.248345e-02 -5.453037
2019-11-01 23:01:59,075 train 050 1.401937e-02 -1.688736
2019-11-01 23:04:44,310 train 100 1.375954e-02 -3.785640
2019-11-01 23:07:27,378 train 150 1.368917e-02 -2.992714
2019-11-01 23:10:09,879 train 200 1.356195e-02 -2.612560
2019-11-01 23:12:49,707 train 250 1.356861e-02 -2.699288
2019-11-01 23:15:27,795 train 300 1.357436e-02 -2.566402
2019-11-01 23:18:10,455 train 350 1.356169e-02 -2.700231
2019-11-01 23:20:50,739 train 400 1.350716e-02 -2.534839
2019-11-01 23:23:32,526 train 450 1.356284e-02 -2.455754
2019-11-01 23:26:11,206 train 500 1.360482e-02 -4.140124
2019-11-01 23:28:51,038 train 550 1.361013e-02 -3.929962
2019-11-01 23:31:34,240 train 600 1.352919e-02 -3.737885
2019-11-01 23:34:18,349 train 650 1.354793e-02 -4.425836
2019-11-01 23:37:00,266 train 700 1.350666e-02 -8.031168
2019-11-01 23:39:39,526 train 750 1.355004e-02 -7.825474
2019-11-01 23:42:19,001 train 800 1.357811e-02 -7.432261
2019-11-01 23:45:01,271 train 850 1.358600e-02 -7.076551
2019-11-01 23:47:41,772 train 900 1.362019e-02 -6.784013
2019-11-01 23:50:23,995 train 950 1.362363e-02 -6.608677
2019-11-01 23:53:03,111 train 1000 1.361281e-02 -6.718853
2019-11-01 23:55:49,178 train 1050 1.361164e-02 -6.570735
2019-11-01 23:58:28,692 train 1100 1.362837e-02 -6.492147
2019-11-02 00:01:10,991 train 1150 1.362009e-02 -6.263217
2019-11-02 00:03:54,283 train 1200 1.361793e-02 -6.063037
2019-11-02 00:06:38,340 train 1250 1.362287e-02 -5.892294
2019-11-02 00:09:20,174 train 1300 1.363547e-02 -5.729470
2019-11-02 00:12:00,374 train 1350 1.360469e-02 -5.603868
2019-11-02 00:14:39,990 train 1400 1.361714e-02 -5.456085
2019-11-02 00:17:22,072 train 1450 1.363909e-02 -5.339585
2019-11-02 00:20:03,436 train 1500 1.363653e-02 -5.707464
2019-11-02 00:22:08,847 training loss; accuracy or R2: 1.363731e-02 -5.608996
2019-11-02 00:22:09,327 valid 000 1.771961e-02 -3.263528
2019-11-02 00:22:17,663 valid 050 1.770915e-02 -5.797295
2019-11-02 00:22:26,003 valid 100 1.782186e-02 -5.193025
2019-11-02 00:22:34,313 valid 150 1.766252e-02 -5.033625
2019-11-02 00:22:42,657 valid 200 1.770257e-02 -4.735275
2019-11-02 00:22:51,326 valid 250 1.769212e-02 -4.628205
2019-11-02 00:23:01,593 valid 300 1.772236e-02 -4.613011
2019-11-02 00:23:11,835 valid 350 1.780572e-02 -4.457166
2019-11-02 00:23:22,079 valid 400 1.781388e-02 -4.380161
2019-11-02 00:23:31,887 valid 450 1.784250e-02 -4.461330
2019-11-02 00:23:40,289 valid 500 1.781188e-02 -4.395791
2019-11-02 00:23:48,644 valid 550 1.775929e-02 -4.339441
2019-11-02 00:23:56,971 valid 600 1.772709e-02 -4.412915
2019-11-02 00:24:05,333 valid 650 1.772404e-02 -4.393012
2019-11-02 00:24:13,650 valid 700 1.771367e-02 -4.310864
2019-11-02 00:24:22,012 valid 750 1.771083e-02 -4.380035
2019-11-02 00:24:30,358 valid 800 1.766882e-02 -4.417307
2019-11-02 00:24:38,704 valid 850 1.768421e-02 -4.505601
2019-11-02 00:24:47,055 valid 900 1.770779e-02 -4.487726
2019-11-02 00:24:55,365 valid 950 1.771588e-02 -4.539099
2019-11-02 00:25:03,674 valid 1000 1.770909e-02 -4.534068
2019-11-02 00:25:12,022 valid 1050 1.772740e-02 -4.527367
2019-11-02 00:25:20,341 valid 1100 1.770416e-02 -4.451146
2019-11-02 00:25:28,655 valid 1150 1.768478e-02 -4.438831
2019-11-02 00:25:36,969 valid 1200 1.770182e-02 -4.448033
2019-11-02 00:25:45,299 valid 1250 1.768488e-02 -4.511907
2019-11-02 00:25:53,650 valid 1300 1.766781e-02 -4.508822
2019-11-02 00:26:02,013 valid 1350 1.765727e-02 -4.490898
2019-11-02 00:26:10,328 valid 1400 1.764904e-02 -4.456971
2019-11-02 00:26:18,656 valid 1450 1.765075e-02 -4.531981
2019-11-02 00:26:26,973 valid 1500 1.764831e-02 -4.488840
2019-11-02 00:26:33,450 validation loss; accuracy or R2: 1.767092e-02 -4.486835
2019-11-02 00:26:33,580 epoch 5 lr 9.779754e-03
2019-11-02 00:26:33,581 genotype = Genotype(normal=[('sep_conv_5x5', 1), ('max_pool_3x3', 0), ('dil_conv_5x5', 2), ('dil_conv_5x5', 0), ('sep_conv_5x5', 2), ('sep_conv_5x5', 0), ('sep_conv_5x5', 3), ('max_pool_3x3', 4)], normal_concat=range(2, 6), reduce=[('sep_conv_5x5', 1), ('sep_conv_5x5', 0), ('sep_conv_3x3', 1), ('dil_conv_5x5', 0), ('sep_conv_5x5', 3), ('sep_conv_3x3', 1), ('sep_conv_5x5', 4), ('dil_conv_5x5', 3)], reduce_concat=range(2, 6))
2019-11-02 00:26:33,583 
alphas_normal = Variable containing:
 0.1255  0.1265  0.1241  0.1247  0.1235  0.1249  0.1250  0.1258
 0.1247  0.1251  0.1223  0.1231  0.1235  0.1289  0.1257  0.1268
 0.1245  0.1247  0.1232  0.1233  0.1260  0.1268  0.1242  0.1274
 0.1247  0.1255  0.1240  0.1243  0.1260  0.1245  0.1247  0.1263
 0.1235  0.1254  0.1237  0.1238  0.1249  0.1253  0.1257  0.1277
 0.1253  0.1262  0.1244  0.1245  0.1245  0.1269  0.1240  0.1243
 0.1233  0.1260  0.1245  0.1248  0.1248  0.1265  0.1246  0.1255
 0.1236  0.1256  0.1243  0.1249  0.1260  0.1281  0.1240  0.1235
 0.1237  0.1265  0.1248  0.1254  0.1255  0.1242  0.1250  0.1249
 0.1249  0.1269  0.1244  0.1247  0.1248  0.1246  0.1258  0.1238
 0.1247  0.1269  0.1250  0.1251  0.1249  0.1247  0.1239  0.1248
 0.1241  0.1259  0.1238  0.1244  0.1271  0.1245  0.1254  0.1247
 0.1244  0.1260  0.1236  0.1240  0.1236  0.1274  0.1248  0.1262
 0.1240  0.1273  0.1248  0.1257  0.1238  0.1244  0.1245  0.1255
[torch.cuda.FloatTensor of size 14x8 (GPU 3)]

2019-11-02 00:26:33,584 
alphas_reduce = Variable containing:
 0.1249  0.1259  0.1249  0.1231  0.1244  0.1264  0.1254  0.1250
 0.1254  0.1235  0.1221  0.1253  0.1241  0.1276  0.1258  0.1263
 0.1251  0.1263  0.1261  0.1240  0.1261  0.1236  0.1220  0.1267
 0.1254  0.1248  0.1241  0.1248  0.1274  0.1237  0.1246  0.1253
 0.1250  0.1261  0.1248  0.1247  0.1240  0.1245  0.1253  0.1256
 0.1251  0.1246  0.1240  0.1244  0.1250  0.1261  0.1256  0.1251
 0.1247  0.1247  0.1238  0.1248  0.1262  0.1253  0.1258  0.1247
 0.1248  0.1261  0.1242  0.1249  0.1250  0.1253  0.1248  0.1249
 0.1255  0.1239  0.1225  0.1237  0.1271  0.1274  0.1259  0.1240
 0.1247  0.1245  0.1242  0.1263  0.1259  0.1244  0.1248  0.1251
 0.1245  0.1241  0.1238  0.1255  0.1254  0.1253  0.1262  0.1252
 0.1245  0.1253  0.1236  0.1242  0.1251  0.1258  0.1256  0.1258
 0.1253  0.1247  0.1236  0.1245  0.1266  0.1246  0.1240  0.1268
 0.1252  0.1259  0.1245  0.1246  0.1260  0.1270  0.1233  0.1234
[torch.cuda.FloatTensor of size 14x8 (GPU 3)]

2019-11-02 00:26:37,071 train 000 1.197465e-02 -0.577605
2019-11-02 00:29:18,086 train 050 1.336725e-02 -1.160182
2019-11-02 00:31:58,435 train 100 1.373380e-02 -1.580502
2019-11-02 00:34:43,504 train 150 1.356498e-02 -1.638977
2019-11-02 00:37:26,971 train 200 1.331337e-02 -2.775055
2019-11-02 00:40:11,120 train 250 1.332914e-02 -2.510687
2019-11-02 00:42:53,377 train 300 1.327417e-02 -2.379589
2019-11-02 00:45:35,919 train 350 1.329727e-02 -2.246901
2019-11-02 00:48:19,541 train 400 1.332737e-02 -2.169753
2019-11-02 00:51:01,493 train 450 1.335038e-02 -3.917969
2019-11-02 00:53:46,997 train 500 1.335071e-02 -4.025261
2019-11-02 00:56:35,513 train 550 1.334891e-02 -3.821379
2019-11-02 00:59:19,579 train 600 1.330525e-02 -3.658197
2019-11-02 01:02:03,021 train 650 1.324929e-02 -3.508259
2019-11-02 01:04:45,768 train 700 1.323685e-02 -3.363882
2019-11-02 01:07:27,585 train 750 1.323218e-02 -3.278003
2019-11-02 01:10:08,884 train 800 1.323346e-02 -3.192779
2019-11-02 01:12:51,686 train 850 1.319647e-02 -3.097315
2019-11-02 01:15:33,516 train 900 1.321739e-02 -2.984860
2019-11-02 01:18:15,228 train 950 1.316721e-02 -5.615451
2019-11-02 01:20:57,641 train 1000 1.312420e-02 -5.422468
2019-11-02 01:23:39,624 train 1050 1.311147e-02 -5.247435
2019-11-02 01:26:19,814 train 1100 1.311769e-02 -5.103106
2019-11-02 01:28:58,791 train 1150 1.312075e-02 -5.605198
2019-11-02 01:31:47,399 train 1200 1.314048e-02 -5.461206
2019-11-02 01:34:27,603 train 1250 1.315991e-02 -5.304272
2019-11-02 01:37:11,159 train 1300 1.314327e-02 -5.315605
2019-11-02 01:39:54,895 train 1350 1.314295e-02 -5.185602
2019-11-02 01:42:44,469 train 1400 1.312677e-02 -5.687103
2019-11-02 01:45:27,799 train 1450 1.313677e-02 -5.546214
2019-11-02 01:48:12,158 train 1500 1.312513e-02 -5.449965
2019-11-02 01:50:20,666 training loss; accuracy or R2: 1.313854e-02 -5.351947
2019-11-02 01:50:21,124 valid 000 1.774240e-02 -2.504974
2019-11-02 01:50:29,448 valid 050 1.771275e-02 -4.139101
2019-11-02 01:50:37,761 valid 100 1.832005e-02 -4.447944
2019-11-02 01:50:46,086 valid 150 1.822040e-02 -4.702754
2019-11-02 01:50:54,418 valid 200 1.814135e-02 -4.695127
2019-11-02 01:51:02,731 valid 250 1.814429e-02 -4.956792
2019-11-02 01:51:11,047 valid 300 1.819397e-02 -27.894706
2019-11-02 01:51:19,388 valid 350 1.811564e-02 -25.064463
2019-11-02 01:51:27,688 valid 400 1.812620e-02 -22.517616
2019-11-02 01:51:36,020 valid 450 1.799483e-02 -20.525197
2019-11-02 01:51:44,369 valid 500 1.796011e-02 -18.899940
2019-11-02 01:51:52,693 valid 550 1.794741e-02 -17.690606
2019-11-02 01:52:01,002 valid 600 1.787619e-02 -16.590499
2019-11-02 01:52:09,331 valid 650 1.785357e-02 -15.692924
2019-11-02 01:52:17,680 valid 700 1.788092e-02 -14.963760
2019-11-02 01:52:25,992 valid 750 1.795957e-02 -14.319199
2019-11-02 01:52:34,372 valid 800 1.799501e-02 -13.714120
2019-11-02 01:52:42,697 valid 850 1.797407e-02 -13.188693
2019-11-02 01:52:51,081 valid 900 1.796913e-02 -12.745969
2019-11-02 01:52:59,392 valid 950 1.796612e-02 -12.384858
2019-11-02 01:53:07,733 valid 1000 1.796101e-02 -11.977291
2019-11-02 01:53:16,027 valid 1050 1.795264e-02 -11.606281
2019-11-02 01:53:24,688 valid 1100 1.800232e-02 -11.349170
2019-11-02 01:53:34,956 valid 1150 1.799685e-02 -11.156718
2019-11-02 01:53:45,218 valid 1200 1.800005e-02 -10.850910
2019-11-02 01:53:55,466 valid 1250 1.803424e-02 -10.619232
2019-11-02 01:54:05,715 valid 1300 1.803982e-02 -10.381811
2019-11-02 01:54:15,967 valid 1350 1.805279e-02 -10.186315
2019-11-02 01:54:26,188 valid 1400 1.803820e-02 -10.000412
2019-11-02 01:54:36,399 valid 1450 1.803232e-02 -9.876907
2019-11-02 01:54:46,665 valid 1500 1.801082e-02 -9.692396
2019-11-02 01:54:54,629 validation loss; accuracy or R2: 1.801826e-02 -9.564408
2019-11-02 01:54:54,764 epoch 6 lr 9.683994e-03
2019-11-02 01:54:54,765 genotype = Genotype(normal=[('sep_conv_5x5', 1), ('max_pool_3x3', 0), ('dil_conv_5x5', 0), ('dil_conv_5x5', 1), ('max_pool_3x3', 3), ('max_pool_3x3', 0), ('sep_conv_5x5', 3), ('max_pool_3x3', 4)], normal_concat=range(2, 6), reduce=[('dil_conv_5x5', 1), ('sep_conv_5x5', 0), ('sep_conv_3x3', 1), ('dil_conv_5x5', 2), ('sep_conv_5x5', 1), ('max_pool_3x3', 2), ('sep_conv_5x5', 1), ('dil_conv_3x3', 2)], reduce_concat=range(2, 6))
2019-11-02 01:54:54,767 
alphas_normal = Variable containing:
 0.1256  0.1273  0.1245  0.1249  0.1239  0.1239  0.1233  0.1267
 0.1243  0.1256  0.1232  0.1241  0.1236  0.1281  0.1237  0.1275
 0.1244  0.1244  0.1228  0.1227  0.1271  0.1272  0.1234  0.1280
 0.1248  0.1260  0.1237  0.1240  0.1265  0.1249  0.1236  0.1266
 0.1258  0.1253  0.1236  0.1240  0.1255  0.1259  0.1258  0.1241
 0.1244  0.1267  0.1258  0.1256  0.1235  0.1249  0.1231  0.1261
 0.1240  0.1259  0.1256  0.1257  0.1219  0.1259  0.1253  0.1256
 0.1243  0.1256  0.1253  0.1254  0.1254  0.1259  0.1244  0.1238
 0.1241  0.1270  0.1260  0.1258  0.1247  0.1245  0.1234  0.1247
 0.1248  0.1267  0.1252  0.1254  0.1260  0.1225  0.1247  0.1247
 0.1242  0.1273  0.1258  0.1260  0.1252  0.1238  0.1232  0.1245
 0.1238  0.1274  0.1253  0.1249  0.1254  0.1257  0.1248  0.1226
 0.1232  0.1270  0.1243  0.1243  0.1244  0.1277  0.1232  0.1258
 0.1241  0.1274  0.1251  0.1256  0.1246  0.1244  0.1234  0.1254
[torch.cuda.FloatTensor of size 14x8 (GPU 3)]

2019-11-02 01:54:54,769 
alphas_reduce = Variable containing:
 0.1241  0.1261  0.1245  0.1239  0.1257  0.1267  0.1245  0.1244
 0.1257  0.1245  0.1227  0.1260  0.1227  0.1259  0.1247  0.1278
 0.1248  0.1259  0.1251  0.1244  0.1253  0.1245  0.1249  0.1251
 0.1247  0.1239  0.1229  0.1262  0.1272  0.1249  0.1250  0.1253
 0.1252  0.1256  0.1241  0.1246  0.1244  0.1255  0.1244  0.1261
 0.1246  0.1254  0.1244  0.1245  0.1265  0.1254  0.1245  0.1248
 0.1249  0.1246  0.1234  0.1250  0.1258  0.1266  0.1241  0.1256
 0.1246  0.1265  0.1245  0.1245  0.1259  0.1257  0.1243  0.1239
 0.1247  0.1264  0.1245  0.1246  0.1254  0.1259  0.1230  0.1255
 0.1247  0.1257  0.1252  0.1244  0.1251  0.1258  0.1247  0.1245
 0.1255  0.1229  0.1224  0.1249  0.1263  0.1277  0.1260  0.1242
 0.1258  0.1238  0.1227  0.1243  0.1247  0.1255  0.1271  0.1260
 0.1253  0.1246  0.1236  0.1243  0.1268  0.1247  0.1254  0.1254
 0.1260  0.1255  0.1239  0.1241  0.1257  0.1254  0.1249  0.1245
[torch.cuda.FloatTensor of size 14x8 (GPU 3)]

2019-11-02 01:54:58,430 train 000 1.150039e-02 -0.976926
2019-11-02 01:57:38,230 train 050 1.248738e-02 -1.706300
2019-11-02 02:00:20,384 train 100 1.266579e-02 -1.677438
2019-11-02 02:03:08,620 train 150 1.269403e-02 -3.747792
2019-11-02 02:05:52,550 train 200 1.286535e-02 -3.878863
2019-11-02 02:08:33,404 train 250 1.299386e-02 -3.370540
2019-11-02 02:11:14,503 train 300 1.299265e-02 -3.087195
2019-11-02 02:13:54,827 train 350 1.301917e-02 -3.318310
2019-11-02 02:16:38,962 train 400 1.305922e-02 -5.951300
2019-11-02 02:19:17,732 train 450 1.308635e-02 -6.484014
2019-11-02 02:22:00,153 train 500 1.309703e-02 -6.018027
2019-11-02 02:24:46,887 train 550 1.307910e-02 -5.624945
2019-11-02 02:27:30,005 train 600 1.303941e-02 -7.547714
2019-11-02 02:30:11,308 train 650 1.303192e-02 -7.243441
2019-11-02 02:32:51,893 train 700 1.299692e-02 -6.918386
2019-11-02 02:35:32,937 train 750 1.300471e-02 -6.547161
2019-11-02 02:38:14,240 train 800 1.292431e-02 -6.223068
2019-11-02 02:40:53,535 train 850 1.291672e-02 -5.918117
2019-11-02 02:43:39,409 train 900 1.290149e-02 -5.703240
2019-11-02 02:46:19,985 train 950 1.289253e-02 -5.485041
2019-11-02 02:49:08,367 train 1000 1.289342e-02 -5.287825
2019-11-02 02:51:59,212 train 1050 1.289534e-02 -5.184649
2019-11-02 02:54:40,083 train 1100 1.287713e-02 -5.014058
2019-11-02 02:57:21,303 train 1150 1.287481e-02 -4.877151
2019-11-02 03:00:01,407 train 1200 1.286562e-02 -4.753805
2019-11-02 03:02:43,521 train 1250 1.284055e-02 -4.626472
2019-11-02 03:05:24,650 train 1300 1.284448e-02 -4.520295
2019-11-02 03:08:10,801 train 1350 1.283330e-02 -4.462438
2019-11-02 03:10:51,321 train 1400 1.283718e-02 -4.426608
2019-11-02 03:13:30,613 train 1450 1.282229e-02 -4.327443
2019-11-02 03:16:10,184 train 1500 1.283214e-02 -4.223307
2019-11-02 03:18:13,486 training loss; accuracy or R2: 1.281547e-02 -4.163347
2019-11-02 03:18:13,929 valid 000 1.437654e-02 -2.229856
2019-11-02 03:18:22,230 valid 050 1.303553e-02 -2.112062
2019-11-02 03:18:31,360 valid 100 1.280336e-02 -1.995755
2019-11-02 03:18:41,441 valid 150 1.270179e-02 -21.121181
2019-11-02 03:18:49,741 valid 200 1.281067e-02 -16.493956
2019-11-02 03:18:58,030 valid 250 1.292078e-02 -13.591024
2019-11-02 03:19:06,361 valid 300 1.298606e-02 -11.992382
2019-11-02 03:19:14,665 valid 350 1.298963e-02 -10.581160
2019-11-02 03:19:22,990 valid 400 1.298397e-02 -10.237340
2019-11-02 03:19:31,292 valid 450 1.297533e-02 -9.334273
2019-11-02 03:19:39,611 valid 500 1.301900e-02 -8.674927
2019-11-02 03:19:47,910 valid 550 1.301948e-02 -14.038010
2019-11-02 03:19:56,230 valid 600 1.305216e-02 -13.111192
2019-11-02 03:20:04,513 valid 650 1.309422e-02 -12.316796
2019-11-02 03:20:12,822 valid 700 1.309593e-02 -11.628490
2019-11-02 03:20:21,130 valid 750 1.309024e-02 -11.024971
2019-11-02 03:20:29,488 valid 800 1.313098e-02 -10.521986
2019-11-02 03:20:37,808 valid 850 1.309834e-02 -10.026768
2019-11-02 03:20:46,138 valid 900 1.309277e-02 -9.587902
2019-11-02 03:20:54,426 valid 950 1.313177e-02 -9.247992
2019-11-02 03:21:02,726 valid 1000 1.311977e-02 -9.108285
2019-11-02 03:21:11,009 valid 1050 1.312538e-02 -8.770069
2019-11-02 03:21:19,298 valid 1100 1.314284e-02 -8.518638
2019-11-02 03:21:27,644 valid 1150 1.314788e-02 -8.250947
2019-11-02 03:21:35,916 valid 1200 1.314287e-02 -8.002946
2019-11-02 03:21:44,206 valid 1250 1.315055e-02 -7.790877
2019-11-02 03:21:52,519 valid 1300 1.315563e-02 -7.573861
2019-11-02 03:22:00,825 valid 1350 1.316097e-02 -7.393352
2019-11-02 03:22:09,159 valid 1400 1.315798e-02 -7.215119
2019-11-02 03:22:17,467 valid 1450 1.315148e-02 -7.044429
2019-11-02 03:22:25,806 valid 1500 1.315891e-02 -6.879172
2019-11-02 03:22:32,276 validation loss; accuracy or R2: 1.317863e-02 -6.770512
2019-11-02 03:22:32,400 epoch 7 lr 9.571722e-03
2019-11-02 03:22:32,401 genotype = Genotype(normal=[('sep_conv_5x5', 1), ('max_pool_3x3', 0), ('dil_conv_5x5', 0), ('dil_conv_5x5', 1), ('sep_conv_3x3', 2), ('dil_conv_5x5', 3), ('sep_conv_5x5', 3), ('dil_conv_5x5', 4)], normal_concat=range(2, 6), reduce=[('dil_conv_5x5', 1), ('max_pool_3x3', 0), ('sep_conv_3x3', 1), ('dil_conv_5x5', 2), ('sep_conv_5x5', 2), ('dil_conv_5x5', 3), ('dil_conv_3x3', 1), ('dil_conv_5x5', 4)], reduce_concat=range(2, 6))
2019-11-02 03:22:32,402 
alphas_normal = Variable containing:
 0.1250  0.1281  0.1252  0.1259  0.1243  0.1248  0.1217  0.1249
 0.1249  0.1244  0.1223  0.1230  0.1256  0.1286  0.1237  0.1275
 0.1248  0.1245  0.1227  0.1228  0.1258  0.1261  0.1243  0.1289
 0.1248  0.1250  0.1234  0.1239  0.1262  0.1246  0.1248  0.1274
 0.1255  0.1251  0.1233  0.1234  0.1257  0.1265  0.1267  0.1239
 0.1251  0.1256  0.1247  0.1247  0.1245  0.1260  0.1235  0.1259
 0.1240  0.1258  0.1252  0.1253  0.1243  0.1260  0.1240  0.1255
 0.1244  0.1252  0.1242  0.1247  0.1264  0.1252  0.1251  0.1249
 0.1242  0.1252  0.1243  0.1246  0.1248  0.1257  0.1248  0.1263
 0.1244  0.1257  0.1249  0.1249  0.1265  0.1253  0.1238  0.1246
 0.1250  0.1254  0.1241  0.1244  0.1258  0.1249  0.1255  0.1250
 0.1239  0.1265  0.1249  0.1248  0.1262  0.1262  0.1257  0.1219
 0.1236  0.1263  0.1248  0.1255  0.1251  0.1276  0.1238  0.1233
 0.1240  0.1260  0.1242  0.1249  0.1242  0.1244  0.1251  0.1271
[torch.cuda.FloatTensor of size 14x8 (GPU 3)]

2019-11-02 03:22:32,404 
alphas_reduce = Variable containing:
 0.1238  0.1265  0.1248  0.1244  0.1258  0.1255  0.1238  0.1255
 0.1265  0.1238  0.1226  0.1247  0.1238  0.1253  0.1249  0.1284
 0.1252  0.1258  0.1252  0.1237  0.1241  0.1247  0.1262  0.1250
 0.1254  0.1239  0.1232  0.1242  0.1274  0.1256  0.1249  0.1254
 0.1247  0.1260  0.1246  0.1244  0.1252  0.1239  0.1249  0.1263
 0.1255  0.1258  0.1251  0.1250  0.1251  0.1242  0.1252  0.1240
 0.1249  0.1246  0.1236  0.1248  0.1258  0.1257  0.1248  0.1257
 0.1242  0.1259  0.1249  0.1246  0.1245  0.1263  0.1240  0.1256
 0.1239  0.1258  0.1243  0.1245  0.1251  0.1258  0.1247  0.1259
 0.1249  0.1254  0.1250  0.1249  0.1251  0.1255  0.1239  0.1252
 0.1257  0.1234  0.1228  0.1245  0.1261  0.1253  0.1274  0.1248
 0.1257  0.1253  0.1244  0.1254  0.1243  0.1238  0.1258  0.1253
 0.1247  0.1248  0.1240  0.1244  0.1256  0.1254  0.1254  0.1257
 0.1252  0.1255  0.1247  0.1251  0.1244  0.1255  0.1233  0.1262
[torch.cuda.FloatTensor of size 14x8 (GPU 3)]

2019-11-02 03:22:35,810 train 000 1.414741e-02 -1.845072
2019-11-02 03:25:17,932 train 050 1.202143e-02 -1.895111
2019-11-02 03:27:57,623 train 100 1.207848e-02 -260.267757
2019-11-02 03:30:40,512 train 150 1.221176e-02 -180.140046
2019-11-02 03:33:25,513 train 200 1.218184e-02 -135.996411
2019-11-02 03:36:08,612 train 250 1.215911e-02 -109.260745
2019-11-02 03:38:51,299 train 300 1.224630e-02 -94.584981
2019-11-02 03:41:32,283 train 350 1.232399e-02 -81.420916
2019-11-02 03:44:19,969 train 400 1.238873e-02 -71.468542
2019-11-02 03:47:04,339 train 450 1.245311e-02 -63.692273
2019-11-02 03:49:47,761 train 500 1.244181e-02 -57.478336
2019-11-02 03:52:31,578 train 550 1.244103e-02 -52.406317
2019-11-02 03:55:12,299 train 600 1.249149e-02 -48.357723
2019-11-02 03:57:58,184 train 650 1.247435e-02 -50.104785
2019-11-02 04:00:43,335 train 700 1.248250e-02 -46.667651
2019-11-02 04:03:23,299 train 750 1.248044e-02 -43.680070
2019-11-02 04:06:03,692 train 800 1.247724e-02 -41.057515
2019-11-02 04:08:45,747 train 850 1.247183e-02 -39.955389
2019-11-02 04:11:29,516 train 900 1.244946e-02 -37.979746
2019-11-02 04:14:12,708 train 950 1.243635e-02 -36.072165
2019-11-02 04:16:56,524 train 1000 1.241331e-02 -34.508201
2019-11-02 04:19:41,075 train 1050 1.241265e-02 -33.029022
2019-11-02 04:22:23,300 train 1100 1.242718e-02 -31.646113
2019-11-02 04:25:05,574 train 1150 1.245182e-02 -30.358876
2019-11-02 04:27:46,692 train 1200 1.245029e-02 -29.165068
2019-11-02 04:30:31,362 train 1250 1.245807e-02 -28.070426
2019-11-02 04:33:16,046 train 1300 1.245110e-02 -27.196851
2019-11-02 04:35:59,949 train 1350 1.247650e-02 -26.244083
2019-11-02 04:38:44,843 train 1400 1.248948e-02 -25.357807
2019-11-02 04:41:25,375 train 1450 1.248669e-02 -24.556926
2019-11-02 04:44:11,562 train 1500 1.247101e-02 -23.841010
2019-11-02 04:46:17,764 training loss; accuracy or R2: 1.246853e-02 -23.281732
2019-11-02 04:46:18,261 valid 000 1.204481e-02 -1.161681
2019-11-02 04:46:26,621 valid 050 1.144885e-02 -1.837234
2019-11-02 04:46:35,013 valid 100 1.188170e-02 -1.720719
2019-11-02 04:46:43,322 valid 150 1.182826e-02 -1.539507
2019-11-02 04:46:51,705 valid 200 1.212979e-02 -1.564270
2019-11-02 04:47:00,024 valid 250 1.218844e-02 -1.595789
2019-11-02 04:47:08,393 valid 300 1.229483e-02 -1.589412
2019-11-02 04:47:16,748 valid 350 1.232245e-02 -2.670041
2019-11-02 04:47:25,108 valid 400 1.229148e-02 -2.520835
2019-11-02 04:47:33,484 valid 450 1.232743e-02 -2.500063
2019-11-02 04:47:41,783 valid 500 1.223899e-02 -2.601038
2019-11-02 04:47:50,092 valid 550 1.220459e-02 -2.488071
2019-11-02 04:47:58,420 valid 600 1.220684e-02 -2.555920
2019-11-02 04:48:06,789 valid 650 1.220501e-02 -2.495296
2019-11-02 04:48:15,163 valid 700 1.219989e-02 -2.429979
2019-11-02 04:48:23,492 valid 750 1.221111e-02 -2.374922
2019-11-02 04:48:31,860 valid 800 1.219656e-02 -2.336676
2019-11-02 04:48:40,208 valid 850 1.216456e-02 -2.302699
2019-11-02 04:48:48,595 valid 900 1.212655e-02 -2.306214
2019-11-02 04:48:56,962 valid 950 1.210704e-02 -2.260588
2019-11-02 04:49:05,313 valid 1000 1.208702e-02 -2.225471
2019-11-02 04:49:13,714 valid 1050 1.207323e-02 -2.196455
2019-11-02 04:49:22,072 valid 1100 1.207913e-02 -2.190423
2019-11-02 04:49:30,418 valid 1150 1.210973e-02 -6.246901
2019-11-02 04:49:38,769 valid 1200 1.210581e-02 -6.069847
2019-11-02 04:49:47,144 valid 1250 1.209628e-02 -5.945482
2019-11-02 04:49:55,488 valid 1300 1.210057e-02 -13.329626
2019-11-02 04:50:03,826 valid 1350 1.207956e-02 -12.913766
2019-11-02 04:50:12,135 valid 1400 1.208013e-02 -12.504523
2019-11-02 04:50:20,448 valid 1450 1.208049e-02 -12.133424
2019-11-02 04:50:28,768 valid 1500 1.206842e-02 -11.772368
2019-11-02 04:50:35,223 validation loss; accuracy or R2: 1.205950e-02 -11.518514
2019-11-02 04:50:35,365 epoch 8 lr 9.443380e-03
2019-11-02 04:50:35,366 genotype = Genotype(normal=[('dil_conv_5x5', 1), ('dil_conv_5x5', 0), ('dil_conv_5x5', 0), ('sep_conv_3x3', 1), ('sep_conv_5x5', 2), ('sep_conv_3x3', 0), ('dil_conv_5x5', 1), ('sep_conv_5x5', 2)], normal_concat=range(2, 6), reduce=[('dil_conv_5x5', 1), ('max_pool_3x3', 0), ('max_pool_3x3', 0), ('dil_conv_5x5', 2), ('sep_conv_5x5', 1), ('sep_conv_5x5', 2), ('sep_conv_3x3', 1), ('dil_conv_5x5', 3)], reduce_concat=range(2, 6))
2019-11-02 04:50:35,368 
alphas_normal = Variable containing:
 0.1252  0.1263  0.1251  0.1254  0.1225  0.1248  0.1219  0.1288
 0.1242  0.1239  0.1228  0.1233  0.1230  0.1289  0.1238  0.1302
 0.1262  0.1232  0.1221  0.1226  0.1245  0.1267  0.1263  0.1285
 0.1241  0.1255  0.1248  0.1252  0.1270  0.1258  0.1231  0.1246
 0.1253  0.1247  0.1232  0.1235  0.1253  0.1254  0.1264  0.1261
 0.1241  0.1250  0.1247  0.1246  0.1264  0.1247  0.1241  0.1263
 0.1248  0.1247  0.1250  0.1254  0.1247  0.1242  0.1260  0.1253
 0.1249  0.1243  0.1242  0.1249  0.1250  0.1270  0.1252  0.1246
 0.1253  0.1244  0.1246  0.1250  0.1263  0.1241  0.1254  0.1249
 0.1256  0.1253  0.1247  0.1254  0.1247  0.1254  0.1253  0.1236
 0.1240  0.1245  0.1243  0.1250  0.1261  0.1246  0.1244  0.1271
 0.1243  0.1253  0.1247  0.1249  0.1248  0.1271  0.1246  0.1243
 0.1246  0.1251  0.1249  0.1256  0.1246  0.1254  0.1240  0.1258
 0.1253  0.1244  0.1239  0.1241  0.1253  0.1267  0.1250  0.1253
[torch.cuda.FloatTensor of size 14x8 (GPU 3)]

2019-11-02 04:50:35,370 
alphas_reduce = Variable containing:
 0.1238  0.1277  0.1264  0.1250  0.1253  0.1233  0.1251  0.1234
 0.1265  0.1233  0.1231  0.1248  0.1241  0.1246  0.1254  0.1281
 0.1244  0.1277  0.1270  0.1234  0.1244  0.1247  0.1240  0.1244
 0.1257  0.1231  0.1236  0.1259  0.1264  0.1251  0.1249  0.1253
 0.1246  0.1250  0.1245  0.1242  0.1244  0.1245  0.1257  0.1270
 0.1247  0.1261  0.1255  0.1252  0.1251  0.1250  0.1243  0.1241
 0.1252  0.1241  0.1242  0.1251  0.1252  0.1268  0.1233  0.1261
 0.1250  0.1243  0.1240  0.1243  0.1254  0.1264  0.1242  0.1264
 0.1245  0.1247  0.1246  0.1246  0.1256  0.1254  0.1245  0.1260
 0.1244  0.1254  0.1253  0.1244  0.1258  0.1260  0.1243  0.1244
 0.1255  0.1236  0.1239  0.1241  0.1276  0.1251  0.1257  0.1245
 0.1253  0.1233  0.1238  0.1246  0.1264  0.1260  0.1257  0.1249
 0.1251  0.1235  0.1240  0.1245  0.1268  0.1260  0.1231  0.1272
 0.1254  0.1248  0.1242  0.1244  0.1255  0.1265  0.1240  0.1252
[torch.cuda.FloatTensor of size 14x8 (GPU 3)]

2019-11-02 04:50:38,872 train 000 1.372485e-02 -2.446648
2019-11-02 04:53:19,051 train 050 1.177653e-02 -1.521773
2019-11-02 04:55:59,050 train 100 1.192128e-02 -1.593121
2019-11-02 04:58:42,579 train 150 1.216969e-02 -2.328388
2019-11-02 05:01:25,528 train 200 1.217927e-02 -2.182676
2019-11-02 05:04:06,100 train 250 1.218161e-02 -2.050043
2019-11-02 05:06:48,731 train 300 1.218242e-02 -2.770683
2019-11-02 05:09:29,774 train 350 1.211409e-02 -2.598774
2019-11-02 05:12:14,334 train 400 1.210005e-02 -2.489459
2019-11-02 05:14:57,172 train 450 1.213713e-02 -4.564481
2019-11-02 05:17:41,948 train 500 1.216716e-02 -4.351828
2019-11-02 05:20:23,990 train 550 1.218854e-02 -4.130240
2019-11-02 05:23:07,878 train 600 1.221337e-02 -3.998013
2019-11-02 05:25:52,738 train 650 1.227797e-02 -3.853863
2019-11-02 05:28:33,359 train 700 1.226950e-02 -17.597908
2019-11-02 05:31:14,716 train 750 1.225306e-02 -16.504461
2019-11-02 05:33:53,520 train 800 1.225881e-02 -16.116196
2019-11-02 05:36:45,350 train 850 1.227532e-02 -15.273654
2019-11-02 05:39:28,000 train 900 1.227498e-02 -14.526193
2019-11-02 05:42:08,496 train 950 1.229302e-02 -13.842501
2019-11-02 05:44:50,816 train 1000 1.230924e-02 -13.230822
2019-11-02 05:47:38,081 train 1050 1.231090e-02 -12.669788
2019-11-02 05:50:17,886 train 1100 1.228231e-02 -12.166267
2019-11-02 05:53:01,436 train 1150 1.226460e-02 -11.694169
2019-11-02 05:55:43,136 train 1200 1.225046e-02 -11.307106
2019-11-02 05:58:29,487 train 1250 1.225692e-02 -10.924342
2019-11-02 06:01:10,565 train 1300 1.225798e-02 -10.562386
2019-11-02 06:03:52,213 train 1350 1.222697e-02 -10.217647
2019-11-02 06:06:33,884 train 1400 1.222230e-02 -9.910887
2019-11-02 06:09:18,544 train 1450 1.222461e-02 -9.630839
2019-11-02 06:11:59,508 train 1500 1.221642e-02 -9.351934
2019-11-02 06:14:08,277 training loss; accuracy or R2: 1.221208e-02 -9.158061
2019-11-02 06:14:08,815 valid 000 1.345067e-02 -0.722938
2019-11-02 06:14:19,064 valid 050 1.171579e-02 -1.586812
2019-11-02 06:14:29,276 valid 100 1.148301e-02 -1.542688
2019-11-02 06:14:39,539 valid 150 1.139089e-02 -1.330878
2019-11-02 06:14:49,783 valid 200 1.136255e-02 -1.323431
2019-11-02 06:15:00,028 valid 250 1.133729e-02 -1.300997
2019-11-02 06:15:09,230 valid 300 1.133334e-02 -1.273014
2019-11-02 06:15:17,587 valid 350 1.132691e-02 -1.277647
2019-11-02 06:15:25,944 valid 400 1.125532e-02 -1.372946
2019-11-02 06:15:34,257 valid 450 1.120639e-02 -1.467477
2019-11-02 06:15:42,587 valid 500 1.126115e-02 -1.434578
2019-11-02 06:15:50,884 valid 550 1.130329e-02 -1.483838
2019-11-02 06:15:59,217 valid 600 1.128333e-02 -1.496945
2019-11-02 06:16:07,511 valid 650 1.127584e-02 -1.468480
2019-11-02 06:16:15,823 valid 700 1.122961e-02 -1.480346
2019-11-02 06:16:24,173 valid 750 1.121940e-02 -1.486641
2019-11-02 06:16:32,530 valid 800 1.119781e-02 -1.501122
2019-11-02 06:16:40,865 valid 850 1.118849e-02 -1.464683
2019-11-02 06:16:49,230 valid 900 1.118413e-02 -1.467798
2019-11-02 06:16:57,563 valid 950 1.115696e-02 -1.451960
2019-11-02 06:17:05,893 valid 1000 1.115462e-02 -1.453769
2019-11-02 06:17:14,218 valid 1050 1.119114e-02 -1.438276
2019-11-02 06:17:22,558 valid 1100 1.120782e-02 -1.705998
2019-11-02 06:17:30,883 valid 1150 1.119530e-02 -1.717752
2019-11-02 06:17:39,220 valid 1200 1.119559e-02 -1.781046
2019-11-02 06:17:47,568 valid 1250 1.121367e-02 -1.787201
2019-11-02 06:17:55,919 valid 1300 1.120483e-02 -1.786193
2019-11-02 06:18:04,285 valid 1350 1.120318e-02 -1.773756
2019-11-02 06:18:12,609 valid 1400 1.119765e-02 -1.744507
2019-11-02 06:18:20,982 valid 1450 1.120856e-02 -1.719216
2019-11-02 06:18:29,347 valid 1500 1.121424e-02 -1.710169
2019-11-02 06:18:36,788 validation loss; accuracy or R2: 1.121346e-02 -7.778796
2019-11-02 06:18:36,922 epoch 9 lr 9.299476e-03
2019-11-02 06:18:36,922 genotype = Genotype(normal=[('dil_conv_5x5', 1), ('max_pool_3x3', 0), ('sep_conv_5x5', 0), ('dil_conv_5x5', 2), ('max_pool_3x3', 0), ('sep_conv_5x5', 2), ('dil_conv_3x3', 2), ('dil_conv_5x5', 1)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 0), ('dil_conv_5x5', 1), ('sep_conv_3x3', 1), ('max_pool_3x3', 0), ('max_pool_3x3', 2), ('sep_conv_3x3', 1), ('dil_conv_5x5', 3), ('sep_conv_3x3', 1)], reduce_concat=range(2, 6))
2019-11-02 06:18:36,924 
alphas_normal = Variable containing:
 0.1240  0.1283  0.1271  0.1275  0.1205  0.1248  0.1219  0.1259
 0.1254  0.1235  0.1233  0.1236  0.1236  0.1286  0.1217  0.1303
 0.1234  0.1236  0.1224  0.1227  0.1260  0.1282  0.1268  0.1269
 0.1252  0.1247  0.1253  0.1253  0.1257  0.1246  0.1231  0.1260
 0.1254  0.1237  0.1224  0.1235  0.1258  0.1262  0.1254  0.1275
 0.1225  0.1284  0.1275  0.1271  0.1245  0.1235  0.1215  0.1249
 0.1248  0.1249  0.1258  0.1253  0.1248  0.1234  0.1259  0.1251
 0.1247  0.1252  0.1248  0.1247  0.1249  0.1262  0.1250  0.1246
 0.1246  0.1261  0.1256  0.1259  0.1251  0.1239  0.1237  0.1252
 0.1249  0.1267  0.1257  0.1260  0.1249  0.1251  0.1219  0.1247
 0.1261  0.1235  0.1238  0.1237  0.1257  0.1245  0.1252  0.1275
 0.1251  0.1243  0.1232  0.1244  0.1262  0.1262  0.1277  0.1229
 0.1251  0.1240  0.1233  0.1243  0.1260  0.1272  0.1234  0.1267
 0.1253  0.1243  0.1232  0.1236  0.1256  0.1248  0.1273  0.1260
[torch.cuda.FloatTensor of size 14x8 (GPU 3)]

2019-11-02 06:18:36,926 
alphas_reduce = Variable containing:
 0.1229  0.1282  0.1271  0.1254  0.1256  0.1231  0.1240  0.1236
 0.1269  0.1228  0.1225  0.1251  0.1254  0.1258  0.1249  0.1266
 0.1245  0.1281  0.1272  0.1223  0.1250  0.1251  0.1236  0.1242
 0.1247  0.1239  0.1240  0.1245  0.1290  0.1254  0.1232  0.1254
 0.1245  0.1270  0.1259  0.1253  0.1231  0.1238  0.1241  0.1263
 0.1244  0.1256  0.1247  0.1252  0.1262  0.1254  0.1245  0.1241
 0.1252  0.1245  0.1240  0.1242  0.1266  0.1243  0.1250  0.1262
 0.1253  0.1268  0.1256  0.1255  0.1236  0.1252  0.1240  0.1240
 0.1253  0.1249  0.1240  0.1244  0.1252  0.1253  0.1252  0.1256
 0.1247  0.1260  0.1258  0.1241  0.1260  0.1253  0.1238  0.1242
 0.1253  0.1236  0.1238  0.1241  0.1266  0.1261  0.1259  0.1245
 0.1250  0.1246  0.1245  0.1245  0.1256  0.1256  0.1253  0.1248
 0.1253  0.1242  0.1241  0.1244  0.1250  0.1247  0.1246  0.1277
 0.1262  0.1250  0.1249  0.1249  0.1238  0.1262  0.1240  0.1251
[torch.cuda.FloatTensor of size 14x8 (GPU 3)]

2019-11-02 06:18:40,633 train 000 1.128007e-02 -0.629774
2019-11-02 06:21:25,328 train 050 1.266553e-02 -1.823483
2019-11-02 06:24:05,689 train 100 1.225526e-02 -1.540321
2019-11-02 06:26:47,293 train 150 1.215092e-02 -1.747774
2019-11-02 06:29:29,720 train 200 1.224922e-02 -1.939392
2019-11-02 06:32:17,109 train 250 1.216654e-02 -1.839196
2019-11-02 06:34:58,852 train 300 1.214919e-02 -5.410168
2019-11-02 06:37:41,492 train 350 1.217307e-02 -4.866893
2019-11-02 06:40:22,364 train 400 1.216142e-02 -4.481172
2019-11-02 06:43:03,113 train 450 1.213263e-02 -4.190618
2019-11-02 06:45:42,631 train 500 1.204169e-02 -9.726846
2019-11-02 06:48:34,399 train 550 1.199571e-02 -8.971684
2019-11-02 06:51:15,669 train 600 1.200115e-02 -8.348068
2019-11-02 06:53:57,958 train 650 1.202893e-02 -7.840911
2019-11-02 06:56:37,300 train 700 1.203211e-02 -7.384862
2019-11-02 06:59:16,197 train 750 1.202546e-02 -6.980726
2019-11-02 07:01:59,261 train 800 1.201641e-02 -6.656065
2019-11-02 07:04:41,506 train 850 1.203064e-02 -7.471575
2019-11-02 07:07:20,479 train 900 1.200615e-02 -7.145700
2019-11-02 07:10:01,547 train 950 1.200563e-02 -6.852318
2019-11-02 07:12:42,393 train 1000 1.200199e-02 -6.583518
2019-11-02 07:15:25,767 train 1050 1.200081e-02 -6.339942
2019-11-02 07:18:07,326 train 1100 1.199450e-02 -6.238424
2019-11-02 07:20:48,629 train 1150 1.200152e-02 -6.042253
2019-11-02 07:23:30,586 train 1200 1.202126e-02 -5.853037
2019-11-02 07:26:10,899 train 1250 1.201562e-02 -5.672920
2019-11-02 07:28:51,639 train 1300 1.201184e-02 -5.862644
2019-11-02 07:31:31,533 train 1350 1.200481e-02 -5.699153
2019-11-02 07:34:12,242 train 1400 1.198103e-02 -5.611039
2019-11-02 07:36:58,615 train 1450 1.198167e-02 -5.492022
2019-11-02 07:39:41,807 train 1500 1.197067e-02 -6.185671
2019-11-02 07:41:47,035 training loss; accuracy or R2: 1.196012e-02 -6.073338
2019-11-02 07:41:47,576 valid 000 1.046906e-02 -2.447455
2019-11-02 07:41:56,466 valid 050 1.144882e-02 -3.493463
2019-11-02 07:42:04,788 valid 100 1.154046e-02 -2.884973
2019-11-02 07:42:13,100 valid 150 1.157255e-02 -2.498975
2019-11-02 07:42:21,377 valid 200 1.149547e-02 -3.702773
2019-11-02 07:42:29,756 valid 250 1.153452e-02 -3.497802
2019-11-02 07:42:38,110 valid 300 1.154504e-02 -3.278009
2019-11-02 07:42:46,444 valid 350 1.153225e-02 -3.003869
2019-11-02 07:42:54,827 valid 400 1.145461e-02 -2.888852
2019-11-02 07:43:03,176 valid 450 1.144219e-02 -2.747851
2019-11-02 07:43:11,516 valid 500 1.141747e-02 -2.701494
2019-11-02 07:43:19,859 valid 550 1.141978e-02 -2.648723
2019-11-02 07:43:28,188 valid 600 1.141382e-02 -2.551279
2019-11-02 07:43:36,528 valid 650 1.138030e-02 -2.494195
2019-11-02 07:43:44,889 valid 700 1.141376e-02 -2.435153
2019-11-02 07:43:53,275 valid 750 1.139890e-02 -2.516293
2019-11-02 07:44:01,626 valid 800 1.138270e-02 -2.441077
2019-11-02 07:44:09,984 valid 850 1.137359e-02 -2.381853
2019-11-02 07:44:18,309 valid 900 1.136421e-02 -2.342218
2019-11-02 07:44:26,675 valid 950 1.137018e-02 -2.322232
2019-11-02 07:44:35,021 valid 1000 1.133697e-02 -2.293948
2019-11-02 07:44:43,407 valid 1050 1.134160e-02 -2.274492
2019-11-02 07:44:51,808 valid 1100 1.135466e-02 -2.265781
2019-11-02 07:45:00,236 valid 1150 1.136698e-02 -2.244561
2019-11-02 07:45:08,634 valid 1200 1.135838e-02 -2.207544
2019-11-02 07:45:18,051 valid 1250 1.138258e-02 -2.205158
2019-11-02 07:45:27,941 valid 1300 1.139441e-02 -2.268979
2019-11-02 07:45:36,338 valid 1350 1.137508e-02 -2.234136
2019-11-02 07:45:44,698 valid 1400 1.136101e-02 -2.255171
2019-11-02 07:45:53,077 valid 1450 1.138140e-02 -2.234942
2019-11-02 07:46:01,422 valid 1500 1.139931e-02 -2.209361
2019-11-02 07:46:07,907 validation loss; accuracy or R2: 1.140664e-02 -2.202213
2019-11-02 07:46:08,041 epoch 10 lr 9.140576e-03
2019-11-02 07:46:08,042 genotype = Genotype(normal=[('sep_conv_5x5', 1), ('max_pool_3x3', 0), ('dil_conv_3x3', 2), ('sep_conv_3x3', 0), ('max_pool_3x3', 0), ('dil_conv_5x5', 1), ('sep_conv_5x5', 3), ('max_pool_3x3', 0)], normal_concat=range(2, 6), reduce=[('dil_conv_5x5', 1), ('max_pool_3x3', 0), ('sep_conv_3x3', 1), ('max_pool_3x3', 0), ('sep_conv_5x5', 2), ('dil_conv_5x5', 3), ('max_pool_3x3', 0), ('sep_conv_5x5', 4)], reduce_concat=range(2, 6))
2019-11-02 07:46:08,043 
alphas_normal = Variable containing:
 0.1246  0.1282  0.1262  0.1264  0.1201  0.1249  0.1225  0.1272
 0.1257  0.1241  0.1227  0.1231  0.1249  0.1289  0.1232  0.1276
 0.1242  0.1253  0.1232  0.1236  0.1264  0.1254  0.1259  0.1259
 0.1254  0.1259  0.1242  0.1244  0.1255  0.1260  0.1240  0.1246
 0.1244  0.1245  0.1229  0.1235  0.1262  0.1245  0.1273  0.1267
 0.1243  0.1275  0.1260  0.1257  0.1251  0.1232  0.1235  0.1247
 0.1257  0.1256  0.1244  0.1243  0.1239  0.1245  0.1251  0.1265
 0.1249  0.1251  0.1244  0.1245  0.1253  0.1257  0.1254  0.1246
 0.1240  0.1253  0.1244  0.1248  0.1256  0.1244  0.1256  0.1258
 0.1241  0.1271  0.1263  0.1266  0.1240  0.1248  0.1239  0.1232
 0.1251  0.1243  0.1238  0.1241  0.1270  0.1260  0.1232  0.1264
 0.1243  0.1254  0.1246  0.1250  0.1263  0.1259  0.1260  0.1225
 0.1232  0.1246  0.1238  0.1244  0.1248  0.1285  0.1242  0.1266
 0.1240  0.1250  0.1242  0.1249  0.1257  0.1254  0.1258  0.1250
[torch.cuda.FloatTensor of size 14x8 (GPU 3)]

2019-11-02 07:46:08,045 
alphas_reduce = Variable containing:
 0.1233  0.1268  0.1256  0.1240  0.1266  0.1261  0.1246  0.1230
 0.1267  0.1235  0.1230  0.1245  0.1251  0.1257  0.1246  0.1269
 0.1247  0.1265  0.1255  0.1250  0.1245  0.1253  0.1240  0.1245
 0.1254  0.1234  0.1230  0.1253  0.1275  0.1254  0.1242  0.1260
 0.1254  0.1262  0.1252  0.1252  0.1234  0.1240  0.1250  0.1256
 0.1249  0.1247  0.1243  0.1244  0.1258  0.1264  0.1245  0.1249
 0.1254  0.1244  0.1242  0.1251  0.1254  0.1250  0.1250  0.1255
 0.1254  0.1240  0.1235  0.1241  0.1251  0.1270  0.1250  0.1259
 0.1255  0.1239  0.1231  0.1236  0.1253  0.1264  0.1256  0.1266
 0.1247  0.1265  0.1260  0.1238  0.1243  0.1253  0.1247  0.1245
 0.1249  0.1242  0.1241  0.1248  0.1258  0.1248  0.1260  0.1254
 0.1250  0.1251  0.1249  0.1250  0.1258  0.1241  0.1253  0.1247
 0.1249  0.1254  0.1249  0.1247  0.1264  0.1241  0.1248  0.1249
 0.1240  0.1263  0.1253  0.1253  0.1243  0.1264  0.1242  0.1242
[torch.cuda.FloatTensor of size 14x8 (GPU 3)]

2019-11-02 07:46:11,510 train 000 1.054178e-02 -0.901819
2019-11-02 07:48:49,693 train 050 1.172522e-02 -2.222789
2019-11-02 07:51:29,199 train 100 1.188048e-02 -6.871344
2019-11-02 07:54:10,338 train 150 1.186096e-02 -5.399274
2019-11-02 07:56:52,781 train 200 1.184323e-02 -4.417922
2019-11-02 07:59:36,579 train 250 1.199716e-02 -3.801799
2019-11-02 08:02:21,205 train 300 1.189156e-02 -3.484592
2019-11-02 08:05:02,078 train 350 1.183878e-02 -3.251881
2019-11-02 08:07:42,892 train 400 1.181847e-02 -3.331308
2019-11-02 08:10:24,107 train 450 1.179722e-02 -3.268632
2019-11-02 08:13:05,099 train 500 1.177603e-02 -3.105545
2019-11-02 08:15:48,052 train 550 1.181867e-02 -3.103474
2019-11-02 08:18:29,391 train 600 1.179421e-02 -2.970583
2019-11-02 08:21:08,179 train 650 1.183279e-02 -3.012236
2019-11-02 08:23:52,928 train 700 1.184281e-02 -2.879387
2019-11-02 08:26:33,515 train 750 1.185694e-02 -2.779653
2019-11-02 08:29:14,762 train 800 1.185708e-02 -2.689242
2019-11-02 08:31:57,227 train 850 1.185805e-02 -2.616364
2019-11-02 08:34:38,697 train 900 1.184759e-02 -2.574148
2019-11-02 08:37:18,675 train 950 1.181854e-02 -2.508816
2019-11-02 08:40:02,822 train 1000 1.180328e-02 -2.476359
2019-11-02 08:42:45,615 train 1050 1.179247e-02 -2.425249
2019-11-02 08:45:27,895 train 1100 1.178484e-02 -2.689027
2019-11-02 08:48:14,423 train 1150 1.177362e-02 -2.744121
2019-11-02 08:50:55,745 train 1200 1.175892e-02 -2.825801
2019-11-02 08:53:38,262 train 1250 1.177616e-02 -2.780864
2019-11-02 08:56:19,557 train 1300 1.178245e-02 -2.722168
2019-11-02 08:59:01,806 train 1350 1.177075e-02 -2.686498
2019-11-02 09:01:42,709 train 1400 1.175608e-02 -2.647188
2019-11-02 09:04:23,212 train 1450 1.177914e-02 -2.613434
2019-11-02 09:07:08,258 train 1500 1.179863e-02 -2.573642
2019-11-02 09:09:15,277 training loss; accuracy or R2: 1.178500e-02 -2.550461
2019-11-02 09:09:15,717 valid 000 1.308846e-02 -3.110439
2019-11-02 09:09:24,063 valid 050 1.108534e-02 -1.900842
2019-11-02 09:09:32,415 valid 100 1.118763e-02 -1.969022
2019-11-02 09:09:40,821 valid 150 1.104638e-02 -1.838853
2019-11-02 09:09:49,183 valid 200 1.106126e-02 -1.801669
2019-11-02 09:09:57,560 valid 250 1.111134e-02 -1.904385
2019-11-02 09:10:05,921 valid 300 1.116722e-02 -1.842983
2019-11-02 09:10:14,264 valid 350 1.125727e-02 -1.760254
2019-11-02 09:10:22,634 valid 400 1.125251e-02 -1.734833
2019-11-02 09:10:30,985 valid 450 1.129570e-02 -1.745473
2019-11-02 09:10:39,355 valid 500 1.129969e-02 -1.751393
2019-11-02 09:10:47,701 valid 550 1.131709e-02 -1.937795
2019-11-02 09:10:56,060 valid 600 1.128295e-02 -1.902248
2019-11-02 09:11:04,442 valid 650 1.129204e-02 -1.874745
2019-11-02 09:11:12,829 valid 700 1.131243e-02 -1.838325
2019-11-02 09:11:21,166 valid 750 1.130898e-02 -1.841982
2019-11-02 09:11:29,514 valid 800 1.130231e-02 -1.916695
2019-11-02 09:11:37,817 valid 850 1.128546e-02 -1.857248
2019-11-02 09:11:46,192 valid 900 1.127640e-02 -1.879543
2019-11-02 09:11:54,516 valid 950 1.127473e-02 -1.850015
2019-11-02 09:12:02,852 valid 1000 1.126609e-02 -1.854622
2019-11-02 09:12:11,203 valid 1050 1.127783e-02 -1.895483
2019-11-02 09:12:19,627 valid 1100 1.125910e-02 -1.963373
2019-11-02 09:12:28,001 valid 1150 1.127437e-02 -1.953907
2019-11-02 09:12:36,401 valid 1200 1.128374e-02 -1.934815
2019-11-02 09:12:44,805 valid 1250 1.129650e-02 -2.074971
2019-11-02 09:12:53,149 valid 1300 1.130205e-02 -2.063282
2019-11-02 09:13:01,483 valid 1350 1.130494e-02 -2.062976
2019-11-02 09:13:09,788 valid 1400 1.130839e-02 -2.049716
2019-11-02 09:13:18,111 valid 1450 1.130109e-02 -5.725394
2019-11-02 09:13:26,425 valid 1500 1.131592e-02 -5.586400
2019-11-02 09:13:32,876 validation loss; accuracy or R2: 1.130134e-02 -5.689348
2019-11-02 09:13:33,022 epoch 11 lr 8.967310e-03
2019-11-02 09:13:33,022 genotype = Genotype(normal=[('sep_conv_5x5', 1), ('dil_conv_5x5', 0), ('dil_conv_5x5', 2), ('sep_conv_3x3', 1), ('sep_conv_5x5', 2), ('dil_conv_5x5', 0), ('sep_conv_5x5', 3), ('dil_conv_3x3', 4)], normal_concat=range(2, 6), reduce=[('sep_conv_5x5', 1), ('sep_conv_3x3', 0), ('sep_conv_3x3', 1), ('dil_conv_3x3', 0), ('dil_conv_5x5', 3), ('sep_conv_3x3', 1), ('dil_conv_5x5', 3), ('sep_conv_5x5', 2)], reduce_concat=range(2, 6))
2019-11-02 09:13:33,024 
alphas_normal = Variable containing:
 0.1259  0.1267  0.1239  0.1249  0.1216  0.1256  0.1243  0.1270
 0.1242  0.1256  0.1230  0.1238  0.1250  0.1296  0.1236  0.1253
 0.1246  0.1250  0.1232  0.1241  0.1259  0.1267  0.1234  0.1271
 0.1254  0.1250  0.1239  0.1242  0.1271  0.1230  0.1243  0.1270
 0.1252  0.1246  0.1230  0.1237  0.1261  0.1252  0.1239  0.1283
 0.1249  0.1257  0.1241  0.1245  0.1252  0.1250  0.1242  0.1263
 0.1247  0.1261  0.1250  0.1252  0.1245  0.1234  0.1260  0.1251
 0.1244  0.1255  0.1240  0.1246  0.1240  0.1265  0.1263  0.1248
 0.1249  0.1254  0.1249  0.1257  0.1256  0.1239  0.1238  0.1259
 0.1248  0.1259  0.1246  0.1251  0.1250  0.1267  0.1236  0.1244
 0.1259  0.1251  0.1239  0.1245  0.1248  0.1259  0.1249  0.1250
 0.1247  0.1243  0.1232  0.1243  0.1264  0.1271  0.1250  0.1250
 0.1244  0.1236  0.1227  0.1240  0.1249  0.1296  0.1238  0.1271
 0.1246  0.1240  0.1232  0.1236  0.1246  0.1251  0.1282  0.1268
[torch.cuda.FloatTensor of size 14x8 (GPU 3)]

2019-11-02 09:13:33,025 
alphas_reduce = Variable containing:
 0.1253  0.1248  0.1244  0.1264  0.1264  0.1245  0.1245  0.1238
 0.1249  0.1236  0.1230  0.1230  0.1230  0.1279  0.1275  0.1271
 0.1248  0.1259  0.1253  0.1222  0.1256  0.1251  0.1265  0.1244
 0.1246  0.1245  0.1241  0.1245  0.1280  0.1256  0.1238  0.1250
 0.1251  0.1251  0.1243  0.1247  0.1248  0.1248  0.1248  0.1263
 0.1247  0.1252  0.1244  0.1256  0.1250  0.1250  0.1249  0.1252
 0.1252  0.1244  0.1235  0.1255  0.1258  0.1254  0.1254  0.1248
 0.1247  0.1255  0.1245  0.1245  0.1251  0.1254  0.1254  0.1250
 0.1249  0.1249  0.1244  0.1248  0.1244  0.1264  0.1234  0.1268
 0.1249  0.1243  0.1244  0.1249  0.1259  0.1262  0.1237  0.1258
 0.1254  0.1241  0.1239  0.1249  0.1262  0.1250  0.1258  0.1246
 0.1250  0.1239  0.1237  0.1243  0.1259  0.1263  0.1260  0.1249
 0.1248  0.1245  0.1242  0.1246  0.1251  0.1257  0.1243  0.1268
 0.1261  0.1249  0.1239  0.1246  0.1239  0.1246  0.1262  0.1258
[torch.cuda.FloatTensor of size 14x8 (GPU 3)]

2019-11-02 09:13:36,496 train 000 1.034541e-02 -5.587921
2019-11-02 09:16:18,756 train 050 1.106213e-02 -1.840854
2019-11-02 09:19:01,687 train 100 1.108378e-02 -3.125643
2019-11-02 09:21:45,480 train 150 1.135222e-02 -2.510874
2019-11-02 09:24:28,667 train 200 1.159011e-02 -6.136376
2019-11-02 09:27:14,255 train 250 1.159772e-02 -5.193378
2019-11-02 09:29:58,131 train 300 1.166013e-02 -5.399215
2019-11-02 09:32:39,018 train 350 1.162245e-02 -4.857984
2019-11-02 09:35:16,304 train 400 1.162419e-02 -4.412825
2019-11-02 09:37:50,465 train 450 1.160385e-02 -4.076472
2019-11-02 09:40:27,298 train 500 1.158514e-02 -3.802293
2019-11-02 09:43:14,385 train 550 1.161422e-02 -3.575408
2019-11-02 09:45:59,497 train 600 1.159615e-02 -3.616122
2019-11-02 09:48:43,628 train 650 1.157870e-02 -3.453100
2019-11-02 09:51:35,590 train 700 1.159116e-02 -7.848803
2019-11-02 09:54:22,053 train 750 1.160472e-02 -7.394390
2019-11-02 09:57:03,149 train 800 1.157438e-02 -7.035531
2019-11-02 09:59:52,080 train 850 1.157864e-02 -6.740899
2019-11-02 10:02:31,553 train 900 1.154863e-02 -6.448822
2019-11-02 10:05:19,325 train 950 1.153714e-02 -6.346335
2019-11-02 10:08:07,885 train 1000 1.152951e-02 -6.358290
2019-11-02 10:10:47,184 train 1050 1.151920e-02 -6.621931
2019-11-02 10:13:27,238 train 1100 1.152278e-02 -6.376585
2019-11-02 10:16:08,668 train 1150 1.154273e-02 -6.159430
2019-11-02 10:18:50,988 train 1200 1.153659e-02 -6.085327
2019-11-02 10:21:31,496 train 1250 1.155509e-02 -6.037744
2019-11-02 10:24:14,297 train 1300 1.157437e-02 -6.309234
2019-11-02 10:26:56,285 train 1350 1.156886e-02 -6.138241
2019-11-02 10:29:35,252 train 1400 1.156380e-02 -5.981383
2019-11-02 10:32:21,594 train 1450 1.155385e-02 -5.877867
2019-11-02 10:35:03,708 train 1500 1.156382e-02 -5.730169
2019-11-02 10:37:11,180 training loss; accuracy or R2: 1.157321e-02 -5.619806
2019-11-02 10:37:11,672 valid 000 1.371309e-02 -2.699305
2019-11-02 10:37:19,991 valid 050 1.162664e-02 -1.841023
2019-11-02 10:37:28,347 valid 100 1.176084e-02 -2.831111
2019-11-02 10:37:36,667 valid 150 1.178380e-02 -2.576748
2019-11-02 10:37:45,096 valid 200 1.169845e-02 -2.394129
2019-11-02 10:37:53,588 valid 250 1.167831e-02 -2.216227
2019-11-02 10:38:01,993 valid 300 1.165638e-02 -2.142020
2019-11-02 10:38:10,520 valid 350 1.170196e-02 -2.170591
2019-11-02 10:38:19,211 valid 400 1.167648e-02 -2.079722
2019-11-02 10:38:27,497 valid 450 1.173184e-02 -31.099314
2019-11-02 10:38:35,656 valid 500 1.170756e-02 -28.136802
2019-11-02 10:38:43,564 valid 550 1.169475e-02 -25.706194
2019-11-02 10:38:51,465 valid 600 1.173777e-02 -23.725409
2019-11-02 10:38:59,353 valid 650 1.170088e-02 -22.421314
2019-11-02 10:39:07,183 valid 700 1.171117e-02 -20.981070
2019-11-02 10:39:15,035 valid 750 1.171525e-02 -19.735960
2019-11-02 10:39:22,852 valid 800 1.168337e-02 -18.630165
2019-11-02 10:39:30,670 valid 850 1.169337e-02 -17.659074
2019-11-02 10:39:38,515 valid 900 1.169587e-02 -16.774032
2019-11-02 10:39:46,359 valid 950 1.167935e-02 -15.982911
2019-11-02 10:39:54,206 valid 1000 1.169312e-02 -15.499005
2019-11-02 10:40:02,069 valid 1050 1.171307e-02 -14.848671
2019-11-02 10:40:09,905 valid 1100 1.172397e-02 -14.276253
2019-11-02 10:40:17,761 valid 1150 1.171433e-02 -13.745177
2019-11-02 10:40:25,599 valid 1200 1.170959e-02 -13.276810
2019-11-02 10:40:33,446 valid 1250 1.171867e-02 -12.847102
2019-11-02 10:40:41,293 valid 1300 1.171645e-02 -12.422327
2019-11-02 10:40:49,095 valid 1350 1.170315e-02 -12.037999
2019-11-02 10:40:56,938 valid 1400 1.171140e-02 -11.686567
2019-11-02 10:41:04,751 valid 1450 1.172214e-02 -11.335008
2019-11-02 10:41:12,565 valid 1500 1.172474e-02 -11.022467
2019-11-02 10:41:18,638 validation loss; accuracy or R2: 1.173910e-02 -10.787043
2019-11-02 10:41:18,766 epoch 12 lr 8.780359e-03
2019-11-02 10:41:18,767 genotype = Genotype(normal=[('dil_conv_5x5', 0), ('sep_conv_5x5', 1), ('dil_conv_5x5', 2), ('dil_conv_5x5', 0), ('dil_conv_5x5', 0), ('sep_conv_3x3', 3), ('sep_conv_5x5', 3), ('sep_conv_5x5', 4)], normal_concat=range(2, 6), reduce=[('sep_conv_5x5', 1), ('avg_pool_3x3', 0), ('max_pool_3x3', 0), ('max_pool_3x3', 2), ('dil_conv_5x5', 3), ('dil_conv_3x3', 1), ('sep_conv_3x3', 3), ('dil_conv_3x3', 2)], reduce_concat=range(2, 6))
2019-11-02 10:41:18,768 
alphas_normal = Variable containing:
 0.1257  0.1238  0.1231  0.1237  0.1231  0.1272  0.1248  0.1286
 0.1246  0.1250  0.1236  0.1241  0.1245  0.1283  0.1227  0.1271
 0.1251  0.1230  0.1220  0.1224  0.1272  0.1275  0.1246  0.1283
 0.1241  0.1257  0.1252  0.1254  0.1260  0.1230  0.1234  0.1273
 0.1250  0.1242  0.1223  0.1232  0.1262  0.1260  0.1244  0.1285
 0.1246  0.1257  0.1252  0.1251  0.1244  0.1243  0.1245  0.1263
 0.1244  0.1251  0.1258  0.1258  0.1254  0.1257  0.1229  0.1249
 0.1249  0.1249  0.1246  0.1248  0.1250  0.1259  0.1249  0.1250
 0.1248  0.1258  0.1258  0.1258  0.1261  0.1235  0.1233  0.1250
 0.1251  0.1250  0.1243  0.1245  0.1258  0.1269  0.1223  0.1261
 0.1265  0.1247  0.1243  0.1246  0.1258  0.1250  0.1251  0.1241
 0.1247  0.1241  0.1231  0.1240  0.1257  0.1258  0.1263  0.1263
 0.1257  0.1241  0.1235  0.1239  0.1264  0.1280  0.1239  0.1245
 0.1254  0.1235  0.1230  0.1230  0.1258  0.1270  0.1261  0.1262
[torch.cuda.FloatTensor of size 14x8 (GPU 3)]

2019-11-02 10:41:18,770 
alphas_reduce = Variable containing:
 0.1241  0.1268  0.1268  0.1251  0.1260  0.1229  0.1245  0.1239
 0.1260  0.1237  0.1236  0.1236  0.1236  0.1279  0.1257  0.1260
 0.1238  0.1269  0.1264  0.1240  0.1260  0.1252  0.1236  0.1240
 0.1260  0.1246  0.1238  0.1252  0.1262  0.1255  0.1247  0.1240
 0.1240  0.1265  0.1250  0.1246  0.1253  0.1239  0.1248  0.1259
 0.1236  0.1250  0.1251  0.1249  0.1257  0.1255  0.1250  0.1252
 0.1261  0.1241  0.1238  0.1241  0.1259  0.1241  0.1262  0.1257
 0.1252  0.1256  0.1245  0.1246  0.1253  0.1250  0.1251  0.1247
 0.1256  0.1248  0.1242  0.1247  0.1258  0.1236  0.1244  0.1269
 0.1244  0.1259  0.1259  0.1236  0.1267  0.1248  0.1243  0.1244
 0.1263  0.1243  0.1242  0.1243  0.1255  0.1257  0.1252  0.1246
 0.1254  0.1235  0.1238  0.1246  0.1248  0.1263  0.1271  0.1245
 0.1250  0.1240  0.1237  0.1240  0.1272  0.1252  0.1243  0.1265
 0.1260  0.1249  0.1240  0.1242  0.1231  0.1257  0.1263  0.1258
[torch.cuda.FloatTensor of size 14x8 (GPU 3)]

2019-11-02 10:41:22,106 train 000 1.628632e-02 -1.087009
2019-11-02 10:43:57,860 train 050 1.174994e-02 -22.532483
2019-11-02 10:46:33,786 train 100 1.148789e-02 -12.475661
2019-11-02 10:49:09,216 train 150 1.153679e-02 -8.840990
2019-11-02 10:51:52,192 train 200 1.156038e-02 -26.603911
