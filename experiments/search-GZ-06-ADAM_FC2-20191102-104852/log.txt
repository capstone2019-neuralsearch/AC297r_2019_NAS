2019-11-02 10:48:52,888 gpu device = 2
2019-11-02 10:48:52,888 args = Namespace(arch_learning_rate=0.01, arch_weight_decay=1e-06, batch_size=20, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.3, epochs=50, gpu=2, grad_clip=5, gz_regression=False, init_channels=16, layers=8, learning_rate=0.001, learning_rate_min=0.0001, model_path='saved_models', momentum=0.9, optimizer='Adam', report_freq=50, save='search-GZ_ADAM_FC2-20191102-104852', seed=2, train_portion=0.8, unrolled=True, weight_decay=1e-08)
2019-11-02 10:48:56,385 param size = 2.463125MB
2019-11-02 10:48:56,401 epoch 0 lr 1.000000e-03
2019-11-02 10:48:56,402 genotype = Genotype(normal=[('skip_connect', 1), ('sep_conv_3x3', 0), ('dil_conv_3x3', 0), ('avg_pool_3x3', 2), ('sep_conv_3x3', 0), ('dil_conv_5x5', 3), ('sep_conv_5x5', 0), ('sep_conv_3x3', 1)], normal_concat=range(2, 6), reduce=[('sep_conv_3x3', 1), ('avg_pool_3x3', 0), ('sep_conv_5x5', 1), ('dil_conv_5x5', 2), ('skip_connect', 3), ('sep_conv_3x3', 2), ('sep_conv_3x3', 1), ('skip_connect', 0)], reduce_concat=range(2, 6))
2019-11-02 10:48:56,404 
alphas_normal = Variable containing:
 0.1249  0.1250  0.1250  0.1251  0.1251  0.1249  0.1250  0.1249
 0.1249  0.1252  0.1250  0.1252  0.1250  0.1247  0.1249  0.1252
 0.1250  0.1247  0.1251  0.1249  0.1251  0.1250  0.1252  0.1249
 0.1250  0.1251  0.1251  0.1250  0.1250  0.1248  0.1250  0.1249
 0.1251  0.1252  0.1252  0.1249  0.1247  0.1251  0.1248  0.1251
 0.1248  0.1251  0.1250  0.1250  0.1252  0.1251  0.1250  0.1248
 0.1249  0.1249  0.1251  0.1251  0.1250  0.1249  0.1251  0.1250
 0.1253  0.1250  0.1248  0.1249  0.1249  0.1252  0.1250  0.1251
 0.1249  0.1249  0.1251  0.1250  0.1248  0.1251  0.1250  0.1252
 0.1249  0.1249  0.1250  0.1251  0.1252  0.1252  0.1249  0.1248
 0.1251  0.1251  0.1249  0.1250  0.1251  0.1250  0.1248  0.1249
 0.1249  0.1250  0.1251  0.1249  0.1250  0.1250  0.1250  0.1251
 0.1251  0.1248  0.1251  0.1249  0.1251  0.1250  0.1250  0.1251
 0.1251  0.1248  0.1250  0.1251  0.1251  0.1248  0.1250  0.1251
[torch.cuda.FloatTensor of size 14x8 (GPU 2)]

2019-11-02 10:48:56,406 
alphas_reduce = Variable containing:
 0.1248  0.1248  0.1252  0.1250  0.1252  0.1250  0.1251  0.1249
 0.1247  0.1248  0.1251  0.1251  0.1253  0.1250  0.1249  0.1251
 0.1252  0.1250  0.1250  0.1250  0.1251  0.1251  0.1250  0.1248
 0.1252  0.1248  0.1247  0.1250  0.1250  0.1252  0.1251  0.1250
 0.1252  0.1248  0.1250  0.1248  0.1250  0.1250  0.1250  0.1252
 0.1249  0.1251  0.1250  0.1248  0.1251  0.1250  0.1251  0.1249
 0.1250  0.1250  0.1250  0.1250  0.1251  0.1249  0.1251  0.1250
 0.1250  0.1250  0.1250  0.1248  0.1252  0.1249  0.1250  0.1250
 0.1248  0.1249  0.1250  0.1252  0.1249  0.1249  0.1252  0.1252
 0.1250  0.1251  0.1251  0.1252  0.1249  0.1248  0.1250  0.1249
 0.1249  0.1252  0.1250  0.1248  0.1252  0.1249  0.1250  0.1250
 0.1251  0.1251  0.1250  0.1250  0.1250  0.1251  0.1249  0.1250
 0.1250  0.1250  0.1250  0.1251  0.1250  0.1250  0.1250  0.1249
 0.1251  0.1250  0.1250  0.1250  0.1252  0.1249  0.1248  0.1251
[torch.cuda.FloatTensor of size 14x8 (GPU 2)]

2019-11-02 10:49:03,293 train 000 4.027578e-02 -11.720464
2019-11-02 10:51:46,130 train 050 2.617999e-02 -3.289739
2019-11-02 10:54:28,487 train 100 2.462493e-02 -2.032706
2019-11-02 10:57:05,251 train 150 2.349360e-02 -1.761171
2019-11-02 10:59:42,703 train 200 2.268652e-02 -1.599967
2019-11-02 11:02:17,878 train 250 2.245425e-02 -1.447711
2019-11-02 11:04:52,584 train 300 2.199405e-02 -1.514791
2019-11-02 11:07:27,009 train 350 2.158487e-02 -1.453739
2019-11-02 11:10:01,514 train 400 2.135730e-02 -1.752290
2019-11-02 11:12:35,538 train 450 2.113542e-02 -1.669334
2019-11-02 11:15:09,714 train 500 2.073394e-02 -1.595534
2019-11-02 11:17:46,366 train 550 2.051084e-02 -1.864175
2019-11-02 11:20:23,287 train 600 2.035786e-02 -1.821387
2019-11-02 11:23:00,335 train 650 2.018407e-02 -1.759696
2019-11-02 11:25:41,710 train 700 1.989488e-02 -1.748805
2019-11-02 11:28:26,717 train 750 1.969806e-02 -1.768162
2019-11-02 11:31:11,255 train 800 1.949783e-02 -1.925297
2019-11-02 11:33:49,235 train 850 1.931588e-02 -1.931540
2019-11-02 11:36:29,582 train 900 1.909074e-02 -1.898249
2019-11-02 11:39:09,907 train 950 1.892729e-02 -1.858838
2019-11-02 11:41:50,070 train 1000 1.874546e-02 -1.813123
2019-11-02 11:44:31,055 train 1050 1.858263e-02 -1.783077
2019-11-02 11:47:12,115 train 1100 1.838607e-02 -1.821805
2019-11-02 11:49:53,651 train 1150 1.823438e-02 -1.788745
2019-11-02 11:52:32,619 train 1200 1.810433e-02 -1.755819
2019-11-02 11:55:12,396 train 1250 1.796302e-02 -1.731489
2019-11-02 11:57:52,308 train 1300 1.785694e-02 -1.725305
2019-11-02 12:00:32,174 train 1350 1.774962e-02 -1.694991
2019-11-02 12:03:12,268 train 1400 1.763371e-02 -1.680688
2019-11-02 12:05:52,588 train 1450 1.751836e-02 -1.667801
2019-11-02 12:08:31,510 train 1500 1.738958e-02 -1.649591
2019-11-02 12:11:10,808 train 1550 1.727561e-02 -1.636450
2019-11-02 12:13:50,874 train 1600 1.717714e-02 -1.646911
2019-11-02 12:16:31,178 train 1650 1.706707e-02 -1.637054
2019-11-02 12:19:11,450 train 1700 1.697394e-02 -1.804644
2019-11-02 12:21:52,464 train 1750 1.687453e-02 -1.777843
2019-11-02 12:24:33,740 train 1800 1.679822e-02 -1.763644
2019-11-02 12:27:14,142 train 1850 1.671637e-02 -1.750890
2019-11-02 12:29:54,442 train 1900 1.661933e-02 -1.766590
2019-11-02 12:32:34,898 train 1950 1.655555e-02 -1.767532
2019-11-02 12:35:14,937 train 2000 1.648601e-02 -1.754633
2019-11-02 12:37:55,316 train 2050 1.641075e-02 -2.883493
2019-11-02 12:40:35,548 train 2100 1.634026e-02 -2.935005
2019-11-02 12:43:16,084 train 2150 1.627704e-02 -2.897992
2019-11-02 12:45:56,318 train 2200 1.621825e-02 -2.868844
2019-11-02 12:48:37,012 train 2250 1.617120e-02 -2.834907
2019-11-02 12:51:15,468 train 2300 1.610783e-02 -2.799482
2019-11-02 12:53:55,518 train 2350 1.604984e-02 -2.762995
2019-11-02 12:56:35,878 train 2400 1.599485e-02 -2.730104
2019-11-02 12:59:16,533 train 2450 1.593802e-02 -2.697139
2019-11-02 13:00:00,674 training loss; R2: 1.593060e-02 -2.689078
2019-11-02 13:00:01,149 valid 000 1.778373e-02 -1.071651
2019-11-02 13:00:09,416 valid 050 1.409879e-02 -1.517612
2019-11-02 13:00:17,638 valid 100 1.430128e-02 -8.554780
2019-11-02 13:00:25,863 valid 150 1.408910e-02 -6.205087
2019-11-02 13:00:34,043 valid 200 1.414681e-02 -5.016362
2019-11-02 13:00:42,219 valid 250 1.415401e-02 -4.199800
2019-11-02 13:00:50,403 valid 300 1.418953e-02 -3.596050
2019-11-02 13:00:58,580 valid 350 1.415965e-02 -3.315957
2019-11-02 13:01:06,760 valid 400 1.415115e-02 -4.871446
2019-11-02 13:01:14,979 valid 450 1.411300e-02 -4.453756
2019-11-02 13:01:23,175 valid 500 1.406827e-02 -4.607181
2019-11-02 13:01:31,381 valid 550 1.405787e-02 -4.270191
2019-11-02 13:01:39,584 valid 600 1.401468e-02 -4.140078
2019-11-02 13:01:42,995 validation loss; R2: 1.402678e-02 -4.058640
2019-11-02 13:01:43,119 epoch 1 lr 9.991120e-04
2019-11-02 13:01:43,120 genotype = Genotype(normal=[('skip_connect', 0), ('dil_conv_3x3', 1), ('dil_conv_5x5', 0), ('dil_conv_3x3', 2), ('max_pool_3x3', 0), ('dil_conv_3x3', 1), ('max_pool_3x3', 0), ('dil_conv_5x5', 3)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 0), ('skip_connect', 1), ('dil_conv_5x5', 2), ('max_pool_3x3', 0), ('max_pool_3x3', 0), ('dil_conv_5x5', 2), ('max_pool_3x3', 0), ('max_pool_3x3', 2)], reduce_concat=range(2, 6))
2019-11-02 13:01:43,121 
alphas_normal = Variable containing:
 0.2138  0.1315  0.0404  0.2797  0.0417  0.1115  0.0704  0.1109
 0.4910  0.0397  0.0243  0.0413  0.0445  0.0224  0.1974  0.1393
 0.2966  0.1163  0.0429  0.1479  0.0936  0.0513  0.0832  0.1683
 0.6184  0.0700  0.0310  0.0669  0.0332  0.0259  0.0546  0.1000
 0.4681  0.0477  0.0271  0.0698  0.0919  0.0933  0.1300  0.0720
 0.1847  0.2797  0.0396  0.1133  0.0619  0.1522  0.0653  0.1032
 0.6058  0.0524  0.0339  0.0673  0.0701  0.0278  0.1226  0.0201
 0.6339  0.0680  0.0272  0.0580  0.0593  0.0564  0.0481  0.0493
 0.6509  0.0547  0.0252  0.0598  0.0684  0.0379  0.0482  0.0549
 0.2880  0.2334  0.0441  0.1150  0.0437  0.0505  0.1040  0.1213
 0.7194  0.0504  0.0339  0.0478  0.0425  0.0304  0.0402  0.0353
 0.5071  0.0624  0.0290  0.0575  0.0640  0.0668  0.1199  0.0933
 0.5433  0.0310  0.0258  0.0442  0.0486  0.0619  0.1090  0.1361
 0.5330  0.0283  0.0220  0.0402  0.0783  0.0701  0.1259  0.1021
[torch.cuda.FloatTensor of size 14x8 (GPU 2)]

2019-11-02 13:01:43,123 
alphas_reduce = Variable containing:
 0.0935  0.4036  0.0523  0.1380  0.0600  0.0967  0.0788  0.0772
 0.1145  0.2068  0.0488  0.3364  0.0704  0.0542  0.0648  0.1042
 0.1036  0.3301  0.0714  0.1288  0.0950  0.0614  0.1107  0.0989
 0.2242  0.1816  0.0789  0.1800  0.0841  0.0866  0.0655  0.0991
 0.0446  0.1149  0.0315  0.0522  0.0773  0.0911  0.1044  0.4839
 0.1322  0.5141  0.0735  0.1014  0.0520  0.0463  0.0381  0.0425
 0.1691  0.2814  0.1384  0.0887  0.0677  0.1034  0.0572  0.0942
 0.0599  0.1536  0.0417  0.0723  0.0834  0.0680  0.1334  0.3878
 0.0672  0.1146  0.0454  0.0940  0.0738  0.2031  0.1049  0.2969
 0.1455  0.3228  0.1086  0.1317  0.0500  0.0542  0.0871  0.1001
 0.3836  0.1240  0.0801  0.1220  0.0576  0.0589  0.0788  0.0949
 0.0989  0.3162  0.0493  0.0753  0.0801  0.0922  0.1286  0.1594
 0.1210  0.2527  0.0548  0.0871  0.0775  0.0940  0.2159  0.0970
 0.0982  0.1008  0.0442  0.0592  0.1923  0.1268  0.1096  0.2688
[torch.cuda.FloatTensor of size 14x8 (GPU 2)]

2019-11-02 13:01:47,826 train 000 8.182938e-03 -3.403541
2019-11-02 13:04:28,969 train 050 1.247970e-02 -1.102833
2019-11-02 13:07:09,751 train 100 1.268251e-02 -22.758511
2019-11-02 13:09:49,898 train 150 1.276175e-02 -15.826595
2019-11-02 13:12:29,730 train 200 1.297274e-02 -12.265271
2019-11-02 13:15:08,832 train 250 1.306080e-02 -10.160869
2019-11-02 13:17:48,881 train 300 1.307139e-02 -9.230917
2019-11-02 13:20:29,711 train 350 1.302926e-02 -8.536989
2019-11-02 13:23:10,104 train 400 1.302188e-02 -7.608267
2019-11-02 13:25:50,339 train 450 1.306155e-02 -6.980894
2019-11-02 13:28:30,624 train 500 1.301238e-02 -6.467322
2019-11-02 13:31:10,862 train 550 1.299816e-02 -6.024598
2019-11-02 13:33:50,891 train 600 1.298311e-02 -5.609055
2019-11-02 13:36:30,112 train 650 1.293101e-02 -5.271156
2019-11-02 13:39:09,992 train 700 1.295496e-02 -4.996757
2019-11-02 13:41:49,509 train 750 1.292406e-02 -4.746311
2019-11-02 13:44:28,014 train 800 1.288447e-02 -6.141137
2019-11-02 13:47:07,522 train 850 1.289977e-02 -5.954840
2019-11-02 13:49:46,651 train 900 1.289457e-02 -5.694252
2019-11-02 13:52:26,660 train 950 1.289496e-02 -5.452354
2019-11-02 13:55:06,013 train 1000 1.286176e-02 -5.233834
2019-11-02 13:57:45,480 train 1050 1.282224e-02 -5.030035
2019-11-02 14:00:24,450 train 1100 1.278594e-02 -4.847910
2019-11-02 14:03:03,459 train 1150 1.276924e-02 -4.840160
2019-11-02 14:05:43,361 train 1200 1.273807e-02 -4.678476
2019-11-02 14:08:23,214 train 1250 1.270719e-02 -4.556331
2019-11-02 14:11:03,314 train 1300 1.270433e-02 -4.424416
2019-11-02 14:13:42,758 train 1350 1.270276e-02 -4.383102
2019-11-02 14:16:22,456 train 1400 1.270408e-02 -4.275429
2019-11-02 14:19:02,160 train 1450 1.268075e-02 -4.171653
2019-11-02 14:21:41,140 train 1500 1.268268e-02 -4.062757
2019-11-02 14:24:18,804 train 1550 1.267178e-02 -3.966774
2019-11-02 14:26:56,524 train 1600 1.266075e-02 -4.107022
2019-11-02 14:29:35,123 train 1650 1.264603e-02 -4.037604
2019-11-02 14:32:14,132 train 1700 1.262397e-02 -3.950597
2019-11-02 14:34:54,149 train 1750 1.260922e-02 -3.866382
2019-11-02 14:37:33,563 train 1800 1.259803e-02 -3.789742
2019-11-02 14:40:11,716 train 1850 1.259582e-02 -3.724096
2019-11-02 14:42:51,142 train 1900 1.257986e-02 -3.665032
2019-11-02 14:45:30,860 train 1950 1.255809e-02 -3.675617
2019-11-02 14:48:10,285 train 2000 1.254583e-02 -3.611192
2019-11-02 14:50:49,560 train 2050 1.253944e-02 -3.559080
2019-11-02 14:53:28,699 train 2100 1.251692e-02 -3.501983
2019-11-02 14:56:08,593 train 2150 1.249314e-02 -3.448787
2019-11-02 14:58:48,594 train 2200 1.247714e-02 -3.392549
2019-11-02 15:01:28,312 train 2250 1.246724e-02 -3.342646
2019-11-02 15:04:08,556 train 2300 1.244521e-02 -3.907641
2019-11-02 15:06:48,224 train 2350 1.243388e-02 -3.862295
2019-11-02 15:09:27,789 train 2400 1.242813e-02 -3.810029
2019-11-02 15:12:07,143 train 2450 1.242003e-02 -3.760914
2019-11-02 15:12:48,664 training loss; R2: 1.241947e-02 -3.831760
2019-11-02 15:12:49,137 valid 000 1.569589e-02 -0.611654
2019-11-02 15:12:57,342 valid 050 1.612431e-02 -2.337351
2019-11-02 15:13:05,556 valid 100 1.594183e-02 -2.342272
2019-11-02 15:13:13,745 valid 150 1.563545e-02 -2.543792
2019-11-02 15:13:21,932 valid 200 1.551877e-02 -2.934768
2019-11-02 15:13:30,105 valid 250 1.554673e-02 -2.844244
2019-11-02 15:13:38,272 valid 300 1.553896e-02 -2.886775
2019-11-02 15:13:46,464 valid 350 1.541859e-02 -2.829452
2019-11-02 15:13:54,629 valid 400 1.551370e-02 -3.060579
2019-11-02 15:14:02,788 valid 450 1.550574e-02 -3.032505
2019-11-02 15:14:11,003 valid 500 1.550528e-02 -2.967160
2019-11-02 15:14:19,163 valid 550 1.553226e-02 -3.020347
2019-11-02 15:14:27,333 valid 600 1.551671e-02 -2.993859
2019-11-02 15:14:29,763 validation loss; R2: 1.553217e-02 -2.981658
2019-11-02 15:14:29,894 epoch 2 lr 9.964516e-04
2019-11-02 15:14:29,895 genotype = Genotype(normal=[('skip_connect', 0), ('dil_conv_3x3', 1), ('skip_connect', 0), ('dil_conv_5x5', 1), ('max_pool_3x3', 0), ('skip_connect', 1), ('max_pool_3x3', 0), ('dil_conv_5x5', 2)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 0), ('max_pool_3x3', 1), ('dil_conv_5x5', 2), ('max_pool_3x3', 0), ('dil_conv_5x5', 2), ('max_pool_3x3', 0), ('dil_conv_5x5', 4), ('max_pool_3x3', 3)], reduce_concat=range(2, 6))
2019-11-02 15:14:29,896 
alphas_normal = Variable containing:
 0.2334  0.1378  0.0409  0.3391  0.0272  0.0824  0.0475  0.0917
 0.4912  0.0503  0.0457  0.0732  0.0437  0.0193  0.1574  0.1193
 0.4009  0.1072  0.0377  0.1503  0.0714  0.0465  0.0425  0.1436
 0.4817  0.1374  0.0422  0.0870  0.0230  0.0422  0.0408  0.1457
 0.5958  0.0375  0.0235  0.0551  0.0679  0.0673  0.0874  0.0656
 0.1312  0.4621  0.0371  0.0904  0.0621  0.0671  0.0560  0.0940
 0.5124  0.0914  0.0630  0.1117  0.0796  0.0322  0.0672  0.0426
 0.7067  0.0644  0.0320  0.0549  0.0365  0.0334  0.0338  0.0384
 0.6359  0.0660  0.0348  0.0712  0.0390  0.0405  0.0654  0.0470
 0.2105  0.3932  0.0319  0.0708  0.0435  0.0386  0.0671  0.1445
 0.7136  0.0522  0.0315  0.0365  0.0344  0.0329  0.0613  0.0377
 0.5615  0.0506  0.0204  0.0325  0.0377  0.0426  0.0915  0.1632
 0.6147  0.0446  0.0330  0.0449  0.0460  0.0371  0.1008  0.0789
 0.6086  0.0297  0.0220  0.0323  0.0453  0.0354  0.1327  0.0940
[torch.cuda.FloatTensor of size 14x8 (GPU 2)]

2019-11-02 15:14:29,898 
alphas_reduce = Variable containing:
 0.1416  0.4399  0.0398  0.1497  0.0362  0.0862  0.0610  0.0456
 0.0890  0.2888  0.0394  0.2706  0.0817  0.0508  0.0460  0.1337
 0.2417  0.2227  0.0472  0.1368  0.0963  0.0505  0.1470  0.0579
 0.3052  0.1394  0.0688  0.1135  0.0328  0.0828  0.0601  0.1975
 0.0284  0.0673  0.0232  0.0268  0.0518  0.0798  0.0536  0.6693
 0.2054  0.4295  0.0685  0.1034  0.0467  0.0597  0.0323  0.0545
 0.0962  0.3166  0.1583  0.1144  0.0344  0.0844  0.0574  0.1385
 0.0549  0.0774  0.0424  0.0441  0.0603  0.0375  0.0678  0.6156
 0.0500  0.1108  0.0692  0.0795  0.0879  0.3159  0.0598  0.2268
 0.3926  0.1323  0.0489  0.1388  0.0253  0.0467  0.0640  0.1514
 0.4108  0.0818  0.0635  0.1683  0.0672  0.0799  0.0422  0.0865
 0.0944  0.2737  0.0537  0.0581  0.0656  0.0528  0.1016  0.3001
 0.0944  0.3115  0.1020  0.0858  0.0493  0.0861  0.2204  0.0504
 0.0875  0.0884  0.0528  0.0525  0.1416  0.0794  0.1430  0.3549
[torch.cuda.FloatTensor of size 14x8 (GPU 2)]

2019-11-02 15:14:33,425 train 000 1.579047e-02 -0.345225
2019-11-02 15:17:13,552 train 050 1.223756e-02 -1.183505
2019-11-02 15:19:53,364 train 100 1.221138e-02 -3.620663
2019-11-02 15:22:33,511 train 150 1.222947e-02 -3.153668
2019-11-02 15:25:13,740 train 200 1.215724e-02 -2.756611
2019-11-02 15:27:53,562 train 250 1.206690e-02 -2.399557
2019-11-02 15:30:33,767 train 300 1.202997e-02 -2.206093
2019-11-02 15:33:13,745 train 350 1.199242e-02 -2.159524
2019-11-02 15:35:54,363 train 400 1.197730e-02 -2.063578
2019-11-02 15:38:34,241 train 450 1.192260e-02 -2.060922
2019-11-02 15:41:13,884 train 500 1.188664e-02 -1.979815
2019-11-02 15:43:53,419 train 550 1.180338e-02 -1.902666
2019-11-02 15:46:33,803 train 600 1.174981e-02 -1.825511
2019-11-02 15:49:13,811 train 650 1.179087e-02 -1.889037
2019-11-02 15:51:53,986 train 700 1.178004e-02 -1.979722
2019-11-02 15:54:33,807 train 750 1.177366e-02 -1.924172
2019-11-02 15:57:13,711 train 800 1.176045e-02 -1.914669
2019-11-02 15:59:53,471 train 850 1.174630e-02 -2.015589
2019-11-02 16:02:31,356 train 900 1.174504e-02 -1.979266
2019-11-02 16:05:09,338 train 950 1.177658e-02 -1.930226
2019-11-02 16:07:48,164 train 1000 1.176306e-02 -1.910015
2019-11-02 16:10:28,059 train 1050 1.174043e-02 -1.874954
2019-11-02 16:13:07,831 train 1100 1.173994e-02 -1.834555
2019-11-02 16:15:47,325 train 1150 1.169807e-02 -1.862133
2019-11-02 16:18:27,072 train 1200 1.169679e-02 -1.831113
2019-11-02 16:21:06,441 train 1250 1.170409e-02 -10.756998
2019-11-02 16:23:45,391 train 1300 1.169173e-02 -10.430030
2019-11-02 16:26:25,107 train 1350 1.168944e-02 -10.562004
2019-11-02 16:29:05,110 train 1400 1.168838e-02 -10.240040
2019-11-02 16:31:45,183 train 1450 1.168087e-02 -9.920063
2019-11-02 16:34:25,360 train 1500 1.165895e-02 -9.631024
2019-11-02 16:37:03,095 train 1550 1.164615e-02 -9.356031
2019-11-02 16:39:40,759 train 1600 1.162742e-02 -9.084508
2019-11-02 16:42:20,699 train 1650 1.162609e-02 -10.016969
2019-11-02 16:45:00,735 train 1700 1.163703e-02 -9.757192
2019-11-02 16:47:41,484 train 1750 1.164223e-02 -9.515373
2019-11-02 16:50:21,728 train 1800 1.164610e-02 -9.274902
2019-11-02 16:53:01,201 train 1850 1.164335e-02 -9.063713
2019-11-02 16:55:41,410 train 1900 1.161550e-02 -8.852983
2019-11-02 16:58:22,271 train 1950 1.161430e-02 -8.654503
2019-11-02 17:01:02,363 train 2000 1.160842e-02 -8.458015
2019-11-02 17:03:42,194 train 2050 1.160409e-02 -8.276084
2019-11-02 17:06:22,374 train 2100 1.159631e-02 -8.116470
2019-11-02 17:09:02,458 train 2150 1.158086e-02 -9.226286
2019-11-02 17:11:43,621 train 2200 1.156830e-02 -9.040145
2019-11-02 17:14:22,768 train 2250 1.155544e-02 -8.859745
2019-11-02 17:17:00,926 train 2300 1.156355e-02 -8.771870
2019-11-02 17:19:40,704 train 2350 1.157192e-02 -8.617931
2019-11-02 17:22:19,695 train 2400 1.156127e-02 -8.508398
2019-11-02 17:24:57,909 train 2450 1.156031e-02 -8.356608
2019-11-02 17:25:39,092 training loss; R2: 1.155656e-02 -8.321610
2019-11-02 17:25:39,585 valid 000 1.396316e-02 -30.029166
2019-11-02 17:25:47,774 valid 050 1.266034e-02 -1.215334
2019-11-02 17:25:55,941 valid 100 1.259402e-02 -0.844055
2019-11-02 17:26:04,056 valid 150 1.248285e-02 -0.727387
2019-11-02 17:26:12,204 valid 200 1.257345e-02 -0.739010
2019-11-02 17:26:20,387 valid 250 1.269566e-02 -0.776979
2019-11-02 17:26:28,522 valid 300 1.281402e-02 -1.049481
2019-11-02 17:26:36,677 valid 350 1.277643e-02 -0.955125
2019-11-02 17:26:44,827 valid 400 1.281840e-02 -0.889960
2019-11-02 17:26:52,974 valid 450 1.280899e-02 -5.337661
2019-11-02 17:27:01,114 valid 500 1.278535e-02 -5.071225
2019-11-02 17:27:09,270 valid 550 1.277437e-02 -4.684547
2019-11-02 17:27:17,407 valid 600 1.279562e-02 -4.340807
2019-11-02 17:27:19,827 validation loss; R2: 1.280786e-02 -4.248325
2019-11-02 17:27:20,008 epoch 3 lr 9.920293e-04
2019-11-02 17:27:20,009 genotype = Genotype(normal=[('skip_connect', 0), ('dil_conv_3x3', 1), ('dil_conv_5x5', 0), ('dil_conv_5x5', 1), ('max_pool_3x3', 0), ('dil_conv_3x3', 3), ('max_pool_3x3', 0), ('dil_conv_3x3', 4)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 0), ('max_pool_3x3', 1), ('dil_conv_5x5', 2), ('max_pool_3x3', 0), ('dil_conv_5x5', 2), ('max_pool_3x3', 0), ('dil_conv_5x5', 4), ('dil_conv_5x5', 2)], reduce_concat=range(2, 6))
2019-11-02 17:27:20,011 
alphas_normal = Variable containing:
 0.2486  0.1605  0.0363  0.3118  0.0293  0.0512  0.0543  0.1081
 0.5189  0.0281  0.0390  0.0588  0.0423  0.0149  0.1893  0.1087
 0.3784  0.1234  0.0353  0.1455  0.1030  0.0341  0.0254  0.1548
 0.6733  0.0697  0.0262  0.0499  0.0121  0.0254  0.0179  0.1254
 0.6572  0.0290  0.0149  0.0328  0.0634  0.0468  0.0774  0.0786
 0.1217  0.4410  0.0284  0.0576  0.1268  0.0619  0.0514  0.1113
 0.5644  0.0702  0.0546  0.0839  0.0652  0.0356  0.0602  0.0659
 0.7155  0.0543  0.0271  0.0398  0.0518  0.0421  0.0269  0.0426
 0.5861  0.0538  0.0306  0.0520  0.0304  0.0785  0.0973  0.0713
 0.1457  0.4309  0.0355  0.0596  0.0467  0.0637  0.0717  0.1463
 0.8572  0.0193  0.0166  0.0176  0.0192  0.0216  0.0278  0.0207
 0.6346  0.0434  0.0213  0.0336  0.0315  0.0232  0.0553  0.1570
 0.6802  0.0322  0.0233  0.0361  0.0304  0.0307  0.0935  0.0735
 0.5973  0.0218  0.0162  0.0215  0.0563  0.0323  0.1817  0.0728
[torch.cuda.FloatTensor of size 14x8 (GPU 2)]

2019-11-02 17:27:20,013 
alphas_reduce = Variable containing:
 0.1758  0.4083  0.0271  0.1646  0.0409  0.1037  0.0382  0.0414
 0.0608  0.3166  0.0246  0.1990  0.1500  0.0660  0.0651  0.1178
 0.1777  0.2271  0.0561  0.2106  0.1132  0.0707  0.1021  0.0426
 0.3899  0.1073  0.0548  0.1402  0.0265  0.0881  0.0537  0.1396
 0.0428  0.0997  0.0436  0.0414  0.0361  0.0565  0.0490  0.6308
 0.2321  0.3512  0.0761  0.1318  0.0466  0.0825  0.0320  0.0477
 0.2051  0.2412  0.1377  0.1361  0.0326  0.1146  0.0420  0.0908
 0.0376  0.0419  0.0307  0.0336  0.0429  0.0250  0.0480  0.7404
 0.0949  0.0789  0.0690  0.0723  0.0636  0.3370  0.0503  0.2339
 0.3093  0.0858  0.0581  0.1496  0.0350  0.1114  0.0651  0.1858
 0.2892  0.0705  0.0481  0.3128  0.0645  0.0865  0.0500  0.0783
 0.0653  0.2647  0.0526  0.0556  0.0510  0.0522  0.1204  0.3382
 0.0913  0.3345  0.0908  0.0806  0.0487  0.1223  0.1674  0.0644
 0.0746  0.0708  0.0592  0.0516  0.1700  0.0879  0.1224  0.3635
[torch.cuda.FloatTensor of size 14x8 (GPU 2)]

2019-11-02 17:27:23,433 train 000 1.360390e-02 -0.149966
2019-11-02 17:30:03,510 train 050 1.157162e-02 -1.455961
2019-11-02 17:32:43,267 train 100 1.130853e-02 -1.386763
2019-11-02 17:35:23,206 train 150 1.103252e-02 -1.185035
2019-11-02 17:38:02,623 train 200 1.104078e-02 -1.142299
2019-11-02 17:40:43,632 train 250 1.119331e-02 -1.144144
2019-11-02 17:43:23,499 train 300 1.122563e-02 -1.153161
2019-11-02 17:46:04,640 train 350 1.121647e-02 -1.130975
2019-11-02 17:48:44,065 train 400 1.122821e-02 -1.152961
2019-11-02 17:51:24,778 train 450 1.118874e-02 -1.230644
2019-11-02 17:54:03,067 train 500 1.116884e-02 -1.575030
2019-11-02 17:56:41,194 train 550 1.117019e-02 -1.582054
2019-11-02 17:59:19,840 train 600 1.114719e-02 -1.518575
2019-11-02 18:01:58,314 train 650 1.113628e-02 -1.487353
2019-11-02 18:04:36,274 train 700 1.112270e-02 -1.491626
2019-11-02 18:07:14,053 train 750 1.112059e-02 -1.491700
2019-11-02 18:09:51,755 train 800 1.112846e-02 -1.462561
2019-11-02 18:12:31,412 train 850 1.111658e-02 -1.439646
2019-11-02 18:15:11,520 train 900 1.110892e-02 -1.450637
2019-11-02 18:17:52,473 train 950 1.110703e-02 -1.430169
2019-11-02 18:20:32,375 train 1000 1.111727e-02 -1.427144
2019-11-02 18:23:13,830 train 1050 1.109611e-02 -1.407834
2019-11-02 18:25:54,149 train 1100 1.107347e-02 -1.404150
2019-11-02 18:28:35,971 train 1150 1.106829e-02 -1.392998
2019-11-02 18:31:17,862 train 1200 1.103704e-02 -1.376698
2019-11-02 18:33:59,611 train 1250 1.103556e-02 -1.360594
2019-11-02 18:36:40,635 train 1300 1.102725e-02 -1.403805
2019-11-02 18:39:21,414 train 1350 1.100798e-02 -1.419090
2019-11-02 18:42:02,141 train 1400 1.100348e-02 -1.413525
2019-11-02 18:44:42,843 train 1450 1.100423e-02 -1.431698
2019-11-02 18:47:24,072 train 1500 1.100376e-02 -1.424199
2019-11-02 18:50:04,949 train 1550 1.100810e-02 -1.404646
2019-11-02 18:52:45,708 train 1600 1.101380e-02 -1.395649
2019-11-02 18:55:26,659 train 1650 1.102139e-02 -1.393202
2019-11-02 18:58:07,365 train 1700 1.102450e-02 -1.388222
2019-11-02 19:00:48,327 train 1750 1.103207e-02 -1.395183
2019-11-02 19:03:29,021 train 1800 1.101837e-02 -1.392108
2019-11-02 19:06:09,994 train 1850 1.101635e-02 -1.382206
2019-11-02 19:08:52,097 train 1900 1.101875e-02 -1.561523
2019-11-02 19:11:35,450 train 1950 1.101165e-02 -1.625696
2019-11-02 19:14:18,929 train 2000 1.101552e-02 -1.616818
2019-11-02 19:17:03,052 train 2050 1.101829e-02 -1.604609
2019-11-02 19:19:47,039 train 2100 1.102168e-02 -1.601182
2019-11-02 19:22:31,932 train 2150 1.102417e-02 -2.462537
2019-11-02 19:25:14,961 train 2200 1.102274e-02 -2.431849
2019-11-02 19:27:59,563 train 2250 1.101775e-02 -2.432891
2019-11-02 19:30:44,078 train 2300 1.099732e-02 -2.402372
2019-11-02 19:33:28,135 train 2350 1.100170e-02 -2.370691
2019-11-02 19:36:11,608 train 2400 1.099959e-02 -2.363263
2019-11-02 19:38:56,584 train 2450 1.099252e-02 -2.342308
2019-11-02 19:39:39,300 training loss; R2: 1.098847e-02 -2.338101
2019-11-02 19:39:39,849 valid 000 1.304170e-02 -1.104838
2019-11-02 19:39:48,565 valid 050 1.241075e-02 -4.328025
2019-11-02 19:39:57,137 valid 100 1.263918e-02 -4.500557
2019-11-02 19:40:05,927 valid 150 1.250365e-02 -4.005780
2019-11-02 19:40:14,542 valid 200 1.245971e-02 -3.649172
2019-11-02 19:40:23,014 valid 250 1.239630e-02 -3.551408
2019-11-02 19:40:31,447 valid 300 1.235959e-02 -3.591932
2019-11-02 19:40:40,073 valid 350 1.233371e-02 -27.728051
2019-11-02 19:40:48,506 valid 400 1.239966e-02 -24.634603
2019-11-02 19:40:57,010 valid 450 1.233688e-02 -22.214904
2019-11-02 19:41:05,475 valid 500 1.239705e-02 -20.248579
2019-11-02 19:41:14,044 valid 550 1.234262e-02 -18.645567
2019-11-02 19:41:22,570 valid 600 1.236267e-02 -17.274834
2019-11-02 19:41:25,121 validation loss; R2: 1.237708e-02 -16.897066
2019-11-02 19:41:25,308 epoch 4 lr 9.858624e-04
2019-11-02 19:41:25,309 genotype = Genotype(normal=[('skip_connect', 0), ('dil_conv_3x3', 1), ('max_pool_3x3', 0), ('dil_conv_5x5', 1), ('max_pool_3x3', 0), ('dil_conv_3x3', 3), ('max_pool_3x3', 0), ('dil_conv_5x5', 2)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 0), ('max_pool_3x3', 1), ('dil_conv_5x5', 2), ('max_pool_3x3', 0), ('dil_conv_5x5', 2), ('sep_conv_5x5', 3), ('dil_conv_5x5', 4), ('dil_conv_5x5', 2)], reduce_concat=range(2, 6))
2019-11-02 19:41:25,311 
alphas_normal = Variable containing:
 0.2700  0.1528  0.0309  0.3340  0.0228  0.0608  0.0388  0.0900
 0.5882  0.0243  0.0329  0.0492  0.0537  0.0106  0.1563  0.0849
 0.3613  0.1503  0.0329  0.1432  0.1150  0.0435  0.0246  0.1292
 0.6901  0.0760  0.0265  0.0451  0.0103  0.0288  0.0161  0.1071
 0.6715  0.0287  0.0161  0.0340  0.0550  0.0340  0.0879  0.0727
 0.0793  0.5643  0.0291  0.0530  0.1015  0.0401  0.0426  0.0900
 0.5763  0.0724  0.0688  0.1062  0.0673  0.0322  0.0420  0.0347
 0.6469  0.0601  0.0385  0.0635  0.0690  0.0413  0.0335  0.0471
 0.4541  0.0653  0.0467  0.1071  0.0386  0.0828  0.1183  0.0870
 0.1052  0.4823  0.0334  0.0552  0.0611  0.0451  0.0980  0.1196
 0.8425  0.0246  0.0304  0.0292  0.0169  0.0152  0.0260  0.0151
 0.5343  0.0435  0.0318  0.0530  0.0359  0.0186  0.0514  0.2315
 0.6663  0.0277  0.0288  0.0654  0.0201  0.0232  0.0622  0.1063
 0.5731  0.0333  0.0302  0.0396  0.0421  0.0364  0.1666  0.0787
[torch.cuda.FloatTensor of size 14x8 (GPU 2)]

2019-11-02 19:41:25,312 
alphas_reduce = Variable containing:
 0.2050  0.4173  0.0240  0.1435  0.0479  0.0634  0.0478  0.0513
 0.0542  0.3234  0.0271  0.1974  0.1868  0.0621  0.0560  0.0930
 0.2638  0.1911  0.0575  0.1896  0.0883  0.0543  0.1001  0.0553
 0.4518  0.0870  0.0579  0.1136  0.0290  0.0868  0.0627  0.1112
 0.0535  0.0801  0.0415  0.0594  0.0739  0.0339  0.0573  0.6004
 0.3514  0.2851  0.0481  0.0794  0.0397  0.1222  0.0221  0.0521
 0.1812  0.2867  0.0663  0.1241  0.0351  0.1151  0.0534  0.1382
 0.0384  0.0395  0.0293  0.0366  0.0379  0.0180  0.0726  0.7276
 0.0642  0.0886  0.0753  0.0948  0.0625  0.3588  0.0498  0.2060
 0.4484  0.0528  0.0349  0.0530  0.0513  0.1111  0.0502  0.1982
 0.4290  0.0622  0.0551  0.1950  0.0566  0.0926  0.0458  0.0637
 0.0704  0.2187  0.0388  0.0450  0.0769  0.0702  0.0818  0.3983
 0.1227  0.2675  0.0546  0.0649  0.0415  0.1525  0.2147  0.0817
 0.0505  0.0348  0.0301  0.0306  0.2178  0.0992  0.1067  0.4303
[torch.cuda.FloatTensor of size 14x8 (GPU 2)]

2019-11-02 19:41:28,831 train 000 9.857275e-03 -2.646083
2019-11-02 19:44:13,286 train 050 1.095766e-02 -5.405783
2019-11-02 19:46:57,343 train 100 1.094919e-02 -3.526868
2019-11-02 19:49:41,268 train 150 1.090055e-02 -2.979907
2019-11-02 19:52:25,439 train 200 1.108346e-02 -2.588365
2019-11-02 19:55:07,933 train 250 1.089966e-02 -2.287305
2019-11-02 19:57:52,326 train 300 1.084048e-02 -2.071402
2019-11-02 20:00:36,842 train 350 1.078008e-02 -1.925786
2019-11-02 20:03:18,736 train 400 1.074436e-02 -1.897662
2019-11-02 20:05:59,692 train 450 1.072353e-02 -1.846553
2019-11-02 20:08:40,905 train 500 1.074892e-02 -1.847845
2019-11-02 20:11:21,479 train 550 1.072145e-02 -1.801294
2019-11-02 20:14:03,225 train 600 1.070974e-02 -1.743067
2019-11-02 20:16:45,598 train 650 1.070106e-02 -1.801731
2019-11-02 20:19:31,247 train 700 1.073080e-02 -1.760382
2019-11-02 20:22:17,979 train 750 1.073780e-02 -1.789914
2019-11-02 20:25:03,602 train 800 1.074984e-02 -1.800043
2019-11-02 20:27:49,565 train 850 1.071500e-02 -1.759616
2019-11-02 20:30:35,557 train 900 1.071463e-02 -1.730094
2019-11-02 20:33:20,702 train 950 1.070419e-02 -1.888376
2019-11-02 20:36:06,046 train 1000 1.070682e-02 -1.842024
2019-11-02 20:38:52,360 train 1050 1.070077e-02 -2.250546
2019-11-02 20:41:38,477 train 1100 1.070430e-02 -2.346612
2019-11-02 20:44:23,738 train 1150 1.071883e-02 -2.345312
2019-11-02 20:47:07,861 train 1200 1.068611e-02 -2.291438
2019-11-02 20:49:54,213 train 1250 1.067480e-02 -2.243723
2019-11-02 20:52:39,562 train 1300 1.066251e-02 -2.195615
2019-11-02 20:55:24,451 train 1350 1.067369e-02 -2.152262
2019-11-02 20:58:09,006 train 1400 1.068045e-02 -2.112385
2019-11-02 21:00:53,645 train 1450 1.067676e-02 -2.080945
2019-11-02 21:03:39,282 train 1500 1.069207e-02 -2.060957
2019-11-02 21:06:23,482 train 1550 1.067826e-02 -2.019461
2019-11-02 21:09:09,536 train 1600 1.067442e-02 -2.052971
2019-11-02 21:11:55,384 train 1650 1.067621e-02 -2.029642
2019-11-02 21:14:40,848 train 1700 1.067220e-02 -2.044422
2019-11-02 21:17:26,303 train 1750 1.066831e-02 -2.016470
2019-11-02 21:20:11,167 train 1800 1.065800e-02 -1.990514
2019-11-02 21:22:56,295 train 1850 1.064791e-02 -1.999722
2019-11-02 21:25:41,288 train 1900 1.065264e-02 -3.717660
2019-11-02 21:28:26,351 train 1950 1.063960e-02 -3.645622
2019-11-02 21:31:12,324 train 2000 1.063787e-02 -3.580674
2019-11-02 21:33:58,831 train 2050 1.063771e-02 -3.523355
2019-11-02 21:36:44,050 train 2100 1.064257e-02 -3.483130
2019-11-02 21:39:29,021 train 2150 1.063792e-02 -3.596644
2019-11-02 21:42:14,549 train 2200 1.063774e-02 -3.541144
2019-11-02 21:45:00,320 train 2250 1.063949e-02 -3.491402
2019-11-02 21:47:45,860 train 2300 1.064245e-02 -3.443569
2019-11-02 21:50:31,691 train 2350 1.063459e-02 -3.391839
2019-11-02 21:53:17,690 train 2400 1.062488e-02 -3.347009
2019-11-02 21:56:02,827 train 2450 1.061716e-02 -3.306257
2019-11-02 21:56:46,097 training loss; R2: 1.061583e-02 -3.293612
2019-11-02 21:56:46,550 valid 000 1.020513e-02 -2.995578
2019-11-02 21:56:55,225 valid 050 9.939455e-03 -1.229443
2019-11-02 21:57:03,886 valid 100 1.019515e-02 -1.326195
2019-11-02 21:57:12,534 valid 150 1.016242e-02 -1.254929
2019-11-02 21:57:21,200 valid 200 1.017040e-02 -1.154685
2019-11-02 21:57:29,877 valid 250 1.023683e-02 -1.232811
2019-11-02 21:57:38,546 valid 300 1.020083e-02 -1.236769
2019-11-02 21:57:47,187 valid 350 1.020501e-02 -1.239961
2019-11-02 21:57:55,850 valid 400 1.017026e-02 -14.692168
2019-11-02 21:58:04,514 valid 450 1.021223e-02 -13.157637
2019-11-02 21:58:13,218 valid 500 1.022811e-02 -11.943503
2019-11-02 21:58:21,862 valid 550 1.021935e-02 -10.941471
2019-11-02 21:58:30,568 valid 600 1.021924e-02 -10.896243
2019-11-02 21:58:33,156 validation loss; R2: 1.022436e-02 -10.685094
2019-11-02 21:58:33,297 epoch 5 lr 9.779754e-04
2019-11-02 21:58:33,298 genotype = Genotype(normal=[('skip_connect', 0), ('dil_conv_3x3', 1), ('skip_connect', 0), ('dil_conv_5x5', 1), ('max_pool_3x3', 0), ('dil_conv_3x3', 3), ('max_pool_3x3', 0), ('dil_conv_3x3', 4)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 0), ('max_pool_3x3', 1), ('dil_conv_5x5', 2), ('skip_connect', 0), ('dil_conv_5x5', 2), ('sep_conv_5x5', 3), ('dil_conv_5x5', 2), ('dil_conv_5x5', 4)], reduce_concat=range(2, 6))
2019-11-02 21:58:33,299 
alphas_normal = Variable containing:
 0.1972  0.1348  0.0395  0.4159  0.0264  0.0501  0.0447  0.0915
 0.5321  0.0252  0.0395  0.0548  0.0382  0.0121  0.1971  0.1010
 0.3219  0.1462  0.0559  0.1870  0.0722  0.0302  0.0169  0.1699
 0.5769  0.1013  0.0347  0.0558  0.0141  0.0251  0.0367  0.1553
 0.6807  0.0189  0.0189  0.0286  0.0639  0.0232  0.0669  0.0990
 0.0615  0.5581  0.0330  0.0426  0.1308  0.0390  0.0458  0.0892
 0.5853  0.0767  0.0769  0.1115  0.0535  0.0247  0.0244  0.0471
 0.7498  0.0424  0.0363  0.0458  0.0296  0.0334  0.0205  0.0423
 0.4056  0.0457  0.0477  0.0875  0.0162  0.1036  0.2090  0.0847
 0.0730  0.5151  0.0343  0.0407  0.0490  0.0424  0.1131  0.1324
 0.8675  0.0190  0.0193  0.0176  0.0095  0.0183  0.0272  0.0216
 0.6622  0.0409  0.0252  0.0309  0.0268  0.0157  0.0290  0.1694
 0.7310  0.0211  0.0175  0.0346  0.0192  0.0308  0.0505  0.0953
 0.5574  0.0275  0.0212  0.0255  0.0235  0.0270  0.1889  0.1291
[torch.cuda.FloatTensor of size 14x8 (GPU 2)]

2019-11-02 21:58:33,301 
alphas_reduce = Variable containing:
 0.1840  0.2688  0.0300  0.2533  0.0636  0.0506  0.0791  0.0706
 0.0441  0.2549  0.0261  0.1711  0.2404  0.1011  0.0551  0.1072
 0.3567  0.1203  0.0601  0.1559  0.1556  0.0627  0.0512  0.0375
 0.5313  0.0528  0.0390  0.0727  0.0231  0.1392  0.0527  0.0892
 0.0243  0.0436  0.0299  0.0337  0.0540  0.0228  0.0255  0.7661
 0.3699  0.2940  0.0660  0.0613  0.0395  0.0874  0.0234  0.0585
 0.1838  0.3060  0.0593  0.1303  0.0371  0.1144  0.0310  0.1380
 0.0400  0.0366  0.0344  0.0445  0.0363  0.0170  0.0505  0.7407
 0.0607  0.0800  0.0880  0.1204  0.0365  0.3736  0.0361  0.2045
 0.4524  0.0483  0.0609  0.0618  0.0273  0.1052  0.0543  0.1899
 0.6407  0.0457  0.0476  0.1085  0.0429  0.0519  0.0290  0.0338
 0.0952  0.1434  0.0888  0.0963  0.0672  0.0498  0.0458  0.4133
 0.1655  0.1363  0.0807  0.0910  0.0237  0.1916  0.2072  0.1039
 0.0816  0.0333  0.0508  0.0441  0.2174  0.0821  0.0961  0.3946
[torch.cuda.FloatTensor of size 14x8 (GPU 2)]

2019-11-02 21:58:36,935 train 000 9.043362e-03 -0.807302
2019-11-02 22:01:22,121 train 050 1.074861e-02 -0.986520
2019-11-02 22:04:07,641 train 100 1.046934e-02 -1.065690
2019-11-02 22:06:53,508 train 150 1.044546e-02 -1.069099
2019-11-02 22:09:39,952 train 200 1.034623e-02 -1.191309
2019-11-02 22:12:25,526 train 250 1.036401e-02 -1.648736
2019-11-02 22:15:11,730 train 300 1.035282e-02 -2.054915
2019-11-02 22:17:57,746 train 350 1.042691e-02 -1.957618
2019-11-02 22:20:43,407 train 400 1.045963e-02 -1.848172
2019-11-02 22:23:28,967 train 450 1.044141e-02 -2.220124
2019-11-02 22:26:14,893 train 500 1.047522e-02 -2.088504
2019-11-02 22:29:00,693 train 550 1.047635e-02 -1.991200
2019-11-02 22:31:46,906 train 600 1.047463e-02 -1.916899
2019-11-02 22:34:32,371 train 650 1.046209e-02 -1.850764
2019-11-02 22:37:18,178 train 700 1.047579e-02 -1.806602
2019-11-02 22:40:03,902 train 750 1.051382e-02 -1.749388
2019-11-02 22:42:49,654 train 800 1.050915e-02 -1.699676
2019-11-02 22:45:36,037 train 850 1.052512e-02 -1.673563
2019-11-02 22:48:22,654 train 900 1.052469e-02 -1.638572
2019-11-02 22:51:08,656 train 950 1.049394e-02 -4.312901
2019-11-02 22:53:54,810 train 1000 1.051217e-02 -4.155715
2019-11-02 22:56:40,893 train 1050 1.048178e-02 -4.003567
2019-11-02 22:59:26,819 train 1100 1.045914e-02 -3.867938
2019-11-02 23:02:12,111 train 1150 1.047972e-02 -3.811368
2019-11-02 23:04:56,946 train 1200 1.048156e-02 -3.751600
2019-11-02 23:07:41,699 train 1250 1.047490e-02 -3.653940
2019-11-02 23:10:26,991 train 1300 1.047516e-02 -3.591657
2019-11-02 23:13:13,247 train 1350 1.046980e-02 -3.496058
2019-11-02 23:16:00,391 train 1400 1.043260e-02 -3.401853
2019-11-02 23:18:46,098 train 1450 1.043561e-02 -3.325907
2019-11-02 23:21:31,471 train 1500 1.041862e-02 -3.250653
2019-11-02 23:24:17,588 train 1550 1.040906e-02 -3.608314
2019-11-02 23:27:02,241 train 1600 1.040418e-02 -3.575198
2019-11-02 23:29:46,859 train 1650 1.039513e-02 -3.499598
2019-11-02 23:32:32,414 train 1700 1.039586e-02 -3.453868
2019-11-02 23:35:18,252 train 1750 1.039675e-02 -3.394402
2019-11-02 23:38:03,478 train 1800 1.038634e-02 -3.339791
2019-11-02 23:40:49,293 train 1850 1.038904e-02 -3.275779
2019-11-02 23:43:33,911 train 1900 1.039773e-02 -3.215836
2019-11-02 23:46:19,366 train 1950 1.039980e-02 -3.183587
2019-11-02 23:49:05,356 train 2000 1.040631e-02 -3.142358
2019-11-02 23:51:51,629 train 2050 1.040772e-02 -3.082220
2019-11-02 23:54:37,838 train 2100 1.041249e-02 -3.028491
2019-11-02 23:57:22,618 train 2150 1.040724e-02 -2.976711
2019-11-03 00:00:07,484 train 2200 1.040478e-02 -2.934603
2019-11-03 00:02:53,903 train 2250 1.040884e-02 -2.891207
2019-11-03 00:05:39,732 train 2300 1.040664e-02 -2.851753
2019-11-03 00:08:25,231 train 2350 1.040270e-02 -2.814359
2019-11-03 00:11:10,156 train 2400 1.039888e-02 -2.777741
2019-11-03 00:13:57,108 train 2450 1.040540e-02 -2.754345
2019-11-03 00:14:39,851 training loss; R2: 1.040209e-02 -2.748961
2019-11-03 00:14:40,335 valid 000 8.804591e-03 -0.274395
2019-11-03 00:14:49,035 valid 050 1.009848e-02 -0.854411
2019-11-03 00:14:57,713 valid 100 9.875787e-03 -1.655548
2019-11-03 00:15:06,408 valid 150 9.904684e-03 -1.516568
2019-11-03 00:15:15,131 valid 200 9.878830e-03 -1.283342
2019-11-03 00:15:23,848 valid 250 9.864203e-03 -14.006992
2019-11-03 00:15:32,584 valid 300 9.884832e-03 -11.769333
2019-11-03 00:15:41,293 valid 350 9.841159e-03 -14.685376
2019-11-03 00:15:49,995 valid 400 9.864108e-03 -12.944021
2019-11-03 00:15:58,687 valid 450 9.832703e-03 -11.620975
2019-11-03 00:16:07,411 valid 500 9.863173e-03 -10.555213
2019-11-03 00:16:16,132 valid 550 9.846070e-03 -9.672992
2019-11-03 00:16:24,842 valid 600 9.827895e-03 -8.989432
2019-11-03 00:16:27,427 validation loss; R2: 9.828755e-03 -8.809736
2019-11-03 00:16:27,560 epoch 6 lr 9.683994e-04
2019-11-03 00:16:27,560 genotype = Genotype(normal=[('skip_connect', 0), ('dil_conv_3x3', 1), ('skip_connect', 0), ('dil_conv_5x5', 1), ('max_pool_3x3', 0), ('dil_conv_3x3', 3), ('max_pool_3x3', 0), ('dil_conv_3x3', 4)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 0), ('sep_conv_3x3', 1), ('dil_conv_5x5', 2), ('sep_conv_3x3', 0), ('dil_conv_5x5', 2), ('sep_conv_5x5', 3), ('dil_conv_5x5', 2), ('dil_conv_5x5', 4)], reduce_concat=range(2, 6))
2019-11-03 00:16:27,562 
alphas_normal = Variable containing:
 0.1590  0.1592  0.0405  0.4468  0.0233  0.0402  0.0612  0.0699
 0.5638  0.0185  0.0316  0.0407  0.0365  0.0140  0.1883  0.1066
 0.2960  0.1278  0.0573  0.2131  0.0772  0.0328  0.0202  0.1755
 0.5901  0.1123  0.0377  0.0555  0.0180  0.0227  0.0434  0.1203
 0.6904  0.0186  0.0169  0.0230  0.0474  0.0136  0.0952  0.0948
 0.0565  0.5974  0.0330  0.0359  0.1160  0.0487  0.0395  0.0730
 0.5179  0.0773  0.0929  0.1075  0.0952  0.0192  0.0186  0.0713
 0.8297  0.0256  0.0196  0.0213  0.0345  0.0337  0.0136  0.0220
 0.4580  0.0395  0.0323  0.0600  0.0157  0.1081  0.2189  0.0676
 0.0680  0.4395  0.0426  0.0588  0.0564  0.0278  0.1029  0.2040
 0.8277  0.0312  0.0279  0.0246  0.0098  0.0317  0.0256  0.0216
 0.6922  0.0356  0.0242  0.0307  0.0221  0.0169  0.0225  0.1557
 0.7668  0.0216  0.0164  0.0266  0.0173  0.0311  0.0463  0.0739
 0.4823  0.0302  0.0209  0.0237  0.0289  0.0381  0.2460  0.1300
[torch.cuda.FloatTensor of size 14x8 (GPU 2)]

2019-11-03 00:16:27,564 
alphas_reduce = Variable containing:
 0.1726  0.2685  0.0275  0.2219  0.0935  0.0652  0.0979  0.0530
 0.0554  0.2310  0.0297  0.1519  0.2591  0.1261  0.0572  0.0896
 0.2817  0.0800  0.0405  0.1143  0.2951  0.0732  0.0758  0.0393
 0.3924  0.0449  0.0407  0.0906  0.0373  0.1863  0.1142  0.0937
 0.0520  0.0580  0.0379  0.0437  0.0702  0.0361  0.0256  0.6766
 0.3713  0.2314  0.0704  0.0841  0.0607  0.0885  0.0347  0.0588
 0.2678  0.2032  0.0670  0.1010  0.0602  0.0914  0.0693  0.1401
 0.0336  0.0264  0.0227  0.0272  0.0297  0.0142  0.0277  0.8186
 0.0417  0.0656  0.0812  0.0967  0.0265  0.4724  0.0289  0.1871
 0.3792  0.0446  0.0434  0.0411  0.0309  0.2041  0.0427  0.2139
 0.7517  0.0278  0.0238  0.0412  0.0381  0.0626  0.0243  0.0305
 0.1031  0.1303  0.0424  0.0525  0.0978  0.1185  0.0557  0.3998
 0.1186  0.2087  0.0853  0.0856  0.0272  0.1754  0.1860  0.1133
 0.1036  0.0609  0.0491  0.0460  0.2508  0.0634  0.1044  0.3217
[torch.cuda.FloatTensor of size 14x8 (GPU 2)]

2019-11-03 00:16:31,110 train 000 7.513803e-03 -0.980884
2019-11-03 00:19:15,461 train 050 1.030565e-02 -1.650651
2019-11-03 00:22:00,178 train 100 1.001569e-02 -1.400164
2019-11-03 00:24:46,043 train 150 1.001308e-02 -4.752939
2019-11-03 00:27:31,116 train 200 1.012966e-02 -4.049243
2019-11-03 00:30:16,703 train 250 1.011746e-02 -3.536331
2019-11-03 00:33:02,620 train 300 1.012394e-02 -3.115355
2019-11-03 00:35:48,018 train 350 1.011661e-02 -2.877079
2019-11-03 00:38:33,365 train 400 1.008051e-02 -2.618249
2019-11-03 00:41:19,348 train 450 1.007474e-02 -2.423746
2019-11-03 00:44:05,889 train 500 1.007253e-02 -2.277216
2019-11-03 00:46:52,073 train 550 1.004846e-02 -2.174011
2019-11-03 00:49:37,823 train 600 1.006754e-02 -2.078530
2019-11-03 00:52:21,782 train 650 1.008552e-02 -2.001508
2019-11-03 00:55:08,131 train 700 1.007177e-02 -1.951942
2019-11-03 00:57:53,243 train 750 1.007010e-02 -1.882404
2019-11-03 01:00:38,775 train 800 1.011943e-02 -1.830275
2019-11-03 01:03:24,578 train 850 1.012050e-02 -1.808030
2019-11-03 01:06:10,672 train 900 1.014076e-02 -1.805155
2019-11-03 01:08:56,244 train 950 1.014218e-02 -1.801676
2019-11-03 01:11:41,238 train 1000 1.016430e-02 -1.749181
2019-11-03 01:14:26,587 train 1050 1.016601e-02 -1.713040
2019-11-03 01:17:11,578 train 1100 1.018406e-02 -1.676412
2019-11-03 01:19:55,959 train 1150 1.018896e-02 -1.698225
2019-11-03 01:22:41,293 train 1200 1.019738e-02 -1.678431
2019-11-03 01:25:27,133 train 1250 1.019233e-02 -1.654812
2019-11-03 01:28:12,463 train 1300 1.019624e-02 -1.634045
2019-11-03 01:30:57,782 train 1350 1.019407e-02 -1.614756
2019-11-03 01:33:43,321 train 1400 1.021969e-02 -1.592507
2019-11-03 01:36:29,332 train 1450 1.020355e-02 -1.568276
2019-11-03 01:39:14,144 train 1500 1.020993e-02 -2.661536
2019-11-03 01:42:00,547 train 1550 1.021511e-02 -2.626769
2019-11-03 01:44:46,317 train 1600 1.019706e-02 -3.024478
2019-11-03 01:47:31,722 train 1650 1.017711e-02 -3.017381
2019-11-03 01:50:17,310 train 1700 1.017885e-02 -2.954669
2019-11-03 01:53:02,802 train 1750 1.016235e-02 -2.896687
2019-11-03 01:55:47,820 train 1800 1.015814e-02 -2.912966
2019-11-03 01:58:33,139 train 1850 1.016153e-02 -2.884418
2019-11-03 01:01:19,029 train 1900 1.015441e-02 -2.846361
2019-11-03 01:04:04,459 train 1950 1.014585e-02 -3.192428
2019-11-03 01:06:49,737 train 2000 1.014545e-02 -3.143676
2019-11-03 01:09:35,321 train 2050 1.013332e-02 -3.084390
2019-11-03 01:12:22,313 train 2100 1.012162e-02 -3.031393
2019-11-03 01:15:08,635 train 2150 1.012042e-02 -2.987019
2019-11-03 01:17:55,115 train 2200 1.011465e-02 -2.943720
2019-11-03 01:20:39,955 train 2250 1.011349e-02 -2.905725
2019-11-03 01:23:25,721 train 2300 1.010578e-02 -2.859558
2019-11-03 01:26:11,365 train 2350 1.010596e-02 -2.851893
2019-11-03 01:28:57,047 train 2400 1.009764e-02 -2.823108
2019-11-03 01:31:42,280 train 2450 1.008774e-02 -2.799326
2019-11-03 01:32:24,445 training loss; R2: 1.008862e-02 -2.797585
2019-11-03 01:32:24,976 valid 000 9.188031e-03 -6.679543
2019-11-03 01:32:33,676 valid 050 9.473622e-03 -1.324893
2019-11-03 01:32:42,376 valid 100 9.668100e-03 -1.259881
2019-11-03 01:32:51,077 valid 150 9.691480e-03 -28.523146
2019-11-03 01:32:59,737 valid 200 9.705273e-03 -21.793228
2019-11-03 01:33:08,439 valid 250 9.777805e-03 -17.670269
2019-11-03 01:33:17,103 valid 300 9.779690e-03 -14.886391
2019-11-03 01:33:25,763 valid 350 9.833840e-03 -12.923709
2019-11-03 01:33:34,432 valid 400 9.796769e-03 -11.480395
2019-11-03 01:33:43,082 valid 450 9.843749e-03 -10.408610
2019-11-03 01:33:51,731 valid 500 9.840219e-03 -9.478312
2019-11-03 01:34:00,433 valid 550 9.830078e-03 -8.736416
2019-11-03 01:34:09,150 valid 600 9.830584e-03 -8.181778
2019-11-03 01:34:11,737 validation loss; R2: 9.839839e-03 -8.008580
2019-11-03 01:34:11,871 epoch 7 lr 9.571722e-04
2019-11-03 01:34:11,871 genotype = Genotype(normal=[('skip_connect', 0), ('dil_conv_3x3', 1), ('skip_connect', 0), ('dil_conv_3x3', 2), ('max_pool_3x3', 0), ('dil_conv_3x3', 3), ('max_pool_3x3', 0), ('dil_conv_3x3', 4)], normal_concat=range(2, 6), reduce=[('sep_conv_3x3', 1), ('max_pool_3x3', 0), ('dil_conv_5x5', 2), ('sep_conv_3x3', 0), ('dil_conv_5x5', 2), ('sep_conv_5x5', 3), ('dil_conv_5x5', 4), ('dil_conv_5x5', 2)], reduce_concat=range(2, 6))
2019-11-03 01:34:11,873 
alphas_normal = Variable containing:
 0.2144  0.1281  0.0430  0.4391  0.0270  0.0391  0.0529  0.0564
 0.6218  0.0166  0.0215  0.0270  0.0464  0.0175  0.1562  0.0931
 0.3997  0.1201  0.0516  0.2146  0.0625  0.0227  0.0193  0.1095
 0.6849  0.0815  0.0264  0.0403  0.0143  0.0284  0.0343  0.0899
 0.6083  0.0229  0.0164  0.0260  0.0598  0.0195  0.1240  0.1232
 0.0695  0.6308  0.0260  0.0348  0.1018  0.0347  0.0370  0.0655
 0.5286  0.0920  0.0822  0.0942  0.0777  0.0312  0.0244  0.0697
 0.8297  0.0280  0.0168  0.0231  0.0335  0.0333  0.0165  0.0192
 0.4363  0.0489  0.0293  0.0539  0.0222  0.1173  0.2067  0.0855
 0.0552  0.5100  0.0354  0.0562  0.0760  0.0236  0.0525  0.1911
 0.7760  0.0318  0.0279  0.0278  0.0263  0.0543  0.0320  0.0238
 0.6437  0.0552  0.0293  0.0419  0.0183  0.0178  0.0313  0.1624
 0.7780  0.0324  0.0209  0.0369  0.0175  0.0211  0.0301  0.0631
 0.4034  0.0447  0.0309  0.0359  0.0393  0.0562  0.2699  0.1198
[torch.cuda.FloatTensor of size 14x8 (GPU 2)]

2019-11-03 01:34:11,875 
alphas_reduce = Variable containing:
 0.2334  0.2545  0.0225  0.2340  0.0748  0.0533  0.0762  0.0512
 0.0470  0.2620  0.0246  0.1589  0.2872  0.1035  0.0474  0.0694
 0.4486  0.0757  0.0375  0.0787  0.1470  0.0861  0.0654  0.0609
 0.5386  0.0575  0.0522  0.0538  0.0296  0.1427  0.0790  0.0465
 0.0486  0.0742  0.0516  0.0616  0.0765  0.0369  0.0280  0.6228
 0.3462  0.2310  0.0863  0.0625  0.0481  0.1436  0.0354  0.0469
 0.2353  0.2413  0.0637  0.0837  0.0480  0.0850  0.0484  0.1946
 0.0318  0.0308  0.0330  0.0374  0.0249  0.0194  0.0268  0.7958
 0.0503  0.0643  0.0946  0.0994  0.0637  0.4056  0.0409  0.1811
 0.4286  0.0511  0.0399  0.0441  0.0306  0.1728  0.0525  0.1804
 0.7768  0.0298  0.0302  0.0292  0.0353  0.0522  0.0221  0.0243
 0.0955  0.1085  0.0479  0.0592  0.1436  0.1317  0.0825  0.3310
 0.1280  0.1715  0.0688  0.0690  0.0569  0.1892  0.2052  0.1115
 0.0490  0.0433  0.0402  0.0380  0.2264  0.0669  0.0592  0.4769
[torch.cuda.FloatTensor of size 14x8 (GPU 2)]

2019-11-03 01:34:15,404 train 000 8.713735e-03 -0.250664
2019-11-03 01:36:58,668 train 050 1.001509e-02 -16.960231
2019-11-03 01:39:43,564 train 100 9.870537e-03 -9.114037
2019-11-03 01:42:28,345 train 150 9.838894e-03 -6.409504
2019-11-03 01:45:14,094 train 200 9.832551e-03 -5.118554
2019-11-03 01:47:59,540 train 250 9.900087e-03 -4.357687
2019-11-03 01:50:44,473 train 300 9.921015e-03 -3.833727
2019-11-03 01:53:29,919 train 350 9.919381e-03 -5.214000
2019-11-03 01:56:15,358 train 400 9.905924e-03 -4.880568
2019-11-03 01:59:00,971 train 450 9.870730e-03 -6.716745
2019-11-03 02:01:45,936 train 500 9.898407e-03 -6.185518
2019-11-03 02:04:30,529 train 550 9.908648e-03 -5.687719
2019-11-03 02:07:16,565 train 600 9.880567e-03 -5.322999
2019-11-03 02:10:02,310 train 650 9.863357e-03 -4.985000
2019-11-03 02:12:47,733 train 700 9.859927e-03 -4.873220
2019-11-03 02:15:33,933 train 750 9.879190e-03 -4.628519
2019-11-03 02:18:19,798 train 800 9.877248e-03 -4.410645
2019-11-03 02:21:05,241 train 850 9.886255e-03 -4.218785
2019-11-03 02:23:50,293 train 900 9.909840e-03 -4.060314
2019-11-03 02:26:35,842 train 950 9.925652e-03 -3.906321
2019-11-03 02:29:20,270 train 1000 9.946373e-03 -3.793521
2019-11-03 02:32:06,476 train 1050 9.936902e-03 -3.651628
2019-11-03 02:34:50,700 train 1100 9.919095e-03 -3.542123
2019-11-03 02:37:35,477 train 1150 9.926510e-03 -3.426523
2019-11-03 02:40:19,998 train 1200 9.945662e-03 -3.329174
2019-11-03 02:43:05,465 train 1250 9.951121e-03 -3.239229
2019-11-03 02:45:51,426 train 1300 9.932036e-03 -3.156622
2019-11-03 02:48:35,393 train 1350 9.942926e-03 -3.080217
2019-11-03 02:51:20,163 train 1400 9.928470e-03 -3.014031
2019-11-03 02:54:05,728 train 1450 9.920590e-03 -2.944330
2019-11-03 02:56:52,003 train 1500 9.923753e-03 -2.873084
2019-11-03 02:59:37,837 train 1550 9.924838e-03 -2.810945
2019-11-03 03:02:23,826 train 1600 9.928382e-03 -2.761447
2019-11-03 03:05:08,242 train 1650 9.924101e-03 -2.727349
2019-11-03 03:07:54,100 train 1700 9.914380e-03 -2.681632
2019-11-03 03:10:39,351 train 1750 9.914275e-03 -2.654214
2019-11-03 03:13:24,882 train 1800 9.911547e-03 -2.609516
2019-11-03 03:16:10,322 train 1850 9.901012e-03 -3.467982
2019-11-03 03:18:56,296 train 1900 9.900914e-03 -3.459445
2019-11-03 03:21:41,528 train 1950 9.890709e-03 -3.414791
2019-11-03 03:24:27,027 train 2000 9.880706e-03 -3.372169
2019-11-03 03:27:12,676 train 2050 9.879385e-03 -3.325154
2019-11-03 03:29:58,274 train 2100 9.873739e-03 -3.272822
2019-11-03 03:32:44,645 train 2150 9.873002e-03 -3.238339
2019-11-03 03:35:30,427 train 2200 9.875996e-03 -3.186190
2019-11-03 03:38:15,999 train 2250 9.874104e-03 -3.164938
2019-11-03 03:41:01,279 train 2300 9.880000e-03 -3.121158
2019-11-03 03:43:46,492 train 2350 9.875762e-03 -9.074371
2019-11-03 03:46:32,012 train 2400 9.867391e-03 -8.922757
2019-11-03 03:49:17,084 train 2450 9.866356e-03 -8.760243
2019-11-03 03:50:00,065 training loss; R2: 9.867362e-03 -8.723082
2019-11-03 03:50:00,558 valid 000 1.294954e-02 -0.688276
2019-11-03 03:50:09,273 valid 050 9.372458e-03 -3.232900
2019-11-03 03:50:17,945 valid 100 9.396056e-03 -2.460366
2019-11-03 03:50:26,632 valid 150 9.325666e-03 -2.006874
2019-11-03 03:50:35,315 valid 200 9.229438e-03 -2.011782
2019-11-03 03:50:44,010 valid 250 9.170574e-03 -1.917886
2019-11-03 03:50:52,697 valid 300 9.150669e-03 -13.195999
2019-11-03 03:51:01,374 valid 350 9.192175e-03 -11.524639
2019-11-03 03:51:10,007 valid 400 9.187416e-03 -10.276502
2019-11-03 03:51:18,675 valid 450 9.211807e-03 -9.298963
2019-11-03 03:51:27,317 valid 500 9.197035e-03 -8.487163
2019-11-03 03:51:36,004 valid 550 9.219873e-03 -7.817533
2019-11-03 03:51:44,652 valid 600 9.249408e-03 -7.254962
2019-11-03 03:51:47,226 validation loss; R2: 9.260867e-03 -7.113349
2019-11-03 03:51:47,356 epoch 8 lr 9.443380e-04
2019-11-03 03:51:47,357 genotype = Genotype(normal=[('skip_connect', 0), ('dil_conv_3x3', 1), ('skip_connect', 0), ('dil_conv_5x5', 2), ('max_pool_3x3', 0), ('dil_conv_3x3', 3), ('max_pool_3x3', 0), ('dil_conv_3x3', 4)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 1), ('skip_connect', 0), ('dil_conv_5x5', 2), ('sep_conv_3x3', 0), ('dil_conv_5x5', 2), ('sep_conv_5x5', 3), ('dil_conv_5x5', 4), ('dil_conv_5x5', 2)], reduce_concat=range(2, 6))
2019-11-03 03:51:47,359 
alphas_normal = Variable containing:
 0.2136  0.0981  0.0421  0.4671  0.0346  0.0431  0.0536  0.0478
 0.5360  0.0249  0.0336  0.0416  0.0476  0.0173  0.2008  0.0982
 0.4227  0.1167  0.0528  0.2040  0.0585  0.0294  0.0174  0.0986
 0.6637  0.0770  0.0319  0.0474  0.0154  0.0285  0.0391  0.0970
 0.6151  0.0213  0.0196  0.0293  0.0564  0.0199  0.1095  0.1288
 0.0861  0.6047  0.0294  0.0408  0.1037  0.0402  0.0314  0.0637
 0.4926  0.0892  0.1059  0.1280  0.0758  0.0274  0.0220  0.0590
 0.8252  0.0260  0.0203  0.0292  0.0331  0.0271  0.0191  0.0199
 0.4294  0.0469  0.0394  0.0730  0.0174  0.1235  0.1945  0.0759
 0.0707  0.5212  0.0380  0.0585  0.0702  0.0188  0.0310  0.1917
 0.8063  0.0279  0.0215  0.0214  0.0181  0.0485  0.0339  0.0225
 0.6023  0.0474  0.0234  0.0308  0.0193  0.0195  0.0299  0.2274
 0.7690  0.0277  0.0197  0.0304  0.0169  0.0183  0.0315  0.0866
 0.4386  0.0420  0.0231  0.0273  0.0313  0.0629  0.2428  0.1320
[torch.cuda.FloatTensor of size 14x8 (GPU 2)]

2019-11-03 03:51:47,360 
alphas_reduce = Variable containing:
 0.2502  0.2186  0.0234  0.2490  0.0810  0.0389  0.0739  0.0651
 0.0506  0.2798  0.0271  0.1268  0.2479  0.1347  0.0486  0.0846
 0.4757  0.0700  0.0326  0.0801  0.1644  0.0728  0.0553  0.0492
 0.5737  0.0498  0.0418  0.0568  0.0342  0.1507  0.0613  0.0315
 0.0337  0.0623  0.0414  0.0549  0.0941  0.0501  0.0271  0.6363
 0.5193  0.1650  0.0783  0.0659  0.0367  0.0811  0.0247  0.0291
 0.2163  0.2177  0.0645  0.0969  0.0426  0.1066  0.0524  0.2029
 0.0410  0.0346  0.0397  0.0476  0.0321  0.0182  0.0230  0.7637
 0.0319  0.0481  0.0706  0.0830  0.0647  0.4868  0.0527  0.1622
 0.5389  0.0478  0.0465  0.0488  0.0279  0.1094  0.0354  0.1455
 0.7537  0.0276  0.0364  0.0359  0.0363  0.0440  0.0224  0.0438
 0.0723  0.0550  0.0588  0.0658  0.1850  0.1668  0.0587  0.3376
 0.1177  0.1126  0.0735  0.1198  0.0592  0.1296  0.3103  0.0773
 0.0496  0.0383  0.0411  0.0454  0.2279  0.0532  0.0773  0.4672
[torch.cuda.FloatTensor of size 14x8 (GPU 2)]

2019-11-03 03:51:50,950 train 000 7.175439e-03 -0.574728
2019-11-03 03:54:36,354 train 050 9.371656e-03 -1.132984
2019-11-03 03:57:21,152 train 100 9.597690e-03 -2.340022
2019-11-03 04:00:07,132 train 150 9.615616e-03 -1.915757
2019-11-03 04:02:51,771 train 200 9.630697e-03 -1.689279
2019-11-03 04:05:36,351 train 250 9.583158e-03 -1.548454
2019-11-03 04:08:22,325 train 300 9.598805e-03 -1.708627
2019-11-03 04:11:10,634 train 350 9.619872e-03 -1.588955
2019-11-03 04:13:56,809 train 400 9.607193e-03 -1.569197
2019-11-03 04:16:42,398 train 450 9.656049e-03 -1.605730
2019-11-03 04:19:27,785 train 500 9.704360e-03 -1.599107
2019-11-03 04:22:12,647 train 550 9.708345e-03 -1.649583
2019-11-03 04:24:57,867 train 600 9.703755e-03 -1.676196
2019-11-03 04:27:43,567 train 650 9.690765e-03 -1.615928
2019-11-03 04:30:28,218 train 700 9.707024e-03 -1.645162
2019-11-03 04:33:14,007 train 750 9.696295e-03 -1.585058
2019-11-03 04:36:00,166 train 800 9.667525e-03 -1.563953
2019-11-03 04:38:45,020 train 850 9.685967e-03 -1.580157
2019-11-03 04:41:31,090 train 900 9.693585e-03 -1.609464
2019-11-03 04:44:17,436 train 950 9.685905e-03 -1.589167
2019-11-03 04:47:03,190 train 1000 9.679553e-03 -1.564813
2019-11-03 04:49:48,430 train 1050 9.680489e-03 -1.722400
2019-11-03 04:52:33,806 train 1100 9.683409e-03 -1.695542
2019-11-03 04:55:19,193 train 1150 9.685546e-03 -1.669633
2019-11-03 04:58:04,837 train 1200 9.691036e-03 -1.661743
2019-11-03 05:00:49,066 train 1250 9.696818e-03 -1.645099
2019-11-03 05:03:34,593 train 1300 9.684617e-03 -1.621385
2019-11-03 05:06:20,395 train 1350 9.669767e-03 -1.599954
2019-11-03 05:09:05,962 train 1400 9.674046e-03 -1.568016
2019-11-03 05:11:51,906 train 1450 9.667309e-03 -1.547768
2019-11-03 05:14:37,501 train 1500 9.688968e-03 -1.534626
2019-11-03 05:17:22,589 train 1550 9.682653e-03 -1.513368
2019-11-03 05:20:08,151 train 1600 9.671134e-03 -1.546462
2019-11-03 05:22:52,494 train 1650 9.671912e-03 -1.586080
2019-11-03 05:25:38,170 train 1700 9.678230e-03 -1.632510
2019-11-03 05:28:23,129 train 1750 9.672312e-03 -1.609486
2019-11-03 05:31:09,189 train 1800 9.669685e-03 -1.625856
2019-11-03 05:33:54,697 train 1850 9.663397e-03 -1.648187
2019-11-03 05:36:39,093 train 1900 9.670731e-03 -1.665640
2019-11-03 05:39:24,776 train 1950 9.675274e-03 -1.648275
2019-11-03 05:42:08,888 train 2000 9.672652e-03 -1.634981
2019-11-03 05:44:53,732 train 2050 9.671022e-03 -1.624601
2019-11-03 05:47:39,583 train 2100 9.679238e-03 -1.615983
2019-11-03 05:50:25,694 train 2150 9.674734e-03 -1.625064
2019-11-03 05:53:11,982 train 2200 9.681347e-03 -1.609075
2019-11-03 05:55:57,472 train 2250 9.669974e-03 -1.620731
2019-11-03 05:58:42,468 train 2300 9.674694e-03 -2.486922
2019-11-03 06:01:28,105 train 2350 9.665099e-03 -2.460744
2019-11-03 06:04:13,867 train 2400 9.654896e-03 -2.442790
2019-11-03 06:06:58,950 train 2450 9.660787e-03 -2.411150
2019-11-03 06:07:41,999 training loss; R2: 9.662591e-03 -2.443995
2019-11-03 06:07:42,457 valid 000 1.031240e-02 0.162474
2019-11-03 06:07:51,138 valid 050 9.645016e-03 -0.890913
2019-11-03 06:07:59,878 valid 100 9.734002e-03 -0.768224
2019-11-03 06:08:08,573 valid 150 9.838867e-03 -0.695715
2019-11-03 06:08:17,251 valid 200 9.789245e-03 -0.683540
2019-11-03 06:08:25,906 valid 250 9.727271e-03 -0.689133
2019-11-03 06:08:34,581 valid 300 9.775610e-03 -1.054805
2019-11-03 06:08:43,234 valid 350 9.803016e-03 -0.997963
2019-11-03 06:08:51,982 valid 400 9.816450e-03 -1.072012
2019-11-03 06:09:00,677 valid 450 9.856722e-03 -1.099895
2019-11-03 06:09:09,372 valid 500 9.795027e-03 -1.047827
2019-11-03 06:09:18,019 valid 550 9.813272e-03 -1.019828
2019-11-03 06:09:26,705 valid 600 9.816120e-03 -0.979984
2019-11-03 06:09:29,275 validation loss; R2: 9.820291e-03 -0.973336
2019-11-03 06:09:29,415 epoch 9 lr 9.299476e-04
2019-11-03 06:09:29,416 genotype = Genotype(normal=[('skip_connect', 0), ('dil_conv_3x3', 1), ('skip_connect', 0), ('dil_conv_3x3', 2), ('max_pool_3x3', 0), ('sep_conv_5x5', 3), ('max_pool_3x3', 0), ('dil_conv_3x3', 4)], normal_concat=range(2, 6), reduce=[('sep_conv_3x3', 1), ('max_pool_3x3', 0), ('dil_conv_5x5', 2), ('sep_conv_3x3', 0), ('dil_conv_5x5', 2), ('sep_conv_5x5', 3), ('dil_conv_5x5', 4), ('dil_conv_3x3', 3)], reduce_concat=range(2, 6))
2019-11-03 06:09:29,418 
alphas_normal = Variable containing:
 0.1950  0.1034  0.0586  0.4718  0.0378  0.0517  0.0361  0.0456
 0.4674  0.0308  0.0462  0.0528  0.0589  0.0249  0.1792  0.1399
 0.3724  0.1238  0.0806  0.2077  0.0686  0.0374  0.0166  0.0929
 0.5258  0.1205  0.0587  0.0840  0.0300  0.0159  0.0781  0.0871
 0.5215  0.0247  0.0244  0.0310  0.0740  0.0183  0.1731  0.1331
 0.0669  0.6741  0.0302  0.0311  0.0888  0.0415  0.0250  0.0424
 0.4045  0.1176  0.1318  0.1420  0.1191  0.0222  0.0185  0.0445
 0.8454  0.0265  0.0169  0.0203  0.0357  0.0272  0.0128  0.0150
 0.3421  0.0577  0.0540  0.0713  0.0195  0.1973  0.1875  0.0707
 0.0589  0.5049  0.0433  0.0468  0.0802  0.0283  0.0353  0.2023
 0.8570  0.0191  0.0132  0.0133  0.0156  0.0448  0.0154  0.0217
 0.5349  0.0887  0.0321  0.0346  0.0264  0.0266  0.0405  0.2163
 0.7266  0.0356  0.0240  0.0274  0.0390  0.0443  0.0494  0.0537
 0.3662  0.0712  0.0328  0.0309  0.0403  0.0825  0.2415  0.1346
[torch.cuda.FloatTensor of size 14x8 (GPU 2)]

2019-11-03 06:09:29,419 
alphas_reduce = Variable containing:
 0.3324  0.2016  0.0227  0.1845  0.0715  0.0273  0.0778  0.0822
 0.0331  0.2449  0.0212  0.1609  0.2670  0.1603  0.0384  0.0742
 0.6013  0.0498  0.0237  0.0328  0.1847  0.0441  0.0293  0.0343
 0.5681  0.0876  0.0811  0.0327  0.0281  0.1260  0.0490  0.0274
 0.0338  0.0608  0.0367  0.0464  0.1108  0.0339  0.0200  0.6576
 0.4972  0.2168  0.0791  0.0575  0.0322  0.0557  0.0277  0.0337
 0.1209  0.3050  0.0696  0.0931  0.0540  0.1289  0.0501  0.1784
 0.0550  0.0733  0.0506  0.0696  0.0684  0.0162  0.0349  0.6318
 0.0602  0.0717  0.1102  0.1013  0.0544  0.4662  0.0288  0.1072
 0.6192  0.0397  0.0366  0.0397  0.0241  0.0754  0.0427  0.1225
 0.7942  0.0370  0.0411  0.0295  0.0339  0.0243  0.0161  0.0239
 0.0660  0.0598  0.0679  0.0605  0.1972  0.2114  0.0561  0.2812
 0.1272  0.1069  0.1161  0.1422  0.0256  0.0777  0.3534  0.0508
 0.0601  0.0511  0.0542  0.0568  0.2975  0.0434  0.0821  0.3549
[torch.cuda.FloatTensor of size 14x8 (GPU 2)]

2019-11-03 06:09:33,014 train 000 1.050832e-02 -0.638851
2019-11-03 06:12:19,341 train 050 9.630281e-03 -1.050816
2019-11-03 06:15:04,588 train 100 9.468154e-03 -2.124209
2019-11-03 06:17:50,646 train 150 9.540347e-03 -1.783474
2019-11-03 06:20:36,050 train 200 9.502426e-03 -5.929631
2019-11-03 06:23:20,657 train 250 9.452567e-03 -5.132779
2019-11-03 06:26:05,644 train 300 9.443591e-03 -4.431296
2019-11-03 06:28:50,252 train 350 9.491561e-03 -3.928628
2019-11-03 06:31:35,039 train 400 9.502512e-03 -3.587673
2019-11-03 06:34:20,223 train 450 9.493112e-03 -3.434877
2019-11-03 06:37:05,254 train 500 9.505143e-03 -3.369626
2019-11-03 06:39:50,290 train 550 9.529785e-03 -3.224135
2019-11-03 06:42:35,494 train 600 9.536212e-03 -3.049472
2019-11-03 06:45:20,185 train 650 9.508665e-03 -2.890659
2019-11-03 06:48:05,298 train 700 9.517053e-03 -2.748921
2019-11-03 06:50:51,563 train 750 9.509143e-03 -2.650816
2019-11-03 06:53:35,682 train 800 9.485601e-03 -2.547843
2019-11-03 06:56:21,308 train 850 9.470545e-03 -2.607937
2019-11-03 06:59:07,259 train 900 9.471283e-03 -2.520730
2019-11-03 07:01:52,611 train 950 9.456839e-03 -2.463421
2019-11-03 07:04:38,079 train 1000 9.457870e-03 -2.402109
2019-11-03 07:07:24,693 train 1050 9.438831e-03 -2.335710
2019-11-03 07:10:11,838 train 1100 9.433489e-03 -2.280446
2019-11-03 07:12:57,897 train 1150 9.430464e-03 -2.412238
2019-11-03 07:15:43,487 train 1200 9.439961e-03 -2.359032
2019-11-03 07:18:27,342 train 1250 9.443202e-03 -2.322014
2019-11-03 07:21:13,427 train 1300 9.444041e-03 -2.265035
2019-11-03 07:23:59,941 train 1350 9.441424e-03 -2.232360
2019-11-03 07:26:45,839 train 1400 9.455290e-03 -2.198423
2019-11-03 07:29:30,763 train 1450 9.454223e-03 -2.174645
2019-11-03 07:32:15,922 train 1500 9.454773e-03 -2.134135
2019-11-03 07:35:00,934 train 1550 9.455765e-03 -2.096620
2019-11-03 07:37:46,382 train 1600 9.456861e-03 -2.056218
2019-11-03 07:40:31,891 train 1650 9.466916e-03 -2.025272
2019-11-03 07:43:18,069 train 1700 9.471839e-03 -2.057383
2019-11-03 07:46:02,733 train 1750 9.473598e-03 -2.026159
2019-11-03 07:48:48,199 train 1800 9.478966e-03 -1.995113
2019-11-03 07:51:34,055 train 1850 9.476758e-03 -1.976076
2019-11-03 07:54:18,945 train 1900 9.477082e-03 -1.973317
2019-11-03 07:57:05,409 train 1950 9.481844e-03 -1.950144
2019-11-03 07:59:52,708 train 2000 9.471239e-03 -1.933951
2019-11-03 08:02:38,784 train 2050 9.474347e-03 -1.915250
2019-11-03 08:05:24,133 train 2100 9.475842e-03 -1.893335
2019-11-03 08:08:09,152 train 2150 9.484888e-03 -1.870510
2019-11-03 08:10:55,444 train 2200 9.483703e-03 -1.852801
2019-11-03 08:13:41,392 train 2250 9.483988e-03 -1.907386
2019-11-03 08:16:26,941 train 2300 9.480022e-03 -1.893477
2019-11-03 08:19:13,171 train 2350 9.479833e-03 -1.884791
2019-11-03 08:21:59,283 train 2400 9.484600e-03 -1.871511
2019-11-03 08:24:45,877 train 2450 9.484222e-03 -1.854667
2019-11-03 08:25:28,470 training loss; R2: 9.485613e-03 -1.851796
2019-11-03 08:25:28,969 valid 000 1.053522e-02 -0.959383
2019-11-03 08:25:37,633 valid 050 1.069587e-02 -1.663649
2019-11-03 08:25:46,299 valid 100 1.046288e-02 -1.428832
2019-11-03 08:25:54,977 valid 150 1.045198e-02 -1.386497
2019-11-03 08:26:03,621 valid 200 1.040313e-02 -1.258575
2019-11-03 08:26:12,323 valid 250 1.038926e-02 -1.210222
2019-11-03 08:26:21,010 valid 300 1.042478e-02 -1.152821
2019-11-03 08:26:29,723 valid 350 1.041414e-02 -1.344831
2019-11-03 08:26:38,420 valid 400 1.034014e-02 -1.394803
2019-11-03 08:26:47,116 valid 450 1.028696e-02 -1.405472
2019-11-03 08:26:55,790 valid 500 1.021499e-02 -1.381892
2019-11-03 08:27:04,500 valid 550 1.018985e-02 -1.984383
2019-11-03 08:27:13,185 valid 600 1.021309e-02 -2.235878
2019-11-03 08:27:15,766 validation loss; R2: 1.021188e-02 -2.210989
2019-11-03 08:27:15,900 epoch 10 lr 9.140576e-04
2019-11-03 08:27:15,900 genotype = Genotype(normal=[('skip_connect', 0), ('dil_conv_3x3', 1), ('skip_connect', 0), ('max_pool_3x3', 1), ('max_pool_3x3', 0), ('dil_conv_3x3', 3), ('max_pool_3x3', 0), ('dil_conv_3x3', 4)], normal_concat=range(2, 6), reduce=[('skip_connect', 0), ('max_pool_3x3', 1), ('dil_conv_5x5', 2), ('sep_conv_3x3', 0), ('dil_conv_5x5', 2), ('sep_conv_5x5', 3), ('sep_conv_3x3', 4), ('dil_conv_3x3', 3)], reduce_concat=range(2, 6))
2019-11-03 08:27:15,902 
alphas_normal = Variable containing:
 0.1001  0.1428  0.0897  0.5287  0.0246  0.0402  0.0232  0.0505
 0.5664  0.0289  0.0412  0.0392  0.0406  0.0208  0.1363  0.1265
 0.3484  0.1185  0.1042  0.1926  0.0535  0.0363  0.0278  0.1187
 0.4326  0.1441  0.0520  0.0614  0.0307  0.0215  0.1378  0.1199
 0.6304  0.0250  0.0205  0.0244  0.0445  0.0087  0.1047  0.1417
 0.1697  0.3956  0.0916  0.0952  0.1157  0.0407  0.0367  0.0549
 0.4639  0.0827  0.1444  0.1164  0.0919  0.0212  0.0168  0.0627
 0.8466  0.0149  0.0148  0.0149  0.0273  0.0446  0.0162  0.0207
 0.2849  0.0297  0.0290  0.0390  0.0168  0.2189  0.2779  0.1039
 0.0571  0.5032  0.0623  0.0578  0.0642  0.0468  0.0330  0.1757
 0.7214  0.0743  0.0342  0.0290  0.0169  0.0750  0.0195  0.0297
 0.5890  0.0873  0.0308  0.0364  0.0142  0.0170  0.0267  0.1986
 0.7867  0.0491  0.0204  0.0267  0.0236  0.0409  0.0196  0.0332
 0.4102  0.0748  0.0194  0.0219  0.0356  0.0956  0.2094  0.1332
[torch.cuda.FloatTensor of size 14x8 (GPU 2)]

2019-11-03 08:27:15,904 
alphas_reduce = Variable containing:
 0.1394  0.1846  0.0343  0.3605  0.0703  0.0813  0.0740  0.0556
 0.0838  0.2394  0.0433  0.1672  0.1665  0.2270  0.0213  0.0515
 0.5359  0.0405  0.0376  0.0414  0.2265  0.0578  0.0378  0.0225
 0.6881  0.0350  0.0377  0.0251  0.0192  0.1015  0.0602  0.0333
 0.0337  0.0287  0.0318  0.0290  0.0779  0.0266  0.0220  0.7503
 0.3629  0.1775  0.1022  0.1098  0.0642  0.0747  0.0552  0.0535
 0.1293  0.1440  0.0483  0.1077  0.1083  0.0731  0.0813  0.3080
 0.0643  0.0429  0.0464  0.0466  0.0444  0.0162  0.0257  0.7135
 0.0627  0.0572  0.1067  0.0822  0.0563  0.4834  0.0363  0.1152
 0.4400  0.0501  0.0623  0.0346  0.0354  0.2029  0.0342  0.1404
 0.7638  0.0338  0.0321  0.0300  0.0492  0.0423  0.0271  0.0216
 0.0584  0.0362  0.0665  0.0546  0.2134  0.2129  0.0579  0.3001
 0.1676  0.0882  0.1132  0.1302  0.0467  0.0830  0.3079  0.0633
 0.0727  0.0423  0.0524  0.0470  0.4435  0.0228  0.0520  0.2672
[torch.cuda.FloatTensor of size 14x8 (GPU 2)]

2019-11-03 08:27:19,662 train 000 1.251258e-02 -2.441035
2019-11-03 08:30:04,140 train 050 9.524535e-03 -0.958804
2019-11-03 08:32:50,455 train 100 9.603637e-03 -0.887435
2019-11-03 08:35:35,585 train 150 9.480726e-03 -1.288538
2019-11-03 08:38:21,028 train 200 9.428177e-03 -50.161010
2019-11-03 08:41:07,080 train 250 9.340586e-03 -40.611151
2019-11-03 08:43:52,962 train 300 9.388500e-03 -34.047268
2019-11-03 08:46:38,683 train 350 9.441641e-03 -29.347392
2019-11-03 08:49:23,573 train 400 9.432032e-03 -25.787630
2019-11-03 08:52:09,710 train 450 9.398455e-03 -23.018754
2019-11-03 08:54:54,329 train 500 9.360613e-03 -20.915755
2019-11-03 08:57:40,120 train 550 9.420351e-03 -19.406670
2019-11-03 09:00:25,334 train 600 9.403930e-03 -17.899130
2019-11-03 09:03:11,472 train 650 9.394181e-03 -16.637294
2019-11-03 09:05:57,420 train 700 9.372669e-03 -15.520671
2019-11-03 09:08:43,399 train 750 9.407650e-03 -14.574803
2019-11-03 09:11:28,654 train 800 9.411712e-03 -14.312525
2019-11-03 09:14:14,958 train 850 9.393183e-03 -13.597871
2019-11-03 09:17:01,058 train 900 9.393436e-03 -15.222163
2019-11-03 09:19:47,013 train 950 9.389477e-03 -14.467977
2019-11-03 09:22:32,967 train 1000 9.384246e-03 -13.822242
2019-11-03 09:25:17,254 train 1050 9.387441e-03 -13.220574
