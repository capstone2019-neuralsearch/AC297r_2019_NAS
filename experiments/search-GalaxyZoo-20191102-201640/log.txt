2019-11-02 20:16:40,903 gpu device = 0
2019-11-02 20:16:40,904 args = Namespace(arch_learning_rate=0.01, arch_weight_decay=1e-06, batch_size=16, cutout=False, cutout_length=16, data='../data', dataset='galaxy-zoo', drop_path_prob=0.3, epochs=50, gpu=0, grad_clip=5, gz_regression=False, init_channels=16, layers=8, learning_rate=0.001, learning_rate_min=0.0001, model_path='saved_models', momentum=0.9, optimizer='Adam', report_freq=50, save='search-GZ_FC2_2x2-20191102-201640', seed=2, train_portion=0.8, unrolled=True, weight_decay=1e-08)
2019-11-02 20:16:44,476 param size = 3.278741MB
2019-11-02 20:16:44,490 epoch 0 lr 1.000000e-03
2019-11-02 20:16:44,491 genotype = Genotype(normal=[('dil_conv_3x3', 0), ('skip_connect', 1), ('max_pool_2x2', 0), ('dil_conv_3x3', 1), ('avg_pool_3x3', 3), ('dil_conv_3x3', 0), ('avg_pool_3x3', 4), ('max_pool_2x2', 0)], normal_concat=range(2, 6), reduce=[('dil_conv_3x3', 1), ('max_pool_3x3', 0), ('max_pool_3x3', 2), ('skip_connect', 0), ('sep_conv_5x5', 1), ('skip_connect', 0), ('dil_conv_3x3', 1), ('skip_connect', 3)], reduce_concat=range(2, 6))
2019-11-02 20:16:44,493 
alphas_normal = Variable containing:
 0.1111  0.1112  0.1110  0.1110  0.1111  0.1111  0.1110  0.1113  0.1111
 0.1110  0.1113  0.1110  0.1111  0.1110  0.1112  0.1111  0.1112  0.1110
 0.1111  0.1112  0.1113  0.1112  0.1110  0.1112  0.1109  0.1109  0.1111
 0.1111  0.1108  0.1110  0.1111  0.1111  0.1112  0.1111  0.1113  0.1112
 0.1111  0.1110  0.1112  0.1112  0.1111  0.1112  0.1111  0.1109  0.1111
 0.1110  0.1111  0.1111  0.1111  0.1111  0.1111  0.1112  0.1113  0.1110
 0.1112  0.1111  0.1111  0.1111  0.1111  0.1110  0.1111  0.1111  0.1111
 0.1111  0.1110  0.1112  0.1112  0.1112  0.1110  0.1113  0.1110  0.1111
 0.1111  0.1111  0.1111  0.1112  0.1113  0.1111  0.1110  0.1109  0.1113
 0.1111  0.1109  0.1112  0.1111  0.1112  0.1111  0.1112  0.1110  0.1112
 0.1112  0.1110  0.1112  0.1109  0.1112  0.1112  0.1111  0.1110  0.1112
 0.1110  0.1112  0.1111  0.1111  0.1112  0.1110  0.1112  0.1110  0.1112
 0.1111  0.1111  0.1112  0.1112  0.1111  0.1111  0.1110  0.1111  0.1112
 0.1111  0.1111  0.1112  0.1110  0.1113  0.1110  0.1111  0.1111  0.1111
[torch.cuda.FloatTensor of size 14x9 (GPU 0)]

2019-11-02 20:16:44,495 
alphas_reduce = Variable containing:
 0.1111  0.1111  0.1110  0.1112  0.1112  0.1110  0.1111  0.1112  0.1111
 0.1109  0.1111  0.1112  0.1111  0.1111  0.1111  0.1111  0.1113  0.1111
 0.1112  0.1112  0.1111  0.1111  0.1110  0.1112  0.1111  0.1111  0.1111
 0.1111  0.1111  0.1112  0.1111  0.1110  0.1112  0.1112  0.1110  0.1112
 0.1111  0.1110  0.1111  0.1113  0.1111  0.1112  0.1110  0.1111  0.1112
 0.1112  0.1114  0.1111  0.1111  0.1110  0.1108  0.1112  0.1112  0.1111
 0.1111  0.1110  0.1111  0.1111  0.1111  0.1109  0.1114  0.1111  0.1112
 0.1111  0.1111  0.1112  0.1112  0.1109  0.1112  0.1111  0.1112  0.1109
 0.1108  0.1109  0.1111  0.1111  0.1113  0.1112  0.1112  0.1111  0.1112
 0.1113  0.1111  0.1112  0.1111  0.1110  0.1111  0.1112  0.1111  0.1110
 0.1110  0.1111  0.1109  0.1111  0.1112  0.1111  0.1112  0.1113  0.1111
 0.1112  0.1110  0.1109  0.1111  0.1112  0.1112  0.1111  0.1112  0.1111
 0.1112  0.1113  0.1111  0.1112  0.1111  0.1109  0.1110  0.1109  0.1112
 0.1112  0.1112  0.1111  0.1111  0.1111  0.1112  0.1110  0.1110  0.1112
[torch.cuda.FloatTensor of size 14x9 (GPU 0)]

2019-11-02 20:16:51,874 train 000 4.257381e-02 -7.218439
2019-11-02 20:19:41,488 train 050 3.058931e-02 -54.689776
2019-11-02 20:22:31,219 train 100 2.718397e-02 -29.018033
2019-11-02 20:25:22,134 train 150 2.558624e-02 -19.813441
2019-11-02 20:28:13,634 train 200 2.457392e-02 -15.281572
2019-11-02 20:31:05,335 train 250 2.395507e-02 -12.479972
2019-11-02 20:33:56,704 train 300 2.349647e-02 -10.623974
2019-11-02 20:36:48,046 train 350 2.317255e-02 -9.309757
2019-11-02 20:39:38,985 train 400 2.278305e-02 -8.563084
2019-11-02 20:42:30,629 train 450 2.241414e-02 -7.720367
2019-11-02 20:45:22,575 train 500 2.211696e-02 -7.137477
2019-11-02 20:48:13,368 train 550 2.181374e-02 -6.670773
2019-11-02 20:51:03,747 train 600 2.164778e-02 -6.263797
2019-11-02 20:53:54,771 train 650 2.139920e-02 -5.870785
2019-11-02 20:56:45,599 train 700 2.110423e-02 -5.696383
2019-11-02 20:59:36,164 train 750 2.084774e-02 -5.730285
2019-11-02 21:02:28,118 train 800 2.062946e-02 -5.497812
2019-11-02 21:05:19,509 train 850 2.036306e-02 -5.252964
2019-11-02 21:08:11,620 train 900 2.013923e-02 -5.029249
2019-11-02 21:11:02,685 train 950 1.999295e-02 -4.819859
2019-11-02 21:13:54,866 train 1000 1.983275e-02 -4.815629
2019-11-02 21:16:46,308 train 1050 1.959969e-02 -4.653027
2019-11-02 21:19:37,751 train 1100 1.943612e-02 -4.698264
2019-11-02 21:22:29,090 train 1150 1.926874e-02 -4.568648
2019-11-02 21:25:20,924 train 1200 1.912171e-02 -4.435864
2019-11-02 21:28:13,065 train 1250 1.897233e-02 -4.298353
2019-11-02 21:31:03,751 train 1300 1.883315e-02 -4.208648
2019-11-02 21:33:55,704 train 1350 1.870777e-02 -4.146970
2019-11-02 21:36:47,103 train 1400 1.859569e-02 -4.100571
2019-11-02 21:39:39,119 train 1450 1.847495e-02 -4.003801
2019-11-02 21:42:30,602 train 1500 1.836309e-02 -3.955144
2019-11-02 21:45:22,112 train 1550 1.825481e-02 -3.863525
2019-11-02 21:48:13,573 train 1600 1.815394e-02 -3.791008
2019-11-02 21:51:04,834 train 1650 1.804405e-02 -3.718183
2019-11-02 21:53:56,505 train 1700 1.794821e-02 -3.649824
2019-11-02 21:56:48,364 train 1750 1.785779e-02 -3.635397
2019-11-02 21:59:41,617 train 1800 1.776434e-02 -3.572132
2019-11-02 22:02:32,652 train 1850 1.769485e-02 -3.505766
2019-11-02 22:05:22,920 train 1900 1.760545e-02 -3.478571
2019-11-02 22:08:13,928 train 1950 1.751983e-02 -3.416425
2019-11-02 22:11:04,168 train 2000 1.745768e-02 -4.024828
2019-11-02 22:13:55,505 train 2050 1.739149e-02 -3.958561
2019-11-02 22:16:45,936 train 2100 1.730658e-02 -3.888678
2019-11-02 22:19:37,681 train 2150 1.722924e-02 -3.823134
2019-11-02 22:22:29,231 train 2200 1.715463e-02 -3.778320
2019-11-02 22:25:20,947 train 2250 1.708110e-02 -3.768849
2019-11-02 22:28:11,982 train 2300 1.701141e-02 -3.901191
2019-11-02 22:31:03,722 train 2350 1.693424e-02 -3.837698
2019-11-02 22:33:54,720 train 2400 1.686473e-02 -3.800710
2019-11-02 22:36:46,471 train 2450 1.679000e-02 -3.760535
2019-11-02 22:39:37,866 train 2500 1.672098e-02 -3.723092
2019-11-02 22:42:29,603 train 2550 1.666038e-02 -3.692810
2019-11-02 22:45:18,842 train 2600 1.660238e-02 -3.653563
2019-11-02 22:48:08,171 train 2650 1.655235e-02 -3.656076
2019-11-02 22:50:58,596 train 2700 1.650350e-02 -3.608149
2019-11-02 22:53:48,591 train 2750 1.645276e-02 -3.670974
2019-11-02 22:56:40,790 train 2800 1.640559e-02 -3.630517
2019-11-02 22:59:31,998 train 2850 1.635728e-02 -3.583037
2019-11-02 23:02:23,292 train 2900 1.631086e-02 -3.547474
2019-11-02 23:05:14,816 train 2950 1.626784e-02 -3.541598
2019-11-02 23:08:06,108 train 3000 1.622344e-02 -3.659855
2019-11-02 23:10:57,644 train 3050 1.618026e-02 -3.626860
2019-11-02 23:12:36,125 training loss; R2: 1.615936e-02 -3.624348
2019-11-02 23:12:36,627 valid 000 1.270074e-02 -0.761442
2019-11-02 23:12:46,254 valid 050 1.225527e-02 -1.208143
2019-11-02 23:12:55,837 valid 100 1.219115e-02 -1.191103
2019-11-02 23:13:05,474 valid 150 1.232104e-02 -1.160128
2019-11-02 23:13:15,101 valid 200 1.230126e-02 -1.280605
2019-11-02 23:13:24,720 valid 250 1.227393e-02 -1.245232
2019-11-02 23:13:34,291 valid 300 1.226607e-02 -1.267068
2019-11-02 23:13:43,866 valid 350 1.223421e-02 -1.266782
2019-11-02 23:13:53,434 valid 400 1.215888e-02 -1.223935
2019-11-02 23:14:03,038 valid 450 1.217964e-02 -1.202216
2019-11-02 23:14:12,606 valid 500 1.222121e-02 -1.297797
2019-11-02 23:14:22,203 valid 550 1.217344e-02 -1.294821
2019-11-02 23:14:31,801 valid 600 1.220273e-02 -1.367166
2019-11-02 23:14:41,385 valid 650 1.218886e-02 -5.771151
2019-11-02 23:14:50,955 valid 700 1.217535e-02 -5.438667
2019-11-02 23:15:00,529 valid 750 1.214361e-02 -5.170190
2019-11-02 23:15:04,799 validation loss; R2: 1.216993e-02 -5.077235
2019-11-02 23:15:04,940 epoch 1 lr 9.991120e-04
2019-11-02 23:15:04,941 genotype = Genotype(normal=[('max_pool_3x3', 0), ('dil_conv_5x5', 1), ('max_pool_3x3', 0), ('max_pool_2x2', 2), ('max_pool_3x3', 0), ('max_pool_2x2', 1), ('max_pool_3x3', 0), ('dil_conv_5x5', 3)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 1), ('max_pool_3x3', 0), ('max_pool_3x3', 1), ('max_pool_2x2', 2), ('max_pool_3x3', 1), ('dil_conv_5x5', 3), ('dil_conv_5x5', 2), ('max_pool_3x3', 0)], reduce_concat=range(2, 6))
2019-11-02 23:15:04,943 
alphas_normal = Variable containing:
 0.1520  0.0605  0.1305  0.2275  0.0227  0.0935  0.0804  0.0986  0.1343
 0.3970  0.0840  0.1007  0.0469  0.0371  0.1089  0.0254  0.0890  0.1111
 0.1122  0.0545  0.1089  0.1728  0.0286  0.1336  0.1042  0.1461  0.1391
 0.6049  0.0578  0.0813  0.0525  0.0316  0.0276  0.0360  0.0336  0.0746
 0.4768  0.0922  0.1205  0.0823  0.0335  0.0366  0.0404  0.0546  0.0632
 0.0938  0.0429  0.1324  0.4735  0.0258  0.0367  0.0546  0.0650  0.0753
 0.4679  0.0723  0.1354  0.0968  0.0428  0.0410  0.0335  0.0625  0.0480
 0.4730  0.0572  0.0821  0.0871  0.0295  0.0575  0.0363  0.0781  0.0993
 0.4997  0.0421  0.0649  0.0685  0.0301  0.0341  0.0473  0.1196  0.0936
 0.1846  0.0816  0.1517  0.1609  0.0495  0.0976  0.0762  0.0372  0.1605
 0.3030  0.1342  0.1248  0.0662  0.0723  0.0881  0.0415  0.0696  0.1004
 0.4221  0.0838  0.0692  0.0671  0.0329  0.0709  0.0843  0.0980  0.0718
 0.4752  0.0519  0.0644  0.0571  0.0356  0.0431  0.0827  0.0557  0.1343
 0.5125  0.0381  0.0541  0.0582  0.0286  0.0392  0.0463  0.0900  0.1330
[torch.cuda.FloatTensor of size 14x9 (GPU 0)]

2019-11-02 23:15:04,944 
alphas_reduce = Variable containing:
 0.1095  0.0538  0.1718  0.2391  0.0473  0.1418  0.0871  0.0879  0.0618
 0.0517  0.0879  0.1202  0.4316  0.0371  0.0836  0.0476  0.0471  0.0932
 0.0941  0.1185  0.1224  0.1724  0.0463  0.0781  0.1720  0.0722  0.1240
 0.0946  0.0665  0.0921  0.2316  0.0439  0.1594  0.1658  0.0542  0.0919
 0.0919  0.0693  0.2315  0.2069  0.0308  0.0529  0.0645  0.0579  0.1943
 0.1156  0.1326  0.1358  0.1744  0.0664  0.0663  0.0954  0.0763  0.1373
 0.1336  0.1426  0.1002  0.2131  0.0583  0.0717  0.0903  0.0812  0.1091
 0.1169  0.0838  0.1026  0.1168  0.0419  0.1497  0.1125  0.1008  0.1750
 0.1305  0.0760  0.1143  0.1117  0.0471  0.0729  0.0989  0.1394  0.2093
 0.0992  0.0942  0.1344  0.2115  0.0657  0.1012  0.1107  0.0756  0.1075
 0.0870  0.0924  0.0824  0.1625  0.0543  0.1401  0.2038  0.0755  0.1021
 0.1099  0.0785  0.1145  0.1940  0.0427  0.0695  0.0604  0.0831  0.2473
 0.1599  0.0787  0.1422  0.1959  0.0488  0.0725  0.0658  0.0965  0.1396
 0.1518  0.0816  0.1416  0.1406  0.0575  0.0502  0.1420  0.1332  0.1014
[torch.cuda.FloatTensor of size 14x9 (GPU 0)]

2019-11-02 23:15:09,513 train 000 1.354997e-02 -1.752912
2019-11-02 23:17:59,987 train 050 1.308112e-02 -1.385088
2019-11-02 23:20:51,117 train 100 1.339758e-02 -1.570585
2019-11-02 23:23:41,942 train 150 1.343310e-02 -1.680767
2019-11-02 23:26:33,089 train 200 1.330545e-02 -1.947196
2019-11-02 23:29:24,536 train 250 1.334072e-02 -1.950935
2019-11-02 23:32:15,962 train 300 1.339074e-02 -1.990690
2019-11-02 23:35:07,442 train 350 1.342682e-02 -1.915249
2019-11-02 23:37:59,149 train 400 1.342187e-02 -1.902558
2019-11-02 23:40:50,556 train 450 1.347654e-02 -1.853422
2019-11-02 23:43:41,541 train 500 1.346978e-02 -1.902522
2019-11-02 23:46:32,868 train 550 1.340551e-02 -1.840752
2019-11-02 23:49:24,499 train 600 1.341091e-02 -1.804223
2019-11-02 23:52:15,332 train 650 1.344352e-02 -1.749207
2019-11-02 23:55:05,534 train 700 1.341774e-02 -1.728895
2019-11-02 23:57:56,137 train 750 1.337054e-02 -1.708648
2019-11-03 00:00:47,550 train 800 1.337016e-02 -1.814300
2019-11-03 00:03:38,065 train 850 1.335076e-02 -1.770764
2019-11-03 00:06:29,633 train 900 1.339270e-02 -1.760880
2019-11-03 00:09:20,894 train 950 1.338085e-02 -1.713505
2019-11-03 00:12:12,317 train 1000 1.335174e-02 -1.734745
2019-11-03 00:15:04,198 train 1050 1.334620e-02 -1.716139
2019-11-03 00:17:57,163 train 1100 1.334532e-02 -1.707022
2019-11-03 00:20:48,367 train 1150 1.334583e-02 -1.696621
2019-11-03 00:23:40,146 train 1200 1.332973e-02 -1.670448
2019-11-03 00:26:30,273 train 1250 1.331659e-02 -1.654031
2019-11-03 00:29:21,507 train 1300 1.329481e-02 -1.708803
2019-11-03 00:32:13,353 train 1350 1.325454e-02 -2.053723
2019-11-03 00:35:04,878 train 1400 1.327772e-02 -2.105517
2019-11-03 00:37:55,906 train 1450 1.326976e-02 -2.095068
2019-11-03 00:40:47,186 train 1500 1.325165e-02 -2.061790
2019-11-03 00:43:37,137 train 1550 1.323021e-02 -2.028898
2019-11-03 00:46:27,910 train 1600 1.320315e-02 -2.004557
2019-11-03 00:49:19,053 train 1650 1.317582e-02 -2.091299
2019-11-03 00:52:08,873 train 1700 1.316463e-02 -2.128014
2019-11-03 00:55:00,469 train 1750 1.314766e-02 -2.109781
2019-11-03 00:57:52,437 train 1800 1.313520e-02 -2.129124
2019-11-03 01:00:43,996 train 1850 1.310663e-02 -2.216430
2019-11-03 01:03:34,584 train 1900 1.308473e-02 -2.199082
2019-11-03 01:06:26,294 train 1950 1.305665e-02 -2.170703
2019-11-03 01:09:17,282 train 2000 1.304213e-02 -2.142223
2019-11-03 01:12:09,520 train 2050 1.304129e-02 -2.118981
2019-11-03 01:15:01,153 train 2100 1.302501e-02 -2.509627
2019-11-03 01:17:51,440 train 2150 1.300250e-02 -2.477916
2019-11-03 01:20:43,058 train 2200 1.298414e-02 -2.460516
2019-11-03 01:23:34,942 train 2250 1.297072e-02 -2.544668
2019-11-03 01:26:26,745 train 2300 1.295748e-02 -2.531992
2019-11-03 01:29:18,067 train 2350 1.295149e-02 -2.509854
2019-11-03 01:32:09,606 train 2400 1.294218e-02 -2.487149
2019-11-03 01:35:01,035 train 2450 1.292561e-02 -2.455005
2019-11-03 01:37:52,385 train 2500 1.292491e-02 -2.475481
2019-11-03 01:40:43,585 train 2550 1.290896e-02 -2.448495
2019-11-03 01:43:34,554 train 2600 1.289688e-02 -2.489870
2019-11-03 01:46:25,879 train 2650 1.288660e-02 -2.466944
2019-11-03 01:49:17,205 train 2700 1.287576e-02 -2.456378
2019-11-03 01:52:08,481 train 2750 1.286575e-02 -2.434519
2019-11-03 01:55:00,049 train 2800 1.285642e-02 -2.413845
2019-11-03 01:57:51,069 train 2850 1.284488e-02 -2.405331
2019-11-03 01:00:42,710 train 2900 1.283370e-02 -2.382296
2019-11-03 01:03:34,259 train 2950 1.282021e-02 -2.366703
2019-11-03 01:06:24,925 train 3000 1.280456e-02 -2.348192
2019-11-03 01:09:16,517 train 3050 1.278280e-02 -2.333177
2019-11-03 01:10:52,542 training loss; R2: 1.277895e-02 -2.320085
2019-11-03 01:10:53,020 valid 000 1.502905e-02 -0.547303
2019-11-03 01:11:02,586 valid 050 1.339375e-02 -0.894071
2019-11-03 01:11:12,095 valid 100 1.297457e-02 -0.897715
2019-11-03 01:11:21,685 valid 150 1.298546e-02 -0.933225
2019-11-03 01:11:31,279 valid 200 1.286183e-02 -0.926548
2019-11-03 01:11:40,850 valid 250 1.284103e-02 -0.978896
2019-11-03 01:11:50,416 valid 300 1.274609e-02 -0.963710
2019-11-03 01:11:59,992 valid 350 1.279465e-02 -2.497235
2019-11-03 01:12:09,530 valid 400 1.277230e-02 -2.443999
2019-11-03 01:12:19,085 valid 450 1.280599e-02 -2.261740
2019-11-03 01:12:28,641 valid 500 1.278985e-02 -2.184579
2019-11-03 01:12:38,200 valid 550 1.277134e-02 -2.058869
2019-11-03 01:12:47,726 valid 600 1.280599e-02 -2.275555
2019-11-03 01:12:57,294 valid 650 1.283455e-02 -2.179848
2019-11-03 01:13:06,858 valid 700 1.279256e-02 -2.078799
2019-11-03 01:13:16,409 valid 750 1.275623e-02 -2.039783
2019-11-03 01:13:20,031 validation loss; R2: 1.275861e-02 -2.006224
2019-11-03 01:13:20,242 epoch 2 lr 9.964516e-04
2019-11-03 01:13:20,243 genotype = Genotype(normal=[('max_pool_3x3', 0), ('max_pool_2x2', 1), ('max_pool_3x3', 0), ('max_pool_2x2', 2), ('max_pool_3x3', 0), ('dil_conv_3x3', 3), ('max_pool_3x3', 0), ('max_pool_2x2', 1)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 1), ('max_pool_3x3', 0), ('max_pool_3x3', 2), ('sep_conv_5x5', 0), ('dil_conv_5x5', 3), ('skip_connect', 1), ('max_pool_3x3', 3), ('dil_conv_5x5', 2)], reduce_concat=range(2, 6))
2019-11-03 01:13:20,244 
alphas_normal = Variable containing:
 0.0667  0.1322  0.1606  0.2679  0.0290  0.0792  0.0608  0.0832  0.1205
 0.4517  0.0639  0.1165  0.0610  0.0269  0.1115  0.0290  0.0385  0.1010
 0.0698  0.0681  0.1574  0.2267  0.0346  0.1671  0.0631  0.1039  0.1092
 0.5931  0.0368  0.1006  0.0717  0.0239  0.0365  0.0372  0.0296  0.0706
 0.4163  0.1037  0.1698  0.0953  0.0304  0.0648  0.0249  0.0442  0.0507
 0.0483  0.0302  0.0744  0.6348  0.0226  0.0283  0.0516  0.0622  0.0477
 0.6171  0.0271  0.0693  0.1056  0.0225  0.0303  0.0188  0.0470  0.0623
 0.3894  0.0595  0.0914  0.1263  0.0297  0.0522  0.0347  0.0912  0.1258
 0.4373  0.0384  0.0707  0.0930  0.0273  0.0354  0.0639  0.1313  0.1028
 0.0711  0.0735  0.2121  0.2128  0.0498  0.1223  0.1068  0.0320  0.1196
 0.2877  0.0887  0.1809  0.1643  0.0560  0.0619  0.0343  0.0572  0.0691
 0.3568  0.0916  0.0955  0.0975  0.0338  0.0908  0.0775  0.1032  0.0533
 0.4190  0.0605  0.0897  0.0898  0.0358  0.0410  0.0734  0.0582  0.1327
 0.4685  0.0497  0.0865  0.1132  0.0354  0.0218  0.0186  0.0580  0.1483
[torch.cuda.FloatTensor of size 14x9 (GPU 0)]

2019-11-03 01:13:20,246 
alphas_reduce = Variable containing:
 0.0999  0.0520  0.1365  0.3032  0.0341  0.1741  0.0765  0.0790  0.0447
 0.0439  0.0613  0.0605  0.4972  0.0343  0.1268  0.0592  0.0380  0.0788
 0.1398  0.0681  0.0950  0.1625  0.0438  0.0518  0.3086  0.0553  0.0751
 0.1523  0.1150  0.0772  0.2082  0.0440  0.1188  0.1929  0.0392  0.0524
 0.0561  0.0342  0.1944  0.3163  0.0276  0.0467  0.0553  0.0558  0.2136
 0.2718  0.0868  0.1184  0.2047  0.0562  0.0488  0.0553  0.0633  0.0946
 0.1617  0.2350  0.0824  0.1384  0.0604  0.0841  0.0975  0.0693  0.0714
 0.1293  0.0551  0.1094  0.2229  0.0417  0.1288  0.0544  0.0868  0.1716
 0.0773  0.0494  0.0929  0.1564  0.0534  0.0451  0.0754  0.0926  0.3576
 0.1323  0.0786  0.0917  0.1825  0.0541  0.1195  0.1742  0.0992  0.0681
 0.1364  0.1252  0.0799  0.1078  0.0648  0.1047  0.2268  0.0932  0.0611
 0.0835  0.0475  0.0879  0.2122  0.0467  0.0727  0.0571  0.0632  0.3293
 0.0740  0.0501  0.1188  0.4365  0.0668  0.0567  0.0484  0.0453  0.1036
 0.1089  0.0607  0.1229  0.2051  0.0794  0.0380  0.0967  0.1203  0.1681
[torch.cuda.FloatTensor of size 14x9 (GPU 0)]

2019-11-03 01:13:23,950 train 000 1.262891e-02 -1.019402
2019-11-03 01:16:14,473 train 050 1.237622e-02 -1.652589
2019-11-03 01:19:05,532 train 100 1.220223e-02 -1.609770
2019-11-03 01:21:56,581 train 150 1.230418e-02 -1.443346
2019-11-03 01:24:47,714 train 200 1.233920e-02 -1.357790
2019-11-03 01:27:38,009 train 250 1.218489e-02 -1.344694
2019-11-03 01:30:28,847 train 300 1.215225e-02 -1.324655
2019-11-03 01:33:20,438 train 350 1.221440e-02 -1.342260
2019-11-03 01:36:11,357 train 400 1.218248e-02 -1.361900
2019-11-03 01:39:02,005 train 450 1.217366e-02 -1.645272
2019-11-03 01:41:53,148 train 500 1.211869e-02 -1.756152
2019-11-03 01:44:44,319 train 550 1.210232e-02 -1.707296
2019-11-03 01:47:35,645 train 600 1.209056e-02 -1.668876
2019-11-03 01:50:26,715 train 650 1.204608e-02 -1.711917
2019-11-03 01:53:17,858 train 700 1.204272e-02 -1.886397
2019-11-03 01:56:08,936 train 750 1.206513e-02 -2.374995
2019-11-03 01:59:00,425 train 800 1.208817e-02 -2.312078
2019-11-03 02:01:51,351 train 850 1.204406e-02 -2.257024
2019-11-03 02:04:42,891 train 900 1.204733e-02 -2.222175
2019-11-03 02:07:34,241 train 950 1.204722e-02 -2.205448
2019-11-03 02:10:26,191 train 1000 1.206072e-02 -2.284780
2019-11-03 02:13:16,911 train 1050 1.205670e-02 -2.236242
2019-11-03 02:16:06,462 train 1100 1.205060e-02 -2.300955
2019-11-03 02:18:54,988 train 1150 1.206510e-02 -2.287900
2019-11-03 02:21:45,959 train 1200 1.205918e-02 -2.232906
2019-11-03 02:24:37,501 train 1250 1.203630e-02 -2.291822
2019-11-03 02:27:28,416 train 1300 1.202538e-02 -2.253343
2019-11-03 02:30:19,865 train 1350 1.202454e-02 -2.205809
2019-11-03 02:33:11,001 train 1400 1.201087e-02 -2.171201
2019-11-03 02:36:01,840 train 1450 1.200591e-02 -2.175426
2019-11-03 02:38:53,270 train 1500 1.200984e-02 -2.159490
2019-11-03 02:41:43,897 train 1550 1.203388e-02 -2.192675
2019-11-03 02:44:36,394 train 1600 1.203984e-02 -2.164985
2019-11-03 02:47:25,859 train 1650 1.202557e-02 -2.134224
2019-11-03 02:50:15,506 train 1700 1.203272e-02 -2.206205
2019-11-03 02:53:06,483 train 1750 1.201435e-02 -2.185925
2019-11-03 02:55:57,035 train 1800 1.201278e-02 -2.155219
2019-11-03 02:58:47,385 train 1850 1.200634e-02 -2.138566
2019-11-03 03:01:37,367 train 1900 1.200932e-02 -2.116409
2019-11-03 03:04:29,249 train 1950 1.200501e-02 -2.092322
2019-11-03 03:07:21,738 train 2000 1.200146e-02 -2.060602
2019-11-03 03:10:11,895 train 2050 1.199812e-02 -2.060443
2019-11-03 03:13:03,580 train 2100 1.199640e-02 -2.044733
2019-11-03 03:15:54,520 train 2150 1.200199e-02 -2.106752
2019-11-03 03:18:45,620 train 2200 1.200440e-02 -2.097547
2019-11-03 03:21:35,962 train 2250 1.200005e-02 -2.090082
2019-11-03 03:24:27,484 train 2300 1.199092e-02 -2.100610
2019-11-03 03:27:18,392 train 2350 1.197504e-02 -2.090581
2019-11-03 03:30:09,512 train 2400 1.197922e-02 -2.065273
2019-11-03 03:32:58,937 train 2450 1.196441e-02 -2.055827
2019-11-03 03:35:48,277 train 2500 1.194897e-02 -2.047196
2019-11-03 03:38:38,101 train 2550 1.195215e-02 -2.031655
2019-11-03 03:41:29,763 train 2600 1.192945e-02 -2.019157
2019-11-03 03:44:21,253 train 2650 1.191966e-02 -2.064095
2019-11-03 03:47:12,787 train 2700 1.190982e-02 -2.070704
2019-11-03 03:50:04,194 train 2750 1.190163e-02 -2.060482
2019-11-03 03:52:57,296 train 2800 1.188652e-02 -2.044417
2019-11-03 03:55:47,961 train 2850 1.188552e-02 -2.024618
2019-11-03 03:58:39,650 train 2900 1.187054e-02 -2.006691
2019-11-03 04:01:30,361 train 2950 1.186119e-02 -1.993795
2019-11-03 04:04:20,730 train 3000 1.185884e-02 -1.979401
2019-11-03 04:07:11,655 train 3050 1.184388e-02 -1.984213
2019-11-03 04:08:47,252 training loss; R2: 1.183490e-02 -1.976667
2019-11-03 04:08:47,704 valid 000 9.859396e-03 0.035892
2019-11-03 04:08:57,326 valid 050 1.067925e-02 -4.615767
2019-11-03 04:09:06,865 valid 100 1.078044e-02 -2.965094
2019-11-03 04:09:16,443 valid 150 1.069331e-02 -2.314248
2019-11-03 04:09:25,972 valid 200 1.086317e-02 -2.071865
2019-11-03 04:09:35,590 valid 250 1.088897e-02 -1.887061
2019-11-03 04:09:45,174 valid 300 1.084203e-02 -1.739787
2019-11-03 04:09:54,763 valid 350 1.090616e-02 -1.766035
2019-11-03 04:10:04,316 valid 400 1.091088e-02 -1.689089
2019-11-03 04:10:13,895 valid 450 1.096530e-02 -1.678700
2019-11-03 04:10:23,433 valid 500 1.091792e-02 -1.604225
2019-11-03 04:10:33,012 valid 550 1.096619e-02 -1.562084
2019-11-03 04:10:42,563 valid 600 1.095031e-02 -1.560632
2019-11-03 04:10:52,127 valid 650 1.094617e-02 -1.507216
2019-11-03 04:11:01,661 valid 700 1.093234e-02 -1.478217
2019-11-03 04:11:11,228 valid 750 1.092295e-02 -1.462616
2019-11-03 04:11:14,860 validation loss; R2: 1.093912e-02 -1.446379
2019-11-03 04:11:15,013 epoch 3 lr 9.920293e-04
2019-11-03 04:11:15,015 genotype = Genotype(normal=[('max_pool_3x3', 0), ('max_pool_2x2', 1), ('max_pool_2x2', 0), ('max_pool_2x2', 2), ('max_pool_3x3', 0), ('dil_conv_3x3', 3), ('max_pool_3x3', 1), ('max_pool_3x3', 0)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 1), ('max_pool_3x3', 0), ('max_pool_3x3', 2), ('sep_conv_5x5', 0), ('dil_conv_5x5', 3), ('dil_conv_5x5', 2), ('dil_conv_5x5', 2), ('max_pool_3x3', 3)], reduce_concat=range(2, 6))
2019-11-03 04:11:15,017 
alphas_normal = Variable containing:
 0.0775  0.1417  0.1684  0.1862  0.0274  0.0680  0.0456  0.1016  0.1836
 0.5394  0.0579  0.1194  0.0567  0.0282  0.0739  0.0181  0.0322  0.0743
 0.1212  0.0670  0.1860  0.1775  0.0318  0.1653  0.0571  0.1025  0.0917
 0.6847  0.0278  0.0854  0.0679  0.0216  0.0255  0.0226  0.0235  0.0409
 0.6207  0.0614  0.1235  0.0671  0.0248  0.0254  0.0153  0.0367  0.0251
 0.0492  0.0265  0.0501  0.6320  0.0214  0.0261  0.0602  0.0891  0.0454
 0.6015  0.0305  0.0561  0.0944  0.0272  0.0324  0.0128  0.0726  0.0724
 0.4696  0.0586  0.0793  0.1098  0.0371  0.0387  0.0274  0.0880  0.0914
 0.5548  0.0314  0.0427  0.0623  0.0235  0.0229  0.0360  0.1237  0.1027
 0.1928  0.0469  0.1208  0.2208  0.0335  0.1161  0.1097  0.0377  0.1217
 0.1673  0.0632  0.2116  0.2974  0.0512  0.0624  0.0303  0.0541  0.0625
 0.5909  0.0474  0.0618  0.0784  0.0301  0.0506  0.0246  0.0822  0.0340
 0.5824  0.0360  0.0517  0.0654  0.0269  0.0270  0.0340  0.0578  0.1189
 0.5150  0.0402  0.0796  0.1302  0.0328  0.0118  0.0095  0.0330  0.1481
[torch.cuda.FloatTensor of size 14x9 (GPU 0)]

2019-11-03 04:11:15,019 
alphas_reduce = Variable containing:
 0.0727  0.0699  0.1052  0.2597  0.0320  0.2012  0.0911  0.1054  0.0628
 0.0350  0.0457  0.0443  0.5992  0.0284  0.0703  0.0500  0.0447  0.0825
 0.1932  0.1023  0.0734  0.1152  0.0357  0.0860  0.2490  0.0640  0.0811
 0.1480  0.1918  0.0693  0.2138  0.0482  0.0924  0.1571  0.0354  0.0440
 0.0528  0.0360  0.1618  0.2766  0.0265  0.0352  0.0795  0.0718  0.2599
 0.3808  0.1047  0.0727  0.1209  0.0439  0.0494  0.0447  0.0734  0.1096
 0.1637  0.3024  0.0680  0.1015  0.0547  0.0750  0.0931  0.0826  0.0588
 0.0776  0.0471  0.0967  0.1928  0.0360  0.0993  0.0668  0.0715  0.3123
 0.0502  0.0412  0.0699  0.1231  0.0402  0.0284  0.0608  0.1420  0.4442
 0.1799  0.1610  0.0615  0.0959  0.0479  0.0896  0.2139  0.1025  0.0478
 0.2681  0.1320  0.0771  0.1034  0.0609  0.0559  0.1535  0.0763  0.0726
 0.1064  0.0409  0.0596  0.1145  0.0332  0.0677  0.0444  0.0481  0.4852
 0.0905  0.0393  0.0793  0.4734  0.0428  0.0493  0.0669  0.0472  0.1112
 0.1240  0.0563  0.1154  0.1718  0.0744  0.0429  0.0816  0.1708  0.1627
[torch.cuda.FloatTensor of size 14x9 (GPU 0)]

2019-11-03 04:11:18,627 train 000 1.454126e-02 -1.929009
2019-11-03 04:14:08,910 train 050 1.131652e-02 -2.018018
2019-11-03 04:16:58,922 train 100 1.168335e-02 -1.730255
2019-11-03 04:19:49,303 train 150 1.173006e-02 -1.877157
2019-11-03 04:22:40,147 train 200 1.161740e-02 -1.828291
2019-11-03 04:25:30,841 train 250 1.166634e-02 -1.699990
2019-11-03 04:28:20,484 train 300 1.161855e-02 -1.686506
2019-11-03 04:31:09,576 train 350 1.152887e-02 -1.818117
2019-11-03 04:33:59,384 train 400 1.155695e-02 -2.530676
2019-11-03 04:36:50,065 train 450 1.155626e-02 -2.402799
2019-11-03 04:39:39,216 train 500 1.155279e-02 -2.303167
2019-11-03 04:42:29,355 train 550 1.155611e-02 -2.177114
2019-11-03 04:45:20,190 train 600 1.160285e-02 -2.195721
2019-11-03 04:48:10,613 train 650 1.155199e-02 -4.012442
2019-11-03 04:51:01,610 train 700 1.155744e-02 -3.837614
2019-11-03 04:53:51,833 train 750 1.154325e-02 -3.660659
2019-11-03 04:56:42,371 train 800 1.156141e-02 -3.514915
2019-11-03 04:59:33,934 train 850 1.159694e-02 -3.427510
2019-11-03 05:02:23,615 train 900 1.159216e-02 -3.305015
2019-11-03 05:05:13,226 train 950 1.157820e-02 -3.176794
2019-11-03 05:08:04,588 train 1000 1.156786e-02 -3.185599
2019-11-03 05:10:53,104 train 1050 1.157100e-02 -3.305014
2019-11-03 05:13:43,156 train 1100 1.154797e-02 -3.266974
2019-11-03 05:16:34,127 train 1150 1.151418e-02 -3.175561
2019-11-03 05:19:25,129 train 1200 1.148386e-02 -3.082514
2019-11-03 05:22:16,021 train 1250 1.147454e-02 -3.007461
2019-11-03 05:25:07,266 train 1300 1.145899e-02 -3.069542
2019-11-03 05:27:57,420 train 1350 1.144922e-02 -4.758207
2019-11-03 05:30:48,409 train 1400 1.144028e-02 -4.670893
2019-11-03 05:33:39,213 train 1450 1.143860e-02 -4.554927
2019-11-03 05:36:30,739 train 1500 1.142860e-02 -4.443228
2019-11-03 05:39:21,943 train 1550 1.141685e-02 -4.394279
2019-11-03 05:42:12,932 train 1600 1.144331e-02 -4.298136
2019-11-03 05:45:04,699 train 1650 1.144138e-02 -4.218557
2019-11-03 05:47:55,937 train 1700 1.144180e-02 -4.158867
2019-11-03 05:50:47,324 train 1750 1.144981e-02 -4.076359
2019-11-03 05:53:36,906 train 1800 1.145111e-02 -3.999009
2019-11-03 05:56:27,489 train 1850 1.144894e-02 -3.917807
2019-11-03 05:59:17,829 train 1900 1.143894e-02 -3.845551
2019-11-03 06:02:09,441 train 1950 1.142309e-02 -3.793959
2019-11-03 06:05:00,716 train 2000 1.140899e-02 -3.735198
2019-11-03 06:07:51,252 train 2050 1.140348e-02 -3.722409
2019-11-03 06:10:44,626 train 2100 1.141050e-02 -3.672245
2019-11-03 06:13:35,973 train 2150 1.139139e-02 -3.613846
2019-11-03 06:16:25,546 train 2200 1.140564e-02 -3.578243
2019-11-03 06:19:15,812 train 2250 1.140651e-02 -3.529685
2019-11-03 06:22:07,082 train 2300 1.139652e-02 -3.473408
2019-11-03 06:24:59,015 train 2350 1.139077e-02 -3.587032
2019-11-03 06:27:50,378 train 2400 1.139382e-02 -3.535887
2019-11-03 06:30:40,866 train 2450 1.138998e-02 -3.490752
2019-11-03 06:33:31,993 train 2500 1.138552e-02 -3.721980
2019-11-03 06:36:23,173 train 2550 1.138612e-02 -3.723663
2019-11-03 06:39:14,432 train 2600 1.138339e-02 -3.671115
2019-11-03 06:42:05,755 train 2650 1.138409e-02 -3.634436
2019-11-03 06:44:57,722 train 2700 1.137382e-02 -3.589510
2019-11-03 06:47:49,005 train 2750 1.138052e-02 -3.543955
2019-11-03 06:50:39,350 train 2800 1.138171e-02 -3.504851
2019-11-03 06:53:29,831 train 2850 1.138448e-02 -3.466793
2019-11-03 06:56:21,430 train 2900 1.137470e-02 -3.435418
2019-11-03 06:59:12,727 train 2950 1.136573e-02 -3.402685
2019-11-03 07:02:02,842 train 3000 1.136238e-02 -3.364535
2019-11-03 07:04:54,332 train 3050 1.135124e-02 -3.331465
2019-11-03 07:06:29,958 training loss; R2: 1.135031e-02 -3.492230
2019-11-03 07:06:30,440 valid 000 9.191576e-03 -1.388463
2019-11-03 07:06:40,066 valid 050 1.029583e-02 -1.824597
2019-11-03 07:06:49,617 valid 100 1.017153e-02 -1.295395
2019-11-03 07:06:59,242 valid 150 1.024357e-02 -1.188378
2019-11-03 07:07:08,835 valid 200 1.041121e-02 -1.207607
2019-11-03 07:07:18,377 valid 250 1.056284e-02 -13.148040
2019-11-03 07:07:27,893 valid 300 1.060273e-02 -18.471519
2019-11-03 07:07:37,473 valid 350 1.060580e-02 -15.964758
2019-11-03 07:07:47,012 valid 400 1.061493e-02 -14.119106
2019-11-03 07:07:56,602 valid 450 1.062676e-02 -13.075726
2019-11-03 07:08:06,175 valid 500 1.061978e-02 -11.855744
2019-11-03 07:08:15,763 valid 550 1.062894e-02 -10.875994
2019-11-03 07:08:25,288 valid 600 1.060226e-02 -10.086938
2019-11-03 07:08:34,822 valid 650 1.056883e-02 -9.405060
2019-11-03 07:08:44,377 valid 700 1.057002e-02 -9.504922
2019-11-03 07:08:53,937 valid 750 1.054008e-02 -8.937110
2019-11-03 07:08:57,551 validation loss; R2: 1.053108e-02 -8.754404
2019-11-03 07:08:57,695 epoch 4 lr 9.858624e-04
2019-11-03 07:08:57,696 genotype = Genotype(normal=[('max_pool_2x2', 0), ('max_pool_2x2', 1), ('max_pool_2x2', 0), ('max_pool_2x2', 2), ('max_pool_3x3', 0), ('dil_conv_3x3', 3), ('max_pool_3x3', 1), ('dil_conv_5x5', 4)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 1), ('max_pool_3x3', 0), ('sep_conv_5x5', 0), ('dil_conv_5x5', 2), ('dil_conv_5x5', 3), ('dil_conv_5x5', 2), ('max_pool_3x3', 3), ('dil_conv_5x5', 2)], reduce_concat=range(2, 6))
2019-11-03 07:08:57,698 
alphas_normal = Variable containing:
 0.0486  0.1639  0.2519  0.1458  0.0274  0.0798  0.0617  0.0759  0.1450
 0.5445  0.0436  0.1160  0.0469  0.0230  0.0844  0.0240  0.0521  0.0654
 0.0760  0.0623  0.2296  0.1993  0.0333  0.1534  0.0749  0.0870  0.0843
 0.7087  0.0255  0.0682  0.0559  0.0208  0.0328  0.0236  0.0344  0.0301
 0.5351  0.0763  0.1458  0.0766  0.0264  0.0482  0.0077  0.0513  0.0325
 0.0382  0.0297  0.0444  0.6711  0.0252  0.0368  0.0574  0.0645  0.0326
 0.6185  0.0325  0.0388  0.0867  0.0317  0.0315  0.0197  0.0782  0.0624
 0.4289  0.0719  0.0665  0.0969  0.0381  0.0727  0.0256  0.1123  0.0872
 0.4167  0.0343  0.0439  0.0636  0.0261  0.0284  0.0931  0.1585  0.1353
 0.1108  0.0432  0.1108  0.1968  0.0316  0.1252  0.2453  0.0340  0.1023
 0.1744  0.0644  0.1698  0.3214  0.0524  0.0690  0.0183  0.0432  0.0869
 0.4772  0.0542  0.0596  0.0661  0.0285  0.1412  0.0240  0.1056  0.0436
 0.4991  0.0364  0.0517  0.0565  0.0267  0.0481  0.0519  0.0657  0.1638
 0.3916  0.0275  0.0815  0.1434  0.0255  0.0222  0.0149  0.0285  0.2651
[torch.cuda.FloatTensor of size 14x9 (GPU 0)]

2019-11-03 07:08:57,700 
alphas_reduce = Variable containing:
 0.0607  0.0702  0.0732  0.2631  0.0281  0.2488  0.0940  0.0998  0.0621
 0.0323  0.0393  0.0763  0.5741  0.0350  0.0795  0.0643  0.0398  0.0595
 0.1423  0.0880  0.0778  0.1315  0.0403  0.0487  0.3624  0.0379  0.0711
 0.1317  0.2669  0.0688  0.1868  0.0452  0.1027  0.1133  0.0395  0.0451
 0.0658  0.0455  0.1021  0.2753  0.0279  0.0432  0.0723  0.0769  0.2910
 0.5324  0.0897  0.0608  0.0860  0.0397  0.0363  0.0432  0.0345  0.0774
 0.2827  0.2463  0.0689  0.0771  0.0510  0.0409  0.0970  0.0950  0.0412
 0.1572  0.0571  0.0714  0.1521  0.0369  0.1329  0.0521  0.0663  0.2741
 0.0745  0.0351  0.0520  0.1122  0.0329  0.0308  0.0722  0.0679  0.5225
 0.1285  0.1569  0.0762  0.1018  0.0580  0.0508  0.3114  0.0640  0.0525
 0.3335  0.1074  0.0934  0.0917  0.0636  0.0389  0.1739  0.0504  0.0472
 0.0585  0.0386  0.0404  0.0704  0.0301  0.1052  0.0419  0.0482  0.5666
 0.0554  0.0365  0.0492  0.6230  0.0344  0.0500  0.0491  0.0356  0.0669
 0.1113  0.0481  0.0810  0.1329  0.0514  0.1287  0.1009  0.1732  0.1726
[torch.cuda.FloatTensor of size 14x9 (GPU 0)]

2019-11-03 07:09:01,410 train 000 7.353352e-03 -0.297188
2019-11-03 07:11:51,209 train 050 1.158539e-02 -1.931291
2019-11-03 07:14:42,121 train 100 1.133738e-02 -1.677781
2019-11-03 07:17:33,070 train 150 1.115519e-02 -1.502749
2019-11-03 07:20:22,746 train 200 1.109198e-02 -1.386782
2019-11-03 07:23:14,177 train 250 1.105030e-02 -2.707892
2019-11-03 07:26:04,844 train 300 1.103970e-02 -2.429747
2019-11-03 07:28:56,540 train 350 1.103811e-02 -7.489783
2019-11-03 07:31:47,884 train 400 1.105085e-02 -6.679334
2019-11-03 07:34:39,341 train 450 1.105966e-02 -6.091692
2019-11-03 07:37:30,544 train 500 1.103293e-02 -5.585498
2019-11-03 07:40:22,342 train 550 1.108255e-02 -5.172399
2019-11-03 07:43:13,553 train 600 1.105002e-02 -4.826286
2019-11-03 07:46:05,441 train 650 1.105802e-02 -4.558350
2019-11-03 07:48:57,437 train 700 1.105336e-02 -4.302480
2019-11-03 07:51:48,286 train 750 1.105213e-02 -4.115810
2019-11-03 07:54:40,132 train 800 1.103629e-02 -3.937128
2019-11-03 07:57:31,414 train 850 1.101454e-02 -3.802013
2019-11-03 08:00:23,886 train 900 1.102005e-02 -3.651268
2019-11-03 08:03:15,560 train 950 1.100403e-02 -3.500933
2019-11-03 08:06:06,912 train 1000 1.098998e-02 -3.385153
2019-11-03 08:08:58,218 train 1050 1.101781e-02 -3.363882
2019-11-03 08:11:49,505 train 1100 1.102199e-02 -3.256600
2019-11-03 08:14:39,932 train 1150 1.102975e-02 -3.155316
2019-11-03 08:17:31,688 train 1200 1.102225e-02 -3.083501
2019-11-03 08:20:23,202 train 1250 1.101459e-02 -3.287335
2019-11-03 08:23:14,709 train 1300 1.103230e-02 -3.212161
2019-11-03 08:26:07,434 train 1350 1.102174e-02 -3.146959
2019-11-03 08:29:00,445 train 1400 1.100994e-02 -3.127907
2019-11-03 08:31:52,258 train 1450 1.100947e-02 -3.067932
2019-11-03 08:34:43,583 train 1500 1.099111e-02 -2.995922
2019-11-03 08:37:32,756 train 1550 1.097674e-02 -3.131869
2019-11-03 08:40:24,539 train 1600 1.096698e-02 -3.075249
2019-11-03 08:43:16,403 train 1650 1.096699e-02 -3.094038
2019-11-03 08:46:08,037 train 1700 1.095527e-02 -3.033744
2019-11-03 08:49:00,116 train 1750 1.094311e-02 -2.986018
2019-11-03 08:51:50,897 train 1800 1.095150e-02 -2.930796
2019-11-03 08:54:41,831 train 1850 1.095769e-02 -2.883304
2019-11-03 08:57:33,981 train 1900 1.098188e-02 -2.844352
2019-11-03 09:00:24,988 train 1950 1.098637e-02 -3.096235
2019-11-03 09:03:16,674 train 2000 1.097656e-02 -3.107828
2019-11-03 09:06:08,757 train 2050 1.097301e-02 -3.164656
2019-11-03 09:08:58,860 train 2100 1.098799e-02 -3.117284
2019-11-03 09:11:50,630 train 2150 1.097964e-02 -3.070739
2019-11-03 09:14:42,289 train 2200 1.098760e-02 -3.095312
2019-11-03 09:17:34,073 train 2250 1.098766e-02 -3.178574
2019-11-03 09:20:25,005 train 2300 1.099208e-02 -3.135063
2019-11-03 09:23:16,525 train 2350 1.100222e-02 -3.101996
2019-11-03 09:26:08,826 train 2400 1.098534e-02 -3.064345
2019-11-03 09:28:58,561 train 2450 1.098438e-02 -3.029169
2019-11-03 09:31:43,429 train 2500 1.097214e-02 -2.998843
2019-11-03 09:34:26,154 train 2550 1.097139e-02 -2.957731
2019-11-03 09:37:10,192 train 2600 1.096353e-02 -2.921047
2019-11-03 09:39:51,960 train 2650 1.096043e-02 -2.892171
2019-11-03 09:42:36,004 train 2700 1.096947e-02 -2.862651
2019-11-03 09:45:20,221 train 2750 1.096460e-02 -2.831694
2019-11-03 09:48:04,568 train 2800 1.095750e-02 -2.797213
2019-11-03 09:50:48,924 train 2850 1.096102e-02 -2.767806
2019-11-03 09:53:33,619 train 2900 1.095931e-02 -2.753827
2019-11-03 09:56:17,546 train 2950 1.095964e-02 -2.724968
2019-11-03 09:59:01,737 train 3000 1.095594e-02 -2.715300
2019-11-03 10:01:45,695 train 3050 1.095670e-02 -2.689109
2019-11-03 10:03:18,408 training loss; R2: 1.096004e-02 -2.716965
2019-11-03 10:03:18,869 valid 000 9.258608e-03 -0.819646
2019-11-03 10:03:27,947 valid 050 1.068585e-02 -13.631395
2019-11-03 10:03:37,012 valid 100 1.066146e-02 -7.271216
2019-11-03 10:03:46,084 valid 150 1.055466e-02 -5.360426
2019-11-03 10:03:55,191 valid 200 1.051459e-02 -4.277696
2019-11-03 10:04:04,467 valid 250 1.055360e-02 -3.624407
2019-11-03 10:04:13,510 valid 300 1.056893e-02 -3.295702
2019-11-03 10:04:22,544 valid 350 1.053746e-02 -2.964483
2019-11-03 10:04:31,584 valid 400 1.053138e-02 -2.863352
2019-11-03 10:04:40,644 valid 450 1.054281e-02 -2.680386
2019-11-03 10:04:49,700 valid 500 1.055068e-02 -2.774877
2019-11-03 10:04:58,745 valid 550 1.054503e-02 -2.613286
2019-11-03 10:05:07,782 valid 600 1.051058e-02 -2.490579
2019-11-03 10:05:16,788 valid 650 1.050921e-02 -2.368883
2019-11-03 10:05:25,835 valid 700 1.048752e-02 -2.269600
2019-11-03 10:05:34,903 valid 750 1.046850e-02 -2.223405
2019-11-03 10:05:38,324 validation loss; R2: 1.046630e-02 -2.187177
2019-11-03 10:05:38,486 epoch 5 lr 9.779754e-04
2019-11-03 10:05:38,487 genotype = Genotype(normal=[('skip_connect', 0), ('max_pool_2x2', 1), ('max_pool_2x2', 0), ('max_pool_2x2', 2), ('max_pool_3x3', 0), ('dil_conv_3x3', 3), ('max_pool_3x3', 1), ('max_pool_3x3', 0)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 1), ('max_pool_3x3', 0), ('skip_connect', 1), ('sep_conv_5x5', 0), ('dil_conv_5x5', 3), ('skip_connect', 1), ('dil_conv_5x5', 2), ('max_pool_3x3', 3)], reduce_concat=range(2, 6))
2019-11-03 10:05:38,489 
alphas_normal = Variable containing:
 0.0435  0.2314  0.2163  0.1072  0.0263  0.0809  0.0379  0.0899  0.1665
 0.5948  0.0431  0.1145  0.0639  0.0266  0.0498  0.0283  0.0283  0.0508
 0.1451  0.0568  0.2130  0.1878  0.0262  0.1174  0.0847  0.1119  0.0571
 0.7316  0.0223  0.0609  0.0535  0.0197  0.0276  0.0291  0.0330  0.0223
 0.6016  0.0382  0.1631  0.0752  0.0203  0.0304  0.0097  0.0336  0.0280
 0.0409  0.0279  0.0454  0.6591  0.0230  0.0369  0.0750  0.0563  0.0355
 0.6716  0.0247  0.0297  0.0735  0.0248  0.0226  0.0211  0.0594  0.0727
 0.4784  0.0524  0.0614  0.0884  0.0363  0.0844  0.0237  0.1022  0.0729
 0.4671  0.0380  0.0442  0.0555  0.0323  0.0148  0.1186  0.1443  0.0853
 0.1699  0.0490  0.1009  0.3187  0.0347  0.0625  0.1486  0.0456  0.0701
 0.1666  0.0478  0.1398  0.4115  0.0421  0.0534  0.0263  0.0380  0.0746
 0.4780  0.0350  0.0489  0.0672  0.0245  0.1754  0.0215  0.1105  0.0388
 0.5172  0.0374  0.0545  0.0688  0.0293  0.0396  0.0334  0.0683  0.1517
 0.4542  0.0354  0.0761  0.1208  0.0336  0.0150  0.0094  0.0162  0.2394
[torch.cuda.FloatTensor of size 14x9 (GPU 0)]

2019-11-03 10:05:38,490 
alphas_reduce = Variable containing:
 0.0537  0.0810  0.0605  0.3079  0.0310  0.2061  0.0487  0.1551  0.0561
 0.0413  0.0683  0.0686  0.5713  0.0341  0.0610  0.0663  0.0425  0.0466
 0.1739  0.0852  0.0547  0.0909  0.0398  0.0796  0.2820  0.0585  0.1354
 0.0836  0.3052  0.0641  0.1343  0.0527  0.0866  0.1849  0.0534  0.0351
 0.0689  0.0653  0.1062  0.2614  0.0391  0.0361  0.0787  0.0868  0.2576
 0.5048  0.1047  0.0592  0.0790  0.0461  0.0372  0.0454  0.0411  0.0825
 0.2254  0.3619  0.0390  0.0435  0.0371  0.0317  0.0746  0.1463  0.0405
 0.1346  0.0519  0.0666  0.1490  0.0492  0.0771  0.0698  0.0618  0.3401
 0.0367  0.0290  0.0443  0.0918  0.0350  0.0271  0.0747  0.0337  0.6278
 0.2170  0.1453  0.0672  0.0818  0.0605  0.0411  0.2593  0.0583  0.0694
 0.2725  0.1266  0.0770  0.0744  0.0710  0.0710  0.1514  0.1019  0.0543
 0.0583  0.0408  0.0360  0.0452  0.0332  0.0647  0.0296  0.0306  0.6616
 0.0764  0.0417  0.0529  0.5062  0.0543  0.0363  0.0821  0.0546  0.0955
 0.1148  0.1071  0.0874  0.1001  0.1465  0.0662  0.0576  0.1035  0.2169
[torch.cuda.FloatTensor of size 14x9 (GPU 0)]

2019-11-03 10:05:42,067 train 000 9.187580e-03 -3.700918
2019-11-03 10:08:27,040 train 050 1.030931e-02 -2.495600
2019-11-03 10:11:11,182 train 100 1.046227e-02 -2.196433
2019-11-03 10:13:54,938 train 150 1.056680e-02 -9.817188
2019-11-03 10:16:40,453 train 200 1.089642e-02 -7.645703
2019-11-03 10:19:24,909 train 250 1.087233e-02 -6.445129
2019-11-03 10:22:09,037 train 300 1.083140e-02 -5.640360
2019-11-03 10:24:55,053 train 350 1.083924e-02 -5.032010
2019-11-03 10:27:40,175 train 400 1.079579e-02 -9.331978
2019-11-03 10:30:25,872 train 450 1.082054e-02 -8.408532
2019-11-03 10:33:10,344 train 500 1.082967e-02 -7.707546
2019-11-03 10:35:54,640 train 550 1.082071e-02 -7.146620
2019-11-03 10:38:39,328 train 600 1.081811e-02 -6.676824
2019-11-03 10:41:24,297 train 650 1.079896e-02 -6.990762
2019-11-03 10:44:08,713 train 700 1.076494e-02 -6.549282
2019-11-03 10:46:53,405 train 750 1.076398e-02 -6.264976
2019-11-03 10:49:37,911 train 800 1.075236e-02 -5.969899
2019-11-03 10:52:21,782 train 850 1.073346e-02 -5.678063
2019-11-03 10:55:05,192 train 900 1.072252e-02 -5.418740
2019-11-03 10:57:47,182 train 950 1.073874e-02 -5.205617
2019-11-03 11:00:31,656 train 1000 1.072917e-02 -5.008890
2019-11-03 11:03:15,980 train 1050 1.074459e-02 -4.852992
2019-11-03 11:05:59,900 train 1100 1.075955e-02 -4.742597
2019-11-03 11:08:43,902 train 1150 1.073010e-02 -4.584217
2019-11-03 11:11:28,300 train 1200 1.073393e-02 -4.425950
2019-11-03 11:14:12,603 train 1250 1.072418e-02 -4.303587
2019-11-03 11:16:57,065 train 1300 1.072397e-02 -4.209125
2019-11-03 11:19:41,372 train 1350 1.071652e-02 -4.104676
2019-11-03 11:22:26,027 train 1400 1.071941e-02 -3.985146
2019-11-03 11:25:10,762 train 1450 1.071267e-02 -3.964223
2019-11-03 11:27:56,355 train 1500 1.071271e-02 -3.886170
2019-11-03 11:30:41,273 train 1550 1.070552e-02 -3.798499
2019-11-03 11:33:25,594 train 1600 1.071039e-02 -3.734895
2019-11-03 11:36:10,098 train 1650 1.070747e-02 -3.669932
2019-11-03 11:38:54,979 train 1700 1.070546e-02 -3.599438
2019-11-03 11:41:40,376 train 1750 1.070661e-02 -3.578420
2019-11-03 11:44:25,441 train 1800 1.070183e-02 -3.514017
2019-11-03 11:47:10,309 train 1850 1.070175e-02 -3.448286
2019-11-03 11:49:55,555 train 1900 1.068553e-02 -3.402987
2019-11-03 11:52:40,521 train 1950 1.067781e-02 -3.360273
2019-11-03 11:55:25,892 train 2000 1.066121e-02 -3.301445
2019-11-03 11:58:10,945 train 2050 1.064715e-02 -3.255826
2019-11-03 12:00:56,981 train 2100 1.064396e-02 -3.249544
2019-11-03 12:03:41,832 train 2150 1.063420e-02 -3.193882
2019-11-03 12:06:26,666 train 2200 1.063152e-02 -3.200731
2019-11-03 12:09:11,314 train 2250 1.061545e-02 -3.162789
2019-11-03 12:11:58,394 train 2300 1.061071e-02 -3.115363
2019-11-03 12:14:43,044 train 2350 1.062395e-02 -3.079907
2019-11-03 12:17:32,412 train 2400 1.061783e-02 -3.053293
2019-11-03 12:20:21,096 train 2450 1.060973e-02 -3.008444
2019-11-03 12:23:05,881 train 2500 1.061372e-02 -2.979122
2019-11-03 12:25:50,755 train 2550 1.061033e-02 -2.940854
2019-11-03 12:28:35,617 train 2600 1.060398e-02 -3.040986
2019-11-03 12:31:20,748 train 2650 1.060666e-02 -3.021954
2019-11-03 12:34:05,175 train 2700 1.060538e-02 -2.987533
2019-11-03 12:36:49,333 train 2750 1.061318e-02 -2.948739
2019-11-03 12:39:33,810 train 2800 1.060621e-02 -2.924477
2019-11-03 12:42:18,053 train 2850 1.060902e-02 -2.900423
2019-11-03 12:45:02,101 train 2900 1.062559e-02 -2.869393
2019-11-03 12:47:45,837 train 2950 1.062401e-02 -2.849543
2019-11-03 12:50:29,797 train 3000 1.061668e-02 -2.828556
2019-11-03 12:53:13,928 train 3050 1.061496e-02 -2.812988
2019-11-03 12:54:45,785 training loss; R2: 1.061308e-02 -2.800040
2019-11-03 12:54:46,261 valid 000 8.033193e-03 -0.328762
2019-11-03 12:54:55,307 valid 050 9.735652e-03 -1.342344
2019-11-03 12:55:04,327 valid 100 9.546087e-03 -1.212096
2019-11-03 12:55:13,334 valid 150 9.794805e-03 -1.082695
2019-11-03 12:55:22,373 valid 200 9.793082e-03 -1.038876
2019-11-03 12:55:31,397 valid 250 9.875255e-03 -1.042778
2019-11-03 12:55:40,422 valid 300 9.913909e-03 -1.001450
2019-11-03 12:55:49,430 valid 350 9.926432e-03 -1.137693
2019-11-03 12:55:58,476 valid 400 9.919131e-03 -1.138039
2019-11-03 12:56:07,495 valid 450 1.001181e-02 -1.200514
2019-11-03 12:56:16,467 valid 500 1.002730e-02 -1.182290
2019-11-03 12:56:25,478 valid 550 9.991377e-03 -1.424549
2019-11-03 12:56:34,479 valid 600 9.983785e-03 -7.457198
2019-11-03 12:56:43,487 valid 650 9.933117e-03 -6.940947
2019-11-03 12:56:52,507 valid 700 9.898423e-03 -6.532866
2019-11-03 12:57:01,556 valid 750 9.884131e-03 -6.160901
2019-11-03 12:57:04,957 validation loss; R2: 9.894228e-03 -6.033661
2019-11-03 12:57:05,110 epoch 6 lr 9.683994e-04
2019-11-03 12:57:05,111 genotype = Genotype(normal=[('max_pool_2x2', 0), ('max_pool_2x2', 1), ('max_pool_2x2', 0), ('max_pool_2x2', 2), ('max_pool_3x3', 0), ('sep_conv_5x5', 3), ('max_pool_3x3', 1), ('max_pool_3x3', 0)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 1), ('max_pool_3x3', 0), ('max_pool_3x3', 2), ('sep_conv_5x5', 0), ('dil_conv_5x5', 3), ('dil_conv_5x5', 2), ('dil_conv_5x5', 2), ('max_pool_3x3', 3)], reduce_concat=range(2, 6))
2019-11-03 12:57:05,113 
alphas_normal = Variable containing:
 0.0427  0.2413  0.3033  0.0706  0.0307  0.0934  0.0448  0.0543  0.1189
 0.5543  0.0394  0.1466  0.0531  0.0268  0.0631  0.0321  0.0314  0.0532
 0.1079  0.0639  0.2748  0.1652  0.0321  0.1339  0.1169  0.0635  0.0420
 0.6574  0.0259  0.0873  0.0631  0.0229  0.0428  0.0341  0.0459  0.0206
 0.4594  0.0342  0.2604  0.1094  0.0213  0.0465  0.0068  0.0257  0.0363
 0.0324  0.0338  0.0446  0.6687  0.0278  0.0330  0.1041  0.0359  0.0197
 0.5975  0.0215  0.0281  0.0664  0.0212  0.0363  0.0199  0.0573  0.1518
 0.4509  0.0421  0.0743  0.1281  0.0354  0.1083  0.0277  0.0843  0.0489
 0.2529  0.0387  0.0895  0.1250  0.0388  0.0208  0.2408  0.1009  0.0927
 0.0994  0.0716  0.1219  0.3401  0.0535  0.1109  0.1094  0.0402  0.0529
 0.1203  0.0443  0.1650  0.4599  0.0353  0.0550  0.0251  0.0315  0.0635
 0.3914  0.0405  0.0591  0.0859  0.0306  0.1791  0.0153  0.1578  0.0403
 0.3841  0.0403  0.0945  0.1361  0.0345  0.0640  0.0568  0.0608  0.1289
 0.2271  0.0305  0.1130  0.2344  0.0325  0.0189  0.0168  0.0174  0.3093
[torch.cuda.FloatTensor of size 14x9 (GPU 0)]

2019-11-03 12:57:05,115 
alphas_reduce = Variable containing:
 0.1419  0.0548  0.0710  0.2551  0.0320  0.2355  0.0380  0.1216  0.0501
 0.0230  0.0381  0.0479  0.5934  0.0254  0.1278  0.0740  0.0284  0.0418
 0.2297  0.0994  0.0778  0.1015  0.0516  0.0616  0.2736  0.0338  0.0710
 0.0993  0.1663  0.1058  0.2018  0.0648  0.1237  0.1906  0.0242  0.0235
 0.0414  0.0506  0.0774  0.3047  0.0359  0.0475  0.0565  0.0862  0.2997
 0.6291  0.0606  0.0685  0.0604  0.0406  0.0285  0.0365  0.0371  0.0387
 0.2251  0.2796  0.0547  0.0602  0.0486  0.0722  0.1278  0.0761  0.0558
 0.0858  0.0642  0.0762  0.1406  0.0458  0.1000  0.0827  0.1140  0.2907
 0.0598  0.0428  0.0676  0.1274  0.0447  0.0301  0.0872  0.0580  0.4825
 0.4273  0.1500  0.0763  0.0803  0.0663  0.0188  0.1144  0.0384  0.0281
 0.4510  0.0938  0.0900  0.0798  0.0645  0.0494  0.0957  0.0425  0.0333
 0.0499  0.0355  0.0361  0.0448  0.0310  0.0758  0.0385  0.0252  0.6633
 0.0664  0.0364  0.0486  0.5613  0.0435  0.0336  0.0556  0.0424  0.1122
 0.1021  0.0491  0.0603  0.0841  0.0782  0.1122  0.1325  0.0962  0.2854
[torch.cuda.FloatTensor of size 14x9 (GPU 0)]

2019-11-03 12:57:08,579 train 000 9.415037e-03 -0.382591
2019-11-03 12:59:53,409 train 050 1.056395e-02 -1.529799
2019-11-03 13:02:37,723 train 100 1.055316e-02 -1.280394
2019-11-03 13:05:22,181 train 150 1.041052e-02 -2.096823
2019-11-03 13:08:06,652 train 200 1.043725e-02 -2.283465
2019-11-03 13:10:50,739 train 250 1.040861e-02 -3.628731
2019-11-03 13:13:34,120 train 300 1.037046e-02 -4.161187
2019-11-03 13:16:16,023 train 350 1.038706e-02 -3.760236
2019-11-03 13:18:57,893 train 400 1.036005e-02 -3.533449
2019-11-03 13:21:39,948 train 450 1.040934e-02 -3.284889
2019-11-03 13:24:24,415 train 500 1.042571e-02 -3.056332
2019-11-03 13:27:09,083 train 550 1.045075e-02 -3.121989
2019-11-03 13:29:53,536 train 600 1.040868e-02 -2.936915
2019-11-03 13:32:37,927 train 650 1.044093e-02 -2.837401
2019-11-03 13:35:21,678 train 700 1.042913e-02 -2.760825
2019-11-03 13:38:05,936 train 750 1.043103e-02 -2.640618
2019-11-03 13:40:49,689 train 800 1.044837e-02 -2.573608
2019-11-03 13:43:33,736 train 850 1.041227e-02 -2.493843
2019-11-03 13:46:18,181 train 900 1.040469e-02 -2.468061
2019-11-03 13:49:02,562 train 950 1.040401e-02 -2.425126
2019-11-03 13:51:46,592 train 1000 1.039020e-02 -2.438424
2019-11-03 13:54:30,754 train 1050 1.040226e-02 -2.380239
2019-11-03 13:57:15,246 train 1100 1.039564e-02 -2.342287
2019-11-03 13:59:59,583 train 1150 1.038686e-02 -3.249824
2019-11-03 14:02:43,718 train 1200 1.037632e-02 -3.173515
2019-11-03 14:05:27,843 train 1250 1.038368e-02 -3.093514
2019-11-03 14:08:11,855 train 1300 1.040512e-02 -3.012294
2019-11-03 14:10:55,951 train 1350 1.040016e-02 -2.928491
2019-11-03 14:13:39,765 train 1400 1.040143e-02 -3.037471
2019-11-03 14:16:23,660 train 1450 1.040322e-02 -2.990899
2019-11-03 14:19:08,021 train 1500 1.040755e-02 -2.933904
2019-11-03 14:21:51,059 train 1550 1.041694e-02 -2.893451
2019-11-03 14:24:32,504 train 1600 1.041934e-02 -2.980767
2019-11-03 14:27:16,897 train 1650 1.040325e-02 -2.932039
2019-11-03 14:30:01,284 train 1700 1.039158e-02 -2.886556
2019-11-03 14:32:45,261 train 1750 1.039105e-02 -6.006180
2019-11-03 14:35:29,552 train 1800 1.038549e-02 -5.871206
2019-11-03 14:38:13,582 train 1850 1.038856e-02 -5.884183
2019-11-03 14:40:57,968 train 1900 1.038609e-02 -5.759155
2019-11-03 14:43:42,916 train 1950 1.037143e-02 -5.658930
2019-11-03 14:46:27,316 train 2000 1.037453e-02 -5.542993
2019-11-03 14:49:11,916 train 2050 1.037465e-02 -5.432643
2019-11-03 14:51:56,645 train 2100 1.037468e-02 -5.334318
2019-11-03 14:54:41,247 train 2150 1.036371e-02 -5.231917
2019-11-03 14:57:25,836 train 2200 1.034161e-02 -5.480681
2019-11-03 15:00:10,779 train 2250 1.035655e-02 -5.419929
2019-11-03 15:02:55,397 train 2300 1.035610e-02 -5.348624
2019-11-03 15:05:40,014 train 2350 1.036356e-02 -5.300322
2019-11-03 15:08:24,217 train 2400 1.036266e-02 -5.215380
2019-11-03 15:11:08,769 train 2450 1.036147e-02 -5.127311
2019-11-03 15:13:53,781 train 2500 1.036035e-02 -5.053421
2019-11-03 15:16:38,596 train 2550 1.035919e-02 -4.980488
2019-11-03 15:19:22,982 train 2600 1.035858e-02 -4.908496
2019-11-03 15:22:07,647 train 2650 1.036776e-02 -4.834961
2019-11-03 15:24:52,569 train 2700 1.036819e-02 -4.767874
2019-11-03 15:27:37,645 train 2750 1.036443e-02 -4.705802
2019-11-03 15:30:22,653 train 2800 1.036191e-02 -4.660497
2019-11-03 15:33:07,428 train 2850 1.036172e-02 -4.596505
2019-11-03 15:35:52,242 train 2900 1.037042e-02 -4.532094
2019-11-03 15:38:36,677 train 2950 1.038478e-02 -4.488736
2019-11-03 15:41:21,278 train 3000 1.037969e-02 -4.434486
2019-11-03 15:44:05,861 train 3050 1.037404e-02 -4.383732
2019-11-03 15:45:37,975 training loss; R2: 1.037147e-02 -4.354448
2019-11-03 15:45:38,443 valid 000 7.764169e-03 -0.380415
2019-11-03 15:45:47,535 valid 050 9.539492e-03 -33.780965
2019-11-03 15:45:56,601 valid 100 9.555126e-03 -17.557386
2019-11-03 15:46:05,661 valid 150 9.510477e-03 -12.173837
2019-11-03 15:46:14,702 valid 200 9.520282e-03 -9.381983
2019-11-03 15:46:23,773 valid 250 9.572941e-03 -7.858608
2019-11-03 15:46:32,817 valid 300 9.506750e-03 -6.833232
2019-11-03 15:46:41,870 valid 350 9.504354e-03 -6.059454
2019-11-03 15:46:50,934 valid 400 9.540657e-03 -5.426118
2019-11-03 15:46:59,981 valid 450 9.533989e-03 -5.053525
2019-11-03 15:47:09,047 valid 500 9.546848e-03 -4.624466
2019-11-03 15:47:18,132 valid 550 9.558638e-03 -4.295898
2019-11-03 15:47:27,232 valid 600 9.524244e-03 -4.047283
2019-11-03 15:47:36,304 valid 650 9.521027e-03 -3.819332
2019-11-03 15:47:45,367 valid 700 9.555277e-03 -3.629039
2019-11-03 15:47:54,433 valid 750 9.575521e-03 -3.449472
2019-11-03 15:47:57,859 validation loss; R2: 9.573890e-03 -9.743695
2019-11-03 15:47:58,055 epoch 7 lr 9.571722e-04
2019-11-03 15:47:58,056 genotype = Genotype(normal=[('max_pool_2x2', 0), ('max_pool_2x2', 1), ('max_pool_2x2', 2), ('max_pool_2x2', 0), ('max_pool_3x3', 0), ('max_pool_3x3', 2), ('max_pool_3x3', 1), ('dil_conv_5x5', 4)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 1), ('max_pool_3x3', 0), ('dil_conv_5x5', 2), ('sep_conv_5x5', 0), ('dil_conv_5x5', 3), ('skip_connect', 1), ('dil_conv_5x5', 2), ('max_pool_3x3', 3)], reduce_concat=range(2, 6))
2019-11-03 15:47:58,058 
alphas_normal = Variable containing:
 0.0280  0.2526  0.3674  0.0376  0.0312  0.0501  0.0561  0.0424  0.1346
 0.4944  0.0309  0.1853  0.0731  0.0266  0.0723  0.0487  0.0201  0.0486
 0.1485  0.0628  0.3907  0.1165  0.0337  0.0767  0.0820  0.0554  0.0336
 0.5719  0.0337  0.1333  0.0992  0.0311  0.0297  0.0211  0.0528  0.0271
 0.2182  0.0276  0.4828  0.1635  0.0249  0.0239  0.0035  0.0213  0.0343
 0.0349  0.0379  0.0412  0.6105  0.0309  0.0263  0.1419  0.0504  0.0260
 0.7207  0.0185  0.0205  0.0430  0.0193  0.0218  0.0075  0.0398  0.1088
 0.3897  0.0489  0.0905  0.2066  0.0517  0.0753  0.0235  0.0585  0.0554
 0.1667  0.0433  0.1102  0.2029  0.0569  0.0231  0.2050  0.0992  0.0928
 0.0962  0.0873  0.1004  0.2574  0.0605  0.1694  0.1085  0.0592  0.0611
 0.1216  0.0426  0.1117  0.4797  0.0412  0.0451  0.0476  0.0362  0.0742
 0.3081  0.0356  0.0622  0.0909  0.0285  0.1903  0.0146  0.1474  0.1225
 0.3784  0.0387  0.1284  0.1933  0.0391  0.0326  0.0417  0.0381  0.1096
 0.1345  0.0241  0.1054  0.2854  0.0304  0.0157  0.0132  0.0225  0.3687
[torch.cuda.FloatTensor of size 14x9 (GPU 0)]

2019-11-03 15:47:58,059 
alphas_reduce = Variable containing:
 0.1411  0.0729  0.0910  0.2901  0.0377  0.1104  0.0604  0.1285  0.0679
 0.0189  0.0425  0.0437  0.6609  0.0250  0.0925  0.0576  0.0358  0.0231
 0.2572  0.1061  0.0670  0.0823  0.0455  0.0414  0.2467  0.0687  0.0852
 0.0624  0.1130  0.1333  0.2197  0.0696  0.1575  0.1778  0.0356  0.0311
 0.0544  0.0459  0.0733  0.2339  0.0429  0.0508  0.0605  0.1071  0.3313
 0.6258  0.0632  0.0750  0.0696  0.0524  0.0178  0.0178  0.0280  0.0504
 0.2179  0.3838  0.0517  0.0572  0.0472  0.0470  0.0749  0.0621  0.0582
 0.1616  0.0719  0.0908  0.1323  0.0688  0.0608  0.0658  0.0694  0.2786
 0.0489  0.0492  0.0689  0.1584  0.0593  0.0359  0.0773  0.0550  0.4471
 0.2950  0.1293  0.0643  0.0636  0.0596  0.0399  0.1631  0.0485  0.1368
 0.4008  0.0770  0.0988  0.1092  0.0888  0.0495  0.0790  0.0539  0.0429
 0.0505  0.0342  0.0290  0.0298  0.0341  0.0603  0.0351  0.0283  0.6988
 0.1039  0.0524  0.0552  0.4228  0.0574  0.0510  0.0728  0.0419  0.1426
 0.2528  0.0504  0.0631  0.0591  0.0987  0.0914  0.0533  0.0470  0.2842
[torch.cuda.FloatTensor of size 14x9 (GPU 0)]

2019-11-03 15:48:01,505 train 000 1.156249e-02 -1.963117
2019-11-03 15:50:46,808 train 050 1.028682e-02 -1.270577
2019-11-03 15:53:31,402 train 100 1.027146e-02 -2.595683
2019-11-03 15:56:15,435 train 150 1.013658e-02 -2.241439
2019-11-03 15:58:59,306 train 200 1.015721e-02 -2.303165
2019-11-03 16:01:43,579 train 250 1.006745e-02 -2.194731
2019-11-03 16:04:27,739 train 300 1.004460e-02 -2.038873
2019-11-03 16:07:12,101 train 350 1.006596e-02 -1.959702
2019-11-03 16:09:56,180 train 400 1.013141e-02 -1.862441
2019-11-03 16:12:40,610 train 450 1.014842e-02 -1.900482
2019-11-03 16:15:25,124 train 500 1.013494e-02 -1.811138
2019-11-03 16:18:08,568 train 550 1.012387e-02 -1.729802
2019-11-03 16:20:52,383 train 600 1.010127e-02 -1.680653
2019-11-03 16:23:36,568 train 650 1.009157e-02 -2.028331
2019-11-03 16:26:20,826 train 700 1.010463e-02 -1.975410
2019-11-03 16:29:05,104 train 750 1.009862e-02 -1.909351
2019-11-03 16:31:49,970 train 800 1.009575e-02 -2.580082
2019-11-03 16:34:34,876 train 850 1.009184e-02 -2.492425
2019-11-03 16:37:18,791 train 900 1.009523e-02 -2.410216
2019-11-03 16:40:03,127 train 950 1.012208e-02 -2.387742
2019-11-03 16:42:47,264 train 1000 1.012427e-02 -2.315095
2019-11-03 16:45:31,015 train 1050 1.013249e-02 -2.316135
2019-11-03 16:48:15,956 train 1100 1.013618e-02 -2.277785
2019-11-03 16:51:00,392 train 1150 1.014152e-02 -2.221617
2019-11-03 16:53:44,912 train 1200 1.012030e-02 -2.218424
2019-11-03 16:56:29,625 train 1250 1.013361e-02 -2.171037
2019-11-03 16:59:13,779 train 1300 1.013280e-02 -2.135349
2019-11-03 17:01:58,184 train 1350 1.013402e-02 -2.117924
2019-11-03 17:04:42,691 train 1400 1.015031e-02 -2.085797
2019-11-03 17:07:26,896 train 1450 1.015524e-02 -2.650201
2019-11-03 17:10:11,159 train 1500 1.016228e-02 -2.591554
2019-11-03 17:12:54,860 train 1550 1.017132e-02 -2.541893
2019-11-03 17:15:38,805 train 1600 1.019135e-02 -2.500777
2019-11-03 17:18:23,420 train 1650 1.020285e-02 -5.924372
2019-11-03 17:21:07,913 train 1700 1.019974e-02 -5.819290
2019-11-03 17:23:52,139 train 1750 1.019497e-02 -5.689246
2019-11-03 17:26:36,448 train 1800 1.018361e-02 -5.597759
2019-11-03 17:29:20,885 train 1850 1.018011e-02 -5.483267
2019-11-03 17:32:05,398 train 1900 1.017595e-02 -5.401419
2019-11-03 17:34:49,910 train 1950 1.017004e-02 -5.290020
2019-11-03 17:37:34,225 train 2000 1.015808e-02 -5.185515
2019-11-03 17:40:17,641 train 2050 1.015985e-02 -5.126928
2019-11-03 17:43:01,139 train 2100 1.015569e-02 -5.030688
2019-11-03 17:45:44,920 train 2150 1.015430e-02 -4.938737
2019-11-03 17:48:28,334 train 2200 1.013793e-02 -4.919801
2019-11-03 17:51:12,728 train 2250 1.014040e-02 -4.834809
2019-11-03 17:53:57,111 train 2300 1.012702e-02 -4.759412
2019-11-03 17:56:41,018 train 2350 1.012895e-02 -4.853801
2019-11-03 17:59:25,066 train 2400 1.012179e-02 -4.788066
2019-11-03 18:02:07,091 train 2450 1.012230e-02 -4.747942
2019-11-03 18:04:49,520 train 2500 1.013404e-02 -4.694513
2019-11-03 18:07:33,217 train 2550 1.013454e-02 -4.672084
2019-11-03 18:10:17,949 train 2600 1.013396e-02 -4.608294
2019-11-03 18:13:02,529 train 2650 1.013212e-02 -4.549303
2019-11-03 18:15:46,491 train 2700 1.014241e-02 -4.493157
2019-11-03 18:18:30,271 train 2750 1.013863e-02 -4.448207
2019-11-03 18:21:14,410 train 2800 1.014892e-02 -4.634836
2019-11-03 18:23:57,814 train 2850 1.014458e-02 -4.642474
2019-11-03 18:26:42,060 train 2900 1.014807e-02 -4.636433
2019-11-03 18:29:26,308 train 2950 1.013434e-02 -4.580114
2019-11-03 18:32:10,984 train 3000 1.013558e-02 -4.521992
2019-11-03 18:34:55,004 train 3050 1.013416e-02 -4.463051
2019-11-03 18:36:26,613 training loss; R2: 1.012960e-02 -4.432781
2019-11-03 18:36:27,085 valid 000 9.578080e-03 -2.846514
2019-11-03 18:36:36,261 valid 050 9.046911e-03 -1.658082
2019-11-03 18:36:45,364 valid 100 9.202667e-03 -3.369900
2019-11-03 18:36:54,515 valid 150 9.302194e-03 -2.684720
2019-11-03 18:37:03,625 valid 200 9.387783e-03 -3.431364
2019-11-03 18:37:12,787 valid 250 9.344847e-03 -3.039344
2019-11-03 18:37:21,882 valid 300 9.259668e-03 -2.737381
2019-11-03 18:37:30,993 valid 350 9.265822e-03 -2.821590
2019-11-03 18:37:40,055 valid 400 9.290106e-03 -2.708043
2019-11-03 18:37:49,144 valid 450 9.307127e-03 -2.573899
2019-11-03 18:37:58,214 valid 500 9.290561e-03 -2.452589
2019-11-03 18:38:07,342 valid 550 9.290330e-03 -2.340268
2019-11-03 18:38:16,418 valid 600 9.318369e-03 -2.283555
2019-11-03 18:38:25,547 valid 650 9.314701e-03 -2.227225
2019-11-03 18:38:34,651 valid 700 9.304288e-03 -2.186796
2019-11-03 18:38:43,738 valid 750 9.301357e-03 -13.479567
2019-11-03 18:38:47,173 validation loss; R2: 9.305780e-03 -13.188430
2019-11-03 18:38:47,318 epoch 8 lr 9.443380e-04
2019-11-03 18:38:47,319 genotype = Genotype(normal=[('max_pool_2x2', 0), ('max_pool_2x2', 1), ('max_pool_2x2', 2), ('max_pool_2x2', 0), ('max_pool_3x3', 0), ('sep_conv_5x5', 3), ('max_pool_3x3', 4), ('max_pool_3x3', 1)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 1), ('max_pool_3x3', 0), ('sep_conv_5x5', 0), ('max_pool_3x3', 2), ('dil_conv_5x5', 3), ('skip_connect', 1), ('dil_conv_5x5', 2), ('max_pool_3x3', 3)], reduce_concat=range(2, 6))
2019-11-03 18:38:47,321 
alphas_normal = Variable containing:
 0.0227  0.2139  0.4437  0.0327  0.0293  0.0606  0.0422  0.0426  0.1124
 0.5834  0.0218  0.1645  0.0541  0.0183  0.0569  0.0509  0.0193  0.0306
 0.0792  0.0486  0.4783  0.0855  0.0314  0.0962  0.1064  0.0411  0.0332
 0.6332  0.0226  0.1269  0.0829  0.0219  0.0287  0.0227  0.0399  0.0212
 0.1120  0.0251  0.6085  0.1165  0.0197  0.0511  0.0041  0.0239  0.0391
 0.0269  0.0337  0.0482  0.6784  0.0286  0.0186  0.1078  0.0345  0.0232
 0.6232  0.0129  0.0154  0.0290  0.0128  0.0264  0.0058  0.0635  0.2111
 0.4461  0.0396  0.0732  0.1667  0.0375  0.1130  0.0281  0.0503  0.0454
 0.0933  0.0337  0.1130  0.2495  0.0419  0.0351  0.2567  0.0701  0.1067
 0.0686  0.1230  0.1005  0.2629  0.0797  0.1101  0.1076  0.0779  0.0696
 0.1753  0.0400  0.0887  0.4117  0.0364  0.0530  0.0628  0.0402  0.0920
 0.1921  0.0492  0.0760  0.1063  0.0395  0.2392  0.0219  0.1630  0.1127
 0.1805  0.0435  0.1505  0.2698  0.0441  0.0453  0.0772  0.0357  0.1534
 0.0528  0.0357  0.1225  0.4273  0.0358  0.0143  0.0215  0.0196  0.2705
[torch.cuda.FloatTensor of size 14x9 (GPU 0)]

2019-11-03 18:38:47,323 
alphas_reduce = Variable containing:
 0.1144  0.0559  0.0894  0.2873  0.0352  0.2091  0.0465  0.1217  0.0408
 0.0269  0.0413  0.0459  0.6095  0.0271  0.0905  0.1049  0.0277  0.0261
 0.1929  0.1082  0.0808  0.0840  0.0407  0.0503  0.3114  0.0645  0.0671
 0.0783  0.1460  0.1136  0.2059  0.0638  0.1210  0.1946  0.0508  0.0260
 0.0625  0.0517  0.0749  0.2675  0.0366  0.0889  0.0640  0.1111  0.2430
 0.6509  0.0392  0.0644  0.0778  0.0413  0.0335  0.0330  0.0263  0.0335
 0.2565  0.3226  0.0393  0.0394  0.0368  0.0480  0.0799  0.1062  0.0713
 0.1702  0.0561  0.0664  0.1136  0.0516  0.0999  0.1034  0.1353  0.2035
 0.0563  0.0345  0.0654  0.1671  0.0501  0.0503  0.0671  0.0399  0.4694
 0.4499  0.0696  0.0784  0.0748  0.0527  0.0397  0.1383  0.0422  0.0544
 0.3591  0.0605  0.1414  0.1247  0.1154  0.0450  0.0690  0.0494  0.0353
 0.0717  0.0374  0.0375  0.0425  0.0371  0.1033  0.0289  0.0282  0.6133
 0.0467  0.0299  0.0521  0.5711  0.0444  0.0645  0.0388  0.0365  0.1160
 0.1127  0.0394  0.0568  0.0713  0.0814  0.0912  0.0928  0.0662  0.3884
[torch.cuda.FloatTensor of size 14x9 (GPU 0)]

2019-11-03 18:38:50,886 train 000 9.056009e-03 -0.077393
2019-11-03 18:41:35,228 train 050 9.727250e-03 -1.279330
2019-11-03 18:44:19,247 train 100 9.939697e-03 -2.382914
2019-11-03 18:47:03,435 train 150 9.886383e-03 -1.951671
2019-11-03 18:49:47,513 train 200 9.890727e-03 -4.385263
2019-11-03 18:52:31,745 train 250 9.892740e-03 -3.731632
2019-11-03 18:55:15,450 train 300 9.923178e-03 -3.350827
2019-11-03 18:57:58,748 train 350 9.892809e-03 -3.047971
2019-11-03 19:00:42,577 train 400 9.868132e-03 -2.803575
2019-11-03 19:03:26,875 train 450 9.844936e-03 -2.859893
2019-11-03 19:06:10,564 train 500 9.780491e-03 -2.759322
2019-11-03 19:08:54,928 train 550 9.850976e-03 -2.605649
2019-11-03 19:11:38,836 train 600 9.816560e-03 -2.479776
2019-11-03 19:14:22,464 train 650 9.854252e-03 -2.362414
2019-11-03 19:17:06,231 train 700 9.879288e-03 -2.333517
2019-11-03 19:19:50,432 train 750 9.831205e-03 -2.283228
2019-11-03 19:22:34,433 train 800 9.840492e-03 -2.199046
2019-11-03 19:25:18,696 train 850 9.861136e-03 -2.133370
2019-11-03 19:28:02,011 train 900 9.891022e-03 -2.093693
2019-11-03 19:30:44,471 train 950 9.915339e-03 -2.052301
2019-11-03 19:33:28,794 train 1000 9.915610e-03 -2.024370
2019-11-03 19:36:12,684 train 1050 9.929666e-03 -2.075005
2019-11-03 19:38:56,365 train 1100 9.924385e-03 -2.024138
2019-11-03 19:41:40,047 train 1150 9.942375e-03 -2.013199
2019-11-03 19:44:24,374 train 1200 9.944212e-03 -2.065458
2019-11-03 19:47:08,830 train 1250 9.939928e-03 -2.021652
2019-11-03 19:49:52,983 train 1300 9.921068e-03 -2.201181
2019-11-03 19:52:36,912 train 1350 9.919899e-03 -2.159380
2019-11-03 19:55:21,437 train 1400 9.947436e-03 -2.212923
2019-11-03 19:58:05,729 train 1450 9.944984e-03 -2.166525
2019-11-03 20:00:50,010 train 1500 9.938221e-03 -2.155793
2019-11-03 20:03:33,791 train 1550 9.959144e-03 -2.342930
2019-11-03 20:06:18,121 train 1600 9.951110e-03 -2.369756
2019-11-03 20:09:02,548 train 1650 9.930539e-03 -2.393243
2019-11-03 20:11:46,476 train 1700 9.939638e-03 -2.351396
2019-11-03 20:14:30,556 train 1750 9.928803e-03 -2.799146
2019-11-03 20:17:14,931 train 1800 9.926945e-03 -2.769576
2019-11-03 20:19:59,181 train 1850 9.913411e-03 -2.739181
2019-11-03 20:22:43,193 train 1900 9.924854e-03 -2.691872
2019-11-03 20:25:27,572 train 1950 9.926167e-03 -2.646949
2019-11-03 20:28:11,880 train 2000 9.918719e-03 -2.658649
2019-11-03 20:30:56,583 train 2050 9.917501e-03 -2.621620
2019-11-03 20:33:41,541 train 2100 9.909467e-03 -2.613222
2019-11-03 20:36:26,850 train 2150 9.909306e-03 -2.578059
2019-11-03 20:39:11,998 train 2200 9.903385e-03 -2.542419
2019-11-03 20:41:59,472 train 2250 9.897345e-03 -2.517526
2019-11-03 20:44:47,154 train 2300 9.903799e-03 -2.774955
2019-11-03 20:47:33,646 train 2350 9.904594e-03 -2.741976
2019-11-03 20:50:20,975 train 2400 9.895178e-03 -2.727560
2019-11-03 20:53:08,327 train 2450 9.910350e-03 -2.774069
2019-11-03 20:56:00,017 train 2500 9.904156e-03 -2.758696
2019-11-03 20:58:45,815 train 2550 9.911188e-03 -2.728384
2019-11-03 21:01:28,606 train 2600 9.911350e-03 -2.728314
2019-11-03 21:04:11,124 train 2650 9.910566e-03 -2.727724
2019-11-03 21:06:55,993 train 2700 9.912127e-03 -2.695608
2019-11-03 21:09:41,757 train 2750 9.912487e-03 -2.667258
2019-11-03 21:12:27,210 train 2800 9.906538e-03 -2.675894
2019-11-03 21:15:12,796 train 2850 9.906877e-03 -2.648177
2019-11-03 21:17:58,729 train 2900 9.898515e-03 -2.617652
2019-11-03 21:20:45,577 train 2950 9.897353e-03 -2.592383
2019-11-03 21:23:30,495 train 3000 9.898000e-03 -2.570708
2019-11-03 21:26:15,764 train 3050 9.890731e-03 -2.540731
2019-11-03 21:27:49,860 training loss; R2: 9.893391e-03 -15.447694
2019-11-03 21:27:50,329 valid 000 8.425712e-03 -1.151262
2019-11-03 21:27:59,492 valid 050 9.168580e-03 -1.000145
2019-11-03 21:28:08,598 valid 100 9.306952e-03 -1.144871
2019-11-03 21:28:17,721 valid 150 9.290698e-03 -1.141389
2019-11-03 21:28:26,818 valid 200 9.315278e-03 -1.329671
2019-11-03 21:28:35,957 valid 250 9.298396e-03 -1.415369
2019-11-03 21:28:45,061 valid 300 9.261826e-03 -1.473113
2019-11-03 21:28:54,182 valid 350 9.257690e-03 -1.533342
2019-11-03 21:29:03,243 valid 400 9.208617e-03 -1.491133
2019-11-03 21:29:12,383 valid 450 9.215142e-03 -1.452341
2019-11-03 21:29:21,444 valid 500 9.213260e-03 -1.459433
2019-11-03 21:29:30,550 valid 550 9.198373e-03 -1.431685
2019-11-03 21:29:39,670 valid 600 9.216205e-03 -1.428385
2019-11-03 21:29:48,830 valid 650 9.220650e-03 -1.727767
2019-11-03 21:29:57,948 valid 700 9.202120e-03 -1.764943
2019-11-03 21:30:07,102 valid 750 9.208406e-03 -1.804742
2019-11-03 21:30:10,557 validation loss; R2: 9.215917e-03 -1.797195
2019-11-03 21:30:10,698 epoch 9 lr 9.299476e-04
2019-11-03 21:30:10,698 genotype = Genotype(normal=[('max_pool_2x2', 0), ('max_pool_2x2', 1), ('max_pool_2x2', 2), ('max_pool_2x2', 0), ('max_pool_3x3', 0), ('sep_conv_5x5', 3), ('max_pool_3x3', 1), ('max_pool_3x3', 4)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 1), ('max_pool_3x3', 0), ('sep_conv_5x5', 0), ('max_pool_3x3', 2), ('dil_conv_5x5', 3), ('dil_conv_5x5', 2), ('dil_conv_5x5', 2), ('max_pool_3x3', 3)], reduce_concat=range(2, 6))
2019-11-03 21:30:10,700 
alphas_normal = Variable containing:
 0.0290  0.2322  0.4073  0.0321  0.0246  0.0655  0.0771  0.0298  0.1023
 0.5609  0.0213  0.1818  0.0522  0.0201  0.0517  0.0512  0.0187  0.0421
 0.0616  0.0628  0.4751  0.0887  0.0346  0.1260  0.1000  0.0305  0.0208
 0.5871  0.0230  0.1239  0.0747  0.0223  0.0324  0.0301  0.0698  0.0366
 0.0952  0.0498  0.5567  0.1308  0.0291  0.0659  0.0068  0.0330  0.0327
 0.0371  0.0379  0.0425  0.6992  0.0254  0.0228  0.0852  0.0269  0.0229
 0.6138  0.0186  0.0204  0.0267  0.0186  0.0185  0.0129  0.0700  0.2005
 0.4228  0.0435  0.0602  0.1302  0.0347  0.1592  0.0457  0.0610  0.0428
 0.0867  0.0344  0.0943  0.2181  0.0427  0.0496  0.3113  0.0537  0.1093
 0.0699  0.1343  0.1206  0.3198  0.0673  0.0735  0.0858  0.0703  0.0585
 0.1978  0.0557  0.0921  0.4309  0.0474  0.0346  0.0402  0.0395  0.0619
 0.0947  0.0536  0.0820  0.1157  0.0421  0.2263  0.0169  0.1688  0.1999
 0.1181  0.0502  0.1350  0.2611  0.0485  0.0641  0.0690  0.0374  0.2166
 0.0374  0.0369  0.0997  0.3777  0.0352  0.0146  0.0194  0.0279  0.3514
[torch.cuda.FloatTensor of size 14x9 (GPU 0)]

2019-11-03 21:30:10,702 
alphas_reduce = Variable containing:
 0.0850  0.0694  0.0855  0.3303  0.0393  0.2394  0.0330  0.0865  0.0318
 0.0401  0.0382  0.0386  0.5462  0.0266  0.1453  0.1047  0.0344  0.0258
 0.1626  0.0833  0.0855  0.0775  0.0527  0.0311  0.4062  0.0433  0.0578
 0.1346  0.1167  0.1095  0.2193  0.0696  0.0831  0.1861  0.0569  0.0241
 0.0505  0.0501  0.0725  0.3066  0.0313  0.0635  0.0362  0.1397  0.2497
 0.6409  0.0328  0.0789  0.0753  0.0594  0.0260  0.0375  0.0255  0.0239
 0.2116  0.2686  0.0379  0.0393  0.0370  0.0685  0.1355  0.1009  0.1006
 0.1006  0.0528  0.0510  0.0911  0.0352  0.0815  0.1452  0.1602  0.2824
 0.0524  0.0444  0.0580  0.1651  0.0460  0.0429  0.0613  0.0532  0.4769
 0.3606  0.0659  0.0993  0.0810  0.0921  0.0345  0.1682  0.0366  0.0619
 0.4373  0.0615  0.0977  0.0945  0.1014  0.0530  0.0475  0.0731  0.0339
 0.0734  0.0367  0.0338  0.0457  0.0331  0.1443  0.0418  0.0411  0.5501
 0.0490  0.0355  0.0413  0.4056  0.0398  0.0561  0.0512  0.0581  0.2634
 0.0867  0.0532  0.0759  0.1101  0.0814  0.0829  0.1137  0.0569  0.3393
[torch.cuda.FloatTensor of size 14x9 (GPU 0)]

2019-11-03 21:30:14,324 train 000 9.740758e-03 -1.733228
2019-11-03 21:33:01,215 train 050 9.599260e-03 -0.851638
2019-11-03 21:35:49,415 train 100 9.588208e-03 -0.846157
2019-11-03 21:38:36,465 train 150 9.573611e-03 -0.914560
2019-11-03 21:41:23,513 train 200 9.530823e-03 -1.044556
2019-11-03 21:44:10,207 train 250 9.700728e-03 -1.225683
2019-11-03 21:46:57,299 train 300 9.680700e-03 -1.332924
2019-11-03 21:49:47,058 train 350 9.690760e-03 -2.532378
2019-11-03 21:52:34,649 train 400 9.674160e-03 -2.338315
2019-11-03 21:55:22,058 train 450 9.729339e-03 -2.188339
2019-11-03 21:58:09,952 train 500 9.723441e-03 -2.106774
2019-11-03 22:00:57,896 train 550 9.690452e-03 -2.012806
2019-11-03 22:03:45,476 train 600 9.690776e-03 -1.939147
2019-11-03 22:06:33,420 train 650 9.732540e-03 -1.897186
2019-11-03 22:09:21,975 train 700 9.771420e-03 -1.845684
2019-11-03 22:12:10,058 train 750 9.807703e-03 -1.884138
2019-11-03 22:14:57,297 train 800 9.826495e-03 -1.835926
2019-11-03 22:17:45,237 train 850 9.823221e-03 -1.833109
2019-11-03 22:20:32,426 train 900 9.811127e-03 -1.812359
2019-11-03 22:23:19,710 train 950 9.787934e-03 -1.802077
2019-11-03 22:26:07,023 train 1000 9.763960e-03 -1.753437
2019-11-03 22:28:55,639 train 1050 9.773843e-03 -1.758334
2019-11-03 22:31:50,646 train 1100 9.758978e-03 -1.723773
2019-11-03 22:34:40,189 train 1150 9.745528e-03 -1.712290
2019-11-03 22:37:27,518 train 1200 9.714556e-03 -1.690090
2019-11-03 22:40:17,554 train 1250 9.726052e-03 -1.674162
2019-11-03 22:43:06,616 train 1300 9.731056e-03 -1.645485
2019-11-03 22:45:54,618 train 1350 9.736212e-03 -1.628945
2019-11-03 22:48:43,365 train 1400 9.731914e-03 -1.655728
2019-11-03 22:51:32,003 train 1450 9.755924e-03 -1.896578
2019-11-03 22:54:19,624 train 1500 9.762918e-03 -1.883336
2019-11-03 22:57:07,148 train 1550 9.765951e-03 -1.856135
2019-11-03 22:59:54,446 train 1600 9.758046e-03 -1.843806
2019-11-03 23:02:41,218 train 1650 9.754496e-03 -2.011277
2019-11-03 23:05:28,240 train 1700 9.755490e-03 -1.986368
2019-11-03 23:08:14,275 train 1750 9.745337e-03 -1.965258
2019-11-03 23:11:01,002 train 1800 9.738940e-03 -1.935420
2019-11-03 23:13:47,497 train 1850 9.739576e-03 -2.020579
2019-11-03 23:16:34,565 train 1900 9.726671e-03 -2.007495
2019-11-03 23:19:21,907 train 1950 9.722014e-03 -1.982786
2019-11-03 23:22:09,914 train 2000 9.712548e-03 -1.969300
2019-11-03 23:24:57,199 train 2050 9.721362e-03 -2.003623
2019-11-03 23:27:44,357 train 2100 9.716733e-03 -1.996148
2019-11-03 23:30:28,930 train 2150 9.716684e-03 -1.994824
2019-11-03 23:33:16,411 train 2200 9.721127e-03 -1.975077
2019-11-03 23:36:05,912 train 2250 9.727482e-03 -1.952010
2019-11-03 23:38:54,415 train 2300 9.726906e-03 -1.958046
2019-11-03 23:41:44,589 train 2350 9.732087e-03 -1.934641
2019-11-03 23:44:32,553 train 2400 9.739233e-03 -1.929669
2019-11-03 23:47:19,270 train 2450 9.734700e-03 -1.913938
2019-11-03 23:50:10,992 train 2500 9.747293e-03 -2.015557
2019-11-03 23:53:07,122 train 2550 9.737405e-03 -2.000471
2019-11-03 23:56:09,922 train 2600 9.735383e-03 -1.987345
2019-11-03 23:59:17,122 train 2650 9.739060e-03 -1.969702
2019-11-04 00:02:21,889 train 2700 9.745457e-03 -1.953800
2019-11-04 00:05:26,618 train 2750 9.746919e-03 -1.938004
2019-11-04 00:08:29,751 train 2800 9.745953e-03 -1.936565
2019-11-04 00:11:19,930 train 2850 9.733359e-03 -2.058483
2019-11-04 00:14:10,448 train 2900 9.727854e-03 -2.560999
2019-11-04 00:17:00,206 train 2950 9.727506e-03 -2.533708
2019-11-04 00:19:52,220 train 3000 9.733091e-03 -2.506037
2019-11-04 00:22:48,400 train 3050 9.729611e-03 -2.623779
2019-11-04 00:24:25,929 training loss; R2: 9.731484e-03 -2.608375
2019-11-04 00:24:26,405 valid 000 1.348073e-02 -2.082789
2019-11-04 00:24:35,597 valid 050 9.143063e-03 -1.359549
2019-11-04 00:24:44,786 valid 100 9.231295e-03 -3.551154
2019-11-04 00:24:54,071 valid 150 9.185438e-03 -2.711876
2019-11-04 00:25:03,242 valid 200 9.107641e-03 -2.454978
2019-11-04 00:25:12,383 valid 250 9.109111e-03 -2.257045
2019-11-04 00:25:21,549 valid 300 9.075932e-03 -2.423634
2019-11-04 00:25:30,666 valid 350 9.072645e-03 -2.244346
2019-11-04 00:25:39,810 valid 400 9.096891e-03 -2.129911
2019-11-04 00:25:48,988 valid 450 9.065121e-03 -2.255630
2019-11-04 00:25:58,149 valid 500 9.096828e-03 -10.132180
2019-11-04 00:26:07,270 valid 550 9.093993e-03 -9.326275
2019-11-04 00:26:16,377 valid 600 9.096049e-03 -19.705433
2019-11-04 00:26:25,523 valid 650 9.123564e-03 -18.301533
2019-11-04 00:26:34,737 valid 700 9.147208e-03 -17.076659
2019-11-04 00:26:44,046 valid 750 9.112924e-03 -16.034153
2019-11-04 00:26:47,645 validation loss; R2: 9.110350e-03 -15.665384
2019-11-04 00:26:47,849 epoch 10 lr 9.140576e-04
2019-11-04 00:26:47,850 genotype = Genotype(normal=[('max_pool_2x2', 0), ('max_pool_2x2', 1), ('max_pool_2x2', 2), ('max_pool_2x2', 0), ('max_pool_3x3', 0), ('sep_conv_5x5', 3), ('dil_conv_5x5', 4), ('max_pool_3x3', 1)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 1), ('max_pool_3x3', 0), ('sep_conv_5x5', 0), ('dil_conv_5x5', 2), ('dil_conv_5x5', 3), ('dil_conv_5x5', 2), ('dil_conv_5x5', 2), ('dil_conv_5x5', 4)], reduce_concat=range(2, 6))
2019-11-04 00:26:47,852 
alphas_normal = Variable containing:
 0.0273  0.2668  0.3362  0.0276  0.0292  0.0928  0.0834  0.0306  0.1061
 0.6488  0.0179  0.1318  0.0310  0.0150  0.0280  0.0638  0.0325  0.0312
 0.0739  0.0606  0.4037  0.0718  0.0397  0.1213  0.1396  0.0517  0.0376
 0.6948  0.0200  0.0793  0.0441  0.0187  0.0333  0.0350  0.0495  0.0253
 0.1529  0.0686  0.4880  0.0941  0.0365  0.0693  0.0146  0.0459  0.0301
 0.0429  0.0436  0.0696  0.6679  0.0344  0.0247  0.0643  0.0264  0.0262
 0.6284  0.0195  0.0203  0.0221  0.0198  0.0118  0.0234  0.0961  0.1585
 0.5221  0.0526  0.0509  0.1048  0.0427  0.0918  0.0330  0.0535  0.0487
 0.0881  0.0445  0.0685  0.1740  0.0514  0.0341  0.3111  0.0793  0.1491
 0.0908  0.1206  0.1365  0.2880  0.0643  0.0678  0.0642  0.1129  0.0548
 0.2099  0.0708  0.0954  0.3946  0.0523  0.0497  0.0359  0.0400  0.0515
 0.1821  0.0652  0.0716  0.0913  0.0476  0.1581  0.0153  0.2009  0.1679
 0.1508  0.0536  0.1241  0.2330  0.0596  0.0974  0.0434  0.0355  0.2027
 0.0507  0.0300  0.0604  0.2543  0.0330  0.0120  0.0119  0.0371  0.5106
[torch.cuda.FloatTensor of size 14x9 (GPU 0)]

2019-11-04 00:26:47,854 
alphas_reduce = Variable containing:
 0.0535  0.1216  0.0812  0.2732  0.0342  0.2181  0.0289  0.1448  0.0444
 0.0554  0.0559  0.0749  0.5522  0.0338  0.0746  0.0703  0.0498  0.0331
 0.2227  0.0734  0.0975  0.0871  0.0663  0.0423  0.2739  0.0442  0.0926
 0.1006  0.1291  0.1047  0.1621  0.0661  0.1433  0.2016  0.0551  0.0374
 0.0427  0.0427  0.0685  0.2406  0.0382  0.0874  0.0411  0.1941  0.2446
 0.5397  0.0292  0.0808  0.0715  0.0587  0.0628  0.0471  0.0438  0.0663
 0.1291  0.2271  0.0549  0.0460  0.0412  0.0614  0.1522  0.0994  0.1886
 0.0819  0.0588  0.0605  0.0729  0.0422  0.0693  0.1356  0.1110  0.3678
 0.0552  0.0350  0.0427  0.1032  0.0381  0.0405  0.0465  0.0812  0.5577
 0.3914  0.0794  0.0711  0.0537  0.0602  0.0393  0.1692  0.0486  0.0871
 0.4067  0.0740  0.0902  0.0738  0.0767  0.0675  0.0755  0.0726  0.0630
 0.0558  0.0312  0.0314  0.0363  0.0287  0.1305  0.0433  0.0622  0.5806
 0.0721  0.0403  0.0431  0.3154  0.0408  0.1161  0.0592  0.0696  0.2434
 0.0816  0.0521  0.0570  0.0728  0.0592  0.0916  0.1079  0.1135  0.3643
[torch.cuda.FloatTensor of size 14x9 (GPU 0)]

2019-11-04 00:26:51,595 train 000 1.001663e-02 -1.873939
2019-11-04 00:29:51,417 train 050 9.925854e-03 -1.007960
2019-11-04 00:32:48,298 train 100 9.665509e-03 -1.671501
